{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Gabarit - Templates Data Science","text":"<p>Gabarit provides you with a set of python templates (a.k.a. frameworks) for your Data Science projects. It allows you to generate a code base that includes many features to speed up the production and testing of your AI models. You just have to focus on the core of Data Science.</p>"},{"location":"#philosophy","title":"Philosophy","text":"<p>As a team, we strive to help Data Scientists across the board (and ourselves!) build awesome IA projects by speeding up the development process. This repository contains several frameworks allowing any data scientist, IA enthousiast (or developper of any kind, really) to kickstart an IA project from scratch.  </p> <p>We hate it when a project is left in the infamous POC shadow valley where nice ideas and clever models are forgotten, thus we tried to pack as much production-ready features as we could in these frameworks.  </p> <p>As Hadley Wickhman would say: \"you can't do data science in a GUI\". We are strong believers that during a data science or IA project, you need to be able to fine tune every nooks and crannies to make the best out of your data.  </p> <p>Therefore, these frameworks act as project templates that you can use to generate a code base from nothing (except for a project name). Doing so would allow your fresh and exciting new project to begin with loads of features on which you wouldn't want to focus this early :</p> <ul> <li>Built-in models: from the ever useful TF/IDF + SVM to the more recent transformers</li> <li>Model-agnostic save/load/reload : perfect to embed your model behind a web service</li> <li>Generic training/predict scripts to work with your data as soon as possible</li> <li>DVC &amp; MLFlow integration (you have to configure it to point to your own infrastructures)</li> <li>Streamlit demo tool</li> <li>... and so much more !</li> </ul>"},{"location":"#frameworks","title":"Frameworks","text":"<p>Gabarit contains the following frameworks :</p>"},{"location":"#nlp","title":"NLP","text":"<p>To tackle classification use cases on textual data</p> <ul> <li>Relies on the Words'n fun module for the preprocessing requirements</li> <li>Supports :<ul> <li> Mono Class / Mono Label classification</li> <li> Multi Classes / Mono Label classification</li> <li> Mono Class / Multi Labels classification</li> </ul> </li> </ul>"},{"location":"#numeric","title":"Numeric","text":"<p>To tackle classification and regression use cases on numerical data</p> <ul> <li>Supports :<ul> <li> Regression</li> <li> Multi Classes / Mono Label classification</li> <li> Mono Class / Multi Labels classification</li> </ul> </li> </ul>"},{"location":"#computer-vision","title":"Computer Vision","text":"<p>To tackle classification use cases on images</p> <ul> <li>Supports<ul> <li> Mono Class / Mono Label classification</li> <li> Multi Classes / Mono Label classification</li> <li> Area of interest detection</li> </ul> </li> </ul>"},{"location":"#api","title":"API","text":"<p>Provides a FastAPI for exposing your model to the world</p> <ul> <li>Supports<ul> <li> Gabarit model created with one of the previous package</li> <li> Any model of your own</li> </ul> </li> </ul> <p>These frameworks have been developped to manage different topics but share a common structure and a common philosophy. Once a project made using a framework is in production, any other project can be sent into production following the same process. Using the API template, you can expose framework made models in no time !</p>"},{"location":"#getting-started","title":"Getting started","text":""},{"location":"#installation","title":"Installation","text":"<p>Gabarit supports python &gt;= 3.7. To install it, run the command : </p> <pre><code>pip install gabarit\n</code></pre> <p>This will install <code>gabarit</code> package and all frameworks.</p>"},{"location":"#kickstart-a-new-project","title":"Kickstart a new project","text":"<p>To create a new project from a template, use gabarit entry points : </p> <ul> <li><code>generate_nlp_project</code></li> <li><code>generate_num_project</code></li> <li><code>generate_vision_project</code></li> <li><code>generate_api_project</code></li> </ul> <p>Example : <code>generate_nlp_project -n my_awesome_package -p my_new_project_path -c my_configuration.ini --upload my_instructions.md --dvc dvc_config</code></p> <p>They take several parameters as input :</p> <ul> <li><code>-n</code> or <code>--name</code> : Name of the package/project (lowercase, no whitespace)</li> <li><code>-p</code> or <code>--path</code> : Path (Absolute or relative) where to create the main directory of the project</li> <li><code>-c</code> or <code>--config</code> : Path (Absolute or relative) to a .ini configuration file.     A default configuration file is given alongside each project. (<code>default_config.ini</code>).     It usually contains stuff like default encoding, default separator for .csv files, pip proxy settings, etc.</li> <li><code>--upload</code> or <code>--upload_intructions</code> : Path (Absolute or relative) to a file that contains a list of instructions to upload a trained model to your favorite storage solution.</li> <li><code>--dvc</code> or <code>--dvc_config</code> : Path (Absolute or relative) to a DVC configuration file. If not provided, DVC won't be used.</li> </ul>"},{"location":"#setup-your-new-project","title":"Setup your new project","text":"<ul> <li> <p>(Optionnal) We strongly advise to create a python virtual env</p> <ul> <li><code>pip install virtualenv</code></li> <li><code>python -m venv my_awesome_venv</code></li> <li><code>cd my_awesome_venv/Scripts/ &amp;&amp; activate</code> (windows) or <code>source my_awesome_venv/bin/activate</code> (linux)</li> </ul> </li> <li> <p>Requirements : <code>pip install --no-cache-dir -r requirements.txt</code></p> </li> <li> <p>Setup the project (in develop mode) : <code>python setup.py develop</code></p> </li> </ul> <p>If the <code>make</code> tool is available, you can use the features provided in <code>Makefile</code>:</p> <ul> <li><code>create-virtualenv</code></li> <li><code>init-local-env</code></li> </ul>"},{"location":"#generate-this-documentation-locally","title":"Generate this documentation locally","text":"<p>To generate this documentation locally first clone the gabarit repository : </p> <pre><code>git clone https://github.com/OSS-Pole-Emploi/gabarit.git\ncd gabarit\n</code></pre> <p>Then install mkdocs dependencies :</p> <pre><code>pip install \\\n'mkdocs&gt;=1.4,&lt;2' \\\n'mkdocs-gen-files&gt;=0.4,&lt;1' \\\n'mkdocs-literate-nav&gt;=0.6,&lt;1' \\\n'mkdocs-material&gt;=9.0,&lt;10' \\\n'mkdocs-section-index&gt;=0.3,&lt;1' \\\n'mkdocstrings[python]&gt;=0.8,&lt;1'\n</code></pre> <p>Finally serve the doc locally :  <pre><code>mkdocs serve\n</code></pre></p> <p>Note</p> <p>Generating package references can be long. You can locally disable package references generation by setting an environment variable : <code>export DOC_NO_REF=true</code></p>"},{"location":"#security-warning","title":"Security warning","text":"<p>Gabarit relies on a number of open source packages and therefore may carry on their potential security vulnerabilities. Our philosophy is to be as transparent as possible, which is why we are actively monitoring the dependabot analysis. In order to limit these vulnerabilities, we regularly upgrade these packages as soon as we can. Notice that some packages (namely torch and tensorflow) might lag a few versions behind the actual up to date version due to compatibility issues with CUDA and our own infrastructure.</p> <p>However, we remind you to be vigilant about the security vulnerabilities of the code and models that you will produce with these frameworks. It is your responsibility to ensure that the final product matches the security standards of your organization.</p>"},{"location":"#ethics","title":"Ethics","text":"<p>P\u00f4le emploi intends to include the development and use of artificial intelligence algorithms and solutions in a sustainable and ethical approach. As such, P\u00f4le emploi has adopted an ethical charter, resulting from collaborative and consultative work. The objective is to guarantee a framework of trust, respectful of the values of P\u00f4le emploi, and to minimize the risks associated with the deployment of these technologies.</p> <p>The pdf file is located in pole-emploi.org :</p> <p>PDF - Ethics charter - P\u00f4le emploi</p>"},{"location":"#contacts","title":"Contacts","text":"<p>If you have any question/enquiry feel free to drop us a mail :</p> <p> Contact</p> <p>Maintenance team :</p> <ul> <li>Alexandre GAREL - Data Scientist</li> <li>Nicolas GREFFARD - Data Scientist</li> <li>Gautier SOLARD - Data Scientist</li> <li>Nicolas TOUZOT - Product Owner</li> </ul>"},{"location":"/home/runner/work/gabarit/gabarit/docs/reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>template_api<ul> <li>application</li> <li>core<ul> <li>config</li> <li>logtools</li> <li>resources</li> </ul> </li> <li>model<ul> <li>model_base</li> <li>model_gabarit</li> </ul> </li> <li>routers<ul> <li>functional</li> <li>schemas<ul> <li>functional</li> <li>technical</li> <li>utils</li> </ul> </li> <li>technical</li> </ul> </li> </ul> </li> <li>template_nlp<ul> <li>models_training<ul> <li>hf_metrics<ul> <li>accuracy</li> <li>f1</li> <li>precision</li> <li>recall</li> </ul> </li> <li>model_aggregation</li> <li>model_class</li> <li>model_huggingface</li> <li>models_sklearn<ul> <li>model_pipeline</li> <li>model_tfidf_gbt</li> <li>model_tfidf_lgbm</li> <li>model_tfidf_sgdc</li> <li>model_tfidf_svm</li> </ul> </li> <li>models_tensorflow<ul> <li>model_embedding_cnn</li> <li>model_embedding_lstm</li> <li>model_embedding_lstm_attention</li> <li>model_embedding_lstm_gru</li> <li>model_embedding_lstm_structured_attention</li> <li>model_keras</li> <li>model_tfidf_dense</li> <li>utils_deep_keras</li> </ul> </li> <li>utils_models</li> </ul> </li> <li>monitoring<ul> <li>mlflow_logger</li> <li>model_explainer</li> </ul> </li> <li>preprocessing<ul> <li>preprocess</li> </ul> </li> <li>utils</li> </ul> </li> <li>template_num<ul> <li>models_training<ul> <li>classifiers<ul> <li>model_aggregation_classifier</li> <li>model_classifier</li> <li>model_xgboost_classifier</li> <li>models_sklearn<ul> <li>model_gbt_classifier</li> <li>model_knn_classifier</li> <li>model_lgbm_classifier</li> <li>model_logistic_regression_classifier</li> <li>model_rf_classifier</li> <li>model_ridge_classifier</li> <li>model_sgd_classifier</li> <li>model_svm_classifier</li> </ul> </li> <li>models_tensorflow<ul> <li>model_dense_classifier</li> </ul> </li> </ul> </li> <li>model_class</li> <li>model_keras</li> <li>model_pipeline</li> <li>regressors<ul> <li>model_aggregation_regressor</li> <li>model_regressor</li> <li>model_xgboost_regressor</li> <li>models_sklearn<ul> <li>model_bayesian_ridge_regressor</li> <li>model_elasticnet_regressor</li> <li>model_gbt_regressor</li> <li>model_kernel_ridge_regressor</li> <li>model_knn_regressor</li> <li>model_lgbm_regressor</li> <li>model_pls_regressor</li> <li>model_rf_regressor</li> <li>model_sgd_regressor</li> <li>model_svr_regressor</li> </ul> </li> <li>models_tensorflow<ul> <li>model_dense_regressor</li> </ul> </li> </ul> </li> <li>utils_deep_keras</li> <li>utils_models</li> </ul> </li> <li>monitoring<ul> <li>mlflow_logger</li> <li>model_explainer</li> </ul> </li> <li>preprocessing<ul> <li>column_preprocessors</li> <li>outlier_detection</li> <li>preprocess</li> </ul> </li> <li>utils</li> </ul> </li> <li>template_vision<ul> <li>models_training<ul> <li>classifiers<ul> <li>model_classifier</li> <li>model_cnn_classifier</li> <li>model_transfer_learning_classifier</li> </ul> </li> <li>model_class</li> <li>model_keras</li> <li>object_detectors<ul> <li>model_detectron_faster_rcnn</li> <li>model_keras_faster_rcnn</li> <li>model_object_detector</li> <li>utils_faster_rcnn</li> <li>utils_object_detectors</li> </ul> </li> <li>utils_deep_keras</li> <li>utils_models</li> </ul> </li> <li>monitoring<ul> <li>mlflow_logger</li> <li>model_explainer</li> </ul> </li> <li>preprocessing<ul> <li>manage_white_borders</li> <li>preprocess</li> </ul> </li> <li>utils</li> </ul> </li> </ul>"},{"location":"frameworks/","title":"Frameworks","text":""},{"location":"frameworks/#general-principles","title":"General principles","text":""},{"location":"frameworks/#templates-structure","title":"Templates structure","text":"<ul> <li> <p>Data must be kept in a directory named <code>project_name-data/</code> located at the root folder of the project (i.e. where <code>setup.py</code> is).</p> </li> <li> <p>Any data mapping or lookup can be kept under <code>project_name-data/sources/</code>. Warning : we're talking small files (&lt; 1 Mo). Larger files should be managed through DVC (or git lfs for that matter).</p> </li> <li> <p>Embedding files or equivalent should also be kept under <code>project_name-data/</code>.</p> </li> <li> <p>Transformers models (e.g. Flaubert) should be kept under <code>project_name-transformers/</code> at the root directory of the project.</p> </li> <li> <p>Trained models that you built and trained are automatically saved under <code>project_name-models/</code>.</p> </li> <li> <p>Sklearn preprocessing pipelines (mainly from the numerical framework) are automatically stored within <code>project_name-pipelines/</code>.</p> </li> <li> <p>The Computer Vision template has some more subdirectories in the <code>project_name-data/</code> folder:</p> <ul> <li> <p><code>cache_keras</code>: subfolder that replaces the default keras' cache folder. Used with transfer learning classifiers.</p> </li> <li> <p><code>transfer_learning_weights</code>: subfolder that holds networks weights to be used with custom Faster RCNN implementation.</p> </li> <li> <p><code>detectron2_conf_files</code>: subfolder that holds all necessary configuration files to be used with the detectron2 models.</p> </li> </ul> </li> <li> <p>The <code>tests/</code> directory contains numerous unit tests allowing to automatically validate the intended behaviour of the different features. It is of utter importance to keep them up to date depending on your own developments to ensure that everything is working fine. Feel free to check already existing test files if you need some directions. Note that to launch a specific test case you just have to run : <code>python test_file.py</code>; for instance: <code>python tests/test_model_tfidf_dense.py</code>.</p> </li> <li> <p>Numbered files contained in <code>project_name-scripts/</code> (e.g. <code>2_training.py</code>) hint the main steps of the project. They are indicative but we strongly advise to use them as it can speed up the development steps. It orchestrates the main features of this project: utils functions, preprocessing pipelines and model classes.</p> </li> <li> <p>The <code>preprocess.py</code> file contains the different preprocessing pipeline available by default by the package/project. More specifically, it contains a dictionnary of the pipelines. It will be used to create working datasets (for instance training set, valid test and test set).</p> </li> <li> <p>Beware that the first row of each generated csv file after running a preprocessing will contain the name of the preprocessing pipeline applied such that it can be reused in the future. Hence, this row (e.g. <code>#preprocess_P1</code>) is a metadata and it has to be skipped while parsing the csv file. Our templates provide a function (<code>utils.read_csv</code>) that does it automatically (it also returns the metadata).</p> </li> <li> <p>The modelling part is built as follow :</p> <ul> <li> <p>ModelClass : main class that manages how data / models are saved and how performance metrics are computed</p> </li> <li> <p>ModelPipeline : inherits from ModelClass, manages sklearn pipeline models</p> </li> <li> <p>ModelKeras : inherits from ModelClass, manages Keras/Tensorflow models</p> </li> <li> <p>ModelPyTorch : inherits from ModelClass, manages PyTorch models</p> </li> <li> <p>ModelXXX : built-in implementation of standard models used in the industry, inherits from one of the above classes when appropriate</p> </li> </ul> </li> </ul>"},{"location":"frameworks/#main-steps-of-a-given-project","title":"Main steps of a given project","text":"<p>The intended flow of a project driven by one of these framework is the following:</p> <ul> <li> <p>0 \u2013 Utility files</p> <ul> <li>Split train/valid/test, sampling, embedding download, etc...</li> </ul> </li> <li> <p>1 \u2013 Preprocessing</p> </li> <li> <p>2 \u2013 Model training</p> <ul> <li>You can tune the parameters within the script or update the model class depending on your needs</li> </ul> </li> <li> <p>3 \u2013 Predictions on a dataset</p> </li> <li> <p>4 \u2013 Play with a streamlit demonstrator to showcase your models</p> </li> </ul>"},{"location":"frameworks/#data-formats","title":"Data formats","text":"<p>Input data are supposed to be <code>.csv</code> files and the separator and encoding are to be provided during the generation of the project. It is obviously possible to use another datatype but a transformation step to <code>.csv</code> will be required to use the scripts provided by default.</p> <p>Concerning the prediction target, please refer to <code>2_training.py</code>. Usually we expect One Hot Encoded format for multi-labels use cases. For single-label use cases, a single column (string for classification, float for regression) is expected.</p>"},{"location":"frameworks/#features","title":"Features","text":"<p>Projects generated through the frameworks provide several main features:</p>"},{"location":"frameworks/#model-saving-and-reloading","title":"Model saving and reloading","text":"<p>When a new model is instanciated, a directory is created within <code>project_name-models/</code>. It is named after the model type and its date of creation. Each model class exposes a <code>save</code> function that allow to save everything necessary to load it back:</p> <ul> <li>Configuration file</li> <li>Serialized object (.pkl)</li> <li>\"standalone\" model</li> <li>If Deep Learning : the network weights</li> <li>etc.</li> </ul> <p>Thus any model can be loaded through the <code>utils_models.load_model</code> function. The \"standalone\" mode ensures that the model can be loaded even after its code has been modified. Indeed, the .pkl file could be out of sync with the model class (it it was modified after the model had been saved). In this specific case, you can use <code>0_reload_model.py</code>.</p>"},{"location":"frameworks/#third-party-ai-modules","title":"Third party AI modules","text":"<p>To this day, 3 main AI modules are used:</p> <ul> <li> <p>Scikit Learn</p> </li> <li> <p>TensorFlow (Keras)</p> </li> <li> <p>PyTorch (PyTorch Lightning)</p> </li> </ul> <p>Do no hesitate to extend this list as is the case for LighGBM for instance.</p>"},{"location":"frameworks/#dvc","title":"DVC","text":"<p>A new project can automatically be set up to run in sync with DVC if you supply the necessary configuration during project generation. We strongly advise to use DVC or similar (git lfs could do the trick) to keep both your code and your datasets synchronized to be able to re-train a model in the same conditions sometime down the line. Please refrain to upload large datasets (&gt;1mo) directly on your version control system. Once setup, dvc configuration is available within <code>.dvc/</code></p>"},{"location":"frameworks/#mlflow","title":"MLFlow","text":"<p>A new project can automatically be set up to work alongside a MLFlow instance. If you supply a MLFlow host url during project generation, training metrics will be automatically be send to your MLFlow server. Refer to <code>2_training.py</code> and <code>monitoring/model_logger.py</code> for further informations about this mechanism.</p>"},{"location":"frameworks/#streamlit-demonstrator","title":"Streamlit demonstrator","text":"<p>A generic demonstrator is automatically created when you generate a new project with the frameworks. It relies on Streamlit to expose a handy front-end to showcase your work. The demonstrator script can be easily modified to fit your specific needs. </p>"},{"location":"frameworks/#exploratory-data-analysis-eda","title":"Exploratory Data Analysis (EDA)","text":"<p>Some frameworks provide a generic exploratory data analysis notebook to quickly grasp your datasets (<code>project_name-exploration/EDA/</code>). Feel free to have a go with it before starting heavy modelling work; EDA is an extraordinary opportunity to get to know your data which will greatly help you further down the line.</p>"},{"location":"frameworks/#misc","title":"Misc.","text":"<p>Some additionnal features :</p> <ul> <li>Basic hyper-parameter search is provided within <code>2_training.py</code></li> <li>You can use Tensorflow checkpoints to restart the training of a model without having to start from scratch</li> <li>A custom made Learning Rate Scheduler for Tensorflow is also provided</li> <li>Etc... feel free to explore the generated classes to learn more about what you can do !</li> </ul>"},{"location":"frameworks/#industrialization","title":"Industrialization","text":""},{"location":"frameworks/#principles","title":"Principles","text":"<p>Industrialization of a project generated from one of the framework roughly follows the same pattern. Once you have trained a model which is a release candidate :</p> <ul> <li> <p>Push the actual serialized model to your artifact repository (for instance artifactory or nexus)</p> <ul> <li>Instructions about how to technically push the model are usually specified within the model directory</li> </ul> </li> <li> <p>Push the python module (the project you generated with a framework) to your artifact repository (it could be pypi or any system able to host a python repository)</p> <ul> <li> <p>First you have to build a wheel of the project <code>.whl</code> : <code>python setup.py sdist bdist_wheel</code></p> </li> <li> <p>Then you have to push it to your repository, for instance by using twine : <code>twine upload --username {USER} --password {PWD} --repository-url https://{repository_url} dist/*.whl</code></p> </li> </ul> <p>Note</p> <p>we strongly advise to embed these steps within a Continuous Integration Pipeline and ensuring that all your unit tests are OK (you can use nose to run your test suite : <code>pip install nose nose-cov &amp;&amp; nosetests tests/</code>)</p> <ul> <li> <p>Beware, function <code>utils_models.predict</code> has to be adapted to your project needs (e.g. if some specific computations are required before or after the actual inference).</p> <ul> <li>This is the function that has to be called by the web service that will serve your model. Using <code>utils_models.predict</code> instead of the actual predict method of the model class ensure that your service can stay model agnostic: if one day you decide to change your design, to use another model; the service won't be impacted.</li> </ul> </li> </ul> <p>Warning</p> <p>some libraries (such as torch, detectron2, etc.) may not be hosted on PyPI. You'll need to add an extra <code>--find-links</code> option to your pip installation.</p> <p>If you don't have access to the internet, you'll need to setup a proxy which will host all the needed libraries. You can then use <code>--trusted-host</code> and <code>--index-url</code> options.</p> </li> <li> <p>You can use our API Framework to expose your model, see API section</p> </li> </ul>"},{"location":"frameworks/#update-your-model","title":"Update your model","text":"<p>If you want to update the model exposed by the API, you just have to push a new version of the serialized model to your repository and update your service (typically only the model version). If the actual code base of the model (for instance in the predict method) was updated, you would also have to publish a new version of the python module.  </p>"},{"location":"frameworks/#unit-tests","title":"Unit tests","text":"<p>Numerous unit tests are provided by the framework. Don't forget to adapt them when you modify the code. If you wish to add features, it is obviously advised to add new unit tests.</p>"},{"location":"frameworks/#misc_1","title":"Misc.","text":"<ul> <li>To this day, each framework is tested and integrated on our own continuous integration pipeline.</li> <li>If a GPU is available, some models will automatically try to use it during training and inference</li> </ul>"},{"location":"frameworks/#update-a-project-with-the-latest-gabarit-version","title":"Update a project with the latest Gabarit version","text":"<p>It can be tricky to update a project to a newer version of Gabarit as you probably made changes into the code and don't want them to be removed. As our philosophy is to give you code and let you adapt it for your specific usage, we can't control everything.  </p> <p>However, we still provide an operating procedure that must keep your changes while updating the project to the latest Gabarit version :</p> <ol> <li> <p>Create a new branch from your latest commit C0</p> </li> <li> <p>Find the Gabarit's version last used to generate your project</p> </li> <li> <p>Generate a project ON TOP of your code using this version</p> <ul> <li>Commit the changes (commit C1)</li> </ul> </li> <li> <p>Create a patch : <code>git diff HEAD XXXXXX &gt; local_diff.patch</code> where XXXXXX is the SHA-1 of the latest commit C0</p> <ul> <li>This patch holds every changes you made since you last generated the project, except for new files</li> <li>Note that we don't really care for new files as they are not removed with Gabarit new generation</li> </ul> </li> <li> <p>Generate a project ON TOP of your code, but this time with the latest Gabarit version. Commit the changes (commit C2).</p> <ul> <li>The <code>.gitignore</code> file might change, be careful NOT TO COMMIT files that are \"unignored\".</li> </ul> </li> <li> <p>Apply the patch : <code>git am -3 &lt; local_diff.patch</code></p> <p>RENAMED / MOVED / DELETED FILES</p> <p>this won't work for renamed / moved / deleted files :</p> <ul> <li>You'll have to manage them manually</li> <li>You need to remove files that are no longer in the new Gabarit version BEFORE applying the patch.</li> <li>The patch will then probably crash. You will have to fix it manually.</li> </ul> <ol> <li>You will probably have conflict, resolve them</li> <li>Add files and commit changes (commit C3)</li> <li>You might need to run <code>git am --skip</code> as we only had a single patch to apply</li> </ol> </li> <li> <p>Squash the last commits (you should have 3 commits)</p> <ul> <li><code>git reset --soft HEAD~3</code></li> <li><code>git commit -m \"my_message\"</code></li> </ul> </li> <li> <p>CHECK IF EVERYTHING SEEMS OK</p> </li> <li> <p>Merge your branch &amp; push :) </p> </li> </ol> <p>Be aware that some of your defined functions might need to be updated as the newer Gabarit version might have some breaking changes.</p> <p></p>"},{"location":"frameworks/API/","title":"API Framework","text":""},{"location":"frameworks/API/#project-structure","title":"Project structure","text":"<p>Here is the structure of a project generated with <code>generate_api_project</code> command : </p> <pre><code>.\n\u251c\u2500 template_api                 # your application package\n\u2502   \u251c\u2500 core                     # global config and utilities\n\u2502   \u2502    \u251c\u2500 __init__.py\n\u2502   \u2502    \u251c\u2500 config.py\n\u2502   \u2502    \u251c\u2500 event_handlers.py   # load your model to your app at startup\n\u2502   \u2502    \u2514\u2500 logtools.py\n\u2502   \u2502\n\u2502   \u251c\u2500 model                    # model classes\n\u2502   \u2502    \u251c\u2500 __init__.py\n\u2502   \u2502    \u251c\u2500 model_base.py\n\u2502   \u2502    \u2514\u2500 model_gabarit.py\n\u2502   \u2502\n\u2502   \u251c\u2500 routers                  # applications routes\n\u2502   \u2502    \u251c\u2500 schemas\n\u2502   \u2502    \u251c\u2500 __init__.py\n\u2502   \u2502    \u251c\u2500 functional.py\n\u2502   \u2502    \u251c\u2500 technical.py\n\u2502   \u2502    \u2514\u2500 utils.py\n\u2502   \u2502\n\u2502   \u251c\u2500 __init__.py\n\u2502   \u2514\u2500 application.py\n\u2502\n\u251c\u2500 tests\n\u2502   \u2514\u2500 ...\n.\n.\n.\n\u251c\u2500 .env                         # environement variables for settings\n\u251c\u2500 makefile\n\u251c\u2500 Dockerfile\n\u251c\u2500 pyproject.toml               # your package dependencies and infos\n\u251c\u2500 setup.py\n\u251c\u2500 launch.sh                    # start your application\n\u2514\u2500 README.md\n</code></pre>"},{"location":"frameworks/API/#quickstart","title":"Quickstart","text":"<p>Gabarit has generated a <code>template_api</code> python package that contains all your FastAPI application logic.</p> <p>It contains three main sub-packages (cf. project structure) :</p> <ul> <li> <p><code>core</code> package for configuration and loading your model into your application</p> </li> <li> <p><code>model</code> package for defining how to download your model, to load it and make predictions</p> </li> <li> <p><code>routers</code> package for defining your API routes and how they work</p> </li> </ul> <p>Have a look at your <code>.env</code> file to see the default settings : <pre><code>APP_NAME=\"template_api\"\nAPI_ENTRYPOINT=\"/template_api/rs/v1\"\nMODEL_PATH=\"template_api-models/model.pkl\"\n</code></pre></p>"},{"location":"frameworks/API/#create-a-virtualenv-and-install-your-package","title":"Create a virtualenv and install your package","text":"<p>With make : <pre><code>make run create-virtualenv\nsource .venv/bin/activate\n\nmake init-local-env\n</code></pre></p> <p>Without make : <pre><code>python -m venv .venv\nsource .venv/bin/activate\npip install -e .[dev]\n</code></pre></p>"},{"location":"frameworks/API/#start-your-application","title":"Start your application","text":"<p>To start your FastAPI application activate your virtual environment and then use the script <code>launch.sh</code> or the <code>run</code> command of the makefile :</p> <pre><code>chmod +x launch.sh\nmake run\n</code></pre> <p>This will start a FastAPI thanks to uvicorn that listen on port 5000. Visit http://localhost:5000/docs to see the automatic interactive API documentation (provided by FastAPI and Swagger UI)</p>"},{"location":"frameworks/API/#how-it-works","title":"How it works","text":""},{"location":"frameworks/API/#model-class","title":"Model class","text":"<p>Your application use a <code>Model</code> object to make predictions. You will find a base <code>Model</code> class in <code>template_api.model.model_base</code> :</p> <pre><code>class Model:\n    def __init__(self):\n        self._model: Any = None\n        self._model_conf: dict = None\n        self._model_explainer = None\n        self._loaded: bool = False\n\n    def is_model_loaded(self) -&gt; bool:\n        \"\"\"return the state of the model\"\"\"\n        return self._loaded\n\n    def loading(self, **kwargs):\n        \"\"\"load the model\"\"\"\n        self._model, self._model_conf = self._load_model(**kwargs)\n        self._loaded = True\n\n    def predict(self, *args, **kwargs) -&gt; Any:\n        \"\"\"Make a prediction thanks to the model\"\"\"\n        return self._model.predict(*args, **kwargs)\n\n    def explain_as_json(self, *args, **kwargs) -&gt; Union[dict, list]:\n        \"\"\"Compute explanations about a prediction and return a JSON serializable object\"\"\"\n        return self._model_explainer.explain_instance_as_json(*args, **kwargs)\n\n    def explain_as_html(self, *args, **kwargs) -&gt; str:\n        \"\"\"Compute explanations about a prediction and return an HTML report\"\"\"\n        return self._model_explainer.explain_instance_as_html(*args, **kwargs)\n\n    def _load_model(self, **kwargs) -&gt; Tuple[Any, dict]:\n        \"\"\"Load a model from a file\n\n        Returns:\n            Tuple[Any, dict]: A tuple containing the model and a dict of metadata about it.\n        \"\"\"\n        ...\n\n    @staticmethod\n    def download_model(**kwargs) -&gt; bool:\n        \"\"\"You should implement a download method to automatically download your model\"\"\"\n        ...\n</code></pre> <p>As you can see, a <code>Model</code> object has four main attributes :</p> <ul> <li> <p><code>_model</code> containing your gabarit, scikit-learn or whatever model object</p> </li> <li> <p><code>_model_conf</code> which is a python dict with metadata about your model</p> </li> <li> <p><code>_model_explainer</code> containing your model explainer</p> </li> <li> <p><code>_loaded</code> which is set to <code>True</code> after <code>_load_model</code> has been called</p> </li> </ul> <p>The <code>Model</code> class also define a <code>download_model</code> method that will be used to download your model. By default it does nothing and returns <code>True</code>.</p> <p>You will also find a <code>ModelGabarit</code> class in <code>template_api.model.model_gabarit</code> that is suited to a model constructed thanks to a Gabarit template.</p> <p>It is a great example of how to adapt the base <code>Model</code> class to your use case.</p>"},{"location":"frameworks/API/#load-your-model-at-startup","title":"Load your model at startup","text":"<p>Your model is loaded into your application at startup thanks to <code>template_api.core.event_handlers</code> :</p> <pre><code>from typing import Callable\n\nfrom fastapi import FastAPI\nfrom ..model.model_base import Model\n\ndef _startup_model(app: FastAPI) -&gt; None:\n    \"\"\"Create and Load model\"\"\"\n    model = Model()\n    model.loading()\n    app.state.model = model\n\ndef start_app_handler(app: FastAPI) -&gt; Callable:\n    \"\"\"Startup handler: invoke init actions\"\"\"\n\n    def startup() -&gt; None:\n        logger.info(\"Startup Handler: Load model.\")\n        _startup_model(app)\n\n    return startup\n</code></pre> <p>To change the model used by your application, change the model imported here.</p>"},{"location":"frameworks/API/#functional-and-technical-routers","title":"Functional and technical routers","text":"<p>Routers are split into two categories by default : technical and functional ones.</p> <ul> <li>Technical routers are used for technical purpose such as verifying liveness or getting   infos about your application</li> <li>Functional ones are used to implement your business logic such as model predictions   or model explicability</li> </ul> <p>Since gabarit could not know what data your model is expecting, the default <code>/predict</code> route from <code>template_api.routers.functional</code> use a starlette Request object instead of pydantic.</p> <p>For a cleaner way to handle requests and reponses you should use pydantic as stated in FastAPI documentation</p> <p>You can use routes from template_api.routers.technical as examples of how to create requests and responses schemas thanks to pydantic or have a look at the FastAPI documentation.</p>"},{"location":"frameworks/API/#dockerfile","title":"Dockerfile","text":"<p>A minimal <code>Dockerfile</code> is provided by the template. You should have a look a it, especially if you have to download your model in your containers.</p>"},{"location":"frameworks/NLP/","title":"NLP Framework","text":""},{"location":"frameworks/NLP/#project-structure","title":"Project structure","text":"<p>Here is the structure of a project generated with <code>generate_nlp_project</code> command : </p> <pre><code>.\n\u251c\u2500 template_nlp                 # your application package\n\u2502    \u251c\u2500 models_training         # global config and utilities\n\u2502    \u2502    \u251c\u2500 models_sklearn     # package containing some predefined scikit-learn models\n\u2502    \u2502    \u251c\u2500 models_tensorflow  # package containing some predefined tensorflow models\n\u2502    \u2502    \u251c\u2500 model_class.py     # module containing Model base class\n\u2502    \u2502    \u251c\u2500 ...\n\u2502    \u2502    \u2514\u2500 utils_models.py    # module containing utility functions\n\u2502    \u2502\n\u2502    \u251c\u2500 monitoring              # package containing monitoring utilities (mlflow, model explicability)\n\u2502    \u2502\n\u2502    \u251c\u2500 preprocessing           # package containing preprocessing logic\n\u2502    \u2502\n\u2502    \u251c\u2500 __init__.py\n\u2502    \u2514\u2500 utils.py\n\u2502 \n\u251c\u2500 template_nlp-data            # Folder where to store your data\n\u251c\u2500 template_nlp-exploration     # Folder where to store your exploratory notebooks\n\u251c\u2500 template_nlp-models          # Folder containing trained models\n\u251c\u2500 template_nlp-scripts         # Folder containing script for preprocessing, training, etc.\n\u251c\u2500 template_nlp-tutorials       # Folder containing a tutorial notebook\n.\n.\n.\n\u251c\u2500 makefile\n\u251c\u2500 setup.py\n\u2514\u2500 README.md\n</code></pre> <p>Warning</p> <p>If you used a custom preprocessing function <code>funcA</code> with <code>FunctionTransformer</code>, be aware that the pickled pipeline  may not return wanted results if you later modify <code>funcA</code> definition. </p> <p>Please check gabarit/issues/63</p>"},{"location":"frameworks/NUM/","title":"NUM Framework","text":""},{"location":"frameworks/NUM/#project-structure","title":"Project structure","text":"<p>Here is the structure of a project generated with <code>generate_num_project</code> command : </p> <pre><code>.\n\u251c\u2500 template_num                       # your application package\n\u2502    \u251c\u2500 models_training               # global config and utilities\n\u2502    \u2502    \u251c\u2500 classifiers\n\u2502    \u2502    \u2502    \u251c\u2500 models_sklearn      # package containing some predefined scikit-learn classifiers\n\u2502    \u2502    \u2502    \u2514\u2500 models_tensorflow   # package containing some predefined tensorflow classifiers\n\u2502    \u2502    \u251c\u2500 regressors      \n\u2502    \u2502    \u2502    \u251c\u2500 models_sklearn      # package containing some predefined scikit-learn regressors\n\u2502    \u2502    \u2502    \u2514\u2500 models_tensorflow   # package containing some predefined tensorflow regressors\n\u2502    \u2502    \u251c\u2500 ...\n\u2502    \u2502    \u251c\u2500 model_class.py           # module containing base Model class\n\u2502    \u2502    \u2514\u2500 utils_models.py          # module containing utility functions\n\u2502    \u2502\n\u2502    \u251c\u2500 monitoring                    # package containing monitoring utilities (mlflow, model explicability)\n\u2502    \u2502\n\u2502    \u251c\u2500 preprocessing                 # package containing preprocessing logic\n\u2502    \u2502\n\u2502    \u251c\u2500 __init__.py\n\u2502    \u2514\u2500 utils.py\n\u2502\n\u251c\u2500 template_num-data                  # Folder where to store your data\n\u251c\u2500 template_num-exploration           # Folder where to store your exploratory notebooks\n\u251c\u2500 template_num-models                # Folder containing trained models\n\u251c\u2500 template_num-pipelines             # Folder containing fitted pipelines are stored\n\u251c\u2500 template_num-scripts               # Folder containing script for preprocessing, training, etc.\n\u251c\u2500 template_num-tutorials             # Folder containing a tutorial notebook\n.\n.\n.\n\u251c\u2500 makefile\n\u251c\u2500 setup.py\n\u2514\u2500 README.md\n</code></pre>"},{"location":"frameworks/NUM/#numeric-framewrok-specificities","title":"Numeric framewrok specificities","text":"<ul> <li> <p>Preprocessing has to be computed in a two step fashion to avoid bias:</p> </li> <li> <p>Fit your transformations on the training data (<code>1_preprocess_data.py</code>)</p> </li> <li> <p>Transform your validation/test sets (<code>2_apply_existing_pipeline.py</code>)</p> </li> <li> <p>Preprocessing pipelines are stored in the <code>project_name-pipelines</code> folder</p> </li> <li> <p>They are then stored as a .pkl object in the model folders (so that these can be used during inference)</p> </li> </ul> <p>Warning</p> <p>If you used a custom preprocessing function <code>funcA</code> with <code>FunctionTransformer</code>, be aware that the pickled pipeline  may not return wanted results if you later modify <code>funcA</code> definition. </p> <p>Please check gabarit/issues/63</p>"},{"location":"frameworks/VISION/","title":"VISION Framework","text":""},{"location":"frameworks/VISION/#project-structure","title":"Project structure","text":"<p>Here is the structure of a project generated with <code>generate_vision_project</code> command : </p> <pre><code>.\n\u251c\u2500 template_vision              # your application package\n\u2502    \u251c\u2500 models_training         # global config and utilities\n\u2502    \u2502    \u2514\u2500 classifiers        # package containing some predefined classifiers\n\u2502    \u2502    \u251c\u2500 object_detectors   # package containing some predefined object detectors\n\u2502    \u2502    \u251c\u2500 ...\n\u2502    \u2502    \u251c\u2500 model_class.py     # module containing base Model class\n\u2502    \u2502    \u2514\u2500 utils_models.py    # module containing utility functions\n\u2502    \u2502\n\u2502    \u251c\u2500 monitoring              # package containing monitoring utilities (mlflow, model explicability)\n\u2502    \u2502\n\u2502    \u251c\u2500 preprocessing           # package containing preprocessing logic\n\u2502    \u2502\n\u2502    \u251c\u2500 __init__.py\n\u2502    \u2514\u2500 utils.py\n\u2502\n\u251c\u2500 template_vision-data         # Folder where to store your data\n\u251c\u2500 template_vision-exploration  # Folder where to store your exploratory notebooks\n\u251c\u2500 template_vision-models       # Folder containing trained models\n\u251c\u2500 template_vision-scripts      # Folder containing script for preprocessing, training, etc.\n\u251c\u2500 template_vision-tutorials    # Folder containing a tutorial notebook\n.\n.\n.\n\u251c\u2500 makefile\n\u251c\u2500 setup.py\n\u2514\u2500 README.md\n</code></pre>"},{"location":"frameworks/VISION/#computer-vision-framewrok-specificities","title":"Computer vision framewrok specificities","text":"<ul> <li> <p>The expected input data format is different than in the other frameworks.</p> </li> <li> <p>For image classification, 3 differents formats can be used :</p> <ol> <li>A root folder with a subfolder per class (containing all the images associated with this class)</li> <li>A unique folder containing every image where each image name is prefixed with its class</li> <li>A folder containing all the images and a .csv metadata file containing the image/class matching</li> </ol> </li> <li> <p>For object detection, you must provide a .csv metadata file containing the bounding boxes for each image</p> </li> </ul>"},{"location":"reference/template_api/","title":"Template api","text":""},{"location":"reference/template_api/application/","title":"Application","text":""},{"location":"reference/template_api/application/#template_api.application.declare_application","title":"<code>declare_application()</code>","text":"<p>Create the FastAPI application</p> <p>See https://fastapi.tiangolo.com/tutorial/first-steps/ to learn how to customize your FastAPI application</p> Source code in <code>template_api/application.py</code> <pre><code>def declare_application() -&gt; FastAPI:\n    \"\"\"Create the FastAPI application\n\n    See https://fastapi.tiangolo.com/tutorial/first-steps/ to learn how to\n    customize your FastAPI application\n    \"\"\"\n    app = FastAPI(\n        title=f\"REST API form {settings.app_name}\",\n        description=f\"Use {settings.app_name} thanks to FastAPI\",\n        lifespan=lifespan\n    )\n\n    # Add PrometheusMiddleware\n    app.add_middleware(PrometheusMiddleware)\n    app.add_route(\"/metrics\", metrics)\n\n    # CORS middleware that allows all origins to avoid CORS problems\n    # see https://fastapi.tiangolo.com/tutorial/cors/#use-corsmiddleware\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[\"*\"],\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n\n    #\n    app.include_router(main_routeur, prefix=settings.api_entrypoint)\n\n    return app\n</code></pre>"},{"location":"reference/template_api/core/","title":"Core","text":"<p>Core package contains global configurations and utilities</p>"},{"location":"reference/template_api/core/config/","title":"Config","text":"<p>Config global settings</p> <p>This module handle global app configuration</p>"},{"location":"reference/template_api/core/logtools/","title":"Logtools","text":"<p>Logs utilities</p> <p>This module is used to define log pattern, add log filters, etc.</p>"},{"location":"reference/template_api/core/resources/","title":"Resources","text":"<p>Resources for the FastAPI application</p> <p>This module define resources that need to be instantiated at startup in a global variable resources that can be used in routes.</p> <p>This is the way your machine learning models can be loaded in memory at startup so they can handle requests.</p> <p>To use your own model instead of the base model, create a module in template_api.model such as model_awesome.py and import it as Model instead of the one used here.</p>"},{"location":"reference/template_api/model/","title":"Model","text":"<p>Model package contain model-related logic</p>"},{"location":"reference/template_api/model/model_base/","title":"Model base","text":"<p>This module contains the base Model class</p> <p>Model is the base model class. It contains a loading and downloading methods that are used by default to download your model into your Docker container and load it into your application.</p> <p>To use a custom model class in your application, create a new module such as model_awesome.py in this package and write a custom class that overwrite _load_model, download_model or predict depending on your needs.</p>"},{"location":"reference/template_api/model/model_base/#template_api.model.model_base.Model","title":"<code>Model</code>","text":"<p>Parent model class.</p> <p>This class is given as an exemple, you should probably adapt it to your project. This class loads the model from a .pkl file. The model must have a predict function.</p> Source code in <code>template_api/model/model_base.py</code> <pre><code>class Model:\n    \"\"\"Parent model class.\n\n    This class is given as an exemple, you should probably adapt it to your project.\n    This class loads the model from a .pkl file. The model must have a predict function.\n    \"\"\"\n\n    def __init__(self):\n        '''Init. model class'''\n        self._model = None\n        self._model_conf = None\n        self._model_explainer = None\n        self._loaded = False\n\n    def is_model_loaded(self):\n        \"\"\"return the state of the model\"\"\"\n        return self._loaded\n\n    def loading(self, **kwargs):\n        \"\"\"load the model\"\"\"\n        self._load_model(**kwargs)\n        self._loaded = True\n\n    def predict(self, *args, **kwargs):\n        \"\"\"Make a prediction thanks to the model\"\"\"\n        return self._model.predict(*args, **kwargs)\n\n    def explain_as_json(self, *args, **kwargs) -&gt; Union[dict, list]:\n        \"\"\"Compute explanations about a prediction and return a JSON serializable object\"\"\"\n        return self._model_explainer.explain_instance_as_json(*args, **kwargs)\n\n    def explain_as_html(self, *args, **kwargs) -&gt; str:\n        \"\"\"Compute explanations about a prediction and return an HTML report\"\"\"\n        return self._model_explainer.explain_instance_as_html(*args, **kwargs)\n\n    def _load_model(self, **kwargs) -&gt; None:\n        \"\"\"Load a model from a file\n\n        Returns:\n            Tuple[Any, dict]: A tuple containing the model and a dict of metadata about it.\n        \"\"\"\n        settings = ModelSettings(**kwargs)\n\n        logger.info(f\"Loading the model from {settings.model_path}\")\n        with settings.model_path.open(\"rb\") as f:\n            self._model = pickle.load(f)\n\n        self._model_conf = {\n            \"model_path\": settings.model_path.name,\n            \"model_name\": settings.model_path.stem,\n        }\n        logger.info(f\"Model loaded\")\n\n    @staticmethod\n    def download_model(**kwargs) -&gt; bool:\n        \"\"\"You should implement a download method to automatically download your model\"\"\"\n        logger.info(\"The function download_model is empty. Implement it to automatically download your model.\")\n        return True\n</code></pre>"},{"location":"reference/template_api/model/model_base/#template_api.model.model_base.Model.__init__","title":"<code>__init__()</code>","text":"<p>Init. model class</p> Source code in <code>template_api/model/model_base.py</code> <pre><code>def __init__(self):\n    '''Init. model class'''\n    self._model = None\n    self._model_conf = None\n    self._model_explainer = None\n    self._loaded = False\n</code></pre>"},{"location":"reference/template_api/model/model_base/#template_api.model.model_base.Model.download_model","title":"<code>download_model(**kwargs)</code>  <code>staticmethod</code>","text":"<p>You should implement a download method to automatically download your model</p> Source code in <code>template_api/model/model_base.py</code> <pre><code>@staticmethod\ndef download_model(**kwargs) -&gt; bool:\n    \"\"\"You should implement a download method to automatically download your model\"\"\"\n    logger.info(\"The function download_model is empty. Implement it to automatically download your model.\")\n    return True\n</code></pre>"},{"location":"reference/template_api/model/model_base/#template_api.model.model_base.Model.explain_as_html","title":"<code>explain_as_html(*args, **kwargs)</code>","text":"<p>Compute explanations about a prediction and return an HTML report</p> Source code in <code>template_api/model/model_base.py</code> <pre><code>def explain_as_html(self, *args, **kwargs) -&gt; str:\n    \"\"\"Compute explanations about a prediction and return an HTML report\"\"\"\n    return self._model_explainer.explain_instance_as_html(*args, **kwargs)\n</code></pre>"},{"location":"reference/template_api/model/model_base/#template_api.model.model_base.Model.explain_as_json","title":"<code>explain_as_json(*args, **kwargs)</code>","text":"<p>Compute explanations about a prediction and return a JSON serializable object</p> Source code in <code>template_api/model/model_base.py</code> <pre><code>def explain_as_json(self, *args, **kwargs) -&gt; Union[dict, list]:\n    \"\"\"Compute explanations about a prediction and return a JSON serializable object\"\"\"\n    return self._model_explainer.explain_instance_as_json(*args, **kwargs)\n</code></pre>"},{"location":"reference/template_api/model/model_base/#template_api.model.model_base.Model.is_model_loaded","title":"<code>is_model_loaded()</code>","text":"<p>return the state of the model</p> Source code in <code>template_api/model/model_base.py</code> <pre><code>def is_model_loaded(self):\n    \"\"\"return the state of the model\"\"\"\n    return self._loaded\n</code></pre>"},{"location":"reference/template_api/model/model_base/#template_api.model.model_base.Model.loading","title":"<code>loading(**kwargs)</code>","text":"<p>load the model</p> Source code in <code>template_api/model/model_base.py</code> <pre><code>def loading(self, **kwargs):\n    \"\"\"load the model\"\"\"\n    self._load_model(**kwargs)\n    self._loaded = True\n</code></pre>"},{"location":"reference/template_api/model/model_base/#template_api.model.model_base.Model.predict","title":"<code>predict(*args, **kwargs)</code>","text":"<p>Make a prediction thanks to the model</p> Source code in <code>template_api/model/model_base.py</code> <pre><code>def predict(self, *args, **kwargs):\n    \"\"\"Make a prediction thanks to the model\"\"\"\n    return self._model.predict(*args, **kwargs)\n</code></pre>"},{"location":"reference/template_api/model/model_base/#template_api.model.model_base.ModelSettings","title":"<code>ModelSettings</code>","text":"<p>             Bases: <code>BaseSettings</code></p> <p>Download settings</p> <p>This class is used for settings management purpose, have a look at the pydantic documentation for more details : https://pydantic-docs.helpmanual.io/usage/settings/</p> <p>By default, it looks for environment variables (case insensitive) to set the settings if a variable is not found, it looks for a file name .env in your working directory where you can declare the values of the variables and finally it sets the values to the default ones you can see above.</p> Source code in <code>template_api/model/model_base.py</code> <pre><code>class ModelSettings(BaseSettings):\n    \"\"\"Download settings\n\n    This class is used for settings management purpose, have a look at the pydantic\n    documentation for more details : https://pydantic-docs.helpmanual.io/usage/settings/\n\n    By default, it looks for environment variables (case insensitive) to set the settings\n    if a variable is not found, it looks for a file name .env in your working directory\n    where you can declare the values of the variables and finally it sets the values\n    to the default ones you can see above.\n    \"\"\"\n\n    model_path: Path = DEFAULT_MODEL_PATH\n\n    model_config = SettingsConfigDict(env_file=\".env\", extra='ignore', protected_namespaces=('settings', ))\n</code></pre>"},{"location":"reference/template_api/model/model_gabarit/","title":"Model gabarit","text":"<p>This module contains a ModelGabarit class you can use for your gabarit generated projects</p> ModelGabarit overwrite some methods of the base Model class <ul> <li>download_model method to download a model from a JFrog Artifactory repository ;</li> <li>_load_model method to use the gabarit_package.models_training.utils_models.load_model function from a typical gabarit project ;</li> <li>predict method to use the the gabarit_package.models_training.utils_models.predict function from a typical gabarit project.</li> </ul>"},{"location":"reference/template_api/model/model_gabarit/#template_api.model.model_gabarit.ModelGabarit","title":"<code>ModelGabarit</code>","text":"<p>             Bases: <code>Model</code></p> <p>Model class for a Gabarit generated project</p> <ul> <li>download_model has been redefined to download a model from artifactory based on the settings : ARTIFACTORY_MODEL_URL, ARTIFACTORY_USER, ARTIFACTORY_PASSWORD</li> <li>_load_model has been redefined to use utils_models.load_model</li> <li>predict has been redefined to use utils_models.predict</li> </ul> Source code in <code>template_api/model/model_gabarit.py</code> <pre><code>class ModelGabarit(Model):\n    \"\"\"Model class for a Gabarit generated project\n\n    - download_model has been redefined to download a model from artifactory based on\n    the settings : ARTIFACTORY_MODEL_URL, ARTIFACTORY_USER, ARTIFACTORY_PASSWORD\n    - _load_model has been redefined to use utils_models.load_model\n    - predict has been redefined to use utils_models.predict\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        \"\"\"Object initialization\n        By default, it initialize the attributes _model, _model_config and _loaded\n\n        see the parent __init__ method in template_api.model.model_base.Model\n        \"\"\"\n        super().__init__(*args, **kwargs)\n\n    def predict(self, content: Any, *args, **kwargs) -&gt; Any:\n        \"\"\"Make a prediction by calling utils_models.predict with the loaded model\"\"\"\n        if isinstance(content, list) or isinstance(content, dict):\n            content = pd.DataFrame(content)\n\n        # For APIs, we default to alternative_version = True\n        # It uses `tf.function` and `model.__call__` which is way faster for low number of inputs\n        # It also prevents some memory issues with newest version of TensorFlow\n        # https://github.com/tensorflow/tensorflow/issues/58676\n        # You can change the inference batch size if it doesn't suit your model/project\n        return utils_models.predict(content, model=self._model, model_conf=self._model_conf,\n                                    inference_batch_size=128, alternative_version=True, **kwargs)\n\n    def explain_as_json(self, content: Any, *args, **kwargs) -&gt; Union[dict, list]:\n        \"\"\"Compute explanations about a prediction and return a JSON serializable object\"\"\"\n        if isinstance(content, list) or isinstance(content, dict):\n            content = pd.DataFrame(content)\n\n        return self._model_explainer.explain_instance_as_json(content, *args, **kwargs)\n\n    def explain_as_html(self, content: Any, *args, **kwargs) -&gt; str:\n        \"\"\"Compute explanations about a prediction and return an HTML report\"\"\"\n        if isinstance(content, list) or isinstance(content, dict):\n            content = pd.DataFrame(content)\n\n        return self._model_explainer.explain_instance_as_html(content, *args, **kwargs)\n\n    def _load_model(self, **kwargs) -&gt; None:\n        \"\"\"Load a model in a gabarit fashion\"\"\"\n        settings = ModelSettings(**kwargs)\n\n        # Replace get_data_path method from gabarit.utils to use template_api data directory\n        if hasattr(utils_gabarit, \"get_data_path\"):\n            utils_gabarit.get_data_path = lambda: str(settings.data_dir.resolve())\n\n        # Using is_path=True allow to specify a path instead of a folder relative\n        # to gabarit_package.utils.DIR_PATH\n        model, model_conf = utils_models.load_model(model_dir=settings.model_path, is_path=True)\n\n        # Set attributes\n        self._model = model\n        self._model_conf = model_conf\n\n        # Create a model explainer\n        self._model_explainer = Explainer(model=model, model_conf=model_conf)\n\n    @staticmethod\n    def download_model(**kwargs) -&gt; bool:\n        \"\"\"Download the model from an JFrog Artifactory repository\"\"\"\n        settings = ModelSettings(**kwargs)\n\n        model_path = settings.model_path\n\n        # If the model already exists there is no need to download it\n        if not settings.redownload and model_path.is_dir() and not any(model_path.iterdir()):\n            logger.info(f\"The model is already dowloaded : {model_path} already exists\")\n            return True\n\n        # Create models directory if it doesn not exists\n        models_dir = settings.models_dir\n        models_dir.mkdir(parents=True, exist_ok=True)\n\n        # Download model from artifactory\n        try:\n            from artifactory import ArtifactoryPath\n        except ImportError:\n            raise ImportError(\"Module artifactory not found. Please install it : `pip install dohq-artifactory`\")\n\n        model_artifactory_path = ArtifactoryPath(\n            settings.artifactory_model_url,\n            auth=(settings.artifactory_user, settings.artifactory_password),\n            verify=False,\n        )\n\n        with tempfile.TemporaryDirectory(dir=models_dir) as tmpdir:\n            model_archive_path = Path(tmpdir) / model_artifactory_path.name\n\n            # Download model\n            logger.info(f\"Downloading the model to : {model_path}\")\n            with model_archive_path.open(\"wb\") as out:\n                model_artifactory_path.writeto(out)\n\n            # Unzip model\n            shutil.unpack_archive(model_archive_path, model_path)\n            logger.info(f\"Model downloaded\")\n\n        logger.info(f\"Model archive removed\")\n        return True\n</code></pre>"},{"location":"reference/template_api/model/model_gabarit/#template_api.model.model_gabarit.ModelGabarit.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Object initialization By default, it initialize the attributes _model, _model_config and _loaded</p> <p>see the parent init method in template_api.model.model_base.Model</p> Source code in <code>template_api/model/model_gabarit.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Object initialization\n    By default, it initialize the attributes _model, _model_config and _loaded\n\n    see the parent __init__ method in template_api.model.model_base.Model\n    \"\"\"\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"reference/template_api/model/model_gabarit/#template_api.model.model_gabarit.ModelGabarit.download_model","title":"<code>download_model(**kwargs)</code>  <code>staticmethod</code>","text":"<p>Download the model from an JFrog Artifactory repository</p> Source code in <code>template_api/model/model_gabarit.py</code> <pre><code>@staticmethod\ndef download_model(**kwargs) -&gt; bool:\n    \"\"\"Download the model from an JFrog Artifactory repository\"\"\"\n    settings = ModelSettings(**kwargs)\n\n    model_path = settings.model_path\n\n    # If the model already exists there is no need to download it\n    if not settings.redownload and model_path.is_dir() and not any(model_path.iterdir()):\n        logger.info(f\"The model is already dowloaded : {model_path} already exists\")\n        return True\n\n    # Create models directory if it doesn not exists\n    models_dir = settings.models_dir\n    models_dir.mkdir(parents=True, exist_ok=True)\n\n    # Download model from artifactory\n    try:\n        from artifactory import ArtifactoryPath\n    except ImportError:\n        raise ImportError(\"Module artifactory not found. Please install it : `pip install dohq-artifactory`\")\n\n    model_artifactory_path = ArtifactoryPath(\n        settings.artifactory_model_url,\n        auth=(settings.artifactory_user, settings.artifactory_password),\n        verify=False,\n    )\n\n    with tempfile.TemporaryDirectory(dir=models_dir) as tmpdir:\n        model_archive_path = Path(tmpdir) / model_artifactory_path.name\n\n        # Download model\n        logger.info(f\"Downloading the model to : {model_path}\")\n        with model_archive_path.open(\"wb\") as out:\n            model_artifactory_path.writeto(out)\n\n        # Unzip model\n        shutil.unpack_archive(model_archive_path, model_path)\n        logger.info(f\"Model downloaded\")\n\n    logger.info(f\"Model archive removed\")\n    return True\n</code></pre>"},{"location":"reference/template_api/model/model_gabarit/#template_api.model.model_gabarit.ModelGabarit.explain_as_html","title":"<code>explain_as_html(content, *args, **kwargs)</code>","text":"<p>Compute explanations about a prediction and return an HTML report</p> Source code in <code>template_api/model/model_gabarit.py</code> <pre><code>def explain_as_html(self, content: Any, *args, **kwargs) -&gt; str:\n    \"\"\"Compute explanations about a prediction and return an HTML report\"\"\"\n    if isinstance(content, list) or isinstance(content, dict):\n        content = pd.DataFrame(content)\n\n    return self._model_explainer.explain_instance_as_html(content, *args, **kwargs)\n</code></pre>"},{"location":"reference/template_api/model/model_gabarit/#template_api.model.model_gabarit.ModelGabarit.explain_as_json","title":"<code>explain_as_json(content, *args, **kwargs)</code>","text":"<p>Compute explanations about a prediction and return a JSON serializable object</p> Source code in <code>template_api/model/model_gabarit.py</code> <pre><code>def explain_as_json(self, content: Any, *args, **kwargs) -&gt; Union[dict, list]:\n    \"\"\"Compute explanations about a prediction and return a JSON serializable object\"\"\"\n    if isinstance(content, list) or isinstance(content, dict):\n        content = pd.DataFrame(content)\n\n    return self._model_explainer.explain_instance_as_json(content, *args, **kwargs)\n</code></pre>"},{"location":"reference/template_api/model/model_gabarit/#template_api.model.model_gabarit.ModelGabarit.predict","title":"<code>predict(content, *args, **kwargs)</code>","text":"<p>Make a prediction by calling utils_models.predict with the loaded model</p> Source code in <code>template_api/model/model_gabarit.py</code> <pre><code>def predict(self, content: Any, *args, **kwargs) -&gt; Any:\n    \"\"\"Make a prediction by calling utils_models.predict with the loaded model\"\"\"\n    if isinstance(content, list) or isinstance(content, dict):\n        content = pd.DataFrame(content)\n\n    # For APIs, we default to alternative_version = True\n    # It uses `tf.function` and `model.__call__` which is way faster for low number of inputs\n    # It also prevents some memory issues with newest version of TensorFlow\n    # https://github.com/tensorflow/tensorflow/issues/58676\n    # You can change the inference batch size if it doesn't suit your model/project\n    return utils_models.predict(content, model=self._model, model_conf=self._model_conf,\n                                inference_batch_size=128, alternative_version=True, **kwargs)\n</code></pre>"},{"location":"reference/template_api/model/model_gabarit/#template_api.model.model_gabarit.ModelSettings","title":"<code>ModelSettings</code>","text":"<p>             Bases: <code>BaseSettings</code></p> <p>Download settings</p> <p>This class is used for settings management purpose, have a look at the pydantic documentation for more details : https://pydantic-docs.helpmanual.io/usage/settings/</p> <p>By default, it looks for environment variables (case insensitive) to set the settings if a variable is not found, it looks for a file name .env in your working directory where you can declare the values of the variables and finally it sets the values to the default ones you can see above.</p> Source code in <code>template_api/model/model_gabarit.py</code> <pre><code>class ModelSettings(BaseSettings):\n    \"\"\"Download settings\n\n    This class is used for settings management purpose, have a look at the pydantic\n    documentation for more details : https://pydantic-docs.helpmanual.io/usage/settings/\n\n    By default, it looks for environment variables (case insensitive) to set the settings\n    if a variable is not found, it looks for a file name .env in your working directory\n    where you can declare the values of the variables and finally it sets the values\n    to the default ones you can see above.\n    \"\"\"\n\n    data_dir: Path = DEFAULT_DATA_DIR\n    models_dir: Path = DEFAULT_MODELS_DIR\n    model_path: Path = DEFAULT_MODELS_DIR / \"model\"\n    artifactory_model_url: str = \"\"\n    artifactory_user: str = \"\"\n    artifactory_password: str = \"\"\n    redownload: bool = False\n\n    model_config = SettingsConfigDict(env_file=\".env\", extra='ignore', protected_namespaces=('settings', ))\n</code></pre>"},{"location":"reference/template_api/routers/","title":"Routers","text":"<p>Main router of the REST API</p>"},{"location":"reference/template_api/routers/functional/","title":"Functional","text":""},{"location":"reference/template_api/routers/functional/#template_api.routers.functional.explain","title":"<code>explain(request)</code>  <code>async</code>","text":"<p>Explain route that expose a model explainer in charge of model explicability</p> <p>This function is using starlette Request object instead of pydantic since we can not know what data your model is expecting. See https://fastapi.tiangolo.com/advanced/using-request-directly/ for more infos.</p> <p>We also use the custom starlette JSONResponse class (PredictionResponse) instead of pydantic for the same reasons</p> <p>For a cleaner way to handle requests and reponses you should use pydantic as stated in FastAPI doc : https://fastapi.tiangolo.com/tutorial/body/#create-your-data-model</p> <p>You can use routes from example_api_num.routers.technical as examples of how to create requests and responses schemas thanks to pydantic or have a look at the FastAPI documentation : https://fastapi.tiangolo.com/tutorial/response-model/</p> <p>If there is not explainer or the explainer does not implement explain_as_json or explain_as_html we return a 501 HTTP error : https://developer.mozilla.org/fr/docs/Web/HTTP/Status/501</p> Source code in <code>template_api/routers/functional.py</code> <pre><code>@router.post(\"/explain\")\nasync def explain(request: Request):\n    \"\"\"Explain route that expose a model explainer in charge of model explicability\n\n    This function is using starlette Request object instead of pydantic since we can not\n    know what data your model is expecting.\n    See https://fastapi.tiangolo.com/advanced/using-request-directly/ for more infos.\n\n    We also use the custom starlette JSONResponse class (PredictionResponse)\n    instead of pydantic for the same reasons\n\n    For a cleaner way to handle requests and reponses you should use pydantic as stated in FastAPI\n    doc : https://fastapi.tiangolo.com/tutorial/body/#create-your-data-model\n\n    You can use routes from example_api_num.routers.technical as examples of how to create requests and\n    responses schemas thanks to pydantic or have a look at the FastAPI documentation :\n    https://fastapi.tiangolo.com/tutorial/response-model/\n\n    If there is not explainer or the explainer does not implement explain_as_json or explain_as_html\n    we return a 501 HTTP error : https://developer.mozilla.org/fr/docs/Web/HTTP/Status/501\n    \"\"\"\n    model: Model = RESOURCES.get(RESOURCE_MODEL)\n\n    body = await request.body()\n    body = json.loads(body) if body else {}\n\n    # JSON repsonse (when Accept: application/json in the request)\n    if request.headers.get(\"Accept\") == \"application/json\":\n        try:\n            explanation_json = model.explain_as_json(**body)\n\n        except (AttributeError, NotImplementedError):\n            error_msg = {\n                \"error\": {\n                    \"code\": 501,\n                    \"message\": \"No explainer capable of handling explicability\"\n                }\n            }\n            return Response(\n                content=json.dumps(error_msg),\n                status_code=501,\n                media_type='application/json',\n            )\n        else:\n            return NumpyJSONResponse(explanation_json)\n\n    # HTML repsonse (otherwise)\n    else:\n        try:\n            explanation_html = model.explain_as_html(**body)\n\n        except (AttributeError, NotImplementedError):\n            return Response(\n                content=\"No explainer capable of handling explicability\",\n                status_code=501,\n                media_type='text/plain',\n            )\n        else:\n            return HTMLResponse(explanation_html)\n</code></pre>"},{"location":"reference/template_api/routers/functional/#template_api.routers.functional.predict","title":"<code>predict(request)</code>  <code>async</code>","text":"<p>Predict route that exposes your model</p> <p>This function is using starlette Request object instead of pydantic since we can not know what data your model is expecting. See https://fastapi.tiangolo.com/advanced/using-request-directly/ for more infos.</p> <p>We also use a custom starlette JSONResponse class (NumpyJSONResponse) instead of pydantic for the same reasons</p> <p>For a cleaner way to handle requests and reponses you should use pydantic as stated in FastAPI doc : https://fastapi.tiangolo.com/tutorial/body/#create-your-data-model</p> <p>You can use routes from template_api.routers.technical as examples of how to create requests and responses schemas thanks to pydantic or have a look at the FastAPI documentation : https://fastapi.tiangolo.com/tutorial/response-model/</p> Source code in <code>template_api/routers/functional.py</code> <pre><code>@router.post(\"/predict\")\nasync def predict(request: Request):\n    \"\"\"Predict route that exposes your model\n\n    This function is using starlette Request object instead of pydantic since we can not\n    know what data your model is expecting.\n    See https://fastapi.tiangolo.com/advanced/using-request-directly/ for more infos.\n\n    We also use a custom starlette JSONResponse class (NumpyJSONResponse)\n    instead of pydantic for the same reasons\n\n    For a cleaner way to handle requests and reponses you should use pydantic as stated in FastAPI\n    doc : https://fastapi.tiangolo.com/tutorial/body/#create-your-data-model\n\n    You can use routes from template_api.routers.technical as examples of how to create requests and\n    responses schemas thanks to pydantic or have a look at the FastAPI documentation :\n    https://fastapi.tiangolo.com/tutorial/response-model/\n    \"\"\"\n    model: Model = RESOURCES.get(RESOURCE_MODEL)\n\n    body = await request.body()\n    body = json.loads(body) if body else {}\n\n    prediction = model.predict(**body)\n\n    return NumpyJSONResponse(prediction)\n</code></pre>"},{"location":"reference/template_api/routers/technical/","title":"Technical","text":""},{"location":"reference/template_api/routers/technical/#template_api.routers.technical.get_liveness","title":"<code>get_liveness()</code>  <code>async</code>","text":"<p>Liveness probe for k8s</p> Source code in <code>template_api/routers/technical.py</code> <pre><code>@router.get(\n    \"/liveness\",\n    response_model=ReponseLiveness,\n    name=\"liveness\",\n    tags=[\"technical\"],\n)\nasync def get_liveness() -&gt; ReponseLiveness:\n    \"\"\"Liveness probe for k8s\"\"\"\n    liveness_msg = ReponseLiveness(alive=\"ok\")\n    return liveness_msg\n</code></pre>"},{"location":"reference/template_api/routers/technical/#template_api.routers.technical.get_readiness","title":"<code>get_readiness()</code>  <code>async</code>","text":"<p>Readiness probe for k8s</p> Source code in <code>template_api/routers/technical.py</code> <pre><code>@router.get(\n    \"/readiness\",\n    response_model=ReponseReadiness,\n    name=\"readiness\",\n    tags=[\"technical\"],\n)\nasync def get_readiness() -&gt; ReponseReadiness:\n    \"\"\"Readiness probe for k8s\"\"\"\n    model: Model = RESOURCES.get(RESOURCE_MODEL)\n\n    if model and model.is_model_loaded():\n        return ReponseReadiness(ready=\"ok\")\n    else:\n        return ReponseReadiness(ready=\"ko\")\n</code></pre>"},{"location":"reference/template_api/routers/technical/#template_api.routers.technical.info","title":"<code>info()</code>  <code>async</code>","text":"<p>Rest resource for info</p> Source code in <code>template_api/routers/technical.py</code> <pre><code>@router.get(\n    \"/info\",\n    response_model=ReponseInformation,\n    name=\"information\",\n    tags=[\"technical\"],\n)\nasync def info() -&gt; ReponseInformation:\n    \"\"\"Rest resource for info\"\"\"\n    model: Model = RESOURCES.get(RESOURCE_MODEL)\n\n    return ReponseInformation(\n        application=settings.app_name,\n        version=settings.app_version,\n        model_name=model._model_conf.get(\"model_name\", \"?\"),\n        model_version=model._model_conf.get(\"package_version\", \"?\"),\n    )\n</code></pre>"},{"location":"reference/template_api/routers/schemas/","title":"Schemas","text":""},{"location":"reference/template_api/routers/schemas/functional/","title":"Functional","text":"<p>Functional schemas</p>"},{"location":"reference/template_api/routers/schemas/technical/","title":"Technical","text":"<p>Technical schemas</p>"},{"location":"reference/template_api/routers/schemas/technical/#template_api.routers.schemas.technical.ReponseInformation","title":"<code>ReponseInformation</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Return object for info resource</p> Source code in <code>template_api/routers/schemas/technical.py</code> <pre><code>class ReponseInformation(BaseModel):\n    \"\"\"Return object for info resource\"\"\"\n\n    application: str = Field(None, title=\"Application name\")\n    version: str = Field(None, title=\"Application version\")\n    model_name: str = Field(None, title=\"Model name\")\n    model_version: str = Field(None, title=\"Model version\")\n</code></pre>"},{"location":"reference/template_api/routers/schemas/technical/#template_api.routers.schemas.technical.ReponseLiveness","title":"<code>ReponseLiveness</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Return object for liveness probe</p> Source code in <code>template_api/routers/schemas/technical.py</code> <pre><code>class ReponseLiveness(BaseModel):\n    \"\"\"Return object for liveness probe\"\"\"\n\n    alive: str = Field(None, title=\"Message\")\n</code></pre>"},{"location":"reference/template_api/routers/schemas/technical/#template_api.routers.schemas.technical.ReponseReadiness","title":"<code>ReponseReadiness</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Return object for readiness probe</p> Source code in <code>template_api/routers/schemas/technical.py</code> <pre><code>class ReponseReadiness(BaseModel):\n    \"\"\"Return object for readiness probe\"\"\"\n\n    ready: str = Field(None, title=\"Message\")\n</code></pre>"},{"location":"reference/template_api/routers/schemas/utils/","title":"Utils","text":""},{"location":"reference/template_api/routers/schemas/utils/#template_api.routers.schemas.utils.NumpyArrayEncoder","title":"<code>NumpyArrayEncoder</code>","text":"<p>             Bases: <code>JSONEncoder</code></p> <p>JSONEncoder to store python dict or list containing numpy arrays</p> Source code in <code>template_api/routers/schemas/utils.py</code> <pre><code>class NumpyArrayEncoder(json.JSONEncoder):\n    \"\"\"JSONEncoder to store python dict or list containing numpy arrays\"\"\"\n\n    def default(self, obj: Any) -&gt; Any:\n        \"\"\"Transform numpy arrays into JSON serializable object such as list\n        see : https://docs.python.org/3/library/json.html#json.JSONEncoder.default\n        \"\"\"\n\n        # numpy.ndarray have dtype, astype and tolist attribute and methods that we want\n        # to use to convert their element into JSON serializable objects\n        if hasattr(obj, \"dtype\") and hasattr(obj, \"astype\") and hasattr(obj, \"tolist\"):\n\n            if np.issubdtype(obj.dtype, np.integer):\n                return obj.astype(int).tolist()\n            elif np.issubdtype(obj.dtype, np.number):\n                return obj.astype(float).tolist()\n            else:\n                return obj.tolist()\n\n        # sets are not json serializable\n        elif isinstance(obj, set):\n            return list(obj)\n\n        return json.JSONEncoder.default(self, obj)\n</code></pre>"},{"location":"reference/template_api/routers/schemas/utils/#template_api.routers.schemas.utils.NumpyArrayEncoder.default","title":"<code>default(obj)</code>","text":"<p>Transform numpy arrays into JSON serializable object such as list see : https://docs.python.org/3/library/json.html#json.JSONEncoder.default</p> Source code in <code>template_api/routers/schemas/utils.py</code> <pre><code>def default(self, obj: Any) -&gt; Any:\n    \"\"\"Transform numpy arrays into JSON serializable object such as list\n    see : https://docs.python.org/3/library/json.html#json.JSONEncoder.default\n    \"\"\"\n\n    # numpy.ndarray have dtype, astype and tolist attribute and methods that we want\n    # to use to convert their element into JSON serializable objects\n    if hasattr(obj, \"dtype\") and hasattr(obj, \"astype\") and hasattr(obj, \"tolist\"):\n\n        if np.issubdtype(obj.dtype, np.integer):\n            return obj.astype(int).tolist()\n        elif np.issubdtype(obj.dtype, np.number):\n            return obj.astype(float).tolist()\n        else:\n            return obj.tolist()\n\n    # sets are not json serializable\n    elif isinstance(obj, set):\n        return list(obj)\n\n    return json.JSONEncoder.default(self, obj)\n</code></pre>"},{"location":"reference/template_nlp/","title":"Template nlp","text":""},{"location":"reference/template_nlp/utils/","title":"Utils","text":""},{"location":"reference/template_nlp/utils/#template_nlp.utils.NpEncoder","title":"<code>NpEncoder</code>","text":"<p>             Bases: <code>JSONEncoder</code></p> <p>JSON encoder to manage numpy objects</p> Source code in <code>template_nlp/utils.py</code> <pre><code>class NpEncoder(json.JSONEncoder):\n    '''JSON encoder to manage numpy objects'''\n    def default(self, obj) -&gt; Any:\n        if is_ndarray_convertable(obj):\n            return ndarray_to_builtin_object(obj)\n        elif isinstance(obj, set):\n            return list(obj)\n        else:\n            return super(NpEncoder, self).default(obj)\n</code></pre>"},{"location":"reference/template_nlp/utils/#template_nlp.utils.data_agnostic_str_to_list","title":"<code>data_agnostic_str_to_list(function)</code>","text":"<p>Decorator to transform a string into a list of one element, and retrieve first element of the function returns. Idea: be able to do <code>predict(my_string)</code> Otherwise, we would have to do <code>prediction = predict([my_string])[0]</code></p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>func</code> <p>Function to decorate</p> required <p>Returns:     function: The decorated function</p> Source code in <code>template_nlp/utils.py</code> <pre><code>def data_agnostic_str_to_list(function: Callable) -&gt; Callable:\n    '''Decorator to transform a string into a list of one element,\n    and retrieve first element of the function returns.\n    Idea: be able to do `predict(my_string)`\n    Otherwise, we would have to do `prediction = predict([my_string])[0]`\n\n    Args:\n        function (func): Function to decorate\n    Returns:\n        function: The decorated function\n    '''\n    # Get wrapper\n    def wrapper(self, x, *args, **kwargs):\n        '''Wrapper'''\n        if type(x) == str:\n            # Cast str into a single element list\n            my_list = [x]\n            # Get function result\n            results = function(self, my_list, *args, **kwargs)\n            # Cast back to single element\n            final_result = results[0]\n        else:\n            final_result = function(self, x, *args, **kwargs)\n        # Return\n        return final_result\n    return wrapper\n</code></pre>"},{"location":"reference/template_nlp/utils/#template_nlp.utils.display_shape","title":"<code>display_shape(df)</code>","text":"<p>Displays the number of line and of column of a table.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Table to parse</p> required Source code in <code>template_nlp/utils.py</code> <pre><code>def display_shape(df: pd.DataFrame) -&gt; None:\n    '''Displays the number of line and of column of a table.\n\n    Args:\n        df (pd.DataFrame): Table to parse\n    '''\n    # Display\n    logger.info(f\"Number of lines : {df.shape[0]}. Number of columns : {df.shape[1]}.\")\n</code></pre>"},{"location":"reference/template_nlp/utils/#template_nlp.utils.find_folder_path","title":"<code>find_folder_path(folder_name, base_folder=None)</code>","text":"<p>Find a folder in a base folder and its subfolders. If base_folder is None, considers folder_name as a path and check it exists</p> <p>i.e., with the following structure : - C:/     - base_folder/         - folderA/             - folderB/         - folderC/ find_folder_path(folderA, C:/base_folder) == C:/base_folder/folderA find_folder_path(folderB, C:/base_folder) == C:/base_folder/folderA/folderB find_folder_path(C:/base_folder/folderC, None) == C:/base_folder/folderC find_folder_path(folderB, None) raises an error</p> <p>Parameters:</p> Name Type Description Default <code>folder_name</code> <code>str</code> <p>name of the folder to find. If base_folder is None, consider a path instead.</p> required <p>Kwargs:     base_folder (str): path of the base folder. If None, consider folder_name as a path. Raises:     FileNotFoundError: If we can't find folder_name in base_folder     FileNotFoundError: If folder_name is not a valid path (case where base_folder is None) Returns:     str: path to the wanted folder</p> Source code in <code>template_nlp/utils.py</code> <pre><code>def find_folder_path(folder_name: str, base_folder: Union[str, None] = None) -&gt; str:\n    '''Find a folder in a base folder and its subfolders.\n    If base_folder is None, considers folder_name as a path and check it exists\n\n    i.e., with the following structure :\n    - C:/\n        - base_folder/\n            - folderA/\n                - folderB/\n            - folderC/\n    find_folder_path(folderA, C:/base_folder) == C:/base_folder/folderA\n    find_folder_path(folderB, C:/base_folder) == C:/base_folder/folderA/folderB\n    find_folder_path(C:/base_folder/folderC, None) == C:/base_folder/folderC\n    find_folder_path(folderB, None) raises an error\n\n    Args:\n        folder_name (str): name of the folder to find. If base_folder is None, consider a path instead.\n    Kwargs:\n        base_folder (str): path of the base folder. If None, consider folder_name as a path.\n    Raises:\n        FileNotFoundError: If we can't find folder_name in base_folder\n        FileNotFoundError: If folder_name is not a valid path (case where base_folder is None)\n    Returns:\n        str: path to the wanted folder\n    '''\n    if base_folder is not None:\n        folder_path = None\n        for path, subdirs, files in os.walk(base_folder):\n            for name in subdirs:\n                if name == folder_name:\n                    folder_path = os.path.join(path, name)\n        if folder_path is None:\n            raise FileNotFoundError(f\"Can't find folder {folder_name} inside {base_folder} and its subfolders\")\n    else:\n        folder_path = folder_name\n        if not os.path.exists(folder_path):\n            raise FileNotFoundError(f\"Can't find folder {folder_path} (considered as a path)\")\n    return folder_path\n</code></pre>"},{"location":"reference/template_nlp/utils/#template_nlp.utils.get_chunk_limits","title":"<code>get_chunk_limits(x, chunksize=10000)</code>","text":"<p>Gets chunk limits from a pandas series or dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series or DataFrame</code> <p>Documents to consider</p> required <p>Kwargs:     chunksize (int): The chunk size Raises:     ValueError: If the chunk size is negative Returns:     list: the chunk limits Source code in <code>template_nlp/utils.py</code> <pre><code>def get_chunk_limits(x: Union[pd.DataFrame, pd.Series], chunksize: int = 10000) -&gt; List[Tuple[int]]:\n    '''Gets chunk limits from a pandas series or dataframe.\n\n    Args:\n        x (pd.Series or pd.DataFrame): Documents to consider\n    Kwargs:\n        chunksize (int): The chunk size\n    Raises:\n        ValueError: If the chunk size is negative\n    Returns:\n        list&lt;tuple&gt;: the chunk limits\n    '''\n    if chunksize &lt; 0:\n        raise ValueError('The object chunksize must not be negative.')\n    # Processs\n    if chunksize == 0 or chunksize &gt;= x.shape[0]:\n        chunks_limits = [(0, x.shape[0])]\n    else:\n        chunks_limits = [(i * chunksize, min((i + 1) * chunksize, x.shape[0]))\n                         for i in range(1 + ((x.shape[0] - 1) // chunksize))]\n    return chunks_limits  # type: ignore\n</code></pre>"},{"location":"reference/template_nlp/utils/#template_nlp.utils.get_data_path","title":"<code>get_data_path()</code>","text":"<p>Returns the path to the data folder</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path of the data folder</p> Source code in <code>template_nlp/utils.py</code> <pre><code>def get_data_path() -&gt; str:\n    '''Returns the path to the data folder\n\n    Returns:\n        str: Path of the data folder\n    '''\n    if DIR_PATH is None:\n        dir_path = os.path.join(os.path.dirname(os.path.dirname(os.path.realpath(__file__))), 'template_nlp-data')\n    else:\n        dir_path = os.path.join(os.path.abspath(DIR_PATH), 'template_nlp-data')\n    if not os.path.isdir(dir_path):\n        os.mkdir(dir_path)\n    return os.path.abspath(dir_path)\n</code></pre>"},{"location":"reference/template_nlp/utils/#template_nlp.utils.get_models_path","title":"<code>get_models_path()</code>","text":"<p>Returns the path to the models folder</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path of the models folder</p> Source code in <code>template_nlp/utils.py</code> <pre><code>def get_models_path() -&gt; str:\n    '''Returns the path to the models folder\n\n    Returns:\n        str: Path of the models folder\n    '''\n    if DIR_PATH is None:\n        dir_path = os.path.join(os.path.dirname(os.path.dirname(os.path.realpath(__file__))), 'template_nlp-models')\n    else:\n        dir_path = os.path.join(os.path.abspath(DIR_PATH), 'template_nlp-models')\n    if not os.path.isdir(dir_path):\n        os.mkdir(dir_path)\n    return os.path.abspath(dir_path)\n</code></pre>"},{"location":"reference/template_nlp/utils/#template_nlp.utils.get_new_column_name","title":"<code>get_new_column_name(column_list, wanted_name)</code>","text":"<p>Gets a new column name from a list of existing ones &amp; a wanted name</p> <p>If the wanted name does not exists, return it. Otherwise get a new column prefixed by the wanted name.</p> <p>Parameters:</p> Name Type Description Default <code>column_list</code> <code>list</code> <p>List of existing columns</p> required <code>wanted_name</code> <code>str</code> <p>Wanted name</p> required Source code in <code>template_nlp/utils.py</code> <pre><code>def get_new_column_name(column_list: list, wanted_name: str) -&gt; str:\n    '''Gets a new column name from a list of existing ones &amp; a wanted name\n\n    If the wanted name does not exists, return it.\n    Otherwise get a new column prefixed by the wanted name.\n\n    Args:\n        column_list (list): List of existing columns\n        wanted_name (str): Wanted name\n    '''\n    if wanted_name not in column_list:\n        return wanted_name\n    else:\n        new_name = f'{wanted_name}_{str(uuid.uuid4())[:8]}'\n        # It should not happen, but we still check if new_name is available (bad luck ?)\n        return get_new_column_name(column_list, new_name)\n</code></pre>"},{"location":"reference/template_nlp/utils/#template_nlp.utils.get_package_version","title":"<code>get_package_version()</code>","text":"<p>Returns the current version of the package</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>version of the package</p> Source code in <code>template_nlp/utils.py</code> <pre><code>def get_package_version() -&gt; str:\n    '''Returns the current version of the package\n\n    Returns:\n        str: version of the package\n    '''\n    version = importlib.metadata.version('template_nlp')\n    return version\n</code></pre>"},{"location":"reference/template_nlp/utils/#template_nlp.utils.get_ressources_path","title":"<code>get_ressources_path()</code>","text":"<p>Returns the path to the ressources folder</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path of the ressources folder</p> Source code in <code>template_nlp/utils.py</code> <pre><code>def get_ressources_path() -&gt; str:\n    '''Returns the path to the ressources folder\n\n    Returns:\n        str: Path of the ressources folder\n    '''\n    dir_path = os.path.join(os.path.dirname(os.path.dirname(os.path.realpath(__file__))), 'template_nlp-ressources')\n    if not os.path.isdir(dir_path):\n        os.mkdir(dir_path)\n    return os.path.abspath(dir_path)\n</code></pre>"},{"location":"reference/template_nlp/utils/#template_nlp.utils.get_transformers_path","title":"<code>get_transformers_path()</code>","text":"<p>Returns the path to the transformers folder</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path of the transformers folder</p> Source code in <code>template_nlp/utils.py</code> <pre><code>def get_transformers_path() -&gt; str:\n    '''Returns the path to the transformers folder\n\n    Returns:\n        str: Path of the transformers folder\n    '''\n    if DIR_PATH is None:\n        dir_path = os.path.join(os.path.dirname(os.path.dirname(os.path.realpath(__file__))), 'template_nlp-transformers')\n    else:\n        dir_path = os.path.join(os.path.abspath(DIR_PATH), 'template_nlp-transformers')\n    if not os.path.isdir(dir_path):\n        os.mkdir(dir_path)\n    return os.path.abspath(dir_path)\n</code></pre>"},{"location":"reference/template_nlp/utils/#template_nlp.utils.is_ndarray_convertable","title":"<code>is_ndarray_convertable(obj)</code>","text":"<p>Returns True if the object is covertable to a builtin type in the same way a np.ndarray is</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>an object to test</p> required <p>Returns:     bool: True if the object is covertable to a list as a np.ndarray is</p> Source code in <code>template_nlp/utils.py</code> <pre><code>def is_ndarray_convertable(obj: Any) -&gt; bool:\n    '''Returns True if the object is covertable to a builtin type in the same way a np.ndarray is\n\n    Args:\n        obj (Any): an object to test\n    Returns:\n        bool: True if the object is covertable to a list as a np.ndarray is\n    '''\n    return hasattr(obj, \"dtype\") and hasattr(obj, \"astype\") and hasattr(obj, \"tolist\")\n</code></pre>"},{"location":"reference/template_nlp/utils/#template_nlp.utils.ndarray_to_builtin_object","title":"<code>ndarray_to_builtin_object(obj)</code>","text":"<p>Transform a numpy.ndarray like object to a builtin type like int, float or list</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>An object</p> required <p>Raises:     ValueError: Raise a ValueError when obj is not ndarray convertable Returns:     Any: The object converted to a builtin type like int, float or list</p> Source code in <code>template_nlp/utils.py</code> <pre><code>def ndarray_to_builtin_object(obj: Any) -&gt; Any:\n    '''Transform a numpy.ndarray like object to a builtin type like int, float or list\n\n    Args:\n        obj (Any): An object\n    Raises:\n        ValueError: Raise a ValueError when obj is not ndarray convertable\n    Returns:\n        Any: The object converted to a builtin type like int, float or list\n    '''\n    if is_ndarray_convertable(obj):\n        if np.issubdtype(obj.dtype, np.integer):\n            return obj.astype(int).tolist()\n        elif np.issubdtype(obj.dtype, np.number):\n            return obj.astype(float).tolist()\n        else:\n            return obj.tolist()\n    else:\n        raise ValueError(f\"{obj} is not ndarray convertable\")\n</code></pre>"},{"location":"reference/template_nlp/utils/#template_nlp.utils.read_csv","title":"<code>read_csv(file_path, sep=';', encoding='utf-8', dtype=str, **kwargs)</code>","text":"<p>Reads a .csv file and parses the first line.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the .csv file containing the data</p> required <p>Kwargs:     sep (str): Separator of the data file     encoding (str): Encoding of the data file     kwargs: Pandas' kwargs Raises:     FileNotFoundError: If the file_path object does not point to an existing file Returns:     pd.DataFrame: Data     str: First line of the .csv (None if not beginning with #) and with no line break</p> Source code in <code>template_nlp/utils.py</code> <pre><code>def read_csv(file_path: str, sep: str = ';', encoding: str = 'utf-8', dtype: type = str, **kwargs) -&gt; Tuple[pd.DataFrame, Union[str, None]]:\n    '''Reads a .csv file and parses the first line.\n\n    Args:\n        file_path (str): Path to the .csv file containing the data\n    Kwargs:\n        sep (str): Separator of the data file\n        encoding (str): Encoding of the data file\n        kwargs: Pandas' kwargs\n    Raises:\n        FileNotFoundError: If the file_path object does not point to an existing file\n    Returns:\n        pd.DataFrame: Data\n        str: First line of the .csv (None if not beginning with #) and with no line break\n    '''\n    if not os.path.isfile(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist\")\n\n    # We get the first line\n    with open(file_path, 'r', encoding=encoding) as f:\n        first_line = f.readline()\n    # We check if the first line contains metadata\n    has_metada = True if first_line.startswith('#') else False\n    # We load the dataset\n    if has_metada:\n        df = pd.read_csv(file_path, sep=sep, encoding=encoding, dtype=dtype, skiprows=1, **kwargs).fillna('')\n    else:\n        df = pd.read_csv(file_path, sep=sep, encoding=encoding, dtype=dtype, **kwargs).fillna('')\n\n    # If no metadata, return only the dataframe\n    if not has_metada:\n        return df, None\n    # Else process the first_line\n    else:\n        # Deletion of the line break\n        if first_line is not None and first_line.endswith('\\n'):\n            first_line = first_line[:-1]\n        # Deletion of the return carriage\n        if first_line is not None and first_line.endswith('\\r'):\n            first_line = first_line[:-1]\n        # Return\n        return df, first_line\n</code></pre>"},{"location":"reference/template_nlp/utils/#template_nlp.utils.to_csv","title":"<code>to_csv(df, file_path, first_line=None, sep=';', encoding='utf-8', **kwargs)</code>","text":"<p>Writes a .csv and manages the first line.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Data to write</p> required <code>file_path</code> <code>str</code> <p>Path to the file to create</p> required <p>Kwargs:     first_line (str): First line to write (without line break which is done in this function)     sep (str): Separator for the data file     encoding (str): Encoding of the data file     kwargs: pandas' kwargs</p> Source code in <code>template_nlp/utils.py</code> <pre><code>def to_csv(df: pd.DataFrame, file_path: str, first_line: Union[str, None] = None, sep: str = ';',\n           encoding: str = 'utf-8', **kwargs) -&gt; None:\n    '''Writes a .csv and manages the first line.\n\n    Args:\n        df (pd.DataFrame): Data to write\n        file_path (str): Path to the file to create\n    Kwargs:\n        first_line (str): First line to write (without line break which is done in this function)\n        sep (str): Separator for the data file\n        encoding (str): Encoding of the data file\n        kwargs: pandas' kwargs\n    '''\n    # We get the first line\n    with open(file_path, 'w', encoding=encoding) as f:\n        if first_line is not None:\n            f.write(first_line + '\\n')  # We add the first line if metadata are present\n        df.to_csv(f, sep=sep, encoding=encoding, index=None, **kwargs)\n</code></pre>"},{"location":"reference/template_nlp/utils/#template_nlp.utils.trained_needed","title":"<code>trained_needed(function)</code>","text":"<p>Decorator to ensure that a model has been trained.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>func</code> <p>Function to decorate</p> required <p>Returns:     function: The decorated function</p> Source code in <code>template_nlp/utils.py</code> <pre><code>def trained_needed(function: Callable) -&gt; Callable:\n    '''Decorator to ensure that a model has been trained.\n\n    Args:\n        function (func): Function to decorate\n    Returns:\n        function: The decorated function\n    '''\n    # Get wrapper\n    def wrapper(self, *args, **kwargs):\n        '''Wrapper'''\n        if not self.trained:\n            raise AttributeError(f\"The function {function.__name__} can't be called as long as the model hasn't been fitted\")\n        else:\n            return function(self, *args, **kwargs)\n    return wrapper\n</code></pre>"},{"location":"reference/template_nlp/models_training/","title":"Models training","text":""},{"location":"reference/template_nlp/models_training/model_aggregation/","title":"Model aggregation","text":""},{"location":"reference/template_nlp/models_training/model_aggregation/#template_nlp.models_training.model_aggregation.ModelAggregation","title":"<code>ModelAggregation</code>","text":"<p>             Bases: <code>ModelClass</code></p> <p>Model for aggregating several instances of ModelClass</p> Source code in <code>template_nlp/models_training/model_aggregation.py</code> <pre><code>class ModelAggregation(ModelClass):\n    '''Model for aggregating several instances of ModelClass'''\n\n    _default_name = 'model_aggregation'\n    _dict_aggregation_function = {'majority_vote': {'aggregation_function': majority_vote, 'using_proba': False, 'multi_label': False},\n                                  'proba_argmax': {'aggregation_function': proba_argmax, 'using_proba': True, 'multi_label': False},\n                                  'all_predictions': {'aggregation_function': all_predictions, 'using_proba': False, 'multi_label': True},\n                                  'vote_labels': {'aggregation_function': vote_labels, 'using_proba': False, 'multi_label': True}}\n\n    def __init__(self, list_models: Union[list, None] = None, aggregation_function: Union[Callable, str] = 'majority_vote',\n                 using_proba: bool = False, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelClass for more arguments)\n        This model will aggregate the predictions of several model. The user can choose an aggregation function (with **kwargs if not using a list_classes arg)\n        from existing ones, or create its own. All models must be either mono label or multi label, we do not accept mixes.\n        However, we accept models that do not have the same class / labels. We will consider a meta model with joined classes / labels.\n\n        Kwargs:\n            list_models (list) : The list of models to be aggregated (can be None if reloading from standalones)\n            aggregation_function (Callable or str) : The aggregation function used (custom function must use **kwargs if not using a list_classes arg)\n            using_proba (bool) : Which object is being aggregated (the probabilities or the predictions).\n        Raises:\n            ValueError: All the aggregated sub_models have not the same multi_label attributes\n            ValueError: The multi_label attributes of the aggregated models are inconsistent with multi_label\n        '''\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Set attributes\n        self.using_proba = using_proba\n        self.aggregation_function = aggregation_function\n\n        # Manage submodels\n        self.sub_models = list_models  # Transform the list into a list of dictionnaries [{'name': xxx, 'model': xxx}, ...]\n\n        # Check for multi-labels inconsistencies\n        set_multi_label = {sub_model['model'].multi_label for sub_model in self.sub_models}\n        if len(set_multi_label) &gt; 1:\n            raise ValueError(f\"All the aggregated sub_models do not have the same multi_label attribute\")\n        if len(set_multi_label.union({self.multi_label})) &gt; 1:\n            raise ValueError(f\"The multi_label attributes of the aggregated models are inconsistent with the provided multi label attribute ({self.multi_label}).\")\n\n        # Set trained &amp; classes info from submodels\n        self.trained, self.list_classes, self.dict_classes = self._check_trained()\n        # Set nb_fit to 1 if already trained\n        if self.trained:\n            self.nb_fit = 1\n\n    @property\n    def aggregation_function(self):\n        '''Getter for aggregation_function'''\n        return self._aggregation_function\n\n    @aggregation_function.setter\n    def aggregation_function(self, agg_function: Union[Callable, str]):\n        '''Setter for aggregation_function\n        If a string, try to match a predefined function\n\n        Raises:\n            ValueError: If the object aggregation_function is a str but not found in the dictionary of predefined aggregation functions\n            ValueError: If the object aggregation_function is incompatible with multi_label\n        '''\n        # Retrieve aggregation function from dict if a string\n        if isinstance(agg_function, str):\n            # Get infos\n            if agg_function not in self._dict_aggregation_function.keys():\n                raise ValueError(f\"The aggregation_function ({agg_function}) is not a valid option (must be chosen in {self._dict_aggregation_function.keys()})\")\n            using_proba = self._dict_aggregation_function[agg_function]['using_proba']\n            multi_label = self._dict_aggregation_function[agg_function]['multi_label']\n            agg_function = self._dict_aggregation_function[agg_function]['aggregation_function']  # type: ignore\n            # Apply checks\n            if self.using_proba != using_proba:\n                self.logger.warning(f\"using_proba {self.using_proba} is incompatible with the selected aggregation function '{agg_function}'. We force using_proba to {using_proba}.\")\n                self.using_proba = using_proba  # type: ignore\n            if self.multi_label != multi_label:\n                raise ValueError(f\"multi_label {self.multi_label} is incompatible with the selected aggregation function '{agg_function}'.\")\n        self._aggregation_function = agg_function\n\n    @aggregation_function.deleter\n    def aggregation_function(self):\n        '''Deleter for aggregation_function'''\n        self._aggregation_function = None\n\n    @property\n    def sub_models(self):\n        '''Getter for sub_models'''\n        return self._sub_models\n\n    @sub_models.setter\n    def sub_models(self, list_models: Union[list, None] = None):\n        '''Setter for sub_models\n\n        Kwargs:\n            list_models (list) : The list of models to be aggregated\n        '''\n        list_models = [] if list_models is None else list_models\n        sub_models = []  # Init list of models\n        for model in list_models:\n            # If a string (a model name), reload it\n            if isinstance(model, str):\n                real_model, _ = utils_models.load_model(model)\n                dict_model = {'name': model, 'model': real_model}\n            else:\n                dict_model = {'name': os.path.split(model.model_dir)[-1], 'model': model}\n            sub_models.append(dict_model.copy())\n        self._sub_models = sub_models.copy()\n\n    @sub_models.deleter\n    def sub_models(self):\n        '''Deleter for sub_models'''\n        self._sub_models = None\n\n    def _check_trained(self) -&gt; Tuple[bool, list, dict]:\n        '''Checks and sets various attributes related to the fitting of underlying models\n\n        Returns:\n            bool: is the aggregation model is considered fitted\n            list: list of classes\n            dict: dict of classes\n        '''\n        # Check fitted\n        models_trained = {sub_model['model'].trained for sub_model in self.sub_models}\n        if len(models_trained) &gt; 0 and all(models_trained):\n            # All models trained\n            trained = True\n            # Set list_classes\n            list_classes = list({label for sub_model in self.sub_models for label in sub_model['model'].list_classes})\n            list_classes.sort()\n            # Set dict_classes based on self.list_classes\n            dict_classes = {i: col for i, col in enumerate(list_classes)}\n        # No model or not fitted\n        else:\n            trained, list_classes, dict_classes = False, [], {}\n        return trained, list_classes, dict_classes\n\n    def fit(self, x_train, y_train, x_valid=None, y_valid=None, with_shuffle: bool = True, **kwargs) -&gt; None:\n        '''Fits the model\n\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n            y_train (?): Array-like, shape = [n_samples, n_targets]\n        Kwargs:\n            x_valid (?): Array-like, shape = [n_samples, n_features] - not used by sklearn models\n            y_valid (?): Array-like, shape = [n_samples, n_targets] - not used by sklearn models\n            with_shuffle (bool): If x, y must be shuffled before fitting - not used by sklearn models\n        '''\n        # Fit each model\n        for sub_model in self.sub_models:\n            model = sub_model['model']\n            if not model.trained:\n                model.fit(x_train, y_train, x_valid=x_valid, y_valid=y_valid, with_shuffle=True, **kwargs)\n\n        # Set nb_fit to 1 if not already trained\n        if not self.trained:\n            self.nb_fit = 1\n\n        # Update attributes\n        self.trained, self.list_classes, self.dict_classes = self._check_trained()\n\n    @utils.data_agnostic_str_to_list\n    @utils.trained_needed\n    def predict(self, x_test, return_proba: bool = False, alternative_version: bool = False, **kwargs) -&gt; np.ndarray:\n        '''Prediction\n\n        Args:\n            x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n            return_proba (bool): If the function should return the probabilities instead of the classes\n        Kwargs:\n            alternative_version (bool): If an alternative version (`tf.function` + `model.__call__`) must be used for Keras models. Should be faster with low nb of inputs.\n        Returns:\n            np.ndarray: array of shape = [n_samples]\n        '''\n        # We decide whether to rely on each model's probas or their predictions\n        if return_proba:\n            return self.predict_proba(x_test, alternative_version=alternative_version)\n        else:\n            # Get what we want (probas or preds) and use the aggregation function\n            if self.using_proba:\n                preds_or_probas = self._predict_probas_sub_models(x_test, alternative_version=alternative_version, **kwargs)\n            else:\n                preds_or_probas = self._predict_sub_models(x_test, alternative_version=alternative_version, **kwargs)\n            return np.array([self.aggregation_function(array, list_classes=self.list_classes) for array in preds_or_probas])  # type: ignore\n\n    @utils.data_agnostic_str_to_list\n    @utils.trained_needed\n    def predict_proba(self, x_test, alternative_version: bool = False, **kwargs) -&gt; np.ndarray:\n        '''Predicts the probabilities on the test set\n\n        Args:\n            x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n        Kwargs:\n            alternative_version (bool): If an alternative version (`tf.function` + `model.__call__`) must be used for Keras models. Should be faster with low nb of inputs.\n        Returns:\n            np.ndarray: array of shape = [n_samples, n_classes]\n        '''\n        probas_sub_models = self._predict_probas_sub_models(x_test, alternative_version=alternative_version, **kwargs)\n        # The probas of all models are averaged\n        return np.sum(probas_sub_models, axis=1) / probas_sub_models.shape[1]\n\n    @utils.trained_needed\n    def _predict_probas_sub_models(self, x_test, alternative_version: bool = False, **kwargs) -&gt; np.ndarray:\n        '''Recover the probabilities of each model being aggregated\n\n        Args:\n            x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n        Kwargs:\n            alternative_version (bool): If an alternative version (`tf.function` + `model.__call__`) must be used for Keras models. Should be faster with low nb of inputs.\n        Returns:\n            np.ndarray: array of shape = [n_samples, nb_model, nb_classes]\n        '''\n        array_probas = np.array([self._predict_full_list_classes(sub_model['model'], x_test, return_proba=True, alternative_version=alternative_version)\n                                 for sub_model in self.sub_models])\n        array_probas = np.transpose(array_probas, (1, 0, 2))\n        return array_probas\n\n    @utils.trained_needed\n    def _predict_sub_models(self, x_test, alternative_version: bool = False, **kwargs) -&gt; np.ndarray:\n        '''Recover the predictions of each model being aggregated\n\n        Args:\n            x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n        Kwargs:\n            alternative_version (bool): If an alternative version (`tf.function` + `model.__call__`) must be used for Keras models. Should be faster with low nb of inputs.\n        Returns:\n            np.ndarray: not multi_label : array of shape = [n_samples, nb_model]\n                        multi_label : array of shape = [n_samples, nb_model, n_classes]\n        '''\n        if self.multi_label:\n            array_predict = np.array([self._predict_full_list_classes(sub_model['model'], x_test, return_proba=False, alternative_version=alternative_version)\n                                      for sub_model in self.sub_models])\n            array_predict = np.transpose(array_predict, (1, 0, 2))\n        else:\n            array_predict = np.array([sub_model['model'].predict(x_test, alternative_version=alternative_version) for sub_model in self.sub_models])\n            array_predict = np.transpose(array_predict, (1, 0))\n        return array_predict\n\n    def _predict_full_list_classes(self, model: Type[ModelClass], x_test, return_proba: bool = False, alternative_version: bool = False, **kwargs) -&gt; np.ndarray:\n        '''For multi_label: adds missing columns in the prediction of model (class missing in their list_classes)\n        Or, if return_proba, adds a proba of zero to the missing classes in their list_classes\n\n        Args:\n            model (ModelClass): Model to use\n            x_test (?): Array-like or sparse matrix of shape = [n_samples, n_features]\n            return_proba (bool): If the function should return the probabilities instead of the classes\n        Kwargs:\n            alternative_version (bool): If an alternative version (`tf.function` + `model.__call__`) must be used for Keras models. Should be faster with low nb of inputs.\n        Returns:\n            np.ndarray: The array with the missing columns added\n        '''\n        # Get predictions or probas\n        preds_or_probas = model.predict(x_test, return_proba=return_proba, alternative_version=alternative_version)\n\n        # Manage each cases. Reorder predictions or probas according to aggregation model list_classes\n        # Multi label, proba = True\n        # Multi label, proba = False\n        # Mono label, proba = True\n        if model.multi_label or return_proba:\n            df_all = pd.DataFrame(np.zeros((len(preds_or_probas), len(self.list_classes))), columns=self.list_classes)  # type: ignore\n            df_model = pd.DataFrame(preds_or_probas, columns=model.list_classes)\n            for col in model.list_classes:\n                df_all[col] = df_model[col]\n            return df_all.to_numpy()\n        # Mono label, proba = False\n        else:\n            return preds_or_probas\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        if json_data is None:\n            json_data = {}\n        # Specific aggregation - save some wanted entries\n        train_keys = ['filename', 'filename_valid', 'preprocess_str']\n        default_json_data = {key: json_data.get(key, None) for key in train_keys}\n        default_json_data['aggregator_dir'] = self.model_dir\n        # Save each trained and unsaved model\n        for sub_model in self.sub_models:\n            path_config = os.path.join(sub_model['model'].model_dir, 'configurations.json')\n            if os.path.exists(path_config):\n                with open(path_config, 'r', encoding='utf-8') as f:\n                    configs = json.load(f)\n                    trained = configs.get('trained', False)\n                    if not trained:\n                        sub_model['model'].save(default_json_data)\n            else:\n                sub_model['model'].save(default_json_data)\n\n        # Add some specific information\n        json_data['list_models_name'] = [sub_model['name'] for sub_model in self.sub_models]\n        json_data['using_proba'] = self.using_proba\n\n        # Save aggregation_function if not None &amp; level_save &gt; LOW\n        if (self.aggregation_function is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n            # Manage paths\n            aggregation_function_path = os.path.join(self.model_dir, \"aggregation_function.pkl\")\n            # Save as pickle\n            with open(aggregation_function_path, 'wb') as f:\n                pickle.dump(self.aggregation_function, f)\n\n        # Save\n        models_list = [sub_model['name'] for sub_model in self.sub_models]\n        aggregation_function = self.aggregation_function\n        delattr(self, \"sub_models\")\n        delattr(self, \"aggregation_function\")\n        super().save(json_data=json_data)\n        setattr(self, \"aggregation_function\", aggregation_function)\n        setattr(self, \"sub_models\", models_list)  # Setter needs list of models, not sub_models itself\n\n        # Add message in model_upload_instructions.md\n        md_path = os.path.join(self.model_dir, f\"model_upload_instructions.md\")\n        line = \"/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\   The aggregation model is a special model, please ensure that all sub-models and the aggregation model are manually saved together in order to be able to load it .  /!\\\\/!\\\\/!\\\\/!\\\\/!\\\\ \\n\"\n        self._prepend_line(md_path, line)\n\n    @staticmethod\n    def _prepend_line(file_name: str, line: str) -&gt; None:\n        ''' Insert given string as a new line at the beginning of a file\n\n        Kwargs:\n            file_name (str): Path to file\n            line (str): line to insert\n        '''\n        with open(file_name, 'r+') as f:\n            lines = f.readlines()\n            lines.insert(0, line)\n            f.seek(0)\n            f.writelines(lines)\n\n    def _hook_post_load_model_pkl(self):\n        '''Manages a model specificities post load from a pickle file (i.e. not from standalone files)\n\n        Raises:\n            FileNotFoundError: If the aggregation_function file does not exist\n        '''\n        # Paths\n        aggregation_function_path = os.path.join(self.model_dir, \"aggregation_function.pkl\")\n        configs_path = os.path.join(self.model_dir, 'configurations.json')\n\n        # Manage errors\n        if not os.path.isfile(aggregation_function_path):\n            raise FileNotFoundError(f\"Can't find aggregation_function file ({aggregation_function_path})\")\n        if not os.path.isfile(configs_path):\n            raise FileNotFoundError(f\"Can't find configuration file ({configs_path})\")\n\n        # Reload aggregation function\n        with open(aggregation_function_path, 'rb') as f:\n            self.aggregation_function = pickle.load(f)\n\n        # Reload sub_models\n        configs = self.load_configs(config_path=configs_path)\n        self.sub_models = configs['list_models_name']\n\n    @classmethod\n    def _init_new_instance_from_configs(cls, configs):\n        '''Inits a new instance from a set of configurations\n\n        Args:\n            configs: a set of configurations of a model to be reloaded\n        Returns:\n            ModelClass: the newly generated class\n        '''\n        # Call parent\n        model = super()._init_new_instance_from_configs(configs)\n\n        # Add attributes\n        model.sub_models = configs.get('list_models_name', [])  # Transforms the list into a list of dictionnaries [{'name': xxx, 'model': xxx,}, ...]\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['using_proba']:\n            setattr(model, attribute, configs.get(attribute, getattr(model, attribute)))\n\n        # Return the new model\n        return model\n\n    def _load_standalone_files(self, default_model_dir: Union[str, None] = None,\n                               aggregation_function_path: Union[str, None] = None, *args, **kwargs):\n        '''Loads standalone files for a newly created model via _init_new_instance_from_configs\n\n        Kwargs:\n            default_model_dir (str): a path to look for default file paths\n                                     If None, standalone files path should all be provided\n            aggregation_function_path (str): Path to the aggregation function\n                                             If None, we'll use the default path if default_model_dir is not None\n        Raises:\n            ValueError: If at least one path is not specified and can't be inferred\n            FileNotFoundError: If the aggregation function path does not exist\n        '''\n        # Check if we are able to get all needed paths\n        if default_model_dir is None and aggregation_function_path is None:\n            raise ValueError(\"Aggregation function path is not specified and can't be inferred\")\n\n        # Retrieve file paths\n        if aggregation_function_path is None:\n            aggregation_function_path = os.path.join(default_model_dir, \"aggregation_function.pkl\")\n\n        # Check paths exists\n        if not os.path.isfile(aggregation_function_path):\n            raise FileNotFoundError(f\"Can't find aggregation function path ({aggregation_function_path})\")\n\n        # Reload aggregation function\n        with open(aggregation_function_path, 'rb') as f:\n            self.aggregation_function = pickle.load(f)\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_aggregation/#template_nlp.models_training.model_aggregation.ModelAggregation.aggregation_function","title":"<code>aggregation_function</code>  <code>deletable</code> <code>property</code> <code>writable</code>","text":"<p>Getter for aggregation_function</p>"},{"location":"reference/template_nlp/models_training/model_aggregation/#template_nlp.models_training.model_aggregation.ModelAggregation.sub_models","title":"<code>sub_models</code>  <code>deletable</code> <code>property</code> <code>writable</code>","text":"<p>Getter for sub_models</p>"},{"location":"reference/template_nlp/models_training/model_aggregation/#template_nlp.models_training.model_aggregation.ModelAggregation.__init__","title":"<code>__init__(list_models=None, aggregation_function='majority_vote', using_proba=False, **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass for more arguments) This model will aggregate the predictions of several model. The user can choose an aggregation function (with **kwargs if not using a list_classes arg) from existing ones, or create its own. All models must be either mono label or multi label, we do not accept mixes. However, we accept models that do not have the same class / labels. We will consider a meta model with joined classes / labels.</p> Kwargs <p>list_models (list) : The list of models to be aggregated (can be None if reloading from standalones) aggregation_function (Callable or str) : The aggregation function used (custom function must use **kwargs if not using a list_classes arg) using_proba (bool) : Which object is being aggregated (the probabilities or the predictions).</p> <p>Raises:     ValueError: All the aggregated sub_models have not the same multi_label attributes     ValueError: The multi_label attributes of the aggregated models are inconsistent with multi_label</p> Source code in <code>template_nlp/models_training/model_aggregation.py</code> <pre><code>def __init__(self, list_models: Union[list, None] = None, aggregation_function: Union[Callable, str] = 'majority_vote',\n             using_proba: bool = False, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelClass for more arguments)\n    This model will aggregate the predictions of several model. The user can choose an aggregation function (with **kwargs if not using a list_classes arg)\n    from existing ones, or create its own. All models must be either mono label or multi label, we do not accept mixes.\n    However, we accept models that do not have the same class / labels. We will consider a meta model with joined classes / labels.\n\n    Kwargs:\n        list_models (list) : The list of models to be aggregated (can be None if reloading from standalones)\n        aggregation_function (Callable or str) : The aggregation function used (custom function must use **kwargs if not using a list_classes arg)\n        using_proba (bool) : Which object is being aggregated (the probabilities or the predictions).\n    Raises:\n        ValueError: All the aggregated sub_models have not the same multi_label attributes\n        ValueError: The multi_label attributes of the aggregated models are inconsistent with multi_label\n    '''\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Set attributes\n    self.using_proba = using_proba\n    self.aggregation_function = aggregation_function\n\n    # Manage submodels\n    self.sub_models = list_models  # Transform the list into a list of dictionnaries [{'name': xxx, 'model': xxx}, ...]\n\n    # Check for multi-labels inconsistencies\n    set_multi_label = {sub_model['model'].multi_label for sub_model in self.sub_models}\n    if len(set_multi_label) &gt; 1:\n        raise ValueError(f\"All the aggregated sub_models do not have the same multi_label attribute\")\n    if len(set_multi_label.union({self.multi_label})) &gt; 1:\n        raise ValueError(f\"The multi_label attributes of the aggregated models are inconsistent with the provided multi label attribute ({self.multi_label}).\")\n\n    # Set trained &amp; classes info from submodels\n    self.trained, self.list_classes, self.dict_classes = self._check_trained()\n    # Set nb_fit to 1 if already trained\n    if self.trained:\n        self.nb_fit = 1\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_aggregation/#template_nlp.models_training.model_aggregation.ModelAggregation.fit","title":"<code>fit(x_train, y_train, x_valid=None, y_valid=None, with_shuffle=True, **kwargs)</code>","text":"<p>Fits the model</p> <p>Parameters:</p> Name Type Description Default <code>x_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <code>y_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_targets]</p> required <p>Kwargs:     x_valid (?): Array-like, shape = [n_samples, n_features] - not used by sklearn models     y_valid (?): Array-like, shape = [n_samples, n_targets] - not used by sklearn models     with_shuffle (bool): If x, y must be shuffled before fitting - not used by sklearn models</p> Source code in <code>template_nlp/models_training/model_aggregation.py</code> <pre><code>def fit(self, x_train, y_train, x_valid=None, y_valid=None, with_shuffle: bool = True, **kwargs) -&gt; None:\n    '''Fits the model\n\n    Args:\n        x_train (?): Array-like, shape = [n_samples, n_features]\n        y_train (?): Array-like, shape = [n_samples, n_targets]\n    Kwargs:\n        x_valid (?): Array-like, shape = [n_samples, n_features] - not used by sklearn models\n        y_valid (?): Array-like, shape = [n_samples, n_targets] - not used by sklearn models\n        with_shuffle (bool): If x, y must be shuffled before fitting - not used by sklearn models\n    '''\n    # Fit each model\n    for sub_model in self.sub_models:\n        model = sub_model['model']\n        if not model.trained:\n            model.fit(x_train, y_train, x_valid=x_valid, y_valid=y_valid, with_shuffle=True, **kwargs)\n\n    # Set nb_fit to 1 if not already trained\n    if not self.trained:\n        self.nb_fit = 1\n\n    # Update attributes\n    self.trained, self.list_classes, self.dict_classes = self._check_trained()\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_aggregation/#template_nlp.models_training.model_aggregation.ModelAggregation.predict","title":"<code>predict(x_test, return_proba=False, alternative_version=False, **kwargs)</code>","text":"<p>Prediction</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>array-like or sparse matrix of shape = [n_samples, n_features]</p> required <code>return_proba</code> <code>bool</code> <p>If the function should return the probabilities instead of the classes</p> <code>False</code> <p>Kwargs:     alternative_version (bool): If an alternative version (<code>tf.function</code> + <code>model.__call__</code>) must be used for Keras models. Should be faster with low nb of inputs. Returns:     np.ndarray: array of shape = [n_samples]</p> Source code in <code>template_nlp/models_training/model_aggregation.py</code> <pre><code>@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict(self, x_test, return_proba: bool = False, alternative_version: bool = False, **kwargs) -&gt; np.ndarray:\n    '''Prediction\n\n    Args:\n        x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n        return_proba (bool): If the function should return the probabilities instead of the classes\n    Kwargs:\n        alternative_version (bool): If an alternative version (`tf.function` + `model.__call__`) must be used for Keras models. Should be faster with low nb of inputs.\n    Returns:\n        np.ndarray: array of shape = [n_samples]\n    '''\n    # We decide whether to rely on each model's probas or their predictions\n    if return_proba:\n        return self.predict_proba(x_test, alternative_version=alternative_version)\n    else:\n        # Get what we want (probas or preds) and use the aggregation function\n        if self.using_proba:\n            preds_or_probas = self._predict_probas_sub_models(x_test, alternative_version=alternative_version, **kwargs)\n        else:\n            preds_or_probas = self._predict_sub_models(x_test, alternative_version=alternative_version, **kwargs)\n        return np.array([self.aggregation_function(array, list_classes=self.list_classes) for array in preds_or_probas])  # type: ignore\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_aggregation/#template_nlp.models_training.model_aggregation.ModelAggregation.predict_proba","title":"<code>predict_proba(x_test, alternative_version=False, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>array-like or sparse matrix of shape = [n_samples, n_features]</p> required <p>Kwargs:     alternative_version (bool): If an alternative version (<code>tf.function</code> + <code>model.__call__</code>) must be used for Keras models. Should be faster with low nb of inputs. Returns:     np.ndarray: array of shape = [n_samples, n_classes]</p> Source code in <code>template_nlp/models_training/model_aggregation.py</code> <pre><code>@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict_proba(self, x_test, alternative_version: bool = False, **kwargs) -&gt; np.ndarray:\n    '''Predicts the probabilities on the test set\n\n    Args:\n        x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n    Kwargs:\n        alternative_version (bool): If an alternative version (`tf.function` + `model.__call__`) must be used for Keras models. Should be faster with low nb of inputs.\n    Returns:\n        np.ndarray: array of shape = [n_samples, n_classes]\n    '''\n    probas_sub_models = self._predict_probas_sub_models(x_test, alternative_version=alternative_version, **kwargs)\n    # The probas of all models are averaged\n    return np.sum(probas_sub_models, axis=1) / probas_sub_models.shape[1]\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_aggregation/#template_nlp.models_training.model_aggregation.ModelAggregation.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_nlp/models_training/model_aggregation.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    if json_data is None:\n        json_data = {}\n    # Specific aggregation - save some wanted entries\n    train_keys = ['filename', 'filename_valid', 'preprocess_str']\n    default_json_data = {key: json_data.get(key, None) for key in train_keys}\n    default_json_data['aggregator_dir'] = self.model_dir\n    # Save each trained and unsaved model\n    for sub_model in self.sub_models:\n        path_config = os.path.join(sub_model['model'].model_dir, 'configurations.json')\n        if os.path.exists(path_config):\n            with open(path_config, 'r', encoding='utf-8') as f:\n                configs = json.load(f)\n                trained = configs.get('trained', False)\n                if not trained:\n                    sub_model['model'].save(default_json_data)\n        else:\n            sub_model['model'].save(default_json_data)\n\n    # Add some specific information\n    json_data['list_models_name'] = [sub_model['name'] for sub_model in self.sub_models]\n    json_data['using_proba'] = self.using_proba\n\n    # Save aggregation_function if not None &amp; level_save &gt; LOW\n    if (self.aggregation_function is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n        # Manage paths\n        aggregation_function_path = os.path.join(self.model_dir, \"aggregation_function.pkl\")\n        # Save as pickle\n        with open(aggregation_function_path, 'wb') as f:\n            pickle.dump(self.aggregation_function, f)\n\n    # Save\n    models_list = [sub_model['name'] for sub_model in self.sub_models]\n    aggregation_function = self.aggregation_function\n    delattr(self, \"sub_models\")\n    delattr(self, \"aggregation_function\")\n    super().save(json_data=json_data)\n    setattr(self, \"aggregation_function\", aggregation_function)\n    setattr(self, \"sub_models\", models_list)  # Setter needs list of models, not sub_models itself\n\n    # Add message in model_upload_instructions.md\n    md_path = os.path.join(self.model_dir, f\"model_upload_instructions.md\")\n    line = \"/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\   The aggregation model is a special model, please ensure that all sub-models and the aggregation model are manually saved together in order to be able to load it .  /!\\\\/!\\\\/!\\\\/!\\\\/!\\\\ \\n\"\n    self._prepend_line(md_path, line)\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_aggregation/#template_nlp.models_training.model_aggregation.all_predictions","title":"<code>all_predictions(predictions, **kwargs)</code>","text":"<p>Calculates the sum of the arrays along axis 0 casts it to bool and then to int. Expects a numpy array containing only zeroes and ones. When used as an aggregation function, keeps all the prediction of each model (multi-labels)</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>np.ndarray) </code> <p>Array of shape : (n_models, n_classes)</p> required <p>Return:     np.ndarray: The prediction</p> Source code in <code>template_nlp/models_training/model_aggregation.py</code> <pre><code>def all_predictions(predictions: np.ndarray, **kwargs) -&gt; np.ndarray:\n    '''Calculates the sum of the arrays along axis 0 casts it to bool and then to int.\n    Expects a numpy array containing only zeroes and ones.\n    When used as an aggregation function, keeps all the prediction of each model (multi-labels)\n\n    Args:\n        predictions (np.ndarray) : Array of shape : (n_models, n_classes)\n    Return:\n        np.ndarray: The prediction\n    '''\n    return np.sum(predictions, axis=0, dtype=bool).astype(int)\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_aggregation/#template_nlp.models_training.model_aggregation.majority_vote","title":"<code>majority_vote(predictions, **kwargs)</code>","text":"<p>Gives the class corresponding to the most present prediction in the given predictions. In case of a tie, gives the prediction of the first model involved in the tie Args:     predictions (np.ndarray): The array containing the predictions of each model (shape (n_models)) Returns:     The prediction</p> Source code in <code>template_nlp/models_training/model_aggregation.py</code> <pre><code>def majority_vote(predictions: np.ndarray, **kwargs):\n    '''Gives the class corresponding to the most present prediction in the given predictions.\n    In case of a tie, gives the prediction of the first model involved in the tie\n    Args:\n        predictions (np.ndarray): The array containing the predictions of each model (shape (n_models))\n    Returns:\n        The prediction\n    '''\n    labels, counts = np.unique(predictions, return_counts=True)\n    votes = [(label, count) for label, count in zip(labels, counts)]\n    votes = sorted(votes, key=lambda x: x[1], reverse=True)\n    possible_classes = {vote[0] for vote in votes if vote[1]==votes[0][1]}\n    return [prediction for prediction in predictions if prediction in possible_classes][0]\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_aggregation/#template_nlp.models_training.model_aggregation.proba_argmax","title":"<code>proba_argmax(proba, list_classes, **kwargs)</code>","text":"<p>Gives the class corresponding to the argmax of the average of the given probabilities</p> <p>Parameters:</p> Name Type Description Default <code>proba</code> <code>ndarray</code> <p>The probabilities of each model for each class, array of shape (nb_models, nb_classes)</p> required <code>list_classes</code> <code>list</code> <p>List of classes</p> required <p>Returns:     The prediction</p> Source code in <code>template_nlp/models_training/model_aggregation.py</code> <pre><code>def proba_argmax(proba: np.ndarray, list_classes: list, **kwargs):\n    '''Gives the class corresponding to the argmax of the average of the given probabilities\n\n    Args:\n        proba (np.ndarray): The probabilities of each model for each class, array of shape (nb_models, nb_classes)\n        list_classes (list): List of classes\n    Returns:\n        The prediction\n    '''\n    proba_average = np.sum(proba, axis=0) / proba.shape[0]\n    index_class = np.argmax(proba_average)\n    return list_classes[index_class]\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_aggregation/#template_nlp.models_training.model_aggregation.vote_labels","title":"<code>vote_labels(predictions, **kwargs)</code>","text":"<p>Gives the result of majority_vote applied on the second axis. When used as an aggregation_function, for each class, performs a majority vote for the aggregated models. It gives a multi-labels result</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>ndarray</code> <p>array of shape : (n_models, n_classes)</p> required <p>Return:     np.ndarray: prediction</p> Source code in <code>template_nlp/models_training/model_aggregation.py</code> <pre><code>def vote_labels(predictions: np.ndarray, **kwargs) -&gt; np.ndarray:\n    '''Gives the result of majority_vote applied on the second axis.\n    When used as an aggregation_function, for each class, performs a majority vote for the aggregated models.\n    It gives a multi-labels result\n\n    Args:\n        predictions (np.ndarray): array of shape : (n_models, n_classes)\n    Return:\n        np.ndarray: prediction\n    '''\n    return np.apply_along_axis(majority_vote, 0, predictions)\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/","title":"Model class","text":""},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass","title":"<code>ModelClass</code>","text":"<p>Parent class for the models</p> Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>class ModelClass:\n    '''Parent class for the models'''\n\n    _default_name = 'none'\n\n    # Not implemented :\n    # -&gt; fit (fits a model)\n    # -&gt; predict (predict on new content)\n    # -&gt; predict_proba (predict on new content - returns probas)\n    # -&gt; _load_standalone_files (loads standalone files - for a newly created model)\n\n    # Probably need to be overridden, depending on your model :\n    # -&gt; save (specific save instructions)\n    # -&gt; _init_new_instance_from_configs (loads model attributes - for a newly created model)\n    # -&gt; _hook_post_load_model_pkl (post pkl load function, e.g. load weights from an HDF5 file)\n    # -&gt; _is_gpu_activated (specific instruction to know if a gpu is used)\n\n    def __init__(self, model_dir: Union[str, None] = None, model_name: Union[str, None] = None, x_col: Union[str, int, None] = None,\n                 y_col: Union[str, int, list, None] = None, random_seed: Union[int, None] = None,\n                 level_save: str = 'HIGH', multi_label: bool = False, **kwargs) -&gt; None:\n        '''Initialization of the parent class.\n\n        Kwargs:\n            model_dir (str): Folder where to save the model\n                If None, creates a directory based on the model's name and the date (most common usage)\n            model_name (str): The name of the model\n            x_col (str | int): Name of the columns used for the training - x\n            y_col (str | int | list if multi-labels): Name of the model's target column(s) - y\n            random_seed (int): Seed to use for packages randomness\n            level_save (str): Level of saving\n                LOW: stats + configurations + logger keras - /!\\\\ The model can't be reused /!\\\\ -\n                MEDIUM: LOW + hdf5 + pkl + plots\n                HIGH: MEDIUM + predictions\n            multi_label (bool): If the classification is multi-labels\n        Raises:\n            ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\n            NotADirectoryError: If a provided model directory is not a directory (i.e. it's a file)\n        '''\n        if level_save not in ['LOW', 'MEDIUM', 'HIGH']:\n            raise ValueError(f\"The object level_save ({level_save}) is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\")\n\n        # Get logger\n        self.logger = logging.getLogger(__name__)\n\n        # Model name\n        self.model_name = self._default_name if model_name is None else model_name\n\n        # Names of the columns used\n        self.x_col = x_col\n        self.y_col = y_col\n\n        # Random seed\n        self.random_seed = random_seed\n\n        # Model folder\n        if model_dir is None:\n            self.model_dir = self._get_new_model_dir()\n        else:\n            if not os.path.exists(model_dir):\n                os.makedirs(model_dir)\n            if not os.path.isdir(model_dir):\n                raise NotADirectoryError(f\"{model_dir} is not a valid directory\")\n            self.model_dir = os.path.abspath(model_dir)\n\n        # List of classes to consider (set on fit)\n        self.list_classes: Optional[List[Any]] = None\n        self.dict_classes: Optional[Dict[Any, Any]] = None\n\n        # Multi-labels ?\n        self.multi_label: bool = multi_label\n\n        # Other options\n        self.level_save = level_save\n\n        # is trained ?\n        self.trained = False\n        self.nb_fit = 0\n\n        # Configuration dict. to be logged. Set on save.\n        self.json_dict: Dict[Any, Any] = {}\n\n    def fit(self, x_train, y_train, **kwargs) -&gt; None:\n        '''Trains the model\n\n        Args:\n            x_train (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n            y_train (?): Array-like, shape = [n_samples, n_features]\n        '''\n        raise NotImplementedError(\"'fit' needs to be overridden\")\n\n    @utils.data_agnostic_str_to_list\n    def predict(self, x_test, **kwargs) -&gt; np.ndarray:\n        '''Predictions on the test set\n\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n        raise NotImplementedError(\"'predict' needs to be overridden\")\n\n    @utils.data_agnostic_str_to_list\n    def predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n        '''Predicts probabilities on the test dataset\n\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n        raise NotImplementedError(\"'predict_proba' needs to be overridden\")\n\n    @utils.trained_needed\n    def predict_with_proba(self, x_test, **kwargs) -&gt; Tuple[np.ndarray, np.ndarray]:\n        '''Predicts on the test set with probabilities\n\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n        Returns:\n            predicted_class (np.ndarray): The predicted classes, shape = [n_samples, n_classes]\n            predicted_proba (np.ndarray): The predicted probabilities for each class, shape = [n_samples, n_classes]\n        '''\n        # Process\n        predicted_proba = self.predict(x_test, return_proba=True)\n        predicted_class = self.get_classes_from_proba(predicted_proba)\n        return predicted_class, predicted_proba\n\n    @utils.trained_needed\n    def get_predict_position(self, x_test, y_true) -&gt; np.ndarray:\n        '''Gets the order of predictions of y_true.\n        Positions start at 1 (not 0)\n\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n            y_true (?): Array-like, shape = [n_samples, n_features]\n        Raises:\n            ValueError: Not available in multi-labels case\n        Returns:\n            np.ndarray: Array, shape = [n_samples]\n        '''\n        if self.multi_label:\n            raise ValueError(\"The method 'get_predict_position'is unavailable in the multi-labels case\")\n        # Process\n        # Cast en pd.Series\n        y_true = pd.Series(y_true)\n        # Get predicted proba\n        predicted_proba = self.predict(x_test, return_proba=True)\n        # Get position\n        order = predicted_proba.argsort()\n        ranks = len(self.list_classes) - order.argsort()  # type: ignore\n        df_probas = pd.DataFrame(ranks, columns=self.list_classes)  # type: ignore\n        predict_positions = np.array([df_probas.loc[i, cl] if cl in df_probas.columns else -1 for i, cl in enumerate(y_true)])\n        return predict_positions\n\n    def get_classes_from_proba(self, predicted_proba: np.ndarray) -&gt; np.ndarray:\n        '''Gets the classes from probabilities\n\n        Args:\n            predicted_proba (np.ndarray): The probabilities predicted by the model, shape = [n_samples, n_classes]\n        Returns:\n            predicted_class (np.ndarray): Shape = [n_samples, n_classes] if multi-labels, shape = [n_samples] otherwise\n        '''\n        if not self.multi_label:\n            predicted_class = np.vectorize(lambda x: self.dict_classes[x])(predicted_proba.argmax(axis=-1))\n        else:\n            # If multi-labels, returns a list of 0 and 1\n            predicted_class = np.rint(predicted_proba)  # 1 if x &gt; 0.5 else 0\n        return predicted_class\n\n    def get_top_n_from_proba(self, predicted_proba: np.ndarray, n: int = 5) -&gt; Tuple[list, list]:\n        '''Gets the Top n predictions from probabilities\n\n        Args:\n            predicted_proba (np.ndarray): The probabilities predicted by the model, shape = [n_samples, n_classes]\n        kwargs:\n            n (int): Number of classes to return\n        Raises:\n            ValueError: If the number of classes to return is greater than the number of classes of the model\n        Returns:\n            list: top n predictions\n            list: top n probabilities\n        '''\n        # TODO: Make this method available with multi-labels tasks\n        if self.multi_label:\n            raise ValueError(\"The method 'get_top_n_from_proba' is unavailable with multi-labels tasks\")\n        if self.list_classes is not None and n &gt; len(self.list_classes):  # type: ignore\n            raise ValueError(\"The number of classes to return is greater than the number of classes of the model\")\n        # Process\n        idx = predicted_proba.argsort()[:, -n:][:, ::-1]\n        top_n_proba = list(np.take_along_axis(predicted_proba, idx, axis=1))\n        top_n = list(np.vectorize(lambda x: self.dict_classes[x])(idx))  # type: ignore\n        return top_n, top_n_proba\n\n    def inverse_transform(self, y: Union[list, np.ndarray]) -&gt; Union[list, tuple]:\n        '''Gets a list of classes from the predictions\n\n        Args:\n            y (?): Array-like, shape = [n_samples, n_classes], arrays of 0s and 1s\n                   OR 1D array shape = [n_classes] (only one prediction)\n        Raises:\n            ValueError: If the size of y does not correspond to the number of classes of the model\n        Returns:\n            List of tuple if multi-labels and several predictions\n            Tuple if multi-labels and one prediction\n            List of classes if mono-label\n        '''\n        # If multi-label, get classes in tuple\n        if self.multi_label:\n            # Cast to np array\n            if not isinstance(y, np.ndarray):\n                y = np.array(y)\n            if y.shape[-1] != len(self.list_classes):  # We consider \"-1\" in order to take care of the case where y is 1D\n                raise ValueError(f\"The size of y ({y.shape[-1]}) does not correspond\"\n                                 f\" to the number of classes of the model : ({len(self.list_classes)})\")\n            # Manage 1D array (only one pred)\n            if len(y.shape) == 1:\n                # TODO : shoudln't we return a list here ?\n                return tuple(np.array(self.list_classes).compress(y))\n            # Several preds\n            else:\n                return [tuple(np.array(self.list_classes).compress(indicators)) for indicators in y]\n        # If mono-label, just cast in list if y is np array\n        else:\n            return list(y) if isinstance(y, np.ndarray) else y\n\n    def get_and_save_metrics(self, y_true, y_pred, x=None, series_to_add: Union[List[pd.Series], None] = None,\n                             type_data: str = '') -&gt; pd.DataFrame:\n        '''Gets and saves the metrics of a model\n\n        Args:\n            y_true (?): Array-like, shape = [n_samples, n_features]\n            y_pred (?): Array-like, shape = [n_samples, n_features]\n        Kwargs:\n            x (?): Input data - Array-like, shape = [n_samples]\n            series_to_add (list&lt;pd.Series&gt;): List of pd.Series to add to the dataframe\n            type_data (str): Type of dataset (validation, test, ...)\n        Returns:\n            pd.DataFrame: The dataframe containing the statistics\n        '''\n\n        # Cast to np.array\n        y_true = np.array(y_true)\n        y_pred = np.array(y_pred)\n\n        # Check shapes\n        if not self.multi_label:\n            if len(y_true.shape) == 2 and y_true.shape[1] == 1:\n                y_true = np.ravel(y_true)\n            if len(y_pred.shape) == 2 and y_pred.shape[1] == 1:\n                y_pred = np.ravel(y_pred)\n\n        # Save a predictionn file if wanted\n        if self.level_save == 'HIGH':\n            # Inverse transform\n            y_true_df = list(self.inverse_transform(y_true))\n            y_pred_df = list(self.inverse_transform(y_pred))\n\n            # Concat in a dataframe\n            if x is not None:\n                df = pd.DataFrame({'x': list(x), 'y_true': y_true_df, 'y_pred': y_pred_df})\n            else:\n                df = pd.DataFrame({'y_true': y_true_df, 'y_pred': y_pred_df})\n            # Add a matched column\n            df.loc[:, 'matched'] = df[['y_true', 'y_pred']].apply(lambda x: 1 if x.y_true == x.y_pred else 0, axis=1)\n            # Add some more columns\n            if series_to_add is not None:\n                for ser in series_to_add:\n                    df[ser.name] = ser.reset_index(drop=True).reindex(index=df.index)  # Reindex\n\n            # Save predictions\n            file_path = os.path.join(self.model_dir, f\"predictions{'_' + type_data if len(type_data) &gt; 0 else ''}.csv\")\n            df.sort_values('matched', ascending=True).to_csv(file_path, sep=';', index=None, encoding='utf-8')\n\n        # Gets global f1 score / acc_tot / trues / falses / precision / recall / support\n        if self.multi_label:\n            f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n            trues = np.sum(np.all(np.equal(y_true, y_pred), axis=1))\n            falses = len(y_true) - trues\n            acc_tot = trues / len(y_true)\n            precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n            recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n            support = list(pd.DataFrame(y_true).sum().values)\n            support = [_ / sum(support) for _ in support] + [1.0]\n        else:\n            # We use 'weighted' even in the mono-label case since there can be several classes !\n            f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n            trues = np.sum(y_true == y_pred)\n            falses = np.sum(y_true != y_pred)\n            acc_tot = accuracy_score(y_true, y_pred)\n            precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n            recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n            labels_tmp, counts_tmp = np.unique(y_true, return_counts=True)\n            support = [0.0] * len(self.list_classes) + [1.0]  # type: ignore\n            for i, cl in enumerate(self.list_classes):  # type: ignore\n                if cl in labels_tmp:\n                    idx_tmp = list(labels_tmp).index(cl)\n                    support[i] = counts_tmp[idx_tmp] / y_pred.shape[0]\n\n        # Global Statistics\n        self.logger.info('-- * * * * * * * * * * * * * * --')\n        self.logger.info(f\"Statistics f1-score{' ' + type_data if len(type_data) &gt; 0 else ''}\")\n        self.logger.info('--------------------------------')\n        self.logger.info(f\"Total accuracy : {round(acc_tot * 100, 2)}% \\t Trues: {trues} \\t Falses: {falses}\")\n        self.logger.info(f\"F1-score (weighted) : {round(f1_weighted, 5)}\")\n        self.logger.info(f\"Precision (weighted) : {round(precision_weighted, 5)}\")\n        self.logger.info(f\"Recall (weighted) : {round(recall_weighted, 5)}\")\n        self.logger.info('--------------------------------')\n\n        # Metrics file\n        dict_df_stats = {}\n\n        # Add metrics depending on mono/multi labels &amp; manage confusion matrices\n        labels = self.list_classes\n        log_stats = len(labels) &lt; 50  # type: ignore\n\n        if self.multi_label:\n            # Details per category\n            mcm = multilabel_confusion_matrix(y_true, y_pred)\n            for i, label in enumerate(labels):  # type: ignore\n                c_mat = mcm[i]\n                dict_df_stats[i] = self._update_info_from_c_mat(c_mat, label, log_info=log_stats)\n                # Plot individual confusion matrix if level_save &gt; LOW\n                if self.level_save in ['MEDIUM', 'HIGH']:\n                    none_class = 'not_' + label\n                    tmp_label = re.sub(r',|:|\\s', '_', label)\n                    self._plot_confusion_matrix(c_mat, [none_class, label], type_data=f\"{tmp_label}_{type_data}\",\n                                                normalized=False, subdir=type_data)\n                    self._plot_confusion_matrix(c_mat, [none_class, label], type_data=f\"{tmp_label}_{type_data}\",\n                                                normalized=True, subdir=type_data)\n        else:\n            # Plot confusion matrices if level_save &gt; LOW\n            if self.level_save in ['MEDIUM', 'HIGH']:\n                if len(labels) &gt; 50:\n                    self.logger.warning(\n                        f\"Warning, there are {len(labels)} categories to plot in the confusion matrix.\\n\"\n                        \"Heavy chances of slowness/display bugs/crashes...\\n\"\n                        \"SKIP the plots\"\n                    )\n                else:\n                    # Global stats\n                    c_mat = confusion_matrix(y_true, y_pred, labels=labels)\n                    self._plot_confusion_matrix(c_mat, labels, type_data=type_data, normalized=False)  # type: ignore\n                    self._plot_confusion_matrix(c_mat, labels, type_data=type_data, normalized=True)  # type: ignore\n\n            # Get stats per class\n            for i, label in enumerate(labels):  # type: ignore\n                label_str = str(label)  # Fix : If label is an int, can cause some problems (e.g. only zeroes in the confusion matrix)\n                none_class = 'None' if label_str != 'None' else 'others'  # Check that the class is not already 'None'\n                y_true_tmp = [label_str if _ == label else none_class for _ in y_true]\n                y_pred_tmp = [label_str if _ == label else none_class for _ in y_pred]\n                c_mat_tmp = confusion_matrix(y_true_tmp, y_pred_tmp, labels=[none_class, label_str])\n                dict_df_stats[i] = self._update_info_from_c_mat(c_mat_tmp, label, log_info=False)\n\n\n        # Add global statistics\n        dict_df_stats[i+1] = {\n            'Label': 'All',\n            'F1-Score': f1_weighted,\n            'Accuracy': acc_tot,\n            'Precision': precision_weighted,\n            'Recall': recall_weighted,\n            'Trues': trues,\n            'Falses': falses,\n            'True positive': None,\n            'True negative': None,\n            'False positive': None,\n            'False negative': None,\n            'Condition positive': None,\n            'Condition negative': None,\n            'Predicted positive': None,\n            'Predicted negative': None,\n        }\n\n        df_stats = pd.DataFrame.from_dict(dict_df_stats, orient='index')\n\n        # Add support\n        df_stats['Support'] = support\n\n        # Save .csv\n        file_path = os.path.join(self.model_dir, f\"f1{'_' + type_data if len(type_data) &gt; 0 else ''}@{f1_weighted}.csv\")\n        df_stats.to_csv(file_path, sep=';', index=False, encoding='utf-8')\n\n        # Save accuracy\n        acc_path = os.path.join(self.model_dir, f\"acc{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(acc_tot, 5)}\")\n        with open(acc_path, 'w'):\n            pass\n\n        return df_stats\n\n    def get_metrics_simple_monolabel(self, y_true, y_pred) -&gt; pd.DataFrame:\n        '''Gets metrics on mono-label predictions\n        Same as the method get_and_save_metrics but without all the fluff (save, etc.)\n\n        Args:\n            y_true (?): Array-like, shape = [n_samples, n_features]\n            y_pred (?): Array-like, shape = [n_samples, n_features]\n        Raises:\n            ValueError: If not in mono-label mode\n        Returns:\n            pd.DataFrame: The dataframe containing statistics\n        '''\n        if self.multi_label:\n            raise ValueError(\"The method get_metrics_simple_monolabel only works for the mono-label case\")\n\n        # Cast to np.array\n        y_true = np.array(y_true)\n        y_pred = np.array(y_pred)\n\n        # Check shapes\n        if len(y_true.shape) == 2 and y_true.shape[1] == 1:\n            y_true = np.ravel(y_true)\n        if len(y_pred.shape) == 2 and y_pred.shape[1] == 1:\n            y_pred = np.ravel(y_pred)\n\n        # Gets global f1 score / acc_tot / trues / falses / precision / recall / support\n        f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n        trues = np.sum(y_true == y_pred)\n        falses = np.sum(y_true != y_pred)\n        acc_tot = accuracy_score(y_true, y_pred)\n        precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n        recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n        labels_tmp, counts_tmp = np.unique(y_true, return_counts=True)\n        support = [0.] * len(self.list_classes) + [1.0]\n        for i, cl in enumerate(self.list_classes):\n            if cl in labels_tmp:\n                idx_tmp = list(labels_tmp).index(cl)\n                support[i] = counts_tmp[idx_tmp] / y_pred.shape[0]\n\n        # DataFrame metrics\n        dict_df_stats = {}\n\n        # Get stats per class\n        labels = self.list_classes\n        for i, label in enumerate(labels):\n            label_str = str(label)  # Fix : If label is an int, can cause some problems (e.g. only zeroes in the confusion matrix)\n            none_class = 'None' if label_str != 'None' else 'others'  # Check that the class is not already 'None'\n            y_true_tmp = [label_str if _ == label else none_class for _ in y_true]\n            y_pred_tmp = [label_str if _ == label else none_class for _ in y_pred]\n            c_mat_tmp = confusion_matrix(y_true_tmp, y_pred_tmp, labels=[none_class, label_str])\n            dict_df_stats[i] = self._update_info_from_c_mat(c_mat_tmp, label, log_info=False)\n\n\n        # Add global statistics\n        dict_df_stats[i+1] = {\n            'Label': 'All',\n            'F1-Score': f1_weighted,\n            'Accuracy': acc_tot,\n            'Precision': precision_weighted,\n            'Recall': recall_weighted,\n            'Trues': trues,\n            'Falses': falses,\n            'True positive': None,\n            'True negative': None,\n            'False positive': None,\n            'False negative': None,\n            'Condition positive': None,\n            'Condition negative': None,\n            'Predicted positive': None,\n            'Predicted negative': None,\n        }\n        df_stats = pd.DataFrame.from_dict(dict_df_stats, orient='index')\n\n        # Add support\n        df_stats['Support'] = support\n\n        # Return dataframe\n        return df_stats\n\n    def get_metrics_simple_multilabel(self, y_true, y_pred) -&gt; pd.DataFrame:\n        '''Gets metrics on multi-label predictions\n        Same as the method get_and_save_metrics but without all the fluff (save, etc.)\n\n        Args:\n            y_true (?): Array-like, shape = [n_samples, n_features]\n            y_pred (?): Array-like, shape = [n_samples, n_features]\n        Raises:\n            ValueError: If not with multi-labels tasks\n        Returns:\n            pd.DataFrame: The dataframe containing statistics\n        '''\n        if not self.multi_label:\n            raise ValueError(\"The method get_metrics_simple_multilabel only works for multi-labels cases\")\n\n        # Cast to np.array\n        y_true = np.array(y_true)\n        y_pred = np.array(y_pred)\n\n        # Gets global f1 score / acc_tot / trues / falses / precision / recall / support\n        f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n        trues = np.sum(np.all(np.equal(y_true, y_pred), axis=1))\n        falses = len(y_true) - trues\n        acc_tot = trues / len(y_true)\n        precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n        recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n        support = list(pd.DataFrame(y_true).sum().values)\n        support = [_ / sum(support) for _ in support] + [1.0]\n\n        # DataFrame metrics\n        dict_df_stats = {}\n\n        # Add metrics\n        labels = self.list_classes\n        # Details per category\n        mcm = multilabel_confusion_matrix(y_true, y_pred)\n        for i, label in enumerate(labels):\n            c_mat = mcm[i]\n            dict_df_stats[i] = self._update_info_from_c_mat(c_mat, label, log_info=False)\n\n\n        # Add global statistics\n        dict_df_stats[i+1] = {\n            'Label': 'All',\n            'F1-Score': f1_weighted,\n            'Accuracy': acc_tot,\n            'Precision': precision_weighted,\n            'Recall': recall_weighted,\n            'Trues': trues,\n            'Falses': falses,\n            'True positive': None,\n            'True negative': None,\n            'False positive': None,\n            'False negative': None,\n            'Condition positive': None,\n            'Condition negative': None,\n            'Predicted positive': None,\n            'Predicted negative': None,\n        }\n        df_stats = pd.DataFrame.from_dict(dict_df_stats, orient='index')\n\n        # Add support\n        df_stats['Support'] = support\n\n        # Return dataframe\n        return df_stats\n\n    def _update_info_from_c_mat(self, c_mat: np.ndarray, label: str, log_info: bool = True) -&gt; dict:\n        '''Updates a dataframe for the method get_and_save_metrics, given a confusion matrix\n\n        Args:\n            c_mat (np.ndarray): Confusion matrix\n            label (str): Label to use\n        Kwargs:\n            log_info (bool): If the statistics must be logged\n        Returns:\n            dict: Dictionary with the information for the update of the dataframe\n        '''\n        # Extract all needed info from c_mat\n        true_negative = c_mat[0][0]\n        true_positive = c_mat[1][1]\n        false_negative = c_mat[1][0]\n        false_positive = c_mat[0][1]\n        condition_positive = false_negative + true_positive\n        condition_negative = false_positive + true_negative\n        predicted_positive = false_positive + true_positive\n        predicted_negative = false_negative + true_negative\n        trues_cat = true_negative + true_positive\n        falses_cat = false_negative + false_positive\n        accuracy = (true_negative + true_positive) / (true_negative + true_positive + false_negative + false_positive)\n        precision = 0 if predicted_positive == 0 else true_positive / predicted_positive\n        recall = 0 if condition_positive == 0 else true_positive / condition_positive\n        f1 = 0 if precision + recall == 0 else 2 * precision * recall / (precision + recall)\n\n        # Display some info\n        if log_info:\n            self.logger.info(\n                f\"F1-score: {round(f1, 5)}  \\t Precision: {round(100 * precision, 2)}% \\t\"\n                f\"Recall: {round(100 * recall, 2)}% \\t Trues: {trues_cat} \\t Falses: {falses_cat} \\t\\t --- {label} \"\n            )\n\n        # Return result\n        return {\n            'Label': f'{label}',\n            'F1-Score': f1,\n            'Accuracy': accuracy,\n            'Precision': precision,\n            'Recall': recall,\n            'Trues': trues_cat,\n            'Falses': falses_cat,\n            'True positive': true_positive,\n            'True negative': true_negative,\n            'False positive': false_positive,\n            'False negative': false_negative,\n            'Condition positive': condition_positive,\n            'Condition negative': condition_negative,\n            'Predicted positive': predicted_positive,\n            'Predicted negative': predicted_negative,\n        }\n\n    def _plot_confusion_matrix(self, c_mat: np.ndarray, labels: list, type_data: str = '',\n                               normalized: bool = False, subdir: Union[str, None] = None) -&gt; None:\n        '''Plots a confusion matrix\n\n        Args:\n            c_mat (np.ndarray): Confusion matrix\n            labels (list): Labels to plot\n        Kwargs:\n            type_data (str): Type of dataset (validation, test, ...)\n            normalized (bool): If the confusion matrix should be normalized\n            subdir (str): Sub-directory for writing the plot\n        '''\n\n        # Get title\n        if normalized:\n            title = f\"Normalized confusion matrix{' - ' + type_data if len(type_data) &gt; 0 else ''}\"\n        else:\n            title = f\"Confusion matrix, without normalization{' - ' + type_data if len(type_data) &gt; 0 else ''}\"\n\n        # Init. plot\n        width = round(10 + 0.5 * len(c_mat))\n        height = round(4 / 5 * width)\n        fig, ax = plt.subplots(figsize=(width, height))\n\n        # Plot\n        if normalized:\n            c_mat = c_mat.astype('float') / c_mat.sum(axis=1)[:, np.newaxis]\n            sns.heatmap(c_mat, annot=True, fmt=\".2f\", cmap=plt.cm.Blues, ax=ax) # type: ignore\n        else:\n            sns.heatmap(c_mat, annot=True, fmt=\"d\", cmap=plt.cm.Blues, ax=ax) # type: ignore\n\n        # labels, title and ticks\n        ax.set_xlabel('Predicted classes', fontsize=height * 2)\n        ax.set_ylabel('Real classes', fontsize=height * 2)\n        ax.set_title(title, fontsize=width * 2)\n        ax.xaxis.set_ticklabels(labels)\n        ax.yaxis.set_ticklabels(labels)\n        plt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\n        plt.setp(ax.get_yticklabels(), rotation=30, horizontalalignment='right')\n        plt.tight_layout()\n\n        # Save\n        plots_path = os.path.join(self.model_dir, 'plots')\n        if subdir is not None:  # Ajout subdir\n            plots_path = os.path.join(plots_path, subdir)\n        file_name = f\"{type_data + '_' if len(type_data) &gt; 0 else ''}confusion_matrix{'_normalized' if normalized else ''}.png\"\n        if not os.path.exists(plots_path):\n            os.makedirs(plots_path)\n        plt.savefig(os.path.join(plots_path, file_name))\n\n        # Close figures\n        plt.close('all')\n\n    def _get_new_model_dir(self) -&gt; str:\n        '''Gets a folder where to save the model\n\n        Returns:\n            str: Path to the folder\n        '''\n        models_dir = utils.get_models_path()\n        subfolder = os.path.join(models_dir, self.model_name)\n        folder_name = datetime.now().strftime(f\"{self.model_name}_%Y_%m_%d-%H_%M_%S\")\n        model_dir = os.path.join(subfolder, folder_name)\n        if os.path.isdir(model_dir):\n            time.sleep(1)  # Wait 1 second so that the 'date' changes...\n            return self._get_new_model_dir()  # Get new directory name\n        else:\n            os.makedirs(model_dir)\n        return model_dir\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n\n        # Manage paths\n        pkl_path = os.path.join(self.model_dir, f\"{self.model_name}.pkl\")\n        conf_path = os.path.join(self.model_dir, \"configurations.json\")\n\n        # Save the model if level_save &gt; 'LOW'\n        if self.level_save in ['MEDIUM', 'HIGH']:\n            with open(pkl_path, 'wb') as f:\n                pickle.dump(self, f)\n\n        # Save configuration JSON\n        json_dict = {\n            'maintainers': 'Agence DataServices',\n            'gabarit_version': '1.3.4.dev0+local',\n            'date': datetime.now().strftime(\"%d/%m/%Y - %H:%M:%S\"),  # Not the same as the folder's name\n            'package_version': utils.get_package_version(),\n            'model_name': self.model_name,\n            'model_dir': self.model_dir,\n            'trained': self.trained,\n            'nb_fit': self.nb_fit,\n            'list_classes': self.list_classes,\n            'dict_classes': self.dict_classes,\n            'x_col': self.x_col,\n            'y_col': self.y_col,\n            'random_seed': self.random_seed,\n            'multi_label': self.multi_label,\n            'level_save': self.level_save,\n            'librairie': None,\n        }\n        # Merge json_data if not None\n        if json_data is not None:\n            # Priority given to json_data !\n            json_dict = {**json_dict, **json_data}\n\n        # Add conf to attributes\n        self.json_dict = json_dict\n\n        # Save conf\n        with open(conf_path, 'w', encoding='utf-8') as json_file:\n            json.dump(json_dict, json_file, indent=4, cls=utils.NpEncoder)\n\n        # Now, save a properties file for the model upload\n        self._save_upload_properties(json_dict)\n\n    def _save_upload_properties(self, json_dict: Union[dict, None] = None) -&gt; None:\n        '''Prepares a configuration file for a future export (e.g on an artifactory)\n\n        Kwargs:\n            json_dict: Configurations to save\n        '''\n        if json_dict is None:\n            json_dict = {}\n\n        # Manage paths\n        properties_path = os.path.join(self.model_dir, \"properties.json\")\n        vanilla_model_upload_instructions = os.path.join(utils.get_ressources_path(), 'model_upload_instructions.md')\n        specific_model_upload_instructions = os.path.join(self.model_dir, \"model_upload_instructions.md\")\n\n        # First, we define a list of \"allowed\" properties\n        allowed_properties = [\"maintainers\", \"gabarit_version\", \"date\", \"package_version\", \"model_name\", \"list_classes\",\n                              \"librairie\", \"fit_time\"]\n        # Now we filter these properties\n        final_dict = {k: v for k, v in json_dict.items() if k in allowed_properties}\n        # Save\n        with open(properties_path, 'w', encoding='utf-8') as f:\n            json.dump(final_dict, f, indent=4, cls=utils.NpEncoder)\n\n        # Add instructions to upload a model to a storage solution (e.g. Artifactory)\n        with open(vanilla_model_upload_instructions, 'r', encoding='utf-8') as f:\n            content = f.read()\n        # TODO: to be improved\n        new_content = content.replace('model_dir_path_identifier', os.path.abspath(self.model_dir))\n        with open(specific_model_upload_instructions, 'w', encoding='utf-8') as f:\n            f.write(new_content)\n\n    @classmethod\n    def load_model(cls, model_dir: str, config_path: Union[str, None] = None, **kwargs) -&gt; Tuple[Any, dict]:\n        '''Loads a model from a path or a model name\n\n        Args:\n            model_dir (str): Absolute path of the folder containing the model to load\n        Kwargs:\n            config_path (str): Absolute path to a configuration file. Backup on the model_dir defaults configuration file.\n                               Most of the time, you should leave this empty.\n        Returns:\n            ModelClass: The loaded model\n            dict: The model configuration\n        '''\n        # First load the model configurations\n        configs = cls.load_configs(model_dir=model_dir, config_path=config_path)\n\n        # Load the model object from a pickle file\n        pkl_path = os.path.join(model_dir, f\"{configs['model_name']}.pkl\")\n        with open(pkl_path, 'rb') as f:\n            model = pickle.load(f)\n\n        # Change model_dir to the input model_dir (usually when the model has been trained on another computer)\n        configs['model_dir'] = model_dir\n        model.model_dir = model_dir\n\n        # Post load specificities\n        model._hook_post_load_model_pkl()\n\n        # Display if GPU is being used\n        model.display_if_gpu_activated()\n\n        # Return model\n        return model, configs\n\n    @staticmethod\n    def load_configs(model_dir: Union[str, None] = None, config_path: Union[str, None] = None) -&gt; dict:\n        '''Loads a model's configuration file as a dictionary\n\n        Kwargs:\n            model_dir (str): Absolute path of the model.\n                             Can be None to load a configuration path as it is, but config_path can't also be None.\n            config_path (str): Absolute path to a configuration file. Backup on the model_dir defaults configuration file.\n                               Most of the time, you should leave this empty.\n        Raises:\n            ValueError: If both model_dir and config_path are None.\n        Returns:\n            dict: A model's configurations\n        '''\n        # Manage errors\n        if model_dir is None and config_path is None:\n            raise ValueError(\"model_dir and config_path can't both be None in load_configs function\")\n\n        # Get configurations\n        configuration_path = os.path.join(model_dir, 'configurations.json') if config_path is None else config_path\n        with open(configuration_path, 'r', encoding='utf-8') as f:\n            configs = json.load(f)\n\n        # Can't set int as keys in json, so need to cast it after reloading\n        # dict_classes keys are always ints\n        if 'dict_classes' in configs.keys() and configs['dict_classes'] is not None:\n            configs['dict_classes'] = {int(i): col for i, col in configs['dict_classes'].items()}\n        elif 'list_classes' in configs.keys() and configs['list_classes'] is not None:\n            configs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n\n        # Return configs\n        return configs\n\n    def _hook_post_load_model_pkl(self):\n        '''Manages a model specificities post load from a pickle file (i.e. not from standalone files)'''\n        pass\n\n    @classmethod\n    def init_from_standalone_files(cls, model_dir: Union[str, None] = None, config_path: Union[str, None] = None, **kwargs) -&gt; Tuple[Any, dict]:\n        '''Init. a new model from a config file and standalone files.\n\n        The main purpose of this function is to be able to use an old model trained with an old version which is not\n        unpicklable anymore.\n        We should be able to recreate a new class object as this library tries to save all infos in a configuration file,\n        and all models / tokenizers / etc. in standalone files.\n\n        The standalone files will be inferred from the model_dir argument, except if specific **kwargs are provided.\n        To see which kwargs are available for your model, checks it's own `_load_standalone_files` function.\n        Of course, the function will raise an error if `model_dir` is None and no **kwargs arguments are provided.\n\n        WARNING : It will create a new folder for the reloaded model and copy many files in this new folder.\n                  That may take some space in your hard disk.\n        WARNING : This function should be called with the class of the model to be reloaded.\n                  e.g. ModelEmbeddingLstm.init_from_standalone_files(...)\n\n        Kwargs:\n            model_dir (str): Absolute path of the folder containing the model to load.\n                             If None, config_path must be set.\n            config_path (str): Absolute path to a configuration file.\n                               If None, backups on the model_dir defaults configuration file.\n                               If None, model_dir must be set.\n        Raises:\n            ValueError: If both model_dir and config_path are None.\n        Returns:\n            ModelClass: The loaded model\n            dict: The model configuration\n        '''\n        if model_dir is None and config_path is None:\n            raise ValueError(\"Either model_dir or config_path must be set\")\n\n        # First load the model configurations\n        configs = cls.load_configs(model_dir=model_dir, config_path=config_path)\n\n        # Init model from configurations\n        model = cls._init_new_instance_from_configs(configs)\n\n        # Load standalone files\n        model._load_standalone_files(default_model_dir=model_dir, **kwargs)\n\n        # Set configs to new model dir\n        configs['model_dir'] = model.model_dir\n\n        # Return model\n        return model, configs\n\n    @classmethod\n    def _init_new_instance_from_configs(cls, configs) -&gt; Any:\n        '''Inits a new instance from a set of configurations\n\n        Args:\n            configs: a set of configurations of a model to be reloaded\n        Returns:\n            ModelClass: the newly generated class\n        '''\n        # Create class\n        model = cls()\n\n        # Set class attributes from config\n        # model.model_name = # Keep the created name\n        # model.model_dir = # Keep the created folder\n        model.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n        model.trained = configs.get('trained', True)  # Consider trained by default\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['x_col', 'y_col', 'list_classes', 'dict_classes', 'multi_label', 'level_save', 'random_seed']:\n            setattr(model, attribute, configs.get(attribute, getattr(model, attribute)))\n\n        # Return the new model\n        return model\n\n    def _load_standalone_files(self, default_model_dir: Union[str, None] = None, *args, **kwargs):\n        '''Loads standalone files for a newly created model via _init_new_instance_from_configs\n\n        Kwargs:\n            default_model_dir (str): a path to look for default file paths\n                                     If None, standalone files path should all be provided\n        '''\n        raise NotImplementedError(\"'_load_standalone_files' needs to be overridden\")\n\n    @classmethod\n    def reload_from_standalone(cls, *args, **kwargs) -&gt; Any:\n        '''Deprecated'''\n        print(\"DEPRECATED : use load_model class method instead\")\n        return cls.load_model(*args, **kwargs)\n\n    def display_if_gpu_activated(self) -&gt; None:\n        '''Displays if a GPU is being used'''\n        if self._is_gpu_activated():\n            self.logger.info(\"GPU activated\")\n\n    def _is_gpu_activated(self) -&gt; bool:\n        '''Checks if we use a GPU\n\n        Returns:\n            bool: whether GPU is available or not\n        '''\n        # By default, no GPU\n        return False\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass.__init__","title":"<code>__init__(model_dir=None, model_name=None, x_col=None, y_col=None, random_seed=None, level_save='HIGH', multi_label=False, **kwargs)</code>","text":"<p>Initialization of the parent class.</p> Kwargs <p>model_dir (str): Folder where to save the model     If None, creates a directory based on the model's name and the date (most common usage) model_name (str): The name of the model x_col (str | int): Name of the columns used for the training - x y_col (str | int | list if multi-labels): Name of the model's target column(s) - y random_seed (int): Seed to use for packages randomness level_save (str): Level of saving     LOW: stats + configurations + logger keras - /! The model can't be reused /! -     MEDIUM: LOW + hdf5 + pkl + plots     HIGH: MEDIUM + predictions multi_label (bool): If the classification is multi-labels</p> <p>Raises:     ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])     NotADirectoryError: If a provided model directory is not a directory (i.e. it's a file)</p> Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>def __init__(self, model_dir: Union[str, None] = None, model_name: Union[str, None] = None, x_col: Union[str, int, None] = None,\n             y_col: Union[str, int, list, None] = None, random_seed: Union[int, None] = None,\n             level_save: str = 'HIGH', multi_label: bool = False, **kwargs) -&gt; None:\n    '''Initialization of the parent class.\n\n    Kwargs:\n        model_dir (str): Folder where to save the model\n            If None, creates a directory based on the model's name and the date (most common usage)\n        model_name (str): The name of the model\n        x_col (str | int): Name of the columns used for the training - x\n        y_col (str | int | list if multi-labels): Name of the model's target column(s) - y\n        random_seed (int): Seed to use for packages randomness\n        level_save (str): Level of saving\n            LOW: stats + configurations + logger keras - /!\\\\ The model can't be reused /!\\\\ -\n            MEDIUM: LOW + hdf5 + pkl + plots\n            HIGH: MEDIUM + predictions\n        multi_label (bool): If the classification is multi-labels\n    Raises:\n        ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\n        NotADirectoryError: If a provided model directory is not a directory (i.e. it's a file)\n    '''\n    if level_save not in ['LOW', 'MEDIUM', 'HIGH']:\n        raise ValueError(f\"The object level_save ({level_save}) is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\")\n\n    # Get logger\n    self.logger = logging.getLogger(__name__)\n\n    # Model name\n    self.model_name = self._default_name if model_name is None else model_name\n\n    # Names of the columns used\n    self.x_col = x_col\n    self.y_col = y_col\n\n    # Random seed\n    self.random_seed = random_seed\n\n    # Model folder\n    if model_dir is None:\n        self.model_dir = self._get_new_model_dir()\n    else:\n        if not os.path.exists(model_dir):\n            os.makedirs(model_dir)\n        if not os.path.isdir(model_dir):\n            raise NotADirectoryError(f\"{model_dir} is not a valid directory\")\n        self.model_dir = os.path.abspath(model_dir)\n\n    # List of classes to consider (set on fit)\n    self.list_classes: Optional[List[Any]] = None\n    self.dict_classes: Optional[Dict[Any, Any]] = None\n\n    # Multi-labels ?\n    self.multi_label: bool = multi_label\n\n    # Other options\n    self.level_save = level_save\n\n    # is trained ?\n    self.trained = False\n    self.nb_fit = 0\n\n    # Configuration dict. to be logged. Set on save.\n    self.json_dict: Dict[Any, Any] = {}\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass.display_if_gpu_activated","title":"<code>display_if_gpu_activated()</code>","text":"<p>Displays if a GPU is being used</p> Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>def display_if_gpu_activated(self) -&gt; None:\n    '''Displays if a GPU is being used'''\n    if self._is_gpu_activated():\n        self.logger.info(\"GPU activated\")\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass.fit","title":"<code>fit(x_train, y_train, **kwargs)</code>","text":"<p>Trains the model</p> <p>Parameters:</p> Name Type Description Default <code>x_train</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples, n_features]</p> required <code>y_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>def fit(self, x_train, y_train, **kwargs) -&gt; None:\n    '''Trains the model\n\n    Args:\n        x_train (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n        y_train (?): Array-like, shape = [n_samples, n_features]\n    '''\n    raise NotImplementedError(\"'fit' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass.get_and_save_metrics","title":"<code>get_and_save_metrics(y_true, y_pred, x=None, series_to_add=None, type_data='')</code>","text":"<p>Gets and saves the metrics of a model</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <code>y_pred</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <p>Kwargs:     x (?): Input data - Array-like, shape = [n_samples]     series_to_add (list): List of pd.Series to add to the dataframe     type_data (str): Type of dataset (validation, test, ...) Returns:     pd.DataFrame: The dataframe containing the statistics Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>def get_and_save_metrics(self, y_true, y_pred, x=None, series_to_add: Union[List[pd.Series], None] = None,\n                         type_data: str = '') -&gt; pd.DataFrame:\n    '''Gets and saves the metrics of a model\n\n    Args:\n        y_true (?): Array-like, shape = [n_samples, n_features]\n        y_pred (?): Array-like, shape = [n_samples, n_features]\n    Kwargs:\n        x (?): Input data - Array-like, shape = [n_samples]\n        series_to_add (list&lt;pd.Series&gt;): List of pd.Series to add to the dataframe\n        type_data (str): Type of dataset (validation, test, ...)\n    Returns:\n        pd.DataFrame: The dataframe containing the statistics\n    '''\n\n    # Cast to np.array\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Check shapes\n    if not self.multi_label:\n        if len(y_true.shape) == 2 and y_true.shape[1] == 1:\n            y_true = np.ravel(y_true)\n        if len(y_pred.shape) == 2 and y_pred.shape[1] == 1:\n            y_pred = np.ravel(y_pred)\n\n    # Save a predictionn file if wanted\n    if self.level_save == 'HIGH':\n        # Inverse transform\n        y_true_df = list(self.inverse_transform(y_true))\n        y_pred_df = list(self.inverse_transform(y_pred))\n\n        # Concat in a dataframe\n        if x is not None:\n            df = pd.DataFrame({'x': list(x), 'y_true': y_true_df, 'y_pred': y_pred_df})\n        else:\n            df = pd.DataFrame({'y_true': y_true_df, 'y_pred': y_pred_df})\n        # Add a matched column\n        df.loc[:, 'matched'] = df[['y_true', 'y_pred']].apply(lambda x: 1 if x.y_true == x.y_pred else 0, axis=1)\n        # Add some more columns\n        if series_to_add is not None:\n            for ser in series_to_add:\n                df[ser.name] = ser.reset_index(drop=True).reindex(index=df.index)  # Reindex\n\n        # Save predictions\n        file_path = os.path.join(self.model_dir, f\"predictions{'_' + type_data if len(type_data) &gt; 0 else ''}.csv\")\n        df.sort_values('matched', ascending=True).to_csv(file_path, sep=';', index=None, encoding='utf-8')\n\n    # Gets global f1 score / acc_tot / trues / falses / precision / recall / support\n    if self.multi_label:\n        f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n        trues = np.sum(np.all(np.equal(y_true, y_pred), axis=1))\n        falses = len(y_true) - trues\n        acc_tot = trues / len(y_true)\n        precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n        recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n        support = list(pd.DataFrame(y_true).sum().values)\n        support = [_ / sum(support) for _ in support] + [1.0]\n    else:\n        # We use 'weighted' even in the mono-label case since there can be several classes !\n        f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n        trues = np.sum(y_true == y_pred)\n        falses = np.sum(y_true != y_pred)\n        acc_tot = accuracy_score(y_true, y_pred)\n        precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n        recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n        labels_tmp, counts_tmp = np.unique(y_true, return_counts=True)\n        support = [0.0] * len(self.list_classes) + [1.0]  # type: ignore\n        for i, cl in enumerate(self.list_classes):  # type: ignore\n            if cl in labels_tmp:\n                idx_tmp = list(labels_tmp).index(cl)\n                support[i] = counts_tmp[idx_tmp] / y_pred.shape[0]\n\n    # Global Statistics\n    self.logger.info('-- * * * * * * * * * * * * * * --')\n    self.logger.info(f\"Statistics f1-score{' ' + type_data if len(type_data) &gt; 0 else ''}\")\n    self.logger.info('--------------------------------')\n    self.logger.info(f\"Total accuracy : {round(acc_tot * 100, 2)}% \\t Trues: {trues} \\t Falses: {falses}\")\n    self.logger.info(f\"F1-score (weighted) : {round(f1_weighted, 5)}\")\n    self.logger.info(f\"Precision (weighted) : {round(precision_weighted, 5)}\")\n    self.logger.info(f\"Recall (weighted) : {round(recall_weighted, 5)}\")\n    self.logger.info('--------------------------------')\n\n    # Metrics file\n    dict_df_stats = {}\n\n    # Add metrics depending on mono/multi labels &amp; manage confusion matrices\n    labels = self.list_classes\n    log_stats = len(labels) &lt; 50  # type: ignore\n\n    if self.multi_label:\n        # Details per category\n        mcm = multilabel_confusion_matrix(y_true, y_pred)\n        for i, label in enumerate(labels):  # type: ignore\n            c_mat = mcm[i]\n            dict_df_stats[i] = self._update_info_from_c_mat(c_mat, label, log_info=log_stats)\n            # Plot individual confusion matrix if level_save &gt; LOW\n            if self.level_save in ['MEDIUM', 'HIGH']:\n                none_class = 'not_' + label\n                tmp_label = re.sub(r',|:|\\s', '_', label)\n                self._plot_confusion_matrix(c_mat, [none_class, label], type_data=f\"{tmp_label}_{type_data}\",\n                                            normalized=False, subdir=type_data)\n                self._plot_confusion_matrix(c_mat, [none_class, label], type_data=f\"{tmp_label}_{type_data}\",\n                                            normalized=True, subdir=type_data)\n    else:\n        # Plot confusion matrices if level_save &gt; LOW\n        if self.level_save in ['MEDIUM', 'HIGH']:\n            if len(labels) &gt; 50:\n                self.logger.warning(\n                    f\"Warning, there are {len(labels)} categories to plot in the confusion matrix.\\n\"\n                    \"Heavy chances of slowness/display bugs/crashes...\\n\"\n                    \"SKIP the plots\"\n                )\n            else:\n                # Global stats\n                c_mat = confusion_matrix(y_true, y_pred, labels=labels)\n                self._plot_confusion_matrix(c_mat, labels, type_data=type_data, normalized=False)  # type: ignore\n                self._plot_confusion_matrix(c_mat, labels, type_data=type_data, normalized=True)  # type: ignore\n\n        # Get stats per class\n        for i, label in enumerate(labels):  # type: ignore\n            label_str = str(label)  # Fix : If label is an int, can cause some problems (e.g. only zeroes in the confusion matrix)\n            none_class = 'None' if label_str != 'None' else 'others'  # Check that the class is not already 'None'\n            y_true_tmp = [label_str if _ == label else none_class for _ in y_true]\n            y_pred_tmp = [label_str if _ == label else none_class for _ in y_pred]\n            c_mat_tmp = confusion_matrix(y_true_tmp, y_pred_tmp, labels=[none_class, label_str])\n            dict_df_stats[i] = self._update_info_from_c_mat(c_mat_tmp, label, log_info=False)\n\n\n    # Add global statistics\n    dict_df_stats[i+1] = {\n        'Label': 'All',\n        'F1-Score': f1_weighted,\n        'Accuracy': acc_tot,\n        'Precision': precision_weighted,\n        'Recall': recall_weighted,\n        'Trues': trues,\n        'Falses': falses,\n        'True positive': None,\n        'True negative': None,\n        'False positive': None,\n        'False negative': None,\n        'Condition positive': None,\n        'Condition negative': None,\n        'Predicted positive': None,\n        'Predicted negative': None,\n    }\n\n    df_stats = pd.DataFrame.from_dict(dict_df_stats, orient='index')\n\n    # Add support\n    df_stats['Support'] = support\n\n    # Save .csv\n    file_path = os.path.join(self.model_dir, f\"f1{'_' + type_data if len(type_data) &gt; 0 else ''}@{f1_weighted}.csv\")\n    df_stats.to_csv(file_path, sep=';', index=False, encoding='utf-8')\n\n    # Save accuracy\n    acc_path = os.path.join(self.model_dir, f\"acc{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(acc_tot, 5)}\")\n    with open(acc_path, 'w'):\n        pass\n\n    return df_stats\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass.get_classes_from_proba","title":"<code>get_classes_from_proba(predicted_proba)</code>","text":"<p>Gets the classes from probabilities</p> <p>Parameters:</p> Name Type Description Default <code>predicted_proba</code> <code>ndarray</code> <p>The probabilities predicted by the model, shape = [n_samples, n_classes]</p> required <p>Returns:     predicted_class (np.ndarray): Shape = [n_samples, n_classes] if multi-labels, shape = [n_samples] otherwise</p> Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>def get_classes_from_proba(self, predicted_proba: np.ndarray) -&gt; np.ndarray:\n    '''Gets the classes from probabilities\n\n    Args:\n        predicted_proba (np.ndarray): The probabilities predicted by the model, shape = [n_samples, n_classes]\n    Returns:\n        predicted_class (np.ndarray): Shape = [n_samples, n_classes] if multi-labels, shape = [n_samples] otherwise\n    '''\n    if not self.multi_label:\n        predicted_class = np.vectorize(lambda x: self.dict_classes[x])(predicted_proba.argmax(axis=-1))\n    else:\n        # If multi-labels, returns a list of 0 and 1\n        predicted_class = np.rint(predicted_proba)  # 1 if x &gt; 0.5 else 0\n    return predicted_class\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass.get_metrics_simple_monolabel","title":"<code>get_metrics_simple_monolabel(y_true, y_pred)</code>","text":"<p>Gets metrics on mono-label predictions Same as the method get_and_save_metrics but without all the fluff (save, etc.)</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <code>y_pred</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <p>Raises:     ValueError: If not in mono-label mode Returns:     pd.DataFrame: The dataframe containing statistics</p> Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>def get_metrics_simple_monolabel(self, y_true, y_pred) -&gt; pd.DataFrame:\n    '''Gets metrics on mono-label predictions\n    Same as the method get_and_save_metrics but without all the fluff (save, etc.)\n\n    Args:\n        y_true (?): Array-like, shape = [n_samples, n_features]\n        y_pred (?): Array-like, shape = [n_samples, n_features]\n    Raises:\n        ValueError: If not in mono-label mode\n    Returns:\n        pd.DataFrame: The dataframe containing statistics\n    '''\n    if self.multi_label:\n        raise ValueError(\"The method get_metrics_simple_monolabel only works for the mono-label case\")\n\n    # Cast to np.array\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Check shapes\n    if len(y_true.shape) == 2 and y_true.shape[1] == 1:\n        y_true = np.ravel(y_true)\n    if len(y_pred.shape) == 2 and y_pred.shape[1] == 1:\n        y_pred = np.ravel(y_pred)\n\n    # Gets global f1 score / acc_tot / trues / falses / precision / recall / support\n    f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n    trues = np.sum(y_true == y_pred)\n    falses = np.sum(y_true != y_pred)\n    acc_tot = accuracy_score(y_true, y_pred)\n    precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n    recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n    labels_tmp, counts_tmp = np.unique(y_true, return_counts=True)\n    support = [0.] * len(self.list_classes) + [1.0]\n    for i, cl in enumerate(self.list_classes):\n        if cl in labels_tmp:\n            idx_tmp = list(labels_tmp).index(cl)\n            support[i] = counts_tmp[idx_tmp] / y_pred.shape[0]\n\n    # DataFrame metrics\n    dict_df_stats = {}\n\n    # Get stats per class\n    labels = self.list_classes\n    for i, label in enumerate(labels):\n        label_str = str(label)  # Fix : If label is an int, can cause some problems (e.g. only zeroes in the confusion matrix)\n        none_class = 'None' if label_str != 'None' else 'others'  # Check that the class is not already 'None'\n        y_true_tmp = [label_str if _ == label else none_class for _ in y_true]\n        y_pred_tmp = [label_str if _ == label else none_class for _ in y_pred]\n        c_mat_tmp = confusion_matrix(y_true_tmp, y_pred_tmp, labels=[none_class, label_str])\n        dict_df_stats[i] = self._update_info_from_c_mat(c_mat_tmp, label, log_info=False)\n\n\n    # Add global statistics\n    dict_df_stats[i+1] = {\n        'Label': 'All',\n        'F1-Score': f1_weighted,\n        'Accuracy': acc_tot,\n        'Precision': precision_weighted,\n        'Recall': recall_weighted,\n        'Trues': trues,\n        'Falses': falses,\n        'True positive': None,\n        'True negative': None,\n        'False positive': None,\n        'False negative': None,\n        'Condition positive': None,\n        'Condition negative': None,\n        'Predicted positive': None,\n        'Predicted negative': None,\n    }\n    df_stats = pd.DataFrame.from_dict(dict_df_stats, orient='index')\n\n    # Add support\n    df_stats['Support'] = support\n\n    # Return dataframe\n    return df_stats\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass.get_metrics_simple_multilabel","title":"<code>get_metrics_simple_multilabel(y_true, y_pred)</code>","text":"<p>Gets metrics on multi-label predictions Same as the method get_and_save_metrics but without all the fluff (save, etc.)</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <code>y_pred</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <p>Raises:     ValueError: If not with multi-labels tasks Returns:     pd.DataFrame: The dataframe containing statistics</p> Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>def get_metrics_simple_multilabel(self, y_true, y_pred) -&gt; pd.DataFrame:\n    '''Gets metrics on multi-label predictions\n    Same as the method get_and_save_metrics but without all the fluff (save, etc.)\n\n    Args:\n        y_true (?): Array-like, shape = [n_samples, n_features]\n        y_pred (?): Array-like, shape = [n_samples, n_features]\n    Raises:\n        ValueError: If not with multi-labels tasks\n    Returns:\n        pd.DataFrame: The dataframe containing statistics\n    '''\n    if not self.multi_label:\n        raise ValueError(\"The method get_metrics_simple_multilabel only works for multi-labels cases\")\n\n    # Cast to np.array\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Gets global f1 score / acc_tot / trues / falses / precision / recall / support\n    f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n    trues = np.sum(np.all(np.equal(y_true, y_pred), axis=1))\n    falses = len(y_true) - trues\n    acc_tot = trues / len(y_true)\n    precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n    recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n    support = list(pd.DataFrame(y_true).sum().values)\n    support = [_ / sum(support) for _ in support] + [1.0]\n\n    # DataFrame metrics\n    dict_df_stats = {}\n\n    # Add metrics\n    labels = self.list_classes\n    # Details per category\n    mcm = multilabel_confusion_matrix(y_true, y_pred)\n    for i, label in enumerate(labels):\n        c_mat = mcm[i]\n        dict_df_stats[i] = self._update_info_from_c_mat(c_mat, label, log_info=False)\n\n\n    # Add global statistics\n    dict_df_stats[i+1] = {\n        'Label': 'All',\n        'F1-Score': f1_weighted,\n        'Accuracy': acc_tot,\n        'Precision': precision_weighted,\n        'Recall': recall_weighted,\n        'Trues': trues,\n        'Falses': falses,\n        'True positive': None,\n        'True negative': None,\n        'False positive': None,\n        'False negative': None,\n        'Condition positive': None,\n        'Condition negative': None,\n        'Predicted positive': None,\n        'Predicted negative': None,\n    }\n    df_stats = pd.DataFrame.from_dict(dict_df_stats, orient='index')\n\n    # Add support\n    df_stats['Support'] = support\n\n    # Return dataframe\n    return df_stats\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass.get_predict_position","title":"<code>get_predict_position(x_test, y_true)</code>","text":"<p>Gets the order of predictions of y_true. Positions start at 1 (not 0)</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples, n_features]</p> required <code>y_true</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <p>Raises:     ValueError: Not available in multi-labels case Returns:     np.ndarray: Array, shape = [n_samples]</p> Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>@utils.trained_needed\ndef get_predict_position(self, x_test, y_true) -&gt; np.ndarray:\n    '''Gets the order of predictions of y_true.\n    Positions start at 1 (not 0)\n\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n        y_true (?): Array-like, shape = [n_samples, n_features]\n    Raises:\n        ValueError: Not available in multi-labels case\n    Returns:\n        np.ndarray: Array, shape = [n_samples]\n    '''\n    if self.multi_label:\n        raise ValueError(\"The method 'get_predict_position'is unavailable in the multi-labels case\")\n    # Process\n    # Cast en pd.Series\n    y_true = pd.Series(y_true)\n    # Get predicted proba\n    predicted_proba = self.predict(x_test, return_proba=True)\n    # Get position\n    order = predicted_proba.argsort()\n    ranks = len(self.list_classes) - order.argsort()  # type: ignore\n    df_probas = pd.DataFrame(ranks, columns=self.list_classes)  # type: ignore\n    predict_positions = np.array([df_probas.loc[i, cl] if cl in df_probas.columns else -1 for i, cl in enumerate(y_true)])\n    return predict_positions\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass.get_top_n_from_proba","title":"<code>get_top_n_from_proba(predicted_proba, n=5)</code>","text":"<p>Gets the Top n predictions from probabilities</p> <p>Parameters:</p> Name Type Description Default <code>predicted_proba</code> <code>ndarray</code> <p>The probabilities predicted by the model, shape = [n_samples, n_classes]</p> required <p>kwargs:     n (int): Number of classes to return Raises:     ValueError: If the number of classes to return is greater than the number of classes of the model Returns:     list: top n predictions     list: top n probabilities</p> Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>def get_top_n_from_proba(self, predicted_proba: np.ndarray, n: int = 5) -&gt; Tuple[list, list]:\n    '''Gets the Top n predictions from probabilities\n\n    Args:\n        predicted_proba (np.ndarray): The probabilities predicted by the model, shape = [n_samples, n_classes]\n    kwargs:\n        n (int): Number of classes to return\n    Raises:\n        ValueError: If the number of classes to return is greater than the number of classes of the model\n    Returns:\n        list: top n predictions\n        list: top n probabilities\n    '''\n    # TODO: Make this method available with multi-labels tasks\n    if self.multi_label:\n        raise ValueError(\"The method 'get_top_n_from_proba' is unavailable with multi-labels tasks\")\n    if self.list_classes is not None and n &gt; len(self.list_classes):  # type: ignore\n        raise ValueError(\"The number of classes to return is greater than the number of classes of the model\")\n    # Process\n    idx = predicted_proba.argsort()[:, -n:][:, ::-1]\n    top_n_proba = list(np.take_along_axis(predicted_proba, idx, axis=1))\n    top_n = list(np.vectorize(lambda x: self.dict_classes[x])(idx))  # type: ignore\n    return top_n, top_n_proba\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass.init_from_standalone_files","title":"<code>init_from_standalone_files(model_dir=None, config_path=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Init. a new model from a config file and standalone files.</p> <p>The main purpose of this function is to be able to use an old model trained with an old version which is not unpicklable anymore. We should be able to recreate a new class object as this library tries to save all infos in a configuration file, and all models / tokenizers / etc. in standalone files.</p> <p>The standalone files will be inferred from the model_dir argument, except if specific kwargs are provided. To see which kwargs are available for your model, checks it's own <code>_load_standalone_files</code> function. Of course, the function will raise an error if <code>model_dir</code> is None and no kwargs arguments are provided.</p> It will create a new folder for the reloaded model and copy many files in this new folder. <p>That may take some space in your hard disk.</p> <p>WARNING : This function should be called with the class of the model to be reloaded.           e.g. ModelEmbeddingLstm.init_from_standalone_files(...)</p> Kwargs <p>model_dir (str): Absolute path of the folder containing the model to load.                  If None, config_path must be set. config_path (str): Absolute path to a configuration file.                    If None, backups on the model_dir defaults configuration file.                    If None, model_dir must be set.</p> <p>Raises:     ValueError: If both model_dir and config_path are None. Returns:     ModelClass: The loaded model     dict: The model configuration</p> Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>@classmethod\ndef init_from_standalone_files(cls, model_dir: Union[str, None] = None, config_path: Union[str, None] = None, **kwargs) -&gt; Tuple[Any, dict]:\n    '''Init. a new model from a config file and standalone files.\n\n    The main purpose of this function is to be able to use an old model trained with an old version which is not\n    unpicklable anymore.\n    We should be able to recreate a new class object as this library tries to save all infos in a configuration file,\n    and all models / tokenizers / etc. in standalone files.\n\n    The standalone files will be inferred from the model_dir argument, except if specific **kwargs are provided.\n    To see which kwargs are available for your model, checks it's own `_load_standalone_files` function.\n    Of course, the function will raise an error if `model_dir` is None and no **kwargs arguments are provided.\n\n    WARNING : It will create a new folder for the reloaded model and copy many files in this new folder.\n              That may take some space in your hard disk.\n    WARNING : This function should be called with the class of the model to be reloaded.\n              e.g. ModelEmbeddingLstm.init_from_standalone_files(...)\n\n    Kwargs:\n        model_dir (str): Absolute path of the folder containing the model to load.\n                         If None, config_path must be set.\n        config_path (str): Absolute path to a configuration file.\n                           If None, backups on the model_dir defaults configuration file.\n                           If None, model_dir must be set.\n    Raises:\n        ValueError: If both model_dir and config_path are None.\n    Returns:\n        ModelClass: The loaded model\n        dict: The model configuration\n    '''\n    if model_dir is None and config_path is None:\n        raise ValueError(\"Either model_dir or config_path must be set\")\n\n    # First load the model configurations\n    configs = cls.load_configs(model_dir=model_dir, config_path=config_path)\n\n    # Init model from configurations\n    model = cls._init_new_instance_from_configs(configs)\n\n    # Load standalone files\n    model._load_standalone_files(default_model_dir=model_dir, **kwargs)\n\n    # Set configs to new model dir\n    configs['model_dir'] = model.model_dir\n\n    # Return model\n    return model, configs\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass.inverse_transform","title":"<code>inverse_transform(y)</code>","text":"<p>Gets a list of classes from the predictions</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>?</code> <p>Array-like, shape = [n_samples, n_classes], arrays of 0s and 1s    OR 1D array shape = [n_classes] (only one prediction)</p> required <p>Raises:     ValueError: If the size of y does not correspond to the number of classes of the model Returns:     List of tuple if multi-labels and several predictions     Tuple if multi-labels and one prediction     List of classes if mono-label</p> Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>def inverse_transform(self, y: Union[list, np.ndarray]) -&gt; Union[list, tuple]:\n    '''Gets a list of classes from the predictions\n\n    Args:\n        y (?): Array-like, shape = [n_samples, n_classes], arrays of 0s and 1s\n               OR 1D array shape = [n_classes] (only one prediction)\n    Raises:\n        ValueError: If the size of y does not correspond to the number of classes of the model\n    Returns:\n        List of tuple if multi-labels and several predictions\n        Tuple if multi-labels and one prediction\n        List of classes if mono-label\n    '''\n    # If multi-label, get classes in tuple\n    if self.multi_label:\n        # Cast to np array\n        if not isinstance(y, np.ndarray):\n            y = np.array(y)\n        if y.shape[-1] != len(self.list_classes):  # We consider \"-1\" in order to take care of the case where y is 1D\n            raise ValueError(f\"The size of y ({y.shape[-1]}) does not correspond\"\n                             f\" to the number of classes of the model : ({len(self.list_classes)})\")\n        # Manage 1D array (only one pred)\n        if len(y.shape) == 1:\n            # TODO : shoudln't we return a list here ?\n            return tuple(np.array(self.list_classes).compress(y))\n        # Several preds\n        else:\n            return [tuple(np.array(self.list_classes).compress(indicators)) for indicators in y]\n    # If mono-label, just cast in list if y is np array\n    else:\n        return list(y) if isinstance(y, np.ndarray) else y\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass.load_configs","title":"<code>load_configs(model_dir=None, config_path=None)</code>  <code>staticmethod</code>","text":"<p>Loads a model's configuration file as a dictionary</p> Kwargs <p>model_dir (str): Absolute path of the model.                  Can be None to load a configuration path as it is, but config_path can't also be None. config_path (str): Absolute path to a configuration file. Backup on the model_dir defaults configuration file.                    Most of the time, you should leave this empty.</p> <p>Raises:     ValueError: If both model_dir and config_path are None. Returns:     dict: A model's configurations</p> Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>@staticmethod\ndef load_configs(model_dir: Union[str, None] = None, config_path: Union[str, None] = None) -&gt; dict:\n    '''Loads a model's configuration file as a dictionary\n\n    Kwargs:\n        model_dir (str): Absolute path of the model.\n                         Can be None to load a configuration path as it is, but config_path can't also be None.\n        config_path (str): Absolute path to a configuration file. Backup on the model_dir defaults configuration file.\n                           Most of the time, you should leave this empty.\n    Raises:\n        ValueError: If both model_dir and config_path are None.\n    Returns:\n        dict: A model's configurations\n    '''\n    # Manage errors\n    if model_dir is None and config_path is None:\n        raise ValueError(\"model_dir and config_path can't both be None in load_configs function\")\n\n    # Get configurations\n    configuration_path = os.path.join(model_dir, 'configurations.json') if config_path is None else config_path\n    with open(configuration_path, 'r', encoding='utf-8') as f:\n        configs = json.load(f)\n\n    # Can't set int as keys in json, so need to cast it after reloading\n    # dict_classes keys are always ints\n    if 'dict_classes' in configs.keys() and configs['dict_classes'] is not None:\n        configs['dict_classes'] = {int(i): col for i, col in configs['dict_classes'].items()}\n    elif 'list_classes' in configs.keys() and configs['list_classes'] is not None:\n        configs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n\n    # Return configs\n    return configs\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass.load_model","title":"<code>load_model(model_dir, config_path=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Loads a model from a path or a model name</p> <p>Parameters:</p> Name Type Description Default <code>model_dir</code> <code>str</code> <p>Absolute path of the folder containing the model to load</p> required <p>Kwargs:     config_path (str): Absolute path to a configuration file. Backup on the model_dir defaults configuration file.                        Most of the time, you should leave this empty. Returns:     ModelClass: The loaded model     dict: The model configuration</p> Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>@classmethod\ndef load_model(cls, model_dir: str, config_path: Union[str, None] = None, **kwargs) -&gt; Tuple[Any, dict]:\n    '''Loads a model from a path or a model name\n\n    Args:\n        model_dir (str): Absolute path of the folder containing the model to load\n    Kwargs:\n        config_path (str): Absolute path to a configuration file. Backup on the model_dir defaults configuration file.\n                           Most of the time, you should leave this empty.\n    Returns:\n        ModelClass: The loaded model\n        dict: The model configuration\n    '''\n    # First load the model configurations\n    configs = cls.load_configs(model_dir=model_dir, config_path=config_path)\n\n    # Load the model object from a pickle file\n    pkl_path = os.path.join(model_dir, f\"{configs['model_name']}.pkl\")\n    with open(pkl_path, 'rb') as f:\n        model = pickle.load(f)\n\n    # Change model_dir to the input model_dir (usually when the model has been trained on another computer)\n    configs['model_dir'] = model_dir\n    model.model_dir = model_dir\n\n    # Post load specificities\n    model._hook_post_load_model_pkl()\n\n    # Display if GPU is being used\n    model.display_if_gpu_activated()\n\n    # Return model\n    return model, configs\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass.predict","title":"<code>predict(x_test, **kwargs)</code>","text":"<p>Predictions on the test set</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples, n_features]</p> required <p>Returns:     (np.ndarray): Array, shape = [n_samples, n_classes]</p> Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>@utils.data_agnostic_str_to_list\ndef predict(self, x_test, **kwargs) -&gt; np.ndarray:\n    '''Predictions on the test set\n\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n    raise NotImplementedError(\"'predict' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts probabilities on the test dataset</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples, n_features]</p> required <p>Returns:     (np.ndarray): Array, shape = [n_samples, n_classes]</p> Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>@utils.data_agnostic_str_to_list\ndef predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n    '''Predicts probabilities on the test dataset\n\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n    raise NotImplementedError(\"'predict_proba' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass.predict_with_proba","title":"<code>predict_with_proba(x_test, **kwargs)</code>","text":"<p>Predicts on the test set with probabilities</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples, n_features]</p> required <p>Returns:     predicted_class (np.ndarray): The predicted classes, shape = [n_samples, n_classes]     predicted_proba (np.ndarray): The predicted probabilities for each class, shape = [n_samples, n_classes]</p> Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>@utils.trained_needed\ndef predict_with_proba(self, x_test, **kwargs) -&gt; Tuple[np.ndarray, np.ndarray]:\n    '''Predicts on the test set with probabilities\n\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n    Returns:\n        predicted_class (np.ndarray): The predicted classes, shape = [n_samples, n_classes]\n        predicted_proba (np.ndarray): The predicted probabilities for each class, shape = [n_samples, n_classes]\n    '''\n    # Process\n    predicted_proba = self.predict(x_test, return_proba=True)\n    predicted_class = self.get_classes_from_proba(predicted_proba)\n    return predicted_class, predicted_proba\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass.reload_from_standalone","title":"<code>reload_from_standalone(*args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Deprecated</p> Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>@classmethod\ndef reload_from_standalone(cls, *args, **kwargs) -&gt; Any:\n    '''Deprecated'''\n    print(\"DEPRECATED : use load_model class method instead\")\n    return cls.load_model(*args, **kwargs)\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n\n    # Manage paths\n    pkl_path = os.path.join(self.model_dir, f\"{self.model_name}.pkl\")\n    conf_path = os.path.join(self.model_dir, \"configurations.json\")\n\n    # Save the model if level_save &gt; 'LOW'\n    if self.level_save in ['MEDIUM', 'HIGH']:\n        with open(pkl_path, 'wb') as f:\n            pickle.dump(self, f)\n\n    # Save configuration JSON\n    json_dict = {\n        'maintainers': 'Agence DataServices',\n        'gabarit_version': '1.3.4.dev0+local',\n        'date': datetime.now().strftime(\"%d/%m/%Y - %H:%M:%S\"),  # Not the same as the folder's name\n        'package_version': utils.get_package_version(),\n        'model_name': self.model_name,\n        'model_dir': self.model_dir,\n        'trained': self.trained,\n        'nb_fit': self.nb_fit,\n        'list_classes': self.list_classes,\n        'dict_classes': self.dict_classes,\n        'x_col': self.x_col,\n        'y_col': self.y_col,\n        'random_seed': self.random_seed,\n        'multi_label': self.multi_label,\n        'level_save': self.level_save,\n        'librairie': None,\n    }\n    # Merge json_data if not None\n    if json_data is not None:\n        # Priority given to json_data !\n        json_dict = {**json_dict, **json_data}\n\n    # Add conf to attributes\n    self.json_dict = json_dict\n\n    # Save conf\n    with open(conf_path, 'w', encoding='utf-8') as json_file:\n        json.dump(json_dict, json_file, indent=4, cls=utils.NpEncoder)\n\n    # Now, save a properties file for the model upload\n    self._save_upload_properties(json_dict)\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_huggingface/","title":"Model huggingface","text":""},{"location":"reference/template_nlp/models_training/model_huggingface/#template_nlp.models_training.model_huggingface.ModelHuggingFace","title":"<code>ModelHuggingFace</code>","text":"<p>             Bases: <code>ModelClass</code></p> <p>Generic model for Huggingface NN</p> Source code in <code>template_nlp/models_training/model_huggingface.py</code> <pre><code>class ModelHuggingFace(ModelClass):\n    '''Generic model for Huggingface NN'''\n\n    _default_name = 'model_huggingface'\n\n    # TODO: perhaps it would be smarter to have this class behaving as the abstract class for all the model types\n    # implemented on the HF hub and to create model specific subclasses.\n    # =&gt; might change it as use cases grow\n\n    def __init__(self, batch_size: int = 8, epochs: int = 99, validation_split: float = 0.2, patience: int = 5,\n                 transformer_name: str = 'Geotrend/distilbert-base-fr-cased', transformer_params: Union[dict, None] = None,\n                 trainer_params: Union[dict, None] = None, model_max_length: int = None, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelClass for more arguments)\n\n        Kwargs:\n            batch_size (int): Batch size\n            epochs (int): Number of epochs\n            validation_split (float): Percentage for the validation set split\n                Only used if no input validation set when fitting\n            patience (int): Early stopping patience\n            transformer_name (str) : The name of the transformer backbone to use\n            transformer_params (dict): Parameters used by the Transformer model.\n                The purpose of this dictionary is for the user to use it as they wants in the _get_model function\n                This parameter was initially added in order to do an hyperparameters search\n            trainer_params (dict): A set of parameters to be use by the Trainer. It is recommended to use the default params (leave this empty).\n        '''\n        # TODO: learning rate should be an attribute !\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Param. model\n        self.batch_size = batch_size\n        self.epochs = epochs\n        self.validation_split = validation_split\n        self.patience = patience\n        self.transformer_name = transformer_name\n        self.model_max_length = model_max_length\n\n        # transformer_params has no use as of 14/12/2022\n        # we still leave it for compatibility with Keras models and future usage\n        self.transformer_params = transformer_params\n\n        # Trainer params\n        if trainer_params is None:\n            trainer_params = {\n                'output_dir': self.model_dir,\n                'learning_rate': 2e-5,\n                'per_device_train_batch_size': self.batch_size,\n                'per_device_eval_batch_size': self.batch_size,\n                'num_train_epochs': self.epochs,\n                'weight_decay': 0.0,\n                'evaluation_strategy': 'epoch',\n                'save_strategy': 'epoch',\n                'logging_strategy': 'epoch',\n                'save_total_limit': 1,\n                'load_best_model_at_end': True\n            }\n        # TODO: maybe we should keep the default dict &amp; only add/replace keys in provided dict ?\n\n        # By default huggingface uses seed = 42 if not specified\n        if 'seed' not in trainer_params:\n            trainer_params['seed'] = self.random_seed if self.random_seed is not None else 42\n        self.trainer_params = trainer_params\n\n        # Model set on fit or on reload\n        self.model: Any = None\n        self.pipe: Any = None  # Set on first predict\n\n        # Tokenizer set on fit or on reload\n        self.tokenizer: Any = None\n\n    def fit(self, x_train, y_train, x_valid=None, y_valid=None, with_shuffle: bool = True, **kwargs) -&gt; None:\n        '''Fits the model\n\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n            y_train (?): Array-like, shape = [n_samples, n_targets]\n            x_valid (?): Array-like, shape = [n_samples, n_features]\n            y_valid (?): Array-like, shape = [n_samples, n_targets]\n        Kwargs:\n            with_shuffle (bool): If x, y must be shuffled before fitting\n                Experimental: We must verify if it works as intended depending on the formats of x and y\n                This should be used if y is not shuffled as the split_validation takes the lines in order.\n                Thus, the validation set might get classes which are not in the train set ...\n        Raises:\n            ValueError: If different classes when comparing an already fitted model and a new dataset\n        '''\n        ##############################################\n        # Manage retrain\n        ##############################################\n\n        # If a model has already been fitted, we make a new folder in order not to overwrite the existing one !\n        # And we save the old conf\n        if self.trained:\n            # Get src files to save\n            src_files = [os.path.join(self.model_dir, \"configurations.json\")]\n            if self.nb_fit &gt; 1:\n                for i in range(1, self.nb_fit):\n                    src_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n            # Change model dir\n            self.model_dir = self._get_new_model_dir()\n            # Get dst files\n            dst_files = [os.path.join(self.model_dir, f\"configurations_fit_{self.nb_fit}.json\")]\n            if self.nb_fit &gt; 1:\n                for i in range(1, self.nb_fit):\n                    dst_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n            # Copies\n            for src, dst in zip(src_files, dst_files):\n                try:\n                    shutil.copyfile(src, dst)\n                except Exception as e:\n                    self.logger.error(f\"Impossible to copy {src} to {dst}\")\n                    self.logger.error(\"We still continue ...\")\n                    self.logger.error(repr(e))\n\n        ##############################################\n        # Prepare x_train, x_valid, y_train &amp; y_valid\n        # Also extract list of classes\n        ##############################################\n\n        # If not multilabel, transform y_train as dummies (should already be the case for multi-labels)\n        if not self.multi_label:\n            # If len(array.shape)==2, we flatten the array if the second dimension is useless\n            if isinstance(y_train, np.ndarray) and len(y_train.shape) == 2 and y_train.shape[1] == 1:\n                y_train = np.ravel(y_train)\n            if isinstance(y_valid, np.ndarray) and len(y_valid.shape) == 2 and y_valid.shape[1] == 1:\n                y_valid = np.ravel(y_valid)\n            # Transformation dummies\n            y_train_dummies = pd.get_dummies(y_train)\n            y_valid_dummies = pd.get_dummies(y_valid) if y_valid is not None else None\n            # Important : get_dummies reorder the columns in alphabetical order\n            # Thus, there is no problem if we fit again on a new dataframe with shuffled data\n            list_classes = list(y_train_dummies.columns)\n            # FIX: valid test might miss some classes, hence we need to add them back to y_valid_dummies\n            if y_valid_dummies is not None and y_train_dummies.shape[1] != y_valid_dummies.shape[1]:\n                for cl in list_classes:\n                    # Add missing columns\n                    if cl not in y_valid_dummies.columns:\n                        y_valid_dummies[cl] = 0\n                y_valid_dummies = y_valid_dummies[list_classes]  # Reorder\n        # Else keep it as it is\n        else:\n            y_train_dummies = y_train\n            y_valid_dummies = y_valid\n            if hasattr(y_train_dummies, 'columns'):\n                list_classes = list(y_train_dummies.columns)\n            else:\n                self.logger.warning(\n                    \"Can't read the name of the columns of y_train -&gt; inverse transformation won't be possible\"\n                )\n                # We still create a list of classes in order to be compatible with other functions\n                list_classes = [str(_) for _ in range(pd.DataFrame(y_train_dummies).shape[1])]\n\n        # Set dict_classes based on list classes\n        dict_classes = {i: col for i, col in enumerate(list_classes)}\n\n        # Validate classes if already trained, else set them\n        if self.trained:\n            if self.list_classes != list_classes:\n                raise ValueError(\"Error: the new dataset does not match with the already fitted model\")\n            if self.dict_classes != dict_classes:\n                raise ValueError(\"Error: the new dataset does not match with the already fitted model\")\n        else:\n            self.list_classes = list_classes\n            self.dict_classes = dict_classes\n\n        # Shuffle x, y if wanted\n        # It is advised as validation_split from keras does not shufle the data\n        # Hence we might have classes in the validation data that we never met in the training data\n        rng = np.random.RandomState(self.random_seed)\n        if with_shuffle:\n            p = rng.permutation(len(x_train))\n            x_train = np.array(x_train)[p]\n            y_train_dummies = np.array(y_train_dummies)[p]\n        # Else still transform to numpy array\n        else:\n            x_train = np.array(x_train)\n            y_train_dummies = np.array(y_train_dummies)\n\n        # Also get y_valid_dummies as numpy\n        y_valid_dummies = np.array(y_valid_dummies)\n\n        # If no valid set, split train set according to validation_split\n        if y_valid is None:\n            self.logger.warning(f\"Warning, no validation set. The training set will be splitted (validation fraction = {self.validation_split})\")\n            x_train, x_valid, y_train_dummies, y_valid_dummies = train_test_split(x_train, y_train_dummies, test_size=self.validation_split,\n                                                                                  random_state=self.random_seed)\n\n        ##############################################\n        # Get model &amp; prepare datasets\n        ##############################################\n\n        # Get model (if already fitted, _get_model returns instance model)\n        self.model = self._get_model(num_labels=y_train_dummies.shape[1])\n\n        # Get tokenizer (if already fitted, _get_tokenizer returns instance tokenizer)\n        self.tokenizer = self._get_tokenizer()\n\n        # Preprocess datasets\n        train_dataset = self._prepare_x_train(x_train, y_train_dummies)\n        valid_dataset = self._prepare_x_valid(x_valid, y_valid_dummies)\n\n        ##############################################\n        # Fit\n        ##############################################\n\n        # Fit\n        try:\n            # TODO: remove the checkpoints !\n            # Prepare trainer\n            trainer = Trainer(\n                model=self.model,\n                args=TrainingArguments(**self.trainer_params),\n                train_dataset=train_dataset,\n                eval_dataset=valid_dataset,\n                tokenizer=self.tokenizer,  # Only use for padding, dataset are already preprocessed. Pby not needed as we define a collator.\n                data_collator=DataCollatorWithPadding(tokenizer=self.tokenizer),  # Pad batches\n                compute_metrics=self._compute_metrics_mono_label if not self.multi_label else self._compute_metrics_multi_label,\n                optimizers=self._get_optimizers(),\n            )\n            # Add callbacks\n            trainer.add_callback(MetricsTrainCallback(trainer))\n            trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=self.patience))\n            # Fit\n            trainer.train()\n            # Save model &amp; tokenizer\n            hf_model_dir = os.path.join(self.model_dir, 'hf_model')\n            hf_tokenizer_dir = os.path.join(self.model_dir, 'hf_tokenizer')\n            self.model.save_pretrained(save_directory=hf_model_dir)\n            self.tokenizer.save_pretrained(save_directory=hf_tokenizer_dir)\n            # Remove checkpoint dir if save total limit is set to 1 (no need to keep this as we resave the model)\n            if self.trainer_params.get('save_total_limit', None) == 1:\n                checkpoint_dirs = [_ for _ in os.listdir(self.model_dir) if _.startswith('checkpoint-')]\n                if len(checkpoint_dirs) == 0:\n                    self.logger.warning(\"Can't find a checkpoint dir to be removed.\")\n                else:\n                    for checkpoint_dir in checkpoint_dirs:\n                        shutil.rmtree(os.path.join(self.model_dir, checkpoint_dir))\n        except (RuntimeError, SystemError, SystemExit, EnvironmentError, KeyboardInterrupt, Exception) as e:\n            self.logger.error(repr(e))\n            raise RuntimeError(\"Error during model training\")\n\n        # Print accuracy &amp; loss if level_save &gt; 'LOW'\n        if self.level_save in ['MEDIUM', 'HIGH']:\n            # Plot accuracy\n            fit_history = trainer.state.log_history\n            self._plot_metrics_and_loss(fit_history)\n            # Reload best model ?\n            # Default trainer has load_best_model_at_end = True\n            # Hence we consider the best model is already reloaded\n\n        # Set trained\n        self.trained = True\n        self.nb_fit += 1\n\n    @utils.data_agnostic_str_to_list\n    @utils.trained_needed\n    def predict(self, x_test, return_proba: bool = False, **kwargs) -&gt; np.ndarray:\n        '''Predictions on test set\n\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples]\n        Kwargs:\n            return_proba (bool): If the function should return the probabilities instead of the classes\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n        # Predict probas\n        predicted_proba = self.predict_proba(x_test)\n\n        # We return the probabilities if wanted\n        if return_proba:\n            return predicted_proba\n\n        # Finally, we get the classes predictions\n        return self.get_classes_from_proba(predicted_proba)\n\n    @utils.data_agnostic_str_to_list\n    @utils.trained_needed\n    def predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n        '''Predicts probabilities on the test dataset\n\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n        # Does not work with np array nor pandas Series\n        if isinstance(x_test, (np.ndarray, pd.Series)):\n            x_test = x_test.tolist()\n        # Prepare predict\n        if self.model.training:\n            self.model.eval()\n        if self.pipe is None:\n            # Set model on gpu if available\n            self.model = self.model.to('cuda') if self._is_gpu_activated() else self.model.to('cpu')\n            device = 0 if self._is_gpu_activated() else -1\n            self.pipe = TextClassificationPipeline(model=self.model, tokenizer=self.tokenizer, return_all_scores=True, device=device)\n        # Predict\n        # As we are using the pipeline, we do not need to prepare x_test (done inside the pipeline)\n        # However, we still need to set the tokenizer params (truncate &amp; padding)\n        tokenizer_kwargs = {'padding': False, 'truncation': True}\n        results = np.array(self.pipe(x_test, **tokenizer_kwargs))\n        predicted_proba = np.array([[x['score'] for x in x] for x in results])\n        return predicted_proba\n\n    def _prepare_x_train(self, x_train, y_train_dummies) -&gt; Dataset:\n        '''Prepares the input data for the model - train\n\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            (datasets.Dataset): Prepared dataset\n        '''\n        # TMP FIX : https://github.com/OSS-Pole-Emploi/gabarit/issues/98\n        # We can't call this function if the tokenizer is not set. We will pby change this object to a property.\n        # This isn't really a problem as this function should not be called outside the class &amp; tokenizer is set in the fit function.\n        if self.tokenizer is None:\n            self.tokenizer = self._get_tokenizer()\n        # Check np format (should be the case if using fit)\n        if not isinstance(x_train, np.ndarray):\n            x_train = np.array(x_train)\n        if not isinstance(y_train_dummies, np.ndarray):\n            y_train_dummies = np.array(y_train_dummies)\n        # It seems that HF does not manage dummies targets for non multilabel\n        if not self.multi_label:\n            labels = np.argmax(y_train_dummies, axis=-1).astype(int).tolist()\n        else:\n            labels = y_train_dummies.astype(np.float32).tolist()\n        return Dataset.from_dict({'text': x_train.tolist(), 'label': labels}).map(self._tokenize_function, batched=True)\n\n    def _prepare_x_valid(self, x_valid, y_valid_dummies) -&gt; Dataset:\n        '''Prepares the input data for the model - valid\n\n        Args:\n            x_valid (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            (datasets.Dataset): Prepared dataset\n        '''\n        # Same as train (we don't fit any tokenizer)\n        return self._prepare_x_train(x_valid, y_valid_dummies)\n\n    def _prepare_x_test(self, x_test) -&gt; Dataset:\n        '''Prepares the input data for the model - test\n\n        Args:\n            x_test (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            (datasets.Dataset): Prepared dataset\n        '''\n        # Check np format\n        if not isinstance(x_test, np.ndarray):\n            x_test = np.array(x_test)\n        # /!\\ We don't use it as we are using a TextClassificationPipeline\n        # yet we are leaving this here in case we need it later\n        return Dataset.from_dict({'text': x_test.tolist()}).map(self._tokenize_function, batched=True)\n\n    def _tokenize_function(self, examples: Dict[str, List]) -&gt; BatchEncoding:\n        '''Tokenizes input data\n\n        Args:\n            examples (Batch): input data (Dataset Batch)\n        Returns:\n            BatchEncoding: tokenized data\n        '''\n        # Padding to False as we will use a Trainer and a DataCollatorWithPadding that will manage padding for us (better limit the memory impact)\n        # We leave max_length to None -&gt; backup on model max length\n        # https://stackoverflow.com/questions/74657367/how-do-i-know-which-parameters-to-use-with-a-pretrained-tokenizer\n        return self.tokenizer(examples[\"text\"], padding=False, truncation=True)\n\n    def _get_model(self, model_path: str = None, num_labels: int = None) -&gt; Any:\n        '''Gets a model structure - returns the instance model instead if already defined\n\n        Returns:\n            (Any): a HF model\n        '''\n        # Return model if already set\n        if self.model is not None:\n            return self.model\n        # We must use a random generator since the from_pretrained method apparently use some random\n        generator = torch.Generator()\n        if self.random_seed is not None:\n            generator.manual_seed(self.random_seed)\n        with torch.random.fork_rng():\n            torch.random.set_rng_state(generator.get_state())\n            model = AutoModelForSequenceClassification.from_pretrained(\n                    self.transformer_name if model_path is None else model_path,\n                    num_labels=len(self.list_classes) if num_labels is None else num_labels,\n                    problem_type=\"multi_label_classification\" if self.multi_label else \"single_label_classification\",\n                    cache_dir=HF_CACHE_DIR)\n\n        # Set model on gpu if available\n        model = model.to('cuda') if self._is_gpu_activated() else model.to('cpu')\n        return model\n\n    def _get_tokenizer(self, model_path: str = None) -&gt; PreTrainedTokenizer:\n        '''Gets a tokenizer\n\n        Returns:\n            (PreTrainedTokenizer): a HF tokenizer\n        '''\n        # Return tokenizer if already set\n        if self.tokenizer is not None:\n            return self.tokenizer\n\n        tokenizer = AutoTokenizer.from_pretrained(self.transformer_name if model_path is None else model_path,\n                                                  cache_dir=HF_CACHE_DIR)\n\n        if self.model_max_length:\n            tokenizer.model_max_length = self.model_max_length\n\n        # If the model name is not in tokenizer.max_model_input_sizes it is likely that the attribute model_max_length is not well\n        # initialized. If it is set to VERY_LARGE_INTEGER we warn the user that there is a risk of errors with long sequences\n        elif self.transformer_name not in tokenizer.max_model_input_sizes and tokenizer.model_max_length == VERY_LARGE_INTEGER:\n            self.logger.warning(f\"The model name '{self.transformer_name}' is not present in tokenizer.max_model_input_sizes : '{tokenizer.max_model_input_sizes}' \"\n                                f\"and tokenizer.model_max_length is set to VERY_LARGE_INTEGER. You may encounter errors with long sequences. \"\n                                f\"see. https://huggingface.co/transformers/v4.0.1/main_classes/tokenizer.html?highlight=very_large_integer#transformers.PreTrainedTokenizer\")\n\n        return tokenizer\n\n    def _get_optimizers(self) -&gt; Tuple[Any, Any]:\n        '''Fonction to define the Trainer optimizers\n           -&gt; per default return (None, None), i.e. default optimizers (cf HF Trainer doc)\n\n        Returns:\n            Tuple (Optimizer, LambdaLR): An optimizer/scheduler couple\n        '''\n        # e.g.\n        # Here, your custom Optimizer / scheduler couple\n        # (check https://huggingface.co/docs/transformers/v4.24.0/en/main_classes/optimizer_schedules)\n        return (None, None)\n\n    def _compute_metrics_mono_label(self, eval_pred: EvalPrediction) -&gt; dict:\n        '''Computes some metrics for mono label cases\n\n        Args:\n            eval_pred: predicted &amp; ground truth values to be considered\n        Returns:\n            dict: dictionnary with computed metrics\n        '''\n        # Load metrics\n        metric_accuracy = load_metric(hf_metrics.accuracy.__file__)\n        metric_precision = load_metric(hf_metrics.precision.__file__)\n        metric_recall = load_metric(hf_metrics.recall.__file__)\n        metric_f1 = load_metric(hf_metrics.f1.__file__)\n        # Get predictions\n        logits, labels = eval_pred\n        predictions = np.argmax(logits, axis=-1)\n\n        # Compute metrics\n        accuracy = metric_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n        precision = metric_precision.compute(predictions=predictions, references=labels, average='weighted')[\"precision\"]\n        recall = metric_recall.compute(predictions=predictions, references=labels, average='weighted')[\"recall\"]\n        f1 = metric_f1.compute(predictions=predictions, references=labels, average='weighted')[\"f1\"]\n        # Return dict of metrics\n        return {'accuracy': accuracy, 'weighted_precision': precision, 'weighted_recall': recall, 'weighted_f1': f1}\n\n    def _compute_metrics_multi_label(self, eval_pred: EvalPrediction) -&gt; dict:\n        '''Computes some metrics for mono label cases\n\n        Args:\n            eval_pred: predicted &amp; ground truth values to be considered\n        Returns:\n            dict: dictionnary with computed metrics\n        '''\n        # Sigmoid activation (multi_label)\n        sigmoid = torch.nn.Sigmoid()\n        # Get probas\n        logits, labels = eval_pred\n        probas = sigmoid(torch.Tensor(logits))\n        # Get predictions (probas &gt;= 0.5)\n        predictions = np.zeros(probas.shape)\n        predictions[np.where(probas &gt;= 0.5)] = 1\n        # Compute metrics (we can't use HF metrics, it sucks)\n        accuracy = accuracy_score(y_true=labels, y_pred=predictions)  # Must be exact match on all labels\n        f1 = f1_score(y_true=labels, y_pred=predictions, average='weighted')\n        precision = precision_score(y_true=labels, y_pred=predictions, average='weighted')\n        recall = recall_score(y_true=labels, y_pred=predictions, average='weighted')\n        # return as dictionary\n        return {'accuracy': accuracy, 'weighted_precision': precision, 'weighted_recall': recall, 'weighted_f1': f1}\n\n    def _plot_metrics_and_loss(self, fit_history) -&gt; None:\n        '''Plots TrainOutput, for legacy and compatibility purpose\n\n        Arguments:\n            fit_history (list) : fit history - actually list of logs\n        '''\n        # Manage dir\n        plots_path = os.path.join(self.model_dir, 'plots')\n        if not os.path.exists(plots_path):\n            os.makedirs(plots_path)\n\n        # Rework fit_history to better match Keras fit history\n        fit_history_dict: Dict[str, list] = {}\n        for log in fit_history:\n            for key, value in log.items():\n                if key not in fit_history_dict.keys():\n                    fit_history_dict[key] = [value]\n                else:\n                    fit_history_dict[key] += [value]\n\n        # Get a dictionnary of possible metrics/loss plots\n        metrics_dir = {\n            'loss': ['Loss', 'loss'],\n            'accuracy': ['Accuracy', 'accuracy'],\n            'weighted_f1': ['Weighted F1-score', 'weighted_f1_score'],\n            'weighted_precision': ['Weighted Precision', 'weighted_precision'],\n            'weighted_recall': ['Weighted Recall', 'weighted_recall'],\n        }\n\n        # Plot each available metric\n        for metric in metrics_dir.keys():\n            if any([f'{dataset}_{metric}' in fit_history_dict.keys() for dataset in ['train_metrics', 'eval']]):\n                title = metrics_dir[metric][0]\n                filename = metrics_dir[metric][1]\n                plt.figure(figsize=(10, 8))\n                legend = []\n                for dataset in ['train_metrics', 'eval']:\n                    if f'{dataset}_{metric}' in fit_history_dict.keys():\n                        plt.plot(fit_history_dict[f'{dataset}_{metric}'])\n                        legend += ['Train'] if dataset == 'train_metrics' else ['Validation']\n                plt.title(f\"Model {title}\")\n                plt.ylabel(title)\n                plt.xlabel('Epoch')\n                plt.legend(legend, loc='upper left')\n                # Save\n                filename = f\"{filename}.jpeg\"\n                plt.savefig(os.path.join(plots_path, filename))\n\n                # Close figures\n                plt.close('all')\n\n    @no_type_check  # We do not check the type, because it is complicated with managing custom_objects_str\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save configuration JSON\n        if json_data is None:\n            json_data = {}\n\n        json_data['librairie'] = 'huggingface'\n        json_data['batch_size'] = self.batch_size\n        json_data['epochs'] = self.epochs\n        json_data['validation_split'] = self.validation_split\n        json_data['patience'] = self.patience\n        json_data['transformer_name'] = self.transformer_name\n        json_data['transformer_params'] = self.transformer_params\n        json_data['trainer_params'] = self.trainer_params\n        json_data['model_max_length'] = self.model_max_length\n\n        # Add model structure if not none\n        if self.model is not None:\n            json_data['hf_model'] = self.model.__repr__()\n\n        if '_get_model' not in json_data.keys():\n            json_data['_get_model'] = pickle.source.getsourcelines(self._get_model)[0]\n        if '_get_tokenizer' not in json_data.keys():\n            json_data['_get_tokenizer'] = pickle.source.getsourcelines(self._get_tokenizer)[0]\n\n        # Save strategy :\n        # - HuggingFace model &amp; tokenizer are already saved in the fit() function\n        # - We don't want them in the .pkl as they are heavy &amp; already saved\n        # - Also get rid of the pipe (takes too much disk space for nothing),\n        #   will be reloaded automatically at first call to predict functions\n        hf_model = self.model\n        hf_tokenizer = self.tokenizer\n        pipe = self.pipe\n        self.model = None\n        self.tokenizer = None\n        self.pipe = None\n        super().save(json_data=json_data)\n        self.model = hf_model\n        self.tokenizer = hf_tokenizer\n        self.pipe = pipe\n\n    def _hook_post_load_model_pkl(self):\n        '''Manages a model specificities post load from a pickle file (i.e. not from standalone files)\n\n        Raises:\n            FileNotFoundError: If the HF model directory does not exist\n            FileNotFoundError: If the HF tokenizer directory does not exist\n        '''\n        # Paths\n        hf_model_dir = os.path.join(self.model_dir, 'hf_model')\n        hf_tokenizer_dir = os.path.join(self.model_dir, 'hf_tokenizer')\n\n        # Manage errors\n        if not os.path.isdir(hf_model_dir):\n            raise FileNotFoundError(f\"Can't find HF model directory ({hf_model_dir})\")\n        if not os.path.isdir(hf_tokenizer_dir):\n            raise FileNotFoundError(f\"Can't find HF tokenizer directory ({hf_tokenizer_dir})\")\n\n        # Loading the model\n        self.model = self._get_model(hf_model_dir)\n        # Loading the tokenizer\n        self.tokenizer = self._get_tokenizer(hf_tokenizer_dir)\n\n    @classmethod\n    def _init_new_instance_from_configs(cls, configs):\n        '''Inits a new instance from a set of configurations\n\n        Args:\n            configs: a set of configurations of a model to be reloaded\n        Returns:\n            ModelClass: the newly generated class\n        '''\n        # Call parent\n        model = super()._init_new_instance_from_configs(configs)\n\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['batch_size', 'epochs', 'validation_split', 'patience',\n                          'transformer_name', 'transformer_params', 'trainer_params', 'model_max_length']:\n            setattr(model, attribute, configs.get(attribute, getattr(model, attribute)))\n\n        # Return the new model\n        return model\n\n    def _load_standalone_files(self, default_model_dir: Union[str, None] = None, hf_model_dir_path: Union[str, None] = None,\n                               hf_tokenizer_dir_path: Union[str, None] = None, *args, **kwargs):\n        '''Loads standalone files for a newly created model via _init_new_instance_from_configs\n\n        Kwargs:\n            default_model_dir (str): a path to look for default file paths\n                                     If None, standalone files path should all be provided\n            hf_model_dir_path (str): path to HF model directory.\n                                If None, we'll use the default path if default_model_dir is not None\n            hf_tokenizer_dir_path (str): path to HF tokenizer directory.\n                                    If None, we'll use the default path if default_model_dir is not None\n        Raises:\n            ValueError: If at least one path is not specified and can't be inferred\n            FileNotFoundError: If the HF model directory does not exist\n            FileNotFoundError: If the HF tokenizer directory does not exist\n        '''\n        # Check if we are able to get all needed paths\n        if default_model_dir is None and None in [hf_model_dir_path, hf_tokenizer_dir_path]:\n            raise ValueError(\"At least one path is not specified and can't be inferred\")\n\n        # Retrieve file paths\n        if hf_model_dir_path is None:\n            hf_model_dir_path = os.path.join(default_model_dir, \"hf_model\")\n        if hf_tokenizer_dir_path is None:\n            hf_tokenizer_dir_path = os.path.join(default_model_dir, \"hf_tokenizer\")\n\n        # Check paths exists\n        if not os.path.isdir(hf_model_dir_path):\n            raise FileNotFoundError(f\"Can't find HF model directory ({hf_model_dir_path})\")\n        if not os.path.isdir(hf_tokenizer_dir_path):\n            raise FileNotFoundError(f\"Can't find HF tokenizer directory ({hf_tokenizer_dir_path})\")\n\n        # Reload model &amp; tokenizer\n        self.model = self._get_model(hf_model_dir_path)\n        self.tokenizer = self._get_tokenizer(hf_tokenizer_dir_path)\n\n        # Save hf folders in new folder (as this is skipped in save function)\n        new_hf_model_dir_path = os.path.join(self.model_dir, 'hf_model')\n        new_hf_tokenizer_dir_path = os.path.join(self.model_dir, 'hf_tokenizer')\n        shutil.copytree(hf_model_dir_path, new_hf_model_dir_path)\n        shutil.copytree(hf_tokenizer_dir_path, new_hf_tokenizer_dir_path)\n\n    def _is_gpu_activated(self) -&gt; bool:\n        '''Checks if a GPU is used\n\n        Returns:\n            bool: whether GPU is available or not\n        '''\n        # Check for available GPU devices\n        return torch.cuda.is_available()\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_huggingface/#template_nlp.models_training.model_huggingface.ModelHuggingFace.__init__","title":"<code>__init__(batch_size=8, epochs=99, validation_split=0.2, patience=5, transformer_name='Geotrend/distilbert-base-fr-cased', transformer_params=None, trainer_params=None, model_max_length=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass for more arguments)</p> Kwargs <p>batch_size (int): Batch size epochs (int): Number of epochs validation_split (float): Percentage for the validation set split     Only used if no input validation set when fitting patience (int): Early stopping patience transformer_name (str) : The name of the transformer backbone to use transformer_params (dict): Parameters used by the Transformer model.     The purpose of this dictionary is for the user to use it as they wants in the _get_model function     This parameter was initially added in order to do an hyperparameters search trainer_params (dict): A set of parameters to be use by the Trainer. It is recommended to use the default params (leave this empty).</p> Source code in <code>template_nlp/models_training/model_huggingface.py</code> <pre><code>def __init__(self, batch_size: int = 8, epochs: int = 99, validation_split: float = 0.2, patience: int = 5,\n             transformer_name: str = 'Geotrend/distilbert-base-fr-cased', transformer_params: Union[dict, None] = None,\n             trainer_params: Union[dict, None] = None, model_max_length: int = None, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelClass for more arguments)\n\n    Kwargs:\n        batch_size (int): Batch size\n        epochs (int): Number of epochs\n        validation_split (float): Percentage for the validation set split\n            Only used if no input validation set when fitting\n        patience (int): Early stopping patience\n        transformer_name (str) : The name of the transformer backbone to use\n        transformer_params (dict): Parameters used by the Transformer model.\n            The purpose of this dictionary is for the user to use it as they wants in the _get_model function\n            This parameter was initially added in order to do an hyperparameters search\n        trainer_params (dict): A set of parameters to be use by the Trainer. It is recommended to use the default params (leave this empty).\n    '''\n    # TODO: learning rate should be an attribute !\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Param. model\n    self.batch_size = batch_size\n    self.epochs = epochs\n    self.validation_split = validation_split\n    self.patience = patience\n    self.transformer_name = transformer_name\n    self.model_max_length = model_max_length\n\n    # transformer_params has no use as of 14/12/2022\n    # we still leave it for compatibility with Keras models and future usage\n    self.transformer_params = transformer_params\n\n    # Trainer params\n    if trainer_params is None:\n        trainer_params = {\n            'output_dir': self.model_dir,\n            'learning_rate': 2e-5,\n            'per_device_train_batch_size': self.batch_size,\n            'per_device_eval_batch_size': self.batch_size,\n            'num_train_epochs': self.epochs,\n            'weight_decay': 0.0,\n            'evaluation_strategy': 'epoch',\n            'save_strategy': 'epoch',\n            'logging_strategy': 'epoch',\n            'save_total_limit': 1,\n            'load_best_model_at_end': True\n        }\n    # TODO: maybe we should keep the default dict &amp; only add/replace keys in provided dict ?\n\n    # By default huggingface uses seed = 42 if not specified\n    if 'seed' not in trainer_params:\n        trainer_params['seed'] = self.random_seed if self.random_seed is not None else 42\n    self.trainer_params = trainer_params\n\n    # Model set on fit or on reload\n    self.model: Any = None\n    self.pipe: Any = None  # Set on first predict\n\n    # Tokenizer set on fit or on reload\n    self.tokenizer: Any = None\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_huggingface/#template_nlp.models_training.model_huggingface.ModelHuggingFace.fit","title":"<code>fit(x_train, y_train, x_valid=None, y_valid=None, with_shuffle=True, **kwargs)</code>","text":"<p>Fits the model</p> <p>Parameters:</p> Name Type Description Default <code>x_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <code>y_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_targets]</p> required <code>x_valid</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> <code>None</code> <code>y_valid</code> <code>?</code> <p>Array-like, shape = [n_samples, n_targets]</p> <code>None</code> <p>Kwargs:     with_shuffle (bool): If x, y must be shuffled before fitting         Experimental: We must verify if it works as intended depending on the formats of x and y         This should be used if y is not shuffled as the split_validation takes the lines in order.         Thus, the validation set might get classes which are not in the train set ... Raises:     ValueError: If different classes when comparing an already fitted model and a new dataset</p> Source code in <code>template_nlp/models_training/model_huggingface.py</code> <pre><code>def fit(self, x_train, y_train, x_valid=None, y_valid=None, with_shuffle: bool = True, **kwargs) -&gt; None:\n    '''Fits the model\n\n    Args:\n        x_train (?): Array-like, shape = [n_samples, n_features]\n        y_train (?): Array-like, shape = [n_samples, n_targets]\n        x_valid (?): Array-like, shape = [n_samples, n_features]\n        y_valid (?): Array-like, shape = [n_samples, n_targets]\n    Kwargs:\n        with_shuffle (bool): If x, y must be shuffled before fitting\n            Experimental: We must verify if it works as intended depending on the formats of x and y\n            This should be used if y is not shuffled as the split_validation takes the lines in order.\n            Thus, the validation set might get classes which are not in the train set ...\n    Raises:\n        ValueError: If different classes when comparing an already fitted model and a new dataset\n    '''\n    ##############################################\n    # Manage retrain\n    ##############################################\n\n    # If a model has already been fitted, we make a new folder in order not to overwrite the existing one !\n    # And we save the old conf\n    if self.trained:\n        # Get src files to save\n        src_files = [os.path.join(self.model_dir, \"configurations.json\")]\n        if self.nb_fit &gt; 1:\n            for i in range(1, self.nb_fit):\n                src_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n        # Change model dir\n        self.model_dir = self._get_new_model_dir()\n        # Get dst files\n        dst_files = [os.path.join(self.model_dir, f\"configurations_fit_{self.nb_fit}.json\")]\n        if self.nb_fit &gt; 1:\n            for i in range(1, self.nb_fit):\n                dst_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n        # Copies\n        for src, dst in zip(src_files, dst_files):\n            try:\n                shutil.copyfile(src, dst)\n            except Exception as e:\n                self.logger.error(f\"Impossible to copy {src} to {dst}\")\n                self.logger.error(\"We still continue ...\")\n                self.logger.error(repr(e))\n\n    ##############################################\n    # Prepare x_train, x_valid, y_train &amp; y_valid\n    # Also extract list of classes\n    ##############################################\n\n    # If not multilabel, transform y_train as dummies (should already be the case for multi-labels)\n    if not self.multi_label:\n        # If len(array.shape)==2, we flatten the array if the second dimension is useless\n        if isinstance(y_train, np.ndarray) and len(y_train.shape) == 2 and y_train.shape[1] == 1:\n            y_train = np.ravel(y_train)\n        if isinstance(y_valid, np.ndarray) and len(y_valid.shape) == 2 and y_valid.shape[1] == 1:\n            y_valid = np.ravel(y_valid)\n        # Transformation dummies\n        y_train_dummies = pd.get_dummies(y_train)\n        y_valid_dummies = pd.get_dummies(y_valid) if y_valid is not None else None\n        # Important : get_dummies reorder the columns in alphabetical order\n        # Thus, there is no problem if we fit again on a new dataframe with shuffled data\n        list_classes = list(y_train_dummies.columns)\n        # FIX: valid test might miss some classes, hence we need to add them back to y_valid_dummies\n        if y_valid_dummies is not None and y_train_dummies.shape[1] != y_valid_dummies.shape[1]:\n            for cl in list_classes:\n                # Add missing columns\n                if cl not in y_valid_dummies.columns:\n                    y_valid_dummies[cl] = 0\n            y_valid_dummies = y_valid_dummies[list_classes]  # Reorder\n    # Else keep it as it is\n    else:\n        y_train_dummies = y_train\n        y_valid_dummies = y_valid\n        if hasattr(y_train_dummies, 'columns'):\n            list_classes = list(y_train_dummies.columns)\n        else:\n            self.logger.warning(\n                \"Can't read the name of the columns of y_train -&gt; inverse transformation won't be possible\"\n            )\n            # We still create a list of classes in order to be compatible with other functions\n            list_classes = [str(_) for _ in range(pd.DataFrame(y_train_dummies).shape[1])]\n\n    # Set dict_classes based on list classes\n    dict_classes = {i: col for i, col in enumerate(list_classes)}\n\n    # Validate classes if already trained, else set them\n    if self.trained:\n        if self.list_classes != list_classes:\n            raise ValueError(\"Error: the new dataset does not match with the already fitted model\")\n        if self.dict_classes != dict_classes:\n            raise ValueError(\"Error: the new dataset does not match with the already fitted model\")\n    else:\n        self.list_classes = list_classes\n        self.dict_classes = dict_classes\n\n    # Shuffle x, y if wanted\n    # It is advised as validation_split from keras does not shufle the data\n    # Hence we might have classes in the validation data that we never met in the training data\n    rng = np.random.RandomState(self.random_seed)\n    if with_shuffle:\n        p = rng.permutation(len(x_train))\n        x_train = np.array(x_train)[p]\n        y_train_dummies = np.array(y_train_dummies)[p]\n    # Else still transform to numpy array\n    else:\n        x_train = np.array(x_train)\n        y_train_dummies = np.array(y_train_dummies)\n\n    # Also get y_valid_dummies as numpy\n    y_valid_dummies = np.array(y_valid_dummies)\n\n    # If no valid set, split train set according to validation_split\n    if y_valid is None:\n        self.logger.warning(f\"Warning, no validation set. The training set will be splitted (validation fraction = {self.validation_split})\")\n        x_train, x_valid, y_train_dummies, y_valid_dummies = train_test_split(x_train, y_train_dummies, test_size=self.validation_split,\n                                                                              random_state=self.random_seed)\n\n    ##############################################\n    # Get model &amp; prepare datasets\n    ##############################################\n\n    # Get model (if already fitted, _get_model returns instance model)\n    self.model = self._get_model(num_labels=y_train_dummies.shape[1])\n\n    # Get tokenizer (if already fitted, _get_tokenizer returns instance tokenizer)\n    self.tokenizer = self._get_tokenizer()\n\n    # Preprocess datasets\n    train_dataset = self._prepare_x_train(x_train, y_train_dummies)\n    valid_dataset = self._prepare_x_valid(x_valid, y_valid_dummies)\n\n    ##############################################\n    # Fit\n    ##############################################\n\n    # Fit\n    try:\n        # TODO: remove the checkpoints !\n        # Prepare trainer\n        trainer = Trainer(\n            model=self.model,\n            args=TrainingArguments(**self.trainer_params),\n            train_dataset=train_dataset,\n            eval_dataset=valid_dataset,\n            tokenizer=self.tokenizer,  # Only use for padding, dataset are already preprocessed. Pby not needed as we define a collator.\n            data_collator=DataCollatorWithPadding(tokenizer=self.tokenizer),  # Pad batches\n            compute_metrics=self._compute_metrics_mono_label if not self.multi_label else self._compute_metrics_multi_label,\n            optimizers=self._get_optimizers(),\n        )\n        # Add callbacks\n        trainer.add_callback(MetricsTrainCallback(trainer))\n        trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=self.patience))\n        # Fit\n        trainer.train()\n        # Save model &amp; tokenizer\n        hf_model_dir = os.path.join(self.model_dir, 'hf_model')\n        hf_tokenizer_dir = os.path.join(self.model_dir, 'hf_tokenizer')\n        self.model.save_pretrained(save_directory=hf_model_dir)\n        self.tokenizer.save_pretrained(save_directory=hf_tokenizer_dir)\n        # Remove checkpoint dir if save total limit is set to 1 (no need to keep this as we resave the model)\n        if self.trainer_params.get('save_total_limit', None) == 1:\n            checkpoint_dirs = [_ for _ in os.listdir(self.model_dir) if _.startswith('checkpoint-')]\n            if len(checkpoint_dirs) == 0:\n                self.logger.warning(\"Can't find a checkpoint dir to be removed.\")\n            else:\n                for checkpoint_dir in checkpoint_dirs:\n                    shutil.rmtree(os.path.join(self.model_dir, checkpoint_dir))\n    except (RuntimeError, SystemError, SystemExit, EnvironmentError, KeyboardInterrupt, Exception) as e:\n        self.logger.error(repr(e))\n        raise RuntimeError(\"Error during model training\")\n\n    # Print accuracy &amp; loss if level_save &gt; 'LOW'\n    if self.level_save in ['MEDIUM', 'HIGH']:\n        # Plot accuracy\n        fit_history = trainer.state.log_history\n        self._plot_metrics_and_loss(fit_history)\n        # Reload best model ?\n        # Default trainer has load_best_model_at_end = True\n        # Hence we consider the best model is already reloaded\n\n    # Set trained\n    self.trained = True\n    self.nb_fit += 1\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_huggingface/#template_nlp.models_training.model_huggingface.ModelHuggingFace.predict","title":"<code>predict(x_test, return_proba=False, **kwargs)</code>","text":"<p>Predictions on test set</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples]</p> required <p>Kwargs:     return_proba (bool): If the function should return the probabilities instead of the classes Returns:     (np.ndarray): Array, shape = [n_samples, n_classes]</p> Source code in <code>template_nlp/models_training/model_huggingface.py</code> <pre><code>@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict(self, x_test, return_proba: bool = False, **kwargs) -&gt; np.ndarray:\n    '''Predictions on test set\n\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples]\n    Kwargs:\n        return_proba (bool): If the function should return the probabilities instead of the classes\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n    # Predict probas\n    predicted_proba = self.predict_proba(x_test)\n\n    # We return the probabilities if wanted\n    if return_proba:\n        return predicted_proba\n\n    # Finally, we get the classes predictions\n    return self.get_classes_from_proba(predicted_proba)\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_huggingface/#template_nlp.models_training.model_huggingface.ModelHuggingFace.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts probabilities on the test dataset</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples, n_features]</p> required <p>Returns:     (np.ndarray): Array, shape = [n_samples, n_classes]</p> Source code in <code>template_nlp/models_training/model_huggingface.py</code> <pre><code>@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n    '''Predicts probabilities on the test dataset\n\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n    # Does not work with np array nor pandas Series\n    if isinstance(x_test, (np.ndarray, pd.Series)):\n        x_test = x_test.tolist()\n    # Prepare predict\n    if self.model.training:\n        self.model.eval()\n    if self.pipe is None:\n        # Set model on gpu if available\n        self.model = self.model.to('cuda') if self._is_gpu_activated() else self.model.to('cpu')\n        device = 0 if self._is_gpu_activated() else -1\n        self.pipe = TextClassificationPipeline(model=self.model, tokenizer=self.tokenizer, return_all_scores=True, device=device)\n    # Predict\n    # As we are using the pipeline, we do not need to prepare x_test (done inside the pipeline)\n    # However, we still need to set the tokenizer params (truncate &amp; padding)\n    tokenizer_kwargs = {'padding': False, 'truncation': True}\n    results = np.array(self.pipe(x_test, **tokenizer_kwargs))\n    predicted_proba = np.array([[x['score'] for x in x] for x in results])\n    return predicted_proba\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_huggingface/#template_nlp.models_training.model_huggingface.ModelHuggingFace.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_nlp/models_training/model_huggingface.py</code> <pre><code>@no_type_check  # We do not check the type, because it is complicated with managing custom_objects_str\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save configuration JSON\n    if json_data is None:\n        json_data = {}\n\n    json_data['librairie'] = 'huggingface'\n    json_data['batch_size'] = self.batch_size\n    json_data['epochs'] = self.epochs\n    json_data['validation_split'] = self.validation_split\n    json_data['patience'] = self.patience\n    json_data['transformer_name'] = self.transformer_name\n    json_data['transformer_params'] = self.transformer_params\n    json_data['trainer_params'] = self.trainer_params\n    json_data['model_max_length'] = self.model_max_length\n\n    # Add model structure if not none\n    if self.model is not None:\n        json_data['hf_model'] = self.model.__repr__()\n\n    if '_get_model' not in json_data.keys():\n        json_data['_get_model'] = pickle.source.getsourcelines(self._get_model)[0]\n    if '_get_tokenizer' not in json_data.keys():\n        json_data['_get_tokenizer'] = pickle.source.getsourcelines(self._get_tokenizer)[0]\n\n    # Save strategy :\n    # - HuggingFace model &amp; tokenizer are already saved in the fit() function\n    # - We don't want them in the .pkl as they are heavy &amp; already saved\n    # - Also get rid of the pipe (takes too much disk space for nothing),\n    #   will be reloaded automatically at first call to predict functions\n    hf_model = self.model\n    hf_tokenizer = self.tokenizer\n    pipe = self.pipe\n    self.model = None\n    self.tokenizer = None\n    self.pipe = None\n    super().save(json_data=json_data)\n    self.model = hf_model\n    self.tokenizer = hf_tokenizer\n    self.pipe = pipe\n</code></pre>"},{"location":"reference/template_nlp/models_training/utils_models/","title":"Utils models","text":""},{"location":"reference/template_nlp/models_training/utils_models/#template_nlp.models_training.utils_models.display_train_test_shape","title":"<code>display_train_test_shape(df_train, df_test, df_shape=None)</code>","text":"<p>Displays the size of a train/test split</p> <p>Parameters:</p> Name Type Description Default <code>df_train</code> <code>DataFrame</code> <p>Train dataset</p> required <code>df_test</code> <code>DataFrame</code> <p>Test dataset</p> required <p>Kwargs:     df_shape (int): Size of the initial dataset Raises:     ValueError: If the object df_shape is not positive</p> Source code in <code>template_nlp/models_training/utils_models.py</code> <pre><code>def display_train_test_shape(df_train: pd.DataFrame, df_test: pd.DataFrame, df_shape: Union[int, None] = None) -&gt; None:\n    '''Displays the size of a train/test split\n\n    Args:\n        df_train (pd.DataFrame): Train dataset\n        df_test (pd.DataFrame): Test dataset\n    Kwargs:\n        df_shape (int): Size of the initial dataset\n    Raises:\n        ValueError: If the object df_shape is not positive\n    '''\n    if df_shape is not None and df_shape &lt; 1:\n        raise ValueError(\"The object df_shape must be positive\")\n\n    # Process\n    if df_shape is None:\n        df_shape = df_train.shape[0] + df_test.shape[0]\n    logger.info(f\"There are {df_train.shape[0]} lines in the train dataset and {df_test.shape[0]} in the test dataset.\")\n    logger.info(f\"{round(100 * df_train.shape[0] / df_shape, 2)}% of data are in the train set\")\n    logger.info(f\"{round(100 * df_test.shape[0] / df_shape, 2)}% of data are in the test set\")\n</code></pre>"},{"location":"reference/template_nlp/models_training/utils_models/#template_nlp.models_training.utils_models.get_embedding","title":"<code>get_embedding(embedding_name='cc.fr.300.pkl')</code>","text":"<p>Loads an embedding previously saved as a .pkl file</p> <p>Parameters:</p> Name Type Description Default <code>embedding_name</code> <code>str</code> <p>Name of the embedding file (actually a path relative to template_nlp-data)</p> <code>'cc.fr.300.pkl'</code> <p>Raises:     FileNotFoundError: If the embedding file does not exist in template_nlp-data Returns:     dict: Loaded embedding</p> Source code in <code>template_nlp/models_training/utils_models.py</code> <pre><code>def get_embedding(embedding_name: str = \"cc.fr.300.pkl\") -&gt; Dict[str, np.ndarray]:\n    '''Loads an embedding previously saved as a .pkl file\n\n    Args:\n        embedding_name (str): Name of the embedding file (actually a path relative to template_nlp-data)\n    Raises:\n        FileNotFoundError: If the embedding file does not exist in template_nlp-data\n    Returns:\n        dict: Loaded embedding\n    '''\n    logger.info(\"Reloading an embedding ...\")\n\n    # Manage path\n    data_path = utils.get_data_path()\n    embedding_path = os.path.join(data_path, embedding_name)\n    if not os.path.exists(embedding_path):\n        logger.error(f\"The provided embedding file ({embedding_path}) does not exist.\")\n        logger.error(\"If you want to create one, you have to :\")\n        logger.error(\"    1. Download a fasttext embedding (e.g. French -&gt; https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.vec.gz\")\n        logger.error(\"    2. Use the 0_get_embedding_dict.py to generate the pkl. file\")\n        raise FileNotFoundError()\n\n    # Load embedding indexes\n    with open(embedding_path, 'rb') as f:\n        embedding_indexes = pickle.load(f)\n\n    # Return\n    return embedding_indexes\n</code></pre>"},{"location":"reference/template_nlp/models_training/utils_models/#template_nlp.models_training.utils_models.hierarchical_split","title":"<code>hierarchical_split(df, col, test_size=0.25, seed=None)</code>","text":"<p>Splits a DataFrame into train and test sets - Hierarchical strategy</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe containing the data</p> required <code>col</code> <code>str or int</code> <p>column on which to do the hierarchical split</p> required <p>Kwargs:     test_size (float): Proportion representing the size of the expected test set     seed (int): Random seed Raises:     ValueError: If the object test_size is not between 0 and 1 Returns:     DataFrame: Train dataframe     DataFrame: Test dataframe</p> Source code in <code>template_nlp/models_training/utils_models.py</code> <pre><code>def hierarchical_split(df: pd.DataFrame, col: Union[str, int], test_size: float = 0.25, seed: Union[int, None] = None) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    '''Splits a DataFrame into train and test sets - Hierarchical strategy\n\n    Args:\n        df (pd.DataFrame): Dataframe containing the data\n        col (str or int): column on which to do the hierarchical split\n    Kwargs:\n        test_size (float): Proportion representing the size of the expected test set\n        seed (int): Random seed\n    Raises:\n        ValueError: If the object test_size is not between 0 and 1\n    Returns:\n        DataFrame: Train dataframe\n        DataFrame: Test dataframe\n    '''\n    if not 0 &lt;= test_size &lt;= 1:\n        raise ValueError('The object test_size must be between 0 and 1')\n\n    # Hierarchical split\n    logger.info(\"Hierarchical split\")\n    modalities_train, _ = train_test_split(df[col].unique(), test_size=test_size, random_state=seed)\n    train_rows = df[col].isin(modalities_train)\n    df_train = df[train_rows]\n    df_test = df[~train_rows]\n\n    # Display\n    display_train_test_shape(df_train, df_test, df_shape=df.shape[0])\n\n    # Return\n    return df_train, df_test\n</code></pre>"},{"location":"reference/template_nlp/models_training/utils_models/#template_nlp.models_training.utils_models.load_model","title":"<code>load_model(model_dir, is_path=False, **kwargs)</code>","text":"<p>Loads a model from a path or a model name</p> <p>Parameters:</p> Name Type Description Default <code>model_dir</code> <code>str</code> <p>Name of the folder containing the model (e.g. model_autres_2019_11_07-13_43_19) It can also be an absolute path if is_path is set to True</p> required <p>Kwargs:     is_path (bool): If folder path instead of name (allows to load model from anywhere) Returns:     ModelClass: The loaded model     dict: The model configurations</p> Source code in <code>template_nlp/models_training/utils_models.py</code> <pre><code>def load_model(model_dir: str, is_path: bool = False, **kwargs) -&gt; Tuple[Any, dict]:\n    '''Loads a model from a path or a model name\n\n    Args:\n        model_dir (str): Name of the folder containing the model (e.g. model_autres_2019_11_07-13_43_19)\n            It can also be an absolute path if is_path is set to True\n    Kwargs:\n        is_path (bool): If folder path instead of name (allows to load model from anywhere)\n    Returns:\n        ModelClass: The loaded model\n        dict: The model configurations\n    '''\n    # Find model absolute path\n    base_folder = None if is_path else utils.get_models_path()\n    model_dir = utils.find_folder_path(model_dir, base_folder)\n\n    # Load model\n    model, model_conf = ModelClass.load_model(model_dir=model_dir, **kwargs)\n\n    # Return model &amp; its configs\n    return model, model_conf\n</code></pre>"},{"location":"reference/template_nlp/models_training/utils_models/#template_nlp.models_training.utils_models.normal_split","title":"<code>normal_split(df, test_size=0.25, seed=None)</code>","text":"<p>Splits a DataFrame into train and test sets</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe containing the data</p> required <p>Kwargs:     test_size (float): Proportion representing the size of the expected test set     seed (int): random seed Raises:     ValueError: If the object test_size is not between 0 and 1 Returns:     DataFrame: Train dataframe     DataFrame: Test dataframe</p> Source code in <code>template_nlp/models_training/utils_models.py</code> <pre><code>def normal_split(df: pd.DataFrame, test_size: float = 0.25, seed: Union[int, None] = None) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    '''Splits a DataFrame into train and test sets\n\n    Args:\n        df (pd.DataFrame): Dataframe containing the data\n    Kwargs:\n        test_size (float): Proportion representing the size of the expected test set\n        seed (int): random seed\n    Raises:\n        ValueError: If the object test_size is not between 0 and 1\n    Returns:\n        DataFrame: Train dataframe\n        DataFrame: Test dataframe\n    '''\n    if not 0 &lt;= test_size &lt;= 1:\n        raise ValueError('The object test_size must be between 0 and 1')\n\n    # Normal split\n    logger.info(\"Normal split\")\n    df_train, df_test = train_test_split(df, test_size=test_size, random_state=seed)\n\n    # Display\n    display_train_test_shape(df_train, df_test, df_shape=df.shape[0])\n\n    # Return\n    return df_train, df_test\n</code></pre>"},{"location":"reference/template_nlp/models_training/utils_models/#template_nlp.models_training.utils_models.predict","title":"<code>predict(content, model, model_conf, inference_batch_size=128, alternative_version=False, **kwargs)</code>","text":"<p>Gets predictions of a model on a content</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>Union[str, list]</code> <p>New content to be predicted</p> required <code>model</code> <code>ModelClass</code> <p>Model to use</p> required <code>model_conf</code> <code>dict</code> <p>Model configurations</p> required <p>Kwargs:     inference_batch_size (int): size (approximate) of batches     alternative_version (bool): If an alternative version (<code>tf.function</code> + <code>model.__call__</code>) must be used.         Should be faster with low nb of inputs. Only useful for Keras models.         We advise you to set <code>alternative_version</code> to True for APIs to avoid possible memory leaks with <code>model.predict</code> on newest TensorFlow.         https://github.com/tensorflow/tensorflow/issues/58676         Inferences will probably be way faster too. Returns:     list: a list of strings (resp. tuples) in case of mono-label (resp. multi-labels) classification predictions</p> Source code in <code>template_nlp/models_training/utils_models.py</code> <pre><code>def predict(content: Union[str, list], model, model_conf: dict, inference_batch_size: int = 128, alternative_version: bool = False, **kwargs) -&gt; list:\n    '''Gets predictions of a model on a content\n\n    Args:\n        content (Union[str, list]): New content to be predicted\n        model (ModelClass): Model to use\n        model_conf (dict): Model configurations\n    Kwargs:\n        inference_batch_size (int): size (approximate) of batches\n        alternative_version (bool): If an alternative version (`tf.function` + `model.__call__`) must be used.\n            Should be faster with low nb of inputs. Only useful for Keras models.\n            We advise you to set `alternative_version` to True for APIs to avoid possible memory leaks with `model.predict` on newest TensorFlow.\n            https://github.com/tensorflow/tensorflow/issues/58676\n            Inferences will probably be way faster too.\n    Returns:\n        list: a list of strings (resp. tuples) in case of mono-label (resp. multi-labels) classification predictions\n    '''\n    if isinstance(content, str):\n        content = [content]\n\n    # Get preprocessor\n    if 'preprocess_str' in model_conf.keys():\n        preprocess_str = model_conf['preprocess_str']\n    else:\n        preprocess_str = 'no_preprocess'\n    preprocessor = preprocess.get_preprocessor(preprocess_str)\n\n    # Preprocess\n    content = preprocessor(content)\n\n    # Get prediction (some models need an iterable)\n    predictions = model.predict(content, inference_batch_size=inference_batch_size, alternative_version=alternative_version)\n\n    # Return predictions with inverse transform when relevant\n    return model.inverse_transform(predictions)\n</code></pre>"},{"location":"reference/template_nlp/models_training/utils_models/#template_nlp.models_training.utils_models.predict_with_proba","title":"<code>predict_with_proba(content, model, model_conf, inference_batch_size=128, alternative_version=False, **kwargs)</code>","text":"<p>Gets predictions of a model on a content, with probabilities</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>Union[str, list]</code> <p>New content to be predicted</p> required <code>model</code> <code>ModelClass</code> <p>Model to use</p> required <code>model_conf</code> <code>dict</code> <p>Model configurations</p> required <p>Kwargs:     inference_batch_size (int): size (approximate) of batches     alternative_version (bool): If an alternative version (<code>tf.function</code> + <code>model.__call__</code>) must be used.         Should be faster with low nb of inputs. Only useful for Keras models.         We advise you to set <code>alternative_version</code> to True for APIs to avoid possible memory leaks with <code>model.predict</code> on newest TensorFlow.         https://github.com/tensorflow/tensorflow/issues/58676         Inferences will probably be way faster too. Returns:     MONO-LABEL CLASSIFICATION:         List[str]: predictions         List[float]: probabilities     MULTI-LABELS CLASSIFICATION:         List[tuple]: predictions         List[tuple]: probabilities</p> Source code in <code>template_nlp/models_training/utils_models.py</code> <pre><code>def predict_with_proba(content: Union[str, list], model, model_conf: dict, inference_batch_size: int = 128, alternative_version: bool = False,\n                       **kwargs) -&gt; Union[Tuple[List[str], List[float]], Tuple[List[tuple], List[tuple]]]:\n    '''Gets predictions of a model on a content, with probabilities\n\n    Args:\n        content (Union[str, list]): New content to be predicted\n        model (ModelClass): Model to use\n        model_conf (dict): Model configurations\n    Kwargs:\n        inference_batch_size (int): size (approximate) of batches\n        alternative_version (bool): If an alternative version (`tf.function` + `model.__call__`) must be used.\n            Should be faster with low nb of inputs. Only useful for Keras models.\n            We advise you to set `alternative_version` to True for APIs to avoid possible memory leaks with `model.predict` on newest TensorFlow.\n            https://github.com/tensorflow/tensorflow/issues/58676\n            Inferences will probably be way faster too.\n    Returns:\n        MONO-LABEL CLASSIFICATION:\n            List[str]: predictions\n            List[float]: probabilities\n        MULTI-LABELS CLASSIFICATION:\n            List[tuple]: predictions\n            List[tuple]: probabilities\n    '''\n    if isinstance(content, str):\n        content = [content]\n\n    # Get preprocessor\n    if 'preprocess_str' in model_conf.keys():\n        preprocess_str = model_conf['preprocess_str']\n    else:\n        preprocess_str = 'no_preprocess'\n    preprocessor = preprocess.get_preprocessor(preprocess_str)\n\n    # Preprocess\n    content = preprocessor(content)\n\n    # Get prediction (some models need an iterable)\n    # predictions is a ndarray of shape (n_samples, n_classes)\n    # probas is a ndarray of shape (n_samples, n_classes)\n    predictions, probas = model.predict_with_proba(content, inference_batch_size=inference_batch_size, alternative_version=alternative_version)\n\n    # Rework format :\n    if model.multi_label:\n        model_labels = np.array(model.list_classes)\n        all_preds = [tuple(np.compress(content_pred, model_labels)) for content_pred in predictions]\n        all_probs = [tuple(np.compress(content_pred, content_prob)) for content_pred, content_prob in zip(predictions, probas)]\n    else:\n        all_preds = model.inverse_transform(predictions)\n        all_probs = probas.max(axis=1)\n\n    # Return prediction &amp; proba\n    return all_preds, all_probs\n</code></pre>"},{"location":"reference/template_nlp/models_training/utils_models/#template_nlp.models_training.utils_models.preprocess_model_multilabel","title":"<code>preprocess_model_multilabel(df, y_col, classes=None)</code>","text":"<p>Prepares a dataframe for a multi-labels classification</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Training dataset This dataset must be preprocessed. Example:     # Group by &amp; apply tuple to y_col     x_cols = [col for col in list(df.columns) if col != y_col]     df = pd.DataFrame(df.groupby(x_cols)[y_col].apply(tuple))</p> required <code>y_col</code> <code>str or int</code> <p>Name of the column to be used for training - y</p> required <p>Kwargs:     classes (list): List of classes to consider Returns:     DataFrame: Dataframe for training     list: List of 'y' columns</p> Source code in <code>template_nlp/models_training/utils_models.py</code> <pre><code>def preprocess_model_multilabel(df: pd.DataFrame, y_col: Union[str, int], classes: Union[list, None] = None) -&gt; Tuple[pd.DataFrame, list]:\n    '''Prepares a dataframe for a multi-labels classification\n\n    Args:\n        df (pd.DataFrame): Training dataset\n            This dataset must be preprocessed.\n            Example:\n                # Group by &amp; apply tuple to y_col\n                x_cols = [col for col in list(df.columns) if col != y_col]\n                df = pd.DataFrame(df.groupby(x_cols)[y_col].apply(tuple))\n        y_col (str or int): Name of the column to be used for training - y\n    Kwargs:\n        classes (list): List of classes to consider\n    Returns:\n        DataFrame: Dataframe for training\n        list: List of 'y' columns\n    '''\n    # TODO: add possibility to have sparse output\n    logger.info(\"Preprocess dataframe for multi-labels model\")\n    # Process\n    logger.info(\"Preparing dataset for multi-labels format. Might take several minutes.\")\n    # /!\\ The reset_index is compulsory in order to have the same indexes between df, and MLB transformed values\n    df = df.reset_index(drop=True)\n    # Apply MLB\n    mlb = MultiLabelBinarizer(classes=classes)\n    df = df.assign(**pd.DataFrame(mlb.fit_transform(df[y_col]), columns=mlb.classes_))\n    # Return dataframe &amp; y_cols (i.e. classes)\n    return df, list(mlb.classes_)\n</code></pre>"},{"location":"reference/template_nlp/models_training/utils_models/#template_nlp.models_training.utils_models.remove_small_classes","title":"<code>remove_small_classes(df, col, min_rows=2)</code>","text":"<p>Deletes the classes with small numbers of elements</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe containing the data</p> required <code>col</code> <code>str | int</code> <p>Columns containing the classes</p> required <p>Kwargs:     min_rows (int): Minimal number of lines in the training set (default: 2) Raises:     ValueError: If the object min_rows is not positive Returns:     pd.DataFrame: New dataset</p> Source code in <code>template_nlp/models_training/utils_models.py</code> <pre><code>def remove_small_classes(df: pd.DataFrame, col: Union[str, int], min_rows: int = 2) -&gt; pd.DataFrame:\n    '''Deletes the classes with small numbers of elements\n\n    Args:\n        df (pd.DataFrame): Dataframe containing the data\n        col (str | int): Columns containing the classes\n    Kwargs:\n        min_rows (int): Minimal number of lines in the training set (default: 2)\n    Raises:\n        ValueError: If the object min_rows is not positive\n    Returns:\n        pd.DataFrame: New dataset\n    '''\n    if min_rows &lt; 1:\n        raise ValueError(\"The object min_rows must be positive\")\n\n    # Looking for classes with less than min_rows lines\n    v_count = df[col].value_counts()\n    classes_to_remove = list(v_count[v_count &lt; min_rows].index.values)\n    for cl in classes_to_remove:\n        logger.warning(f\"/!\\\\ /!\\\\ /!\\\\ Class {cl} has less than {min_rows} lines in the training set.\")\n        logger.warning(\"/!\\\\ /!\\\\ /!\\\\ This class is automatically removed from the dataset.\")\n    return df[~df[col].isin(classes_to_remove)]\n</code></pre>"},{"location":"reference/template_nlp/models_training/utils_models/#template_nlp.models_training.utils_models.search_hp_cv","title":"<code>search_hp_cv(model_cls, model_params, hp_params, scoring_fn, kwargs_fit, n_splits=5)</code>","text":"<p>Searches for hyperparameters</p> <p>Parameters:</p> Name Type Description Default <code>model_cls</code> <code>?</code> <p>Class of models on which to do a hyperparameters search</p> required <code>model_params</code> <code>dict</code> <p>Set of \"fixed\" parameters of the model (e.g. x_col, y_col). Must contain 'multi_label'.</p> required <code>hp_params</code> <code>dict</code> <p>Set of \"variable\" parameters on which to do a hyperparameters search</p> required <code>scoring_fn</code> <code>str or func</code> <p>Scoring function to maximize This function must take as input a dictionary containing metrics e.g. {'F1-Score': 0.85, 'Accuracy': 0.57, 'Precision': 0.64, 'Recall': 0.90}</p> required <code>kwargs_fit</code> <code>dict</code> <p>Set of kwargs to input in the fit function Must contain 'x_train' and 'y_train'</p> required <p>Kwargs:     n_splits (int): Number of folds to use Raises:     ValueError: If scoring_fn is not a known string     ValueError: If multi_label is not a key in model_params     ValueError: If x_train is not a key in kwargs_fit     ValueError: If y_train is not a key in kwargs_fit     ValueError: If model_params and hp_params share some keys     ValueError: If hp_params values are not the same length     ValueError: If the number of crossvalidation split is less or equal to 1 Returns:     ModelClass: best model to be \"fitted\" on the dataset</p> Source code in <code>template_nlp/models_training/utils_models.py</code> <pre><code>def search_hp_cv(model_cls, model_params: dict, hp_params: dict, scoring_fn: Union[str, Callable],\n                 kwargs_fit: dict, n_splits: int = 5):\n    '''Searches for hyperparameters\n\n    Args:\n        model_cls (?): Class of models on which to do a hyperparameters search\n        model_params (dict): Set of \"fixed\" parameters of the model (e.g. x_col, y_col).\n            Must contain 'multi_label'.\n        hp_params (dict): Set of \"variable\" parameters on which to do a hyperparameters search\n        scoring_fn (str or func): Scoring function to maximize\n            This function must take as input a dictionary containing metrics\n            e.g. {'F1-Score': 0.85, 'Accuracy': 0.57, 'Precision': 0.64, 'Recall': 0.90}\n        kwargs_fit (dict): Set of kwargs to input in the fit function\n            Must contain 'x_train' and 'y_train'\n    Kwargs:\n        n_splits (int): Number of folds to use\n    Raises:\n        ValueError: If scoring_fn is not a known string\n        ValueError: If multi_label is not a key in model_params\n        ValueError: If x_train is not a key in kwargs_fit\n        ValueError: If y_train is not a key in kwargs_fit\n        ValueError: If model_params and hp_params share some keys\n        ValueError: If hp_params values are not the same length\n        ValueError: If the number of crossvalidation split is less or equal to 1\n    Returns:\n        ModelClass: best model to be \"fitted\" on the dataset\n    '''\n    list_known_scoring = ['accuracy', 'f1', 'precision', 'recall']\n\n    #################\n    # Manage errors\n    #################\n\n    if isinstance(scoring_fn, str) and scoring_fn not in list_known_scoring:\n        raise ValueError(f\"The input {scoring_fn} is not a known value for scoring_fn (known values : {list_known_scoring})\")\n\n    if 'multi_label' not in model_params.keys():\n        raise ValueError(\"The key 'multi_label' must be present in the dictionary model_params\")\n\n    if 'x_train' not in kwargs_fit.keys():\n        raise ValueError(\"The key 'x_train' must be present in the dictionary kwargs_fit\")\n\n    if 'y_train' not in kwargs_fit.keys():\n        raise ValueError(\"The key 'y_train' must be present in the dictionary kwargs_fit\")\n\n    if any([k in hp_params.keys() for k in model_params.keys()]):\n        # A key can't be \"fixed\" and \"variable\"\n        raise ValueError(\"The dictionaries model_params and hp_params share at least one key\")\n\n    if len(set([len(_) for _ in hp_params.values()])) != 1:\n        raise ValueError(\"The values of hp_params must have the same length\")\n\n    if n_splits &lt;= 1:\n        raise ValueError(f\"The number of crossvalidation splits ({n_splits}) must be more than 1\")\n\n    #################\n    # Manage scoring\n    #################\n\n    # Get scoring functions\n    if scoring_fn == 'accuracy':\n        scoring_fn = lambda x: x['Accuracy']\n    elif scoring_fn == 'f1':\n        scoring_fn = lambda x: x['F1-Score']\n    elif scoring_fn == 'precision':\n        scoring_fn = lambda x: x['Precision']\n    elif scoring_fn == 'recall':\n        scoring_fn = lambda x: x['Recall']\n\n    #################\n    # Manage x_train &amp; y_train format\n    #################\n\n    if not isinstance(kwargs_fit['x_train'], (pd.DataFrame, pd.Series)):\n        kwargs_fit['x_train'] = pd.Series(kwargs_fit['x_train'].copy())\n\n    if not isinstance(kwargs_fit['y_train'], (pd.DataFrame, pd.Series)):\n        kwargs_fit['y_train'] = pd.Series(kwargs_fit['y_train'].copy())\n\n    #################\n    # Process\n    #################\n\n    # Loop on hyperparameters\n    nb_search = len(list(hp_params.values())[0])\n    logger.info(\"Beginning of hyperparameters search\")\n    logger.info(f\"Number of model fits : {nb_search} (search number) x {n_splits} (CV splits number) = {nb_search * n_splits}\")\n\n    # DataFrame for stocking metrics :\n    metrics_df = pd.DataFrame(columns=['index_params', 'index_fold', 'Score', 'Accuracy', 'F1-Score', 'Precision', 'Recall'])\n    for i in range(nb_search):\n\n        # Display informations\n        logger.info(f\"Search n\u00b0{i + 1}\")\n        tmp_hp_params = {k: v[i] for k, v in hp_params.items()}\n        logger.info(\"Tested hyperparameters : \")\n        logger.info(pprint.pformat(tmp_hp_params))\n\n        # Get folds (shuffle recommended since the classes could be ordered)\n        if model_params['multi_label']:\n            k_fold = KFold(n_splits=n_splits, shuffle=True)  # Can't stratify on multi-labels\n        else:\n            k_fold = StratifiedKFold(n_splits=n_splits, shuffle=True)\n\n        # Process each fold\n        for j, (train_index, valid_index) in enumerate(k_fold.split(kwargs_fit['x_train'], kwargs_fit['y_train'])):\n            logger.info(f\"Search n\u00b0{i + 1}/{nb_search} - fit n\u00b0{j + 1}/{n_splits}\")\n            # get tmp x, y\n            x_train, x_valid = kwargs_fit['x_train'].iloc[train_index], kwargs_fit['x_train'].iloc[valid_index]\n            y_train, y_valid = kwargs_fit['y_train'].iloc[train_index], kwargs_fit['y_train'].iloc[valid_index]\n            # Get tmp model\n            # Manage model_dir\n            tmp_model_dir = os.path.join(utils.get_models_path(), datetime.now().strftime(\"tmp_%Y_%m_%d-%H_%M_%S\"))\n            # The next line prioritize the last dictionary\n            # We force a temporary folder and a low save level (we only want the metrics)\n            model_tmp = model_cls(**{**model_params, **tmp_hp_params, **{'model_dir': tmp_model_dir, 'level_save': 'LOW'}})\n            # Setting log level to ERROR\n            model_tmp.logger.setLevel(logging.ERROR)\n            # Let's fit ! (priority to the last dictionary)\n            model_tmp.fit(**{**kwargs_fit, **{'x_train': x_train, 'y_train': y_train, 'x_valid': x_valid, 'y_valid': y_valid}})\n            # Let's predict !\n            y_pred = model_tmp.predict(x_valid)\n            # Get metrics !\n            metrics_func = model_tmp.get_metrics_simple_multilabel if model_tmp.multi_label else model_tmp.get_metrics_simple_monolabel\n            metrics_tmp = metrics_func(y_valid, y_pred)\n            metrics_tmp = metrics_tmp[metrics_tmp.Label == \"All\"].copy()  # Add .copy() to avoid pandas settingwithcopy\n            metrics_tmp[\"Score\"] = scoring_fn(metrics_tmp.iloc[0].to_dict())  # type: ignore\n            metrics_tmp[\"index_params\"] = i\n            metrics_tmp[\"index_fold\"] = j\n            metrics_tmp = metrics_tmp[metrics_df.columns]  # Keeping only the necessary columns\n            metrics_df = pd.concat([metrics_df, metrics_tmp], ignore_index=True)\n            # Delete the temporary model : the final model must be refitted on the whole dataset\n            del model_tmp\n            gc.collect()\n            shutil.rmtree(tmp_model_dir)\n        # Display score\n        logger.info(f\"Score for search n\u00b0{i + 1}: {metrics_df[metrics_df['index_params'] == i]['Score'].mean()}\")\n\n    # Metric agregation for all the folds\n    metrics_df = metrics_df.join(metrics_df[['index_params', 'Score']].groupby('index_params').mean().rename({'Score': 'mean_score'}, axis=1), on='index_params', how='left')\n\n    # Select the set of parameters with the best mean score (on the folds)\n    best_index = metrics_df[metrics_df.mean_score == metrics_df.mean_score.max()][\"index_params\"].values[0]\n    best_params = {k: v[best_index] for k, v in hp_params.items()}\n    logger.info(f\"Best results for the set of parameter n\u00b0{best_index + 1}: {pprint.pformat(best_params)}\")\n\n    # Instanciation of a model with the best parameters\n    best_model = model_cls(**{**model_params, **best_params})\n\n    # Save the metrics report of the hyperparameters search and the tested parameters\n    csv_path = os.path.join(best_model.model_dir, \"hyper_params_results.csv\")\n    metrics_df.to_csv(csv_path, sep=';', index=False, encoding='utf-8')\n    json_data = {\n        'model_params': model_params,\n        'scoring_fn': pickle.source.getsourcelines(scoring_fn)[0],\n        'n_splits': n_splits,\n        'hp_params_set': {i: {k: v[i] for k, v in hp_params.items()} for i in range(nb_search)},\n    }\n    json_path = os.path.join(best_model.model_dir, \"hyper_params_tested.json\")\n    with open(json_path, 'w', encoding='utf-8') as f:\n        json.dump(json_data, f, indent=4, cls=utils.NpEncoder)\n\n    # TODO: We are forced to reset the logging level which is linked to the class\n    best_model.logger.setLevel(logging.getLogger('template_nlp').getEffectiveLevel())\n\n    # Return model to be fitted\n    return best_model\n</code></pre>"},{"location":"reference/template_nlp/models_training/utils_models/#template_nlp.models_training.utils_models.stratified_split","title":"<code>stratified_split(df, col, test_size=0.25, seed=None)</code>","text":"<p>Splits a DataFrame into train and test sets - Stratified strategy</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe containing the data</p> required <code>col</code> <code>str or int</code> <p>column on which to do the stratified split</p> required <p>Kwargs:     test_size (float): Proportion representing the size of the expected test set     seed (int): Random seed Raises:     ValueError: If the object test_size is not between 0 and 1 Returns:     DataFrame: Train dataframe     DataFrame: Test dataframe</p> Source code in <code>template_nlp/models_training/utils_models.py</code> <pre><code>def stratified_split(df: pd.DataFrame, col: Union[str, int], test_size: float = 0.25, seed: Union[int, None] = None) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    '''Splits a DataFrame into train and test sets - Stratified strategy\n\n    Args:\n        df (pd.DataFrame): Dataframe containing the data\n        col (str or int): column on which to do the stratified split\n    Kwargs:\n        test_size (float): Proportion representing the size of the expected test set\n        seed (int): Random seed\n    Raises:\n        ValueError: If the object test_size is not between 0 and 1\n    Returns:\n        DataFrame: Train dataframe\n        DataFrame: Test dataframe\n    '''\n    if not 0 &lt;= test_size &lt;= 1:\n        raise ValueError('The object test_size must be between 0 and 1')\n\n    # Stratified split\n    logger.info(\"Stratified split\")\n    df = remove_small_classes(df, col, min_rows=math.ceil(1 / test_size))  # minimum lines number per category to split\n    df_train, df_test = train_test_split(df, stratify=df[col], test_size=test_size, random_state=seed)\n\n    # Display\n    display_train_test_shape(df_train, df_test, df_shape=df.shape[0])\n\n    # Return\n    return df_train, df_test\n</code></pre>"},{"location":"reference/template_nlp/models_training/hf_metrics/","title":"Hf metrics","text":""},{"location":"reference/template_nlp/models_training/hf_metrics/accuracy/","title":"Accuracy","text":"<p>Accuracy metric.</p>"},{"location":"reference/template_nlp/models_training/hf_metrics/f1/","title":"F1","text":"<p>F1 metric.</p>"},{"location":"reference/template_nlp/models_training/hf_metrics/precision/","title":"Precision","text":"<p>Precision metric.</p>"},{"location":"reference/template_nlp/models_training/hf_metrics/recall/","title":"Recall","text":"<p>Recall metric.</p>"},{"location":"reference/template_nlp/models_training/models_sklearn/","title":"Models sklearn","text":""},{"location":"reference/template_nlp/models_training/models_sklearn/model_pipeline/","title":"Model pipeline","text":""},{"location":"reference/template_nlp/models_training/models_sklearn/model_pipeline/#template_nlp.models_training.models_sklearn.model_pipeline.ModelPipeline","title":"<code>ModelPipeline</code>","text":"<p>             Bases: <code>ModelClass</code></p> <p>Generic model for sklearn pipeline</p> Source code in <code>template_nlp/models_training/models_sklearn/model_pipeline.py</code> <pre><code>class ModelPipeline(ModelClass):\n    '''Generic model for sklearn pipeline'''\n\n    _default_name = 'model_pipeline'\n\n    # Probably need to be overridden, depending on your model :\n    # -&gt; predict_proba (predict on new content - returns probas) -&gt; some pipelines do not provide proba, or may have specificities\n    # -&gt; save (specific save instructions)\n    # -&gt; _init_new_instance_from_configs (loads model attributes - for a newly created model)\n    # -&gt; _load_standalone_files (loads standalone files - for a newly created model) -&gt; add pipeline elements\n\n    def __init__(self, pipeline: Union[Pipeline, None] = None, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelClass for more arguments)\n\n        Kwargs:\n            pipeline (Pipeline): Pipeline to use\n        '''\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Manage model (to implement for children class)\n        self.pipeline = pipeline\n\n    def fit(self, x_train, y_train, **kwargs) -&gt; None:\n        '''Trains the model\n           **kwargs permits compatibility with Keras model\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n            y_train (?): Array-like, shape = [n_samples, n_targets]\n        Raises:\n            RuntimeError: If the model is already fitted\n        '''\n        if self.trained:\n            self.logger.error(\"We can't train again a pipeline sklearn model\")\n            self.logger.error(\"Please train a new model\")\n            raise RuntimeError(\"We can't train again a pipeline sklearn model\")\n\n        # We \"only\" check if no multi-classes multi-labels (which can't be managed by most SKLEARN pipelines)\n        if self.multi_label:\n            df_tmp = pd.DataFrame(y_train)\n            for col in df_tmp:\n                uniques = df_tmp[col].unique()\n                if len(uniques) &gt; 2:\n                    self.logger.warning(' - /!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\ - ')\n                    self.logger.warning(\"Most sklearn pipelines can't manage multi-classes/multi-labels\")\n                    self.logger.warning(' - /!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\ - ')\n                    # We \"let\" the process crash by itself\n                    break\n\n        # Fit pipeline\n        self.pipeline.fit(x_train, y_train)\n\n        # Set list classes\n        if not self.multi_label:\n            self.list_classes = list(self.pipeline.classes_)\n        # TODO : check pipeline.classes_ for multi-labels\n        else:\n            if hasattr(y_train, 'columns'):\n                self.list_classes = list(y_train.columns)\n            else:\n                self.logger.warning(\n                    \"Can't read the name of the columns of y_train -&gt; inverse transformation won't be possible\"\n                )\n                # We still create a list of classes in order to be compatible with other functions\n                self.list_classes = [str(_) for _ in range(pd.DataFrame(y_train).shape[1])]\n\n        # Set dict_classes based on list classes\n        self.dict_classes = {i: col for i, col in enumerate(self.list_classes)}\n\n        # Set trained\n        self.trained = True\n        self.nb_fit += 1\n\n    @utils.data_agnostic_str_to_list\n    @utils.trained_needed\n    def predict(self, x_test, return_proba: bool = False, **kwargs) -&gt; np.ndarray:\n        '''Predictions on test set\n\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n        Kwargs:\n            return_proba (bool): If the function should return the probabilities instead of the classes (Keras compatibility)\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n        if not return_proba:\n            return np.array(self.pipeline.predict(x_test))\n        else:\n            return self.predict_proba(x_test)\n\n    @utils.data_agnostic_str_to_list\n    @utils.trained_needed\n    def predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n        '''Predicts probabilities on the test dataset\n\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n        probas = np.array(self.pipeline.predict_proba(x_test))\n        # Very specific fix: in some cases, with OvR, strategy, all estimators return 0, which generates a division per 0 when normalizing\n        # Hence, we replace NaNs with 1 / nb_classes\n        if not self.multi_label:\n            probas = np.nan_to_num(probas, nan=1/len(self.list_classes))\n        # If use of MultiOutputClassifier -&gt;  returns probabilities of 0 and 1 for all elements and all classes\n        # Same thing for some base models\n        # Correction in case where we detect a shape of length &gt; 2 (ie. equals to 3)\n        # Reminder : we do not manage multi-labels/multi-classes\n        if len(probas.shape) &gt; 2:\n            probas = np.swapaxes(probas[:, :, 1], 0, 1)\n        return probas\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save model\n        if json_data is None:\n            json_data = {}\n\n        json_data['librairie'] = 'scikit-learn'\n\n        # Add each pipeline steps' conf\n        if self.pipeline is not None:\n            for step in self.pipeline.steps:\n                name = step[0]\n                confs = step[1].get_params()\n                # Get rid of some non serializable conf\n                for special_conf in ['dtype', 'base_estimator', 'estimator', 'estimator__base_estimator',\n                                     'estimator__estimator', 'estimator__estimator__base_estimator']:\n                    if special_conf in confs.keys():\n                        confs[special_conf] = str(confs[special_conf])\n                json_data[f'{name}_confs'] = confs\n\n        # Save\n        super().save(json_data=json_data)\n\n        # Save model standalone if wanted &amp; pipeline is not None &amp; level_save &gt; 'LOW'\n        if self.pipeline is not None and self.level_save in ['MEDIUM', 'HIGH']:\n            pkl_path = os.path.join(self.model_dir, \"sklearn_pipeline_standalone.pkl\")\n            # Save model\n            with open(pkl_path, 'wb') as f:\n                pickle.dump(self.pipeline, f)\n\n    def _load_standalone_files(self, default_model_dir: Union[str, None] = None,\n                               sklearn_pipeline_path: Union[str, None] = None, *args, **kwargs):\n        '''Loads standalone files for a newly created model via _init_new_instance_from_configs\n\n        Kwargs:\n            default_model_dir (str): a path to look for default file paths\n                                     If None, standalone files path should all be provided\n            sklearn_pipeline_path (str): Path to the sklearn pipeline\n                                         If None, we'll use the default path if default_model_dir is not None\n        Raises:\n            ValueError: If the sklearn pipeline is not specified and can't be inferred\n            FileNotFoundError: If the sklearn pipeline path does not exist\n        '''\n        # Check if we are able to get all needed paths\n        if default_model_dir is None and sklearn_pipeline_path is None:\n            raise ValueError(\"Sklearn pipeline path is not specified and can't be inferred\")\n\n        # Retrieve file paths\n        if sklearn_pipeline_path is None:\n            sklearn_pipeline_path = os.path.join(default_model_dir, \"sklearn_pipeline_standalone.pkl\")\n\n        # Check paths exists\n        if not os.path.isfile(sklearn_pipeline_path):\n            raise FileNotFoundError(f\"Can't find sklearn pipeline path ({sklearn_pipeline_path})\")\n\n        # Reload sklearn pipeline\n        with open(sklearn_pipeline_path, 'rb') as f:\n            self.pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_pipeline/#template_nlp.models_training.models_sklearn.model_pipeline.ModelPipeline.__init__","title":"<code>__init__(pipeline=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass for more arguments)</p> Kwargs <p>pipeline (Pipeline): Pipeline to use</p> Source code in <code>template_nlp/models_training/models_sklearn/model_pipeline.py</code> <pre><code>def __init__(self, pipeline: Union[Pipeline, None] = None, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelClass for more arguments)\n\n    Kwargs:\n        pipeline (Pipeline): Pipeline to use\n    '''\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Manage model (to implement for children class)\n    self.pipeline = pipeline\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_pipeline/#template_nlp.models_training.models_sklearn.model_pipeline.ModelPipeline.fit","title":"<code>fit(x_train, y_train, **kwargs)</code>","text":"<p>Trains the model    **kwargs permits compatibility with Keras model Args:     x_train (?): Array-like, shape = [n_samples, n_features]     y_train (?): Array-like, shape = [n_samples, n_targets] Raises:     RuntimeError: If the model is already fitted</p> Source code in <code>template_nlp/models_training/models_sklearn/model_pipeline.py</code> <pre><code>def fit(self, x_train, y_train, **kwargs) -&gt; None:\n    '''Trains the model\n       **kwargs permits compatibility with Keras model\n    Args:\n        x_train (?): Array-like, shape = [n_samples, n_features]\n        y_train (?): Array-like, shape = [n_samples, n_targets]\n    Raises:\n        RuntimeError: If the model is already fitted\n    '''\n    if self.trained:\n        self.logger.error(\"We can't train again a pipeline sklearn model\")\n        self.logger.error(\"Please train a new model\")\n        raise RuntimeError(\"We can't train again a pipeline sklearn model\")\n\n    # We \"only\" check if no multi-classes multi-labels (which can't be managed by most SKLEARN pipelines)\n    if self.multi_label:\n        df_tmp = pd.DataFrame(y_train)\n        for col in df_tmp:\n            uniques = df_tmp[col].unique()\n            if len(uniques) &gt; 2:\n                self.logger.warning(' - /!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\ - ')\n                self.logger.warning(\"Most sklearn pipelines can't manage multi-classes/multi-labels\")\n                self.logger.warning(' - /!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\ - ')\n                # We \"let\" the process crash by itself\n                break\n\n    # Fit pipeline\n    self.pipeline.fit(x_train, y_train)\n\n    # Set list classes\n    if not self.multi_label:\n        self.list_classes = list(self.pipeline.classes_)\n    # TODO : check pipeline.classes_ for multi-labels\n    else:\n        if hasattr(y_train, 'columns'):\n            self.list_classes = list(y_train.columns)\n        else:\n            self.logger.warning(\n                \"Can't read the name of the columns of y_train -&gt; inverse transformation won't be possible\"\n            )\n            # We still create a list of classes in order to be compatible with other functions\n            self.list_classes = [str(_) for _ in range(pd.DataFrame(y_train).shape[1])]\n\n    # Set dict_classes based on list classes\n    self.dict_classes = {i: col for i, col in enumerate(self.list_classes)}\n\n    # Set trained\n    self.trained = True\n    self.nb_fit += 1\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_pipeline/#template_nlp.models_training.models_sklearn.model_pipeline.ModelPipeline.predict","title":"<code>predict(x_test, return_proba=False, **kwargs)</code>","text":"<p>Predictions on test set</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples, n_features]</p> required <p>Kwargs:     return_proba (bool): If the function should return the probabilities instead of the classes (Keras compatibility) Returns:     (np.ndarray): Array, shape = [n_samples, n_classes]</p> Source code in <code>template_nlp/models_training/models_sklearn/model_pipeline.py</code> <pre><code>@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict(self, x_test, return_proba: bool = False, **kwargs) -&gt; np.ndarray:\n    '''Predictions on test set\n\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n    Kwargs:\n        return_proba (bool): If the function should return the probabilities instead of the classes (Keras compatibility)\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n    if not return_proba:\n        return np.array(self.pipeline.predict(x_test))\n    else:\n        return self.predict_proba(x_test)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_pipeline/#template_nlp.models_training.models_sklearn.model_pipeline.ModelPipeline.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts probabilities on the test dataset</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples, n_features]</p> required <p>Returns:     (np.ndarray): Array, shape = [n_samples, n_classes]</p> Source code in <code>template_nlp/models_training/models_sklearn/model_pipeline.py</code> <pre><code>@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n    '''Predicts probabilities on the test dataset\n\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n    probas = np.array(self.pipeline.predict_proba(x_test))\n    # Very specific fix: in some cases, with OvR, strategy, all estimators return 0, which generates a division per 0 when normalizing\n    # Hence, we replace NaNs with 1 / nb_classes\n    if not self.multi_label:\n        probas = np.nan_to_num(probas, nan=1/len(self.list_classes))\n    # If use of MultiOutputClassifier -&gt;  returns probabilities of 0 and 1 for all elements and all classes\n    # Same thing for some base models\n    # Correction in case where we detect a shape of length &gt; 2 (ie. equals to 3)\n    # Reminder : we do not manage multi-labels/multi-classes\n    if len(probas.shape) &gt; 2:\n        probas = np.swapaxes(probas[:, :, 1], 0, 1)\n    return probas\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_pipeline/#template_nlp.models_training.models_sklearn.model_pipeline.ModelPipeline.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_nlp/models_training/models_sklearn/model_pipeline.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save model\n    if json_data is None:\n        json_data = {}\n\n    json_data['librairie'] = 'scikit-learn'\n\n    # Add each pipeline steps' conf\n    if self.pipeline is not None:\n        for step in self.pipeline.steps:\n            name = step[0]\n            confs = step[1].get_params()\n            # Get rid of some non serializable conf\n            for special_conf in ['dtype', 'base_estimator', 'estimator', 'estimator__base_estimator',\n                                 'estimator__estimator', 'estimator__estimator__base_estimator']:\n                if special_conf in confs.keys():\n                    confs[special_conf] = str(confs[special_conf])\n            json_data[f'{name}_confs'] = confs\n\n    # Save\n    super().save(json_data=json_data)\n\n    # Save model standalone if wanted &amp; pipeline is not None &amp; level_save &gt; 'LOW'\n    if self.pipeline is not None and self.level_save in ['MEDIUM', 'HIGH']:\n        pkl_path = os.path.join(self.model_dir, \"sklearn_pipeline_standalone.pkl\")\n        # Save model\n        with open(pkl_path, 'wb') as f:\n            pickle.dump(self.pipeline, f)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_gbt/","title":"Model tfidf gbt","text":""},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_gbt/#template_nlp.models_training.models_sklearn.model_tfidf_gbt.ModelTfidfGbt","title":"<code>ModelTfidfGbt</code>","text":"<p>             Bases: <code>ModelPipeline</code></p> <p>Model for predictions via TF-IDF + GBT</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_gbt.py</code> <pre><code>class ModelTfidfGbt(ModelPipeline):\n    '''Model for predictions via TF-IDF + GBT'''\n\n    _default_name = 'model_tfidf_gbt'\n\n    def __init__(self, tfidf_params: Union[dict, None] = None, gbt_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelPipeline &amp; ModelClass for more arguments)\n\n        Kwargs:\n            tfidf_params (dict) : Parameters for the tfidf\n            gbt_params (dict) : parameters for the gbt\n            multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n        Raises:\n            ValueError: If multiclass_strategy is not 'ovo', 'ovr' or None\n        '''\n        if multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\n            raise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Manage model\n        if tfidf_params is None:\n            tfidf_params = {}\n        self.tfidf = TfidfVectorizer(**tfidf_params)\n        if gbt_params is None:\n            gbt_params = {}\n        gbt_params['random_state'] = self.random_seed\n        self.gbt = GradientBoostingClassifier(**gbt_params)\n        self.multiclass_strategy = multiclass_strategy\n\n        # Can't do multi-labels / multi-classes\n        if not self.multi_label:\n            # If not multi-classes : no impact\n            if multiclass_strategy == 'ovr':\n                self.pipeline = Pipeline([('tfidf', self.tfidf), ('gbt', OneVsRestClassifier(self.gbt))])\n            elif multiclass_strategy == 'ovo':\n                self.pipeline = Pipeline([('tfidf', self.tfidf), ('gbt', OneVsOneClassifier(self.gbt))])\n            else:\n                self.pipeline = Pipeline([('tfidf', self.tfidf), ('gbt', self.gbt)])\n\n        # Manage multi-labels -&gt; add a MultiOutputClassifier\n        # The GBT does not natively support multi-labels\n        if self.multi_label:\n            self.pipeline = Pipeline([('tfidf', self.tfidf), ('gbt', MultiOutputClassifier(self.gbt))])\n\n    @utils.data_agnostic_str_to_list\n    @utils.trained_needed\n    def predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n        '''Probabilities prediction on the test dataset\n            'ovo' can't predict probabilities. By default we return 1 if it is the predicted class, 0 otherwise.\n\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n        # Use super() of Pipeline class if != 'ovo' or multi-labels\n        if self.multi_label or self.multiclass_strategy != 'ovo':\n            return super().predict_proba(x_test, **kwargs)\n        # We return 1 if predicted, otherwise 0\n        else:\n            preds = self.pipeline.predict(x_test)\n            # Format ['a', 'b', 'c', 'a', ..., 'b']\n            # Transform to \"proba\"\n            transform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\n            probas = np.array([transform_dict[x] for x in preds])\n        return probas\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save model\n        if json_data is None:\n            json_data = {}\n\n        # No need to save the parameters of the pipeline steps, it is already done in ModelPipeline\n        json_data['multiclass_strategy'] = self.multiclass_strategy\n\n        # Save\n        super().save(json_data=json_data)\n\n    @classmethod\n    def _init_new_instance_from_configs(cls, configs):\n        '''Inits a new instance from a set of configurations\n\n        Args:\n            configs: a set of configurations of a model to be reloaded\n        Returns:\n            ModelClass: the newly generated class\n        '''\n        # Call parent\n        model = super()._init_new_instance_from_configs(configs)\n\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['multiclass_strategy']:\n            setattr(model, attribute, configs.get(attribute, getattr(model, attribute)))\n\n        # Return the new model\n        return model\n\n    def _load_standalone_files(self, default_model_dir: Union[str, None] = None, *args, **kwargs):\n        '''Loads standalone files for a newly created model via _init_new_instance_from_configs\n\n        Kwargs:\n            default_model_dir (str): a path to look for default file paths\n                                     If None, standalone files path should all be provided\n        '''\n        # Call parent\n        super()._load_standalone_files(default_model_dir=default_model_dir, **kwargs)\n\n        # Reload pipeline elements\n        self.tfidf = self.pipeline['tfidf']\n\n        # Manage multi-labels or multi-classes\n        if not self.multi_label and self.multiclass_strategy is None:\n            self.gbt = self.pipeline['gbt']\n        else:\n            self.gbt = self.pipeline['gbt'].estimator\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_gbt/#template_nlp.models_training.models_sklearn.model_tfidf_gbt.ModelTfidfGbt.__init__","title":"<code>__init__(tfidf_params=None, gbt_params=None, multiclass_strategy=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline &amp; ModelClass for more arguments)</p> Kwargs <p>tfidf_params (dict) : Parameters for the tfidf gbt_params (dict) : parameters for the gbt multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.</p> <p>Raises:     ValueError: If multiclass_strategy is not 'ovo', 'ovr' or None</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_gbt.py</code> <pre><code>def __init__(self, tfidf_params: Union[dict, None] = None, gbt_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelPipeline &amp; ModelClass for more arguments)\n\n    Kwargs:\n        tfidf_params (dict) : Parameters for the tfidf\n        gbt_params (dict) : parameters for the gbt\n        multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n    Raises:\n        ValueError: If multiclass_strategy is not 'ovo', 'ovr' or None\n    '''\n    if multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\n        raise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Manage model\n    if tfidf_params is None:\n        tfidf_params = {}\n    self.tfidf = TfidfVectorizer(**tfidf_params)\n    if gbt_params is None:\n        gbt_params = {}\n    gbt_params['random_state'] = self.random_seed\n    self.gbt = GradientBoostingClassifier(**gbt_params)\n    self.multiclass_strategy = multiclass_strategy\n\n    # Can't do multi-labels / multi-classes\n    if not self.multi_label:\n        # If not multi-classes : no impact\n        if multiclass_strategy == 'ovr':\n            self.pipeline = Pipeline([('tfidf', self.tfidf), ('gbt', OneVsRestClassifier(self.gbt))])\n        elif multiclass_strategy == 'ovo':\n            self.pipeline = Pipeline([('tfidf', self.tfidf), ('gbt', OneVsOneClassifier(self.gbt))])\n        else:\n            self.pipeline = Pipeline([('tfidf', self.tfidf), ('gbt', self.gbt)])\n\n    # Manage multi-labels -&gt; add a MultiOutputClassifier\n    # The GBT does not natively support multi-labels\n    if self.multi_label:\n        self.pipeline = Pipeline([('tfidf', self.tfidf), ('gbt', MultiOutputClassifier(self.gbt))])\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_gbt/#template_nlp.models_training.models_sklearn.model_tfidf_gbt.ModelTfidfGbt.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Probabilities prediction on the test dataset     'ovo' can't predict probabilities. By default we return 1 if it is the predicted class, 0 otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples, n_features]</p> required <p>Returns:     (np.ndarray): Array, shape = [n_samples, n_classes]</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_gbt.py</code> <pre><code>@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n    '''Probabilities prediction on the test dataset\n        'ovo' can't predict probabilities. By default we return 1 if it is the predicted class, 0 otherwise.\n\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n    # Use super() of Pipeline class if != 'ovo' or multi-labels\n    if self.multi_label or self.multiclass_strategy != 'ovo':\n        return super().predict_proba(x_test, **kwargs)\n    # We return 1 if predicted, otherwise 0\n    else:\n        preds = self.pipeline.predict(x_test)\n        # Format ['a', 'b', 'c', 'a', ..., 'b']\n        # Transform to \"proba\"\n        transform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\n        probas = np.array([transform_dict[x] for x in preds])\n    return probas\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_gbt/#template_nlp.models_training.models_sklearn.model_tfidf_gbt.ModelTfidfGbt.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_gbt.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save model\n    if json_data is None:\n        json_data = {}\n\n    # No need to save the parameters of the pipeline steps, it is already done in ModelPipeline\n    json_data['multiclass_strategy'] = self.multiclass_strategy\n\n    # Save\n    super().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_lgbm/","title":"Model tfidf lgbm","text":""},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_lgbm/#template_nlp.models_training.models_sklearn.model_tfidf_lgbm.ModelTfidfLgbm","title":"<code>ModelTfidfLgbm</code>","text":"<p>             Bases: <code>ModelPipeline</code></p> <p>Model for predictions via TF-IDF + LGBM</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_lgbm.py</code> <pre><code>class ModelTfidfLgbm(ModelPipeline):\n    '''Model for predictions via TF-IDF + LGBM'''\n\n    _default_name = 'model_tfidf_lgbm'\n\n    def __init__(self, tfidf_params: Union[dict, None] = None, lgbm_params: Union[dict, None] = None,\n                 multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelPipeline &amp; ModelClass for more arguments)\n\n        Kwargs:\n            tfidf_params (dict) : Parameters for the tfidf\n            lgbm_params (dict) : Parameters for the lgbm\n            multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n        Raises:\n            ValueError: If multiclass_strategy is not 'ovo', 'ovr' or None\n        '''\n        if multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\n            raise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Manage model\n        if tfidf_params is None:\n            tfidf_params = {}\n        self.tfidf = TfidfVectorizer(**tfidf_params)\n        if lgbm_params is None:\n            lgbm_params = {}\n        lgbm_params['random_state'] = self.random_seed\n        self.lgbm = LGBMClassifier(**lgbm_params)\n        self.multiclass_strategy = multiclass_strategy\n\n        # Can't do multi-labels / multi-classes\n        if not self.multi_label:\n            # If not multi-classes : no impact\n            if multiclass_strategy == 'ovr':\n                self.pipeline = Pipeline([('tfidf', self.tfidf), ('lgbm', OneVsRestClassifier(self.lgbm))])\n            elif multiclass_strategy == 'ovo':\n                self.pipeline = Pipeline([('tfidf', self.tfidf), ('lgbm', OneVsOneClassifier(self.lgbm))])\n            else:\n                self.pipeline = Pipeline([('tfidf', self.tfidf), ('lgbm', self.lgbm)])\n\n        # Manage multi-labels -&gt; add a MultiOutputClassifier\n        # The LGBM does not natively support multi-labels\n        if self.multi_label:\n            self.pipeline = Pipeline([('tfidf', self.tfidf), ('lgbm', MultiOutputClassifier(self.lgbm))])\n\n    @utils.data_agnostic_str_to_list\n    @utils.trained_needed\n    def predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n        '''Probabilities prediction on the test dataset\n            'ovo' can't predict probabilities. By default we return 1 if it is the predicted class, 0 otherwise.\n\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n        # Use super() of Pipeline class if != 'ovo' or multi-labels\n        if self.multi_label or self.multiclass_strategy != 'ovo':\n            return super().predict_proba(x_test, **kwargs)\n        # We return 1 if predicted, otherwise 0\n        else:\n            preds = self.pipeline.predict(x_test)\n            # Format ['a', 'b', 'c', 'a', ..., 'b']\n            # Transform to \"proba\"\n            transform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\n            probas = np.array([transform_dict[x] for x in preds])\n        return probas\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save model\n        if json_data is None:\n            json_data = {}\n\n        # No need to save the parameters of the pipeline steps, it is already done in ModelPipeline\n        json_data['multiclass_strategy'] = self.multiclass_strategy\n\n        # Save\n        super().save(json_data=json_data)\n\n    @classmethod\n    def _init_new_instance_from_configs(cls, configs):\n        '''Inits a new instance from a set of configurations\n\n        Args:\n            configs: a set of configurations of a model to be reloaded\n        Returns:\n            ModelClass: the newly generated class\n        '''\n        # Call parent\n        model = super()._init_new_instance_from_configs(configs)\n\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['multiclass_strategy']:\n            setattr(model, attribute, configs.get(attribute, getattr(model, attribute)))\n\n        # Return the new model\n        return model\n\n    def _load_standalone_files(self, default_model_dir: Union[str, None] = None, *args, **kwargs):\n        '''Loads standalone files for a newly created model via _init_new_instance_from_configs\n\n        Kwargs:\n            default_model_dir (str): a path to look for default file paths\n                                     If None, standalone files path should all be provided\n        '''\n        # Call parent\n        super()._load_standalone_files(default_model_dir=default_model_dir, **kwargs)\n\n        # Reload pipeline elements\n        self.tfidf = self.pipeline['tfidf']\n\n        # Manage multi-labels or multi-classes\n        if not self.multi_label and self.multiclass_strategy is None:\n            self.lgbm = self.pipeline['lgbm']\n        else:\n            self.lgbm = self.pipeline['lgbm'].estimator\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_lgbm/#template_nlp.models_training.models_sklearn.model_tfidf_lgbm.ModelTfidfLgbm.__init__","title":"<code>__init__(tfidf_params=None, lgbm_params=None, multiclass_strategy=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline &amp; ModelClass for more arguments)</p> Kwargs <p>tfidf_params (dict) : Parameters for the tfidf lgbm_params (dict) : Parameters for the lgbm multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.</p> <p>Raises:     ValueError: If multiclass_strategy is not 'ovo', 'ovr' or None</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_lgbm.py</code> <pre><code>def __init__(self, tfidf_params: Union[dict, None] = None, lgbm_params: Union[dict, None] = None,\n             multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelPipeline &amp; ModelClass for more arguments)\n\n    Kwargs:\n        tfidf_params (dict) : Parameters for the tfidf\n        lgbm_params (dict) : Parameters for the lgbm\n        multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n    Raises:\n        ValueError: If multiclass_strategy is not 'ovo', 'ovr' or None\n    '''\n    if multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\n        raise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Manage model\n    if tfidf_params is None:\n        tfidf_params = {}\n    self.tfidf = TfidfVectorizer(**tfidf_params)\n    if lgbm_params is None:\n        lgbm_params = {}\n    lgbm_params['random_state'] = self.random_seed\n    self.lgbm = LGBMClassifier(**lgbm_params)\n    self.multiclass_strategy = multiclass_strategy\n\n    # Can't do multi-labels / multi-classes\n    if not self.multi_label:\n        # If not multi-classes : no impact\n        if multiclass_strategy == 'ovr':\n            self.pipeline = Pipeline([('tfidf', self.tfidf), ('lgbm', OneVsRestClassifier(self.lgbm))])\n        elif multiclass_strategy == 'ovo':\n            self.pipeline = Pipeline([('tfidf', self.tfidf), ('lgbm', OneVsOneClassifier(self.lgbm))])\n        else:\n            self.pipeline = Pipeline([('tfidf', self.tfidf), ('lgbm', self.lgbm)])\n\n    # Manage multi-labels -&gt; add a MultiOutputClassifier\n    # The LGBM does not natively support multi-labels\n    if self.multi_label:\n        self.pipeline = Pipeline([('tfidf', self.tfidf), ('lgbm', MultiOutputClassifier(self.lgbm))])\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_lgbm/#template_nlp.models_training.models_sklearn.model_tfidf_lgbm.ModelTfidfLgbm.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Probabilities prediction on the test dataset     'ovo' can't predict probabilities. By default we return 1 if it is the predicted class, 0 otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples, n_features]</p> required <p>Returns:     (np.ndarray): Array, shape = [n_samples, n_classes]</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_lgbm.py</code> <pre><code>@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n    '''Probabilities prediction on the test dataset\n        'ovo' can't predict probabilities. By default we return 1 if it is the predicted class, 0 otherwise.\n\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n    # Use super() of Pipeline class if != 'ovo' or multi-labels\n    if self.multi_label or self.multiclass_strategy != 'ovo':\n        return super().predict_proba(x_test, **kwargs)\n    # We return 1 if predicted, otherwise 0\n    else:\n        preds = self.pipeline.predict(x_test)\n        # Format ['a', 'b', 'c', 'a', ..., 'b']\n        # Transform to \"proba\"\n        transform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\n        probas = np.array([transform_dict[x] for x in preds])\n    return probas\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_lgbm/#template_nlp.models_training.models_sklearn.model_tfidf_lgbm.ModelTfidfLgbm.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_lgbm.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save model\n    if json_data is None:\n        json_data = {}\n\n    # No need to save the parameters of the pipeline steps, it is already done in ModelPipeline\n    json_data['multiclass_strategy'] = self.multiclass_strategy\n\n    # Save\n    super().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_sgdc/","title":"Model tfidf sgdc","text":""},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_sgdc/#template_nlp.models_training.models_sklearn.model_tfidf_sgdc.ModelTfidfSgdc","title":"<code>ModelTfidfSgdc</code>","text":"<p>             Bases: <code>ModelPipeline</code></p> <p>Model for predictions via TF-IDF + SGDClassier</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_sgdc.py</code> <pre><code>class ModelTfidfSgdc(ModelPipeline):\n    '''Model for predictions via TF-IDF + SGDClassier'''\n\n    _default_name = 'model_tfidf_sgdc'\n\n    def __init__(self, tfidf_params: Union[dict, None] = None, sgdc_params: Union[dict, None] = None,\n                 multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelPipeline &amp; ModelClass for more arguments)\n\n        Kwargs:\n            tfidf_params (dict) : Parameters for the tfidf\n            sgdc_params (dict) : Parameters for the sgdc\n            multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n        Raises:\n            ValueError: If multiclass_strategy is not 'ovo', 'ovr' or None\n        '''\n        if multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\n            raise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Manage model\n        if tfidf_params is None:\n            tfidf_params = {}\n        self.tfidf = TfidfVectorizer(**tfidf_params)\n        if sgdc_params is None:\n            sgdc_params = {}\n        sgdc_params['random_state'] = self.random_seed\n        self.sgdc = SGDClassifier(**sgdc_params)\n        self.multiclass_strategy = multiclass_strategy\n\n        # Can't do multi-labels / multi-classes\n        if not self.multi_label:\n            # If not multi-classes : no impact\n            if multiclass_strategy == 'ovr':\n                self.pipeline = Pipeline([('tfidf', self.tfidf), ('sgdc', OneVsRestClassifier(self.sgdc))])\n            elif multiclass_strategy == 'ovo':\n                self.pipeline = Pipeline([('tfidf', self.tfidf), ('sgdc', OneVsOneClassifier(self.sgdc))])\n            else:\n                self.pipeline = Pipeline([('tfidf', self.tfidf), ('sgdc', self.sgdc)])\n\n        # Manage multi-labels -&gt; add a MultiOutputClassifier\n        # The SGDClassifier does not natively support multi-labels\n        if self.multi_label:\n            self.pipeline = Pipeline([('tfidf', self.tfidf), ('sgdc', MultiOutputClassifier(self.sgdc))])\n\n    @utils.data_agnostic_str_to_list\n    @utils.trained_needed\n    def predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n        '''Predicts the probabilities on the test set\n        - /!\\\\ THE SGDC DOES NOT RETURN PROBABILITIES UNDER SOME CONDITIONS HERE WE SIMULATE PROBABILITIES EQUAL TO 0 OR 1 /!\\\\ -\n\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n        # Can't use probabilities if loss not in ['log', 'modified_huber'] or 'ovo' and not multi-labels\n        if self.sgdc.loss not in ['log', 'modified_huber'] or (self.multiclass_strategy == 'ovo' and not self.multi_label):\n            if not self.multi_label:\n                preds = self.pipeline.predict(x_test)\n                # Format ['a', 'b', 'c', 'a', ..., 'b']\n                # Transform to \"proba\"\n                transform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\n                probas = np.array([transform_dict[x] for x in preds])\n            else:\n                preds = self.pipeline.predict(x_test)\n                # Already right format, but in int !\n                probas = np.array([[float(_) for _ in x] for x in preds])\n        # Otherwise, classic probabilities\n        else:\n            probas = super().predict_proba(x_test, **kwargs)\n        return probas\n\n    def get_predict_position(self, x_test, y_true, **kwargs) -&gt; np.ndarray:\n        '''Gets the order of predictions of y_true.\n        Positions start at 1 (not 0)\n\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n            y_true (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            np.ndarray: Array, shape = [n_samples]\n        '''\n        if self.sgdc.loss not in ['log', 'modified_huber']:\n            self.logger.warning(\"Warning, the method get_predict_position is not suitable for a SGDC model (except for the losses 'log' and 'modified_huber')\"\n                                \"(no probabilities, we use 1 or 0)\")\n        return super().get_predict_position(x_test, y_true)\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save model\n        if json_data is None:\n            json_data = {}\n\n        # No need to save the parameters of the pipeline steps, it is already done in ModelPipeline\n        json_data['multiclass_strategy'] = self.multiclass_strategy\n\n        # Save\n        super().save(json_data=json_data)\n\n    @classmethod\n    def _init_new_instance_from_configs(cls, configs):\n        '''Inits a new instance from a set of configurations\n\n        Args:\n            configs: a set of configurations of a model to be reloaded\n        Returns:\n            ModelClass: the newly generated class\n        '''\n        # Call parent\n        model = super()._init_new_instance_from_configs(configs)\n\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['multiclass_strategy']:\n            setattr(model, attribute, configs.get(attribute, getattr(model, attribute)))\n\n        # Return the new model\n        return model\n\n    def _load_standalone_files(self, default_model_dir: Union[str, None] = None, *args, **kwargs):\n        '''Loads standalone files for a newly created model via _init_new_instance_from_configs\n\n        Kwargs:\n            default_model_dir (str): a path to look for default file paths\n                                     If None, standalone files path should all be provided\n        '''\n        # Call parent\n        super()._load_standalone_files(default_model_dir=default_model_dir, **kwargs)\n\n        # Reload pipeline elements\n        self.tfidf = self.pipeline['tfidf']\n\n        # Manage multi-labels or multi-classes\n        if not self.multi_label and self.multiclass_strategy is None:\n            self.sgdc = self.pipeline['sgdc']\n        else:\n            self.sgdc = self.pipeline['sgdc'].estimator\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_sgdc/#template_nlp.models_training.models_sklearn.model_tfidf_sgdc.ModelTfidfSgdc.__init__","title":"<code>__init__(tfidf_params=None, sgdc_params=None, multiclass_strategy=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline &amp; ModelClass for more arguments)</p> Kwargs <p>tfidf_params (dict) : Parameters for the tfidf sgdc_params (dict) : Parameters for the sgdc multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.</p> <p>Raises:     ValueError: If multiclass_strategy is not 'ovo', 'ovr' or None</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_sgdc.py</code> <pre><code>def __init__(self, tfidf_params: Union[dict, None] = None, sgdc_params: Union[dict, None] = None,\n             multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelPipeline &amp; ModelClass for more arguments)\n\n    Kwargs:\n        tfidf_params (dict) : Parameters for the tfidf\n        sgdc_params (dict) : Parameters for the sgdc\n        multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n    Raises:\n        ValueError: If multiclass_strategy is not 'ovo', 'ovr' or None\n    '''\n    if multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\n        raise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Manage model\n    if tfidf_params is None:\n        tfidf_params = {}\n    self.tfidf = TfidfVectorizer(**tfidf_params)\n    if sgdc_params is None:\n        sgdc_params = {}\n    sgdc_params['random_state'] = self.random_seed\n    self.sgdc = SGDClassifier(**sgdc_params)\n    self.multiclass_strategy = multiclass_strategy\n\n    # Can't do multi-labels / multi-classes\n    if not self.multi_label:\n        # If not multi-classes : no impact\n        if multiclass_strategy == 'ovr':\n            self.pipeline = Pipeline([('tfidf', self.tfidf), ('sgdc', OneVsRestClassifier(self.sgdc))])\n        elif multiclass_strategy == 'ovo':\n            self.pipeline = Pipeline([('tfidf', self.tfidf), ('sgdc', OneVsOneClassifier(self.sgdc))])\n        else:\n            self.pipeline = Pipeline([('tfidf', self.tfidf), ('sgdc', self.sgdc)])\n\n    # Manage multi-labels -&gt; add a MultiOutputClassifier\n    # The SGDClassifier does not natively support multi-labels\n    if self.multi_label:\n        self.pipeline = Pipeline([('tfidf', self.tfidf), ('sgdc', MultiOutputClassifier(self.sgdc))])\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_sgdc/#template_nlp.models_training.models_sklearn.model_tfidf_sgdc.ModelTfidfSgdc.get_predict_position","title":"<code>get_predict_position(x_test, y_true, **kwargs)</code>","text":"<p>Gets the order of predictions of y_true. Positions start at 1 (not 0)</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples, n_features]</p> required <code>y_true</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <p>Returns:     np.ndarray: Array, shape = [n_samples]</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_sgdc.py</code> <pre><code>def get_predict_position(self, x_test, y_true, **kwargs) -&gt; np.ndarray:\n    '''Gets the order of predictions of y_true.\n    Positions start at 1 (not 0)\n\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n        y_true (?): Array-like, shape = [n_samples, n_features]\n    Returns:\n        np.ndarray: Array, shape = [n_samples]\n    '''\n    if self.sgdc.loss not in ['log', 'modified_huber']:\n        self.logger.warning(\"Warning, the method get_predict_position is not suitable for a SGDC model (except for the losses 'log' and 'modified_huber')\"\n                            \"(no probabilities, we use 1 or 0)\")\n    return super().get_predict_position(x_test, y_true)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_sgdc/#template_nlp.models_training.models_sklearn.model_tfidf_sgdc.ModelTfidfSgdc.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set - /! THE SGDC DOES NOT RETURN PROBABILITIES UNDER SOME CONDITIONS HERE WE SIMULATE PROBABILITIES EQUAL TO 0 OR 1 /! -</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples, n_features]</p> required <p>Returns:     (np.ndarray): Array, shape = [n_samples, n_classes]</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_sgdc.py</code> <pre><code>@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n    '''Predicts the probabilities on the test set\n    - /!\\\\ THE SGDC DOES NOT RETURN PROBABILITIES UNDER SOME CONDITIONS HERE WE SIMULATE PROBABILITIES EQUAL TO 0 OR 1 /!\\\\ -\n\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n    # Can't use probabilities if loss not in ['log', 'modified_huber'] or 'ovo' and not multi-labels\n    if self.sgdc.loss not in ['log', 'modified_huber'] or (self.multiclass_strategy == 'ovo' and not self.multi_label):\n        if not self.multi_label:\n            preds = self.pipeline.predict(x_test)\n            # Format ['a', 'b', 'c', 'a', ..., 'b']\n            # Transform to \"proba\"\n            transform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\n            probas = np.array([transform_dict[x] for x in preds])\n        else:\n            preds = self.pipeline.predict(x_test)\n            # Already right format, but in int !\n            probas = np.array([[float(_) for _ in x] for x in preds])\n    # Otherwise, classic probabilities\n    else:\n        probas = super().predict_proba(x_test, **kwargs)\n    return probas\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_sgdc/#template_nlp.models_training.models_sklearn.model_tfidf_sgdc.ModelTfidfSgdc.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_sgdc.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save model\n    if json_data is None:\n        json_data = {}\n\n    # No need to save the parameters of the pipeline steps, it is already done in ModelPipeline\n    json_data['multiclass_strategy'] = self.multiclass_strategy\n\n    # Save\n    super().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_svm/","title":"Model tfidf svm","text":""},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_svm/#template_nlp.models_training.models_sklearn.model_tfidf_svm.ModelTfidfSvm","title":"<code>ModelTfidfSvm</code>","text":"<p>             Bases: <code>ModelPipeline</code></p> <p>Model for predictions via TF-IDF + SVM</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_svm.py</code> <pre><code>class ModelTfidfSvm(ModelPipeline):\n    '''Model for predictions via TF-IDF + SVM'''\n\n    _default_name = 'model_tfidf_svm'\n\n    def __init__(self, tfidf_params: Union[dict, None] = None, svc_params: Union[dict, None] = None,\n                 multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelPipeline &amp; ModelClass for more arguments)\n\n        Kwargs:\n            tfidf_params (dict) : Parameters for the tfidf\n            svc_params (dict) : Parameters for the SVC\n            multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n        Raises:\n            ValueError: If multiclass_strategy is not 'ovo', 'ovr' or None\n        '''\n        if multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\n            raise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Manage model\n        if tfidf_params is None:\n            tfidf_params = {}\n        self.tfidf = TfidfVectorizer(**tfidf_params)\n        if svc_params is None:\n            svc_params = {}\n        svc_params['random_state'] = self.random_seed\n        self.svc = LinearSVC(**svc_params)\n        self.multiclass_strategy = multiclass_strategy\n\n        # Can't do multi-labels / multi-classes\n        if not self.multi_label:\n            # If not multi-classes : no impact\n            if multiclass_strategy == 'ovr':\n                self.pipeline = Pipeline([('tfidf', self.tfidf), ('svc', OneVsRestClassifier(self.svc))])\n            elif multiclass_strategy == 'ovo':\n                self.pipeline = Pipeline([('tfidf', self.tfidf), ('svc', OneVsOneClassifier(self.svc))])\n            else:\n                self.pipeline = Pipeline([('tfidf', self.tfidf), ('svc', self.svc)])\n\n        # Manage multi-labels -&gt; add a MultiOutputClassifier\n        # The SVC does not natively support multi-labels\n        if self.multi_label:\n            self.pipeline = Pipeline([('tfidf', self.tfidf), ('svc', MultiOutputClassifier(self.svc))])\n\n    @utils.data_agnostic_str_to_list\n    @utils.trained_needed\n    def predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n        '''Predicts the probabilities on the test set\n        - /!\\\\ THE SVM DOES NOT RETURN PROBABILITIES, HERE WE SIMULATE PROBABILITIES EQUAL TO 0 OR 1 /!\\\\ -\n\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n        if not self.multi_label:\n            preds = self.pipeline.predict(x_test)\n            # Format ['a', 'b', 'c', 'a', ..., 'b']\n            # Transform to \"proba\"\n            transform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\n            probas = np.array([transform_dict[x] for x in preds])\n        else:\n            preds = self.pipeline.predict(x_test)\n            # Already right format, but in int !\n            probas = np.array([[float(_) for _ in x] for x in preds])\n        return probas\n\n    @utils.data_agnostic_str_to_list\n    @utils.trained_needed\n    def decision_function(self, x_test, **kwargs) -&gt; np.ndarray:\n        '''Predict confidence scores for samples\n\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n        Returns:\n            (?): Array, shape = [n_samples]\n        '''\n        if self.multi_label:\n            raise ValueError(\"The method 'decision_function' is not compatible with a multi-labels case.\")\n        return self.pipeline.decision_function(x_test)\n\n    def get_predict_position(self, x_test, y_true, **kwargs) -&gt; np.ndarray:\n        '''Gets the order of predictions of y_true.\n        Positions start at 1 (not 0)\n\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n            y_true (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            np.ndarray: Array, shape = [n_samples]\n        '''\n        self.logger.warning(\"Warning, the method get_predict_position is not suitable for a SVM model\"\n                            \"(no probabilities, we use 1 or 0)\")\n        return super().get_predict_position(x_test, y_true)\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save model\n        if json_data is None:\n            json_data = {}\n\n        # No need to save the parameters of the pipeline steps, it is already done in ModelPipeline\n        json_data['multiclass_strategy'] = self.multiclass_strategy\n\n        # Save\n        super().save(json_data=json_data)\n\n    @classmethod\n    def _init_new_instance_from_configs(cls, configs):\n        '''Inits a new instance from a set of configurations\n\n        Args:\n            configs: a set of configurations of a model to be reloaded\n        Returns:\n            ModelClass: the newly generated class\n        '''\n        # Call parent\n        model = super()._init_new_instance_from_configs(configs)\n\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['multiclass_strategy']:\n            setattr(model, attribute, configs.get(attribute, getattr(model, attribute)))\n\n        # Return the new model\n        return model\n\n    def _load_standalone_files(self, default_model_dir: Union[str, None] = None, *args, **kwargs):\n        '''Loads standalone files for a newly created model via _init_new_instance_from_configs\n\n        Kwargs:\n            default_model_dir (str): a path to look for default file paths\n                                     If None, standalone files path should all be provided\n        '''\n        # Call parent\n        super()._load_standalone_files(default_model_dir=default_model_dir, **kwargs)\n\n        # Reload pipeline elements\n        self.tfidf = self.pipeline['tfidf']\n\n        # Manage multi-labels or multi-classes\n        if not self.multi_label and self.multiclass_strategy is None:\n            self.svc = self.pipeline['svc']\n        else:\n            self.svc = self.pipeline['svc'].estimator\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_svm/#template_nlp.models_training.models_sklearn.model_tfidf_svm.ModelTfidfSvm.__init__","title":"<code>__init__(tfidf_params=None, svc_params=None, multiclass_strategy=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline &amp; ModelClass for more arguments)</p> Kwargs <p>tfidf_params (dict) : Parameters for the tfidf svc_params (dict) : Parameters for the SVC multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.</p> <p>Raises:     ValueError: If multiclass_strategy is not 'ovo', 'ovr' or None</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_svm.py</code> <pre><code>def __init__(self, tfidf_params: Union[dict, None] = None, svc_params: Union[dict, None] = None,\n             multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelPipeline &amp; ModelClass for more arguments)\n\n    Kwargs:\n        tfidf_params (dict) : Parameters for the tfidf\n        svc_params (dict) : Parameters for the SVC\n        multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n    Raises:\n        ValueError: If multiclass_strategy is not 'ovo', 'ovr' or None\n    '''\n    if multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\n        raise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Manage model\n    if tfidf_params is None:\n        tfidf_params = {}\n    self.tfidf = TfidfVectorizer(**tfidf_params)\n    if svc_params is None:\n        svc_params = {}\n    svc_params['random_state'] = self.random_seed\n    self.svc = LinearSVC(**svc_params)\n    self.multiclass_strategy = multiclass_strategy\n\n    # Can't do multi-labels / multi-classes\n    if not self.multi_label:\n        # If not multi-classes : no impact\n        if multiclass_strategy == 'ovr':\n            self.pipeline = Pipeline([('tfidf', self.tfidf), ('svc', OneVsRestClassifier(self.svc))])\n        elif multiclass_strategy == 'ovo':\n            self.pipeline = Pipeline([('tfidf', self.tfidf), ('svc', OneVsOneClassifier(self.svc))])\n        else:\n            self.pipeline = Pipeline([('tfidf', self.tfidf), ('svc', self.svc)])\n\n    # Manage multi-labels -&gt; add a MultiOutputClassifier\n    # The SVC does not natively support multi-labels\n    if self.multi_label:\n        self.pipeline = Pipeline([('tfidf', self.tfidf), ('svc', MultiOutputClassifier(self.svc))])\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_svm/#template_nlp.models_training.models_sklearn.model_tfidf_svm.ModelTfidfSvm.decision_function","title":"<code>decision_function(x_test, **kwargs)</code>","text":"<p>Predict confidence scores for samples</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples, n_features]</p> required <p>Returns:     (?): Array, shape = [n_samples]</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_svm.py</code> <pre><code>@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef decision_function(self, x_test, **kwargs) -&gt; np.ndarray:\n    '''Predict confidence scores for samples\n\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n    Returns:\n        (?): Array, shape = [n_samples]\n    '''\n    if self.multi_label:\n        raise ValueError(\"The method 'decision_function' is not compatible with a multi-labels case.\")\n    return self.pipeline.decision_function(x_test)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_svm/#template_nlp.models_training.models_sklearn.model_tfidf_svm.ModelTfidfSvm.get_predict_position","title":"<code>get_predict_position(x_test, y_true, **kwargs)</code>","text":"<p>Gets the order of predictions of y_true. Positions start at 1 (not 0)</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples, n_features]</p> required <code>y_true</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <p>Returns:     np.ndarray: Array, shape = [n_samples]</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_svm.py</code> <pre><code>def get_predict_position(self, x_test, y_true, **kwargs) -&gt; np.ndarray:\n    '''Gets the order of predictions of y_true.\n    Positions start at 1 (not 0)\n\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n        y_true (?): Array-like, shape = [n_samples, n_features]\n    Returns:\n        np.ndarray: Array, shape = [n_samples]\n    '''\n    self.logger.warning(\"Warning, the method get_predict_position is not suitable for a SVM model\"\n                        \"(no probabilities, we use 1 or 0)\")\n    return super().get_predict_position(x_test, y_true)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_svm/#template_nlp.models_training.models_sklearn.model_tfidf_svm.ModelTfidfSvm.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set - /! THE SVM DOES NOT RETURN PROBABILITIES, HERE WE SIMULATE PROBABILITIES EQUAL TO 0 OR 1 /! -</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples, n_features]</p> required <p>Returns:     (np.ndarray): Array, shape = [n_samples, n_classes]</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_svm.py</code> <pre><code>@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n    '''Predicts the probabilities on the test set\n    - /!\\\\ THE SVM DOES NOT RETURN PROBABILITIES, HERE WE SIMULATE PROBABILITIES EQUAL TO 0 OR 1 /!\\\\ -\n\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n    if not self.multi_label:\n        preds = self.pipeline.predict(x_test)\n        # Format ['a', 'b', 'c', 'a', ..., 'b']\n        # Transform to \"proba\"\n        transform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\n        probas = np.array([transform_dict[x] for x in preds])\n    else:\n        preds = self.pipeline.predict(x_test)\n        # Already right format, but in int !\n        probas = np.array([[float(_) for _ in x] for x in preds])\n    return probas\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_svm/#template_nlp.models_training.models_sklearn.model_tfidf_svm.ModelTfidfSvm.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_svm.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save model\n    if json_data is None:\n        json_data = {}\n\n    # No need to save the parameters of the pipeline steps, it is already done in ModelPipeline\n    json_data['multiclass_strategy'] = self.multiclass_strategy\n\n    # Save\n    super().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/","title":"Models tensorflow","text":""},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_cnn/","title":"Model embedding cnn","text":""},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_cnn/#template_nlp.models_training.models_tensorflow.model_embedding_cnn.ModelEmbeddingCnn","title":"<code>ModelEmbeddingCnn</code>","text":"<p>             Bases: <code>ModelKeras</code></p> <p>Model for predictions via embedding + CNN</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_cnn.py</code> <pre><code>class ModelEmbeddingCnn(ModelKeras):\n    '''Model for predictions via embedding + CNN'''\n\n    _default_name = 'model_embedding_cnn'\n\n    def __init__(self, max_sequence_length: int = 200, max_words: int = 100000,\n                 padding: str = 'pre', truncating: str = 'post',\n                 tokenizer_filters: str = \"\u2019!#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~\\t\\n\\r\\'\\\"\", **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelClass &amp; ModelKeras for more arguments)\n\n        Kwargs:\n            max_sequence_length (int): Maximum number of words per sequence (ie. sentences)\n            max_words (int): Maximum number of words for tokenization\n            padding (str): Padding (add zeros) at the beginning ('pre') or at the end ('post') of the sequences\n            truncating (str): Truncating the beginning ('pre') or the end ('post') of the sequences (if superior to max_sequence_length)\n            tokenizer_filters (str): Filter to use by the tokenizer\n        Raises:\n            ValueError: If the object padding is not a valid choice (['pre', 'post'])\n            ValueError: If the object truncating is not a valid choice (['pre', 'post'])\n        '''\n        if padding not in ['pre', 'post']:\n            raise ValueError(f\"The object padding ({padding}) is not a valid choice (['pre', 'post'])\")\n        if truncating not in ['pre', 'post']:\n            raise ValueError(f\"The object truncating ({truncating}) is not a valid choice (['pre', 'post'])\")\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        self.max_sequence_length = max_sequence_length\n        self.max_words = max_words\n        self.padding = padding\n        self.truncating = truncating\n\n        # Tokenizer set on fit\n        self.tokenizer: Any = None\n        self.tokenizer_filters = tokenizer_filters\n\n    def _prepare_x_train(self, x_train) -&gt; np.ndarray:\n        '''Prepares the input data for the model. Called when fitting the model\n\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Prepared data\n        '''\n        # Get tokenizer &amp; fit on train\n        self.tokenizer = Tokenizer(num_words=self.max_words, filters=self.tokenizer_filters)\n        self.logger.info('Fitting the tokenizer')\n        self.tokenizer.fit_on_texts(x_train)\n        return self._get_sequence(x_train, self.tokenizer, self.max_sequence_length, padding=self.padding, truncating=self.truncating)\n\n    def _prepare_x_test(self, x_test) -&gt; np.ndarray:\n        '''Prepares the input data for the model. Called when fitting the model\n\n        Args:\n            x_test (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Prepared data\n        '''\n        # Get sequences on test (already fitted on train)\n        return self._get_sequence(x_test, self.tokenizer, self.max_sequence_length, padding=self.padding, truncating=self.truncating)\n\n    def _get_model(self, custom_tokenizer=None) -&gt; Any:\n        '''Gets a model structure - returns the instance model instead if already defined\n\n        Kwargs:\n            custom_tokenizer (?): Tokenizer (if different from the one of the class). Permits to manage \"new embeddings\"\n        Returns:\n            (Model): a Keras model\n        '''\n        # Return model if already set\n        if self.model is not None:\n            return self.model\n\n        # Start by getting embedding matrix\n        if custom_tokenizer is not None:\n            embedding_matrix, embedding_size = self._get_embedding_matrix(custom_tokenizer)\n        else:\n            embedding_matrix, embedding_size = self._get_embedding_matrix(self.tokenizer)\n\n        # Get input dim\n        input_dim = embedding_matrix.shape[0]\n\n        # Get model\n        num_classes = len(self.list_classes)\n\n        # Get random_state\n        random_state = np.random.RandomState(self.random_seed)\n        limit = int(1e9)\n\n        # Process\n        model = Sequential()\n\n        model.add(Embedding(input_dim, embedding_size,\n                            weights=[embedding_matrix],\n                            input_length=self.max_sequence_length,\n                            trainable=False))\n\n        model.add(Conv1D(128, 3, activation=None, kernel_initializer=HeUniform(random_state.randint(limit))))\n        model.add(BatchNormalization(momentum=0.9))\n        model.add(ELU(alpha=1.0))\n        model.add(AveragePooling1D(2))\n\n        model.add(Conv1D(128, 3, activation=None, kernel_initializer=HeUniform(random_state.randint(limit))))\n        model.add(BatchNormalization(momentum=0.9))\n        model.add(ELU(alpha=1.0))\n        model.add(GlobalMaxPooling1D())\n\n        model.add(Dense(512, activation=None, kernel_initializer=HeUniform(random_state.randint(limit))))\n        model.add(BatchNormalization(momentum=0.9))\n        model.add(ELU(alpha=1.0))\n        model.add(Dropout(0.5, seed=random_state.randint(limit)))\n\n        model.add(Dense(512, activation=None, kernel_initializer=HeUniform(random_state.randint(limit))))\n        model.add(BatchNormalization(momentum=0.9))\n        model.add(ELU(alpha=1.0))\n        model.add(Dropout(0.5, seed=random_state.randint(limit)))\n\n        # Last layer\n        activation = 'sigmoid' if self.multi_label else 'softmax'\n        model.add(Dense(num_classes, activation=activation, kernel_initializer=GlorotUniform(random_state.randint(limit))))\n\n        # Compile model\n        lr = self.keras_params.get('learning_rate', 0.002)\n        decay = self.keras_params.get('decay', 0.0)\n        self.logger.info(f\"Learning rate: {lr}\")\n        self.logger.info(f\"Decay: {decay}\")\n        optimizer = Adam(lr=lr, decay=decay)\n        # loss = utils_deep_keras.f1_loss if self.multi_label else 'categorical_crossentropy'\n        loss = 'binary_crossentropy' if self.multi_label else 'categorical_crossentropy'  # utils_deep_keras.f1_loss also possible if multi-labels\n        metrics: List[Union[str, Callable]] = ['accuracy'] if not self.multi_label else ['categorical_accuracy', utils_deep_keras.f1, utils_deep_keras.precision, utils_deep_keras.recall]\n        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n        if self.logger.getEffectiveLevel() &lt; logging.ERROR:\n            model.summary()\n\n        # Try to save model as png if level_save &gt; 'LOW'\n        if self.level_save in ['MEDIUM', 'HIGH']:\n            self._save_model_png(model)\n\n        # Return\n        return model\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save configuration JSON\n        if json_data is None:\n            json_data = {}\n\n        # Add specific data\n        json_data['max_sequence_length'] = self.max_sequence_length\n        json_data['max_words'] = self.max_words\n        json_data['padding'] = self.padding\n        json_data['truncating'] = self.truncating\n        json_data['tokenizer_filters'] = self.tokenizer_filters\n\n        # Save tokenizer if not None &amp; level_save &gt; LOW\n        if (self.tokenizer is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n            # Manage paths\n            tokenizer_path = os.path.join(self.model_dir, \"embedding_tokenizer.pkl\")\n            # Save as pickle\n            with open(tokenizer_path, 'wb') as f:\n                pickle.dump(self.tokenizer, f)\n\n        # Save\n        super().save(json_data=json_data)\n\n    @classmethod\n    def _init_new_instance_from_configs(cls, configs):\n        '''Inits a new instance from a set of configurations\n\n        Args:\n            configs: a set of configurations of a model to be reloaded\n        Returns:\n            ModelClass: the newly generated class\n        '''\n        # Call parent\n        model = super()._init_new_instance_from_configs(configs)\n\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['max_sequence_length', 'max_words', 'padding', 'truncating', 'tokenizer_filters']:\n            setattr(model, attribute, configs.get(attribute, getattr(model, attribute)))\n\n        # Return the new model\n        return model\n\n    def _load_standalone_files(self, default_model_dir: Union[str, None] = None,  # type: ignore\n                               tokenizer_path: Union[str, None] = None, *args, **kwargs):\n        '''Loads standalone files for a newly created model via _init_new_instance_from_configs\n\n        Kwargs:\n            default_model_dir (str): a path to look for default file paths\n                                     If None, standalone files path should all be provided\n            tokenizer_path (str): Path to the tokenizer file\n        Raises:\n            ValueError: If the tokenizer file is not specified and can't be inferred\n            FileNotFoundError: If the tokenizer file does not exist\n        '''\n        # Check if we are able to get all needed paths\n        if default_model_dir is None and tokenizer_path is None:\n            raise ValueError(\"The tokenizer file is not specified and can't be inferred\")\n\n        # Call parent\n        super()._load_standalone_files(default_model_dir=default_model_dir, **kwargs)\n\n        # Retrieve file paths\n        if tokenizer_path is None:\n            tokenizer_path = os.path.join(default_model_dir, \"embedding_tokenizer.pkl\")\n\n        # Check paths exists\n        if not os.path.isfile(tokenizer_path):\n            raise FileNotFoundError(f\"Can't find tokenizer file ({tokenizer_path})\")\n\n        # Reload tokenizer\n        with open(tokenizer_path, 'rb') as f:\n            self.tokenizer = pickle.load(f)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_cnn/#template_nlp.models_training.models_tensorflow.model_embedding_cnn.ModelEmbeddingCnn.__init__","title":"<code>__init__(max_sequence_length=200, max_words=100000, padding='pre', truncating='post', tokenizer_filters='\u2019!#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~\\t\\n\\r\\'\"', **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass &amp; ModelKeras for more arguments)</p> Kwargs <p>max_sequence_length (int): Maximum number of words per sequence (ie. sentences) max_words (int): Maximum number of words for tokenization padding (str): Padding (add zeros) at the beginning ('pre') or at the end ('post') of the sequences truncating (str): Truncating the beginning ('pre') or the end ('post') of the sequences (if superior to max_sequence_length) tokenizer_filters (str): Filter to use by the tokenizer</p> <p>Raises:     ValueError: If the object padding is not a valid choice (['pre', 'post'])     ValueError: If the object truncating is not a valid choice (['pre', 'post'])</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_cnn.py</code> <pre><code>def __init__(self, max_sequence_length: int = 200, max_words: int = 100000,\n             padding: str = 'pre', truncating: str = 'post',\n             tokenizer_filters: str = \"\u2019!#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~\\t\\n\\r\\'\\\"\", **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelClass &amp; ModelKeras for more arguments)\n\n    Kwargs:\n        max_sequence_length (int): Maximum number of words per sequence (ie. sentences)\n        max_words (int): Maximum number of words for tokenization\n        padding (str): Padding (add zeros) at the beginning ('pre') or at the end ('post') of the sequences\n        truncating (str): Truncating the beginning ('pre') or the end ('post') of the sequences (if superior to max_sequence_length)\n        tokenizer_filters (str): Filter to use by the tokenizer\n    Raises:\n        ValueError: If the object padding is not a valid choice (['pre', 'post'])\n        ValueError: If the object truncating is not a valid choice (['pre', 'post'])\n    '''\n    if padding not in ['pre', 'post']:\n        raise ValueError(f\"The object padding ({padding}) is not a valid choice (['pre', 'post'])\")\n    if truncating not in ['pre', 'post']:\n        raise ValueError(f\"The object truncating ({truncating}) is not a valid choice (['pre', 'post'])\")\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    self.max_sequence_length = max_sequence_length\n    self.max_words = max_words\n    self.padding = padding\n    self.truncating = truncating\n\n    # Tokenizer set on fit\n    self.tokenizer: Any = None\n    self.tokenizer_filters = tokenizer_filters\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_cnn/#template_nlp.models_training.models_tensorflow.model_embedding_cnn.ModelEmbeddingCnn.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_cnn.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save configuration JSON\n    if json_data is None:\n        json_data = {}\n\n    # Add specific data\n    json_data['max_sequence_length'] = self.max_sequence_length\n    json_data['max_words'] = self.max_words\n    json_data['padding'] = self.padding\n    json_data['truncating'] = self.truncating\n    json_data['tokenizer_filters'] = self.tokenizer_filters\n\n    # Save tokenizer if not None &amp; level_save &gt; LOW\n    if (self.tokenizer is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n        # Manage paths\n        tokenizer_path = os.path.join(self.model_dir, \"embedding_tokenizer.pkl\")\n        # Save as pickle\n        with open(tokenizer_path, 'wb') as f:\n            pickle.dump(self.tokenizer, f)\n\n    # Save\n    super().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm/","title":"Model embedding lstm","text":""},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm/#template_nlp.models_training.models_tensorflow.model_embedding_lstm.ModelEmbeddingLstm","title":"<code>ModelEmbeddingLstm</code>","text":"<p>             Bases: <code>ModelKeras</code></p> <p>Model for prediction via embedding + LSTM</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_lstm.py</code> <pre><code>class ModelEmbeddingLstm(ModelKeras):\n    '''Model for prediction via embedding + LSTM'''\n\n    _default_name = 'model_embedding_lstm'\n\n    def __init__(self, max_sequence_length: int = 200, max_words: int = 100000,\n                 padding: str = 'pre', truncating: str = 'post',\n                 tokenizer_filters=\"\u2019!#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~\\t\\n\\r\\'\\\"\", **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelClass &amp; ModelKeras for more arguments)\n\n        Kwargs:\n            max_sequence_length (int): Maximum number of words per sequence (ie. sentences)\n            max_words (int): Maximum number of words for tokenization\n            padding (str): Padding (add zeros) at the beginning ('pre') or at the end ('post') of the sequences\n            truncating (str): Truncating the beginning ('pre') or the end ('post') of the sequences (if superior to max_sequence_length)\n            tokenizer_filters (str): Filter to use by the tokenizer\n        Raises:\n            ValueError: If the object padding is not a valid choice (['pre', 'post'])\n            ValueError: If the object truncating is not a valid choice (['pre', 'post'])\n        '''\n        if padding not in ['pre', 'post']:\n            raise ValueError(f\"The object padding ({padding}) is not a valid choice (['pre', 'post'])\")\n        if truncating not in ['pre', 'post']:\n            raise ValueError(f\"The object truncating ({truncating}) is not a valid choice (['pre', 'post'])\")\n\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        self.max_sequence_length = max_sequence_length\n        self.max_words = max_words\n        self.padding = padding\n        self.truncating = truncating\n\n        # Tokenizer set on fit\n        self.tokenizer: Any = None\n        self.tokenizer_filters = tokenizer_filters\n\n    def _prepare_x_train(self, x_train) -&gt; np.ndarray:\n        '''Prepares the input data for the model. Called when fitting the model\n\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Prepared data\n        '''\n        # Get tokenizer &amp; fit on train\n        self.tokenizer = Tokenizer(num_words=self.max_words, filters=self.tokenizer_filters)\n        self.logger.info('Fitting the tokenizer')\n        self.tokenizer.fit_on_texts(x_train)\n        return self._get_sequence(x_train, self.tokenizer, self.max_sequence_length, padding=self.padding, truncating=self.truncating)\n\n    def _prepare_x_test(self, x_test) -&gt; np.ndarray:\n        '''Prepares the input data for the model. Called when fitting the model\n\n        Args:\n            x_test (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Prepared data\n        '''\n        # Get sequences on test (already fitted on train)\n        return self._get_sequence(x_test, self.tokenizer, self.max_sequence_length, padding=self.padding, truncating=self.truncating)\n\n    def _get_model(self, custom_tokenizer=None) -&gt; Any:\n        '''Gets a model structure - returns the instance model instead if already defined\n\n        Kwargs:\n            custom_tokenizer (?): Tokenizer (if different from the one of the class). Permits to manage \"new embeddings\"\n        Returns:\n            (Model): a Keras model\n        '''\n        # Return model if already set\n        if self.model is not None:\n            return self.model\n\n        # Start by getting embedding matrix\n        if custom_tokenizer is not None:\n            embedding_matrix, embedding_size = self._get_embedding_matrix(custom_tokenizer)\n        else:\n            embedding_matrix, embedding_size = self._get_embedding_matrix(self.tokenizer)\n\n        # Get input dim\n        input_dim = embedding_matrix.shape[0]\n\n        # Get random_state\n        random_state = np.random.RandomState(self.random_seed)\n        limit = int(1e9)\n\n        # Get model\n        num_classes = len(self.list_classes)\n        # Process\n        LSTM_UNITS = 100\n        DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n        words = Input(shape=(self.max_sequence_length,))\n        x = Embedding(input_dim, embedding_size, weights=[embedding_matrix], trainable=False)(words)\n        x = BatchNormalization(momentum=0.9)(x)\n        x = SpatialDropout1D(0.5, seed=random_state.randint(limit))(x)\n        x = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True, kernel_initializer=GlorotUniform(random_state.randint(limit)), \n                               recurrent_initializer=Orthogonal(seed=random_state.randint(limit))))(x)\n        x = SpatialDropout1D(0.5, seed=random_state.randint(limit))(x)\n        hidden = concatenate([\n            GlobalMaxPooling1D()(x),\n            GlobalAveragePooling1D()(x),\n        ])\n        hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu', kernel_initializer=GlorotUniform(random_state.randint(limit)))(hidden)])\n        # Last layer\n        activation = 'sigmoid' if self.multi_label else 'softmax'\n        out = Dense(num_classes, activation=activation, kernel_initializer=GlorotUniform(random_state.randint(limit)))(hidden)\n\n        # Compile model\n        model = Model(inputs=words, outputs=[out])\n        lr = self.keras_params['learning_rate'] if 'learning_rate' in self.keras_params.keys() else 0.01\n        decay = self.keras_params['decay'] if 'decay' in self.keras_params.keys() else 0.004\n        self.logger.info(f\"Learning rate: {lr}\")\n        self.logger.info(f\"Decay: {decay}\")\n        optimizer = Adam(lr=lr, decay=decay)\n        # loss = utils_deep_keras.f1_loss if self.multi_label else 'categorical_crossentropy'\n        loss = 'binary_crossentropy' if self.multi_label else 'categorical_crossentropy'  # utils_deep_keras.f1_loss also possible if multi-labels\n        metrics: List[Union[str, Callable]] = ['accuracy'] if not self.multi_label else ['categorical_accuracy', utils_deep_keras.f1, utils_deep_keras.precision, utils_deep_keras.recall]\n        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n        if self.logger.getEffectiveLevel() &lt; logging.ERROR:\n            model.summary()\n\n        # Try to save model as png if level_save &gt; 'LOW'\n        if self.level_save in ['MEDIUM', 'HIGH']:\n            self._save_model_png(model)\n\n        # Return\n        return model\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save configuration JSON\n        if json_data is None:\n            json_data = {}\n\n        # Add specific data\n        json_data['max_sequence_length'] = self.max_sequence_length\n        json_data['max_words'] = self.max_words\n        json_data['padding'] = self.padding\n        json_data['truncating'] = self.truncating\n        json_data['tokenizer_filters'] = self.tokenizer_filters\n\n        # Save tokenizer if not None &amp; level_save &gt; LOW\n        if (self.tokenizer is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n            # Manage paths\n            tokenizer_path = os.path.join(self.model_dir, \"embedding_tokenizer.pkl\")\n            # Save as pickle\n            with open(tokenizer_path, 'wb') as f:\n                pickle.dump(self.tokenizer, f)\n\n        # Save\n        super().save(json_data=json_data)\n\n    @classmethod\n    def _init_new_instance_from_configs(cls, configs):\n        '''Inits a new instance from a set of configurations\n\n        Args:\n            configs: a set of configurations of a model to be reloaded\n        Returns:\n            ModelClass: the newly generated class\n        '''\n        # Call parent\n        model = super()._init_new_instance_from_configs(configs)\n\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['max_sequence_length', 'max_words', 'padding', 'truncating', 'tokenizer_filters']:\n            setattr(model, attribute, configs.get(attribute, getattr(model, attribute)))\n\n        # Return the new model\n        return model\n\n    def _load_standalone_files(self, default_model_dir: Union[str, None] = None,  # type: ignore\n                               tokenizer_path: Union[str, None] = None, *args, **kwargs):\n        '''Loads standalone files for a newly created model via _init_new_instance_from_configs\n\n        Kwargs:\n            default_model_dir (str): a path to look for default file paths\n                                     If None, standalone files path should all be provided\n            tokenizer_path (str): Path to the tokenizer file\n        Raises:\n            ValueError: If the tokenizer file is not specified and can't be inferred\n            FileNotFoundError: If the tokenizer file does not exist\n        '''\n        # Check if we are able to get all needed paths\n        if default_model_dir is None and tokenizer_path is None:\n            raise ValueError(\"The tokenizer file is not specified and can't be inferred\")\n\n        # Call parent\n        super()._load_standalone_files(default_model_dir=default_model_dir, **kwargs)\n\n        # Retrieve file paths\n        if tokenizer_path is None:\n            tokenizer_path = os.path.join(default_model_dir, \"embedding_tokenizer.pkl\")\n\n        # Check paths exists\n        if not os.path.isfile(tokenizer_path):\n            raise FileNotFoundError(f\"Can't find tokenizer file ({tokenizer_path})\")\n\n        # Reload tokenizer\n        with open(tokenizer_path, 'rb') as f:\n            self.tokenizer = pickle.load(f)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm/#template_nlp.models_training.models_tensorflow.model_embedding_lstm.ModelEmbeddingLstm.__init__","title":"<code>__init__(max_sequence_length=200, max_words=100000, padding='pre', truncating='post', tokenizer_filters='\u2019!#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~\\t\\n\\r\\'\"', **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass &amp; ModelKeras for more arguments)</p> Kwargs <p>max_sequence_length (int): Maximum number of words per sequence (ie. sentences) max_words (int): Maximum number of words for tokenization padding (str): Padding (add zeros) at the beginning ('pre') or at the end ('post') of the sequences truncating (str): Truncating the beginning ('pre') or the end ('post') of the sequences (if superior to max_sequence_length) tokenizer_filters (str): Filter to use by the tokenizer</p> <p>Raises:     ValueError: If the object padding is not a valid choice (['pre', 'post'])     ValueError: If the object truncating is not a valid choice (['pre', 'post'])</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_lstm.py</code> <pre><code>def __init__(self, max_sequence_length: int = 200, max_words: int = 100000,\n             padding: str = 'pre', truncating: str = 'post',\n             tokenizer_filters=\"\u2019!#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~\\t\\n\\r\\'\\\"\", **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelClass &amp; ModelKeras for more arguments)\n\n    Kwargs:\n        max_sequence_length (int): Maximum number of words per sequence (ie. sentences)\n        max_words (int): Maximum number of words for tokenization\n        padding (str): Padding (add zeros) at the beginning ('pre') or at the end ('post') of the sequences\n        truncating (str): Truncating the beginning ('pre') or the end ('post') of the sequences (if superior to max_sequence_length)\n        tokenizer_filters (str): Filter to use by the tokenizer\n    Raises:\n        ValueError: If the object padding is not a valid choice (['pre', 'post'])\n        ValueError: If the object truncating is not a valid choice (['pre', 'post'])\n    '''\n    if padding not in ['pre', 'post']:\n        raise ValueError(f\"The object padding ({padding}) is not a valid choice (['pre', 'post'])\")\n    if truncating not in ['pre', 'post']:\n        raise ValueError(f\"The object truncating ({truncating}) is not a valid choice (['pre', 'post'])\")\n\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    self.max_sequence_length = max_sequence_length\n    self.max_words = max_words\n    self.padding = padding\n    self.truncating = truncating\n\n    # Tokenizer set on fit\n    self.tokenizer: Any = None\n    self.tokenizer_filters = tokenizer_filters\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm/#template_nlp.models_training.models_tensorflow.model_embedding_lstm.ModelEmbeddingLstm.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_lstm.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save configuration JSON\n    if json_data is None:\n        json_data = {}\n\n    # Add specific data\n    json_data['max_sequence_length'] = self.max_sequence_length\n    json_data['max_words'] = self.max_words\n    json_data['padding'] = self.padding\n    json_data['truncating'] = self.truncating\n    json_data['tokenizer_filters'] = self.tokenizer_filters\n\n    # Save tokenizer if not None &amp; level_save &gt; LOW\n    if (self.tokenizer is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n        # Manage paths\n        tokenizer_path = os.path.join(self.model_dir, \"embedding_tokenizer.pkl\")\n        # Save as pickle\n        with open(tokenizer_path, 'wb') as f:\n            pickle.dump(self.tokenizer, f)\n\n    # Save\n    super().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm_attention/","title":"Model embedding lstm attention","text":""},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm_attention/#template_nlp.models_training.models_tensorflow.model_embedding_lstm_attention.ModelEmbeddingLstmAttention","title":"<code>ModelEmbeddingLstmAttention</code>","text":"<p>             Bases: <code>ModelKeras</code></p> <p>Model for predictions via embedding + LSTM + Attention</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_lstm_attention.py</code> <pre><code>class ModelEmbeddingLstmAttention(ModelKeras):\n    '''Model for predictions via embedding + LSTM + Attention'''\n\n    _default_name = 'model_embedding_lstm_attention'\n\n    def __init__(self, max_sequence_length: int = 200, max_words: int = 100000,\n                 padding: str = 'pre', truncating: str = 'post',\n                 tokenizer_filters: str = \"\u2019!#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~\\t\\n\\r\\'\\\"\", **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelClass &amp; ModelKeras for more arguments)\n\n        Kwargs:\n            max_sequence_length (int): Maximum number of words per sequence (ie. sentences)\n            max_words (int): Maximum number of words for tokenization\n            padding (str): Padding (add zeros) at the beginning ('pre') or at the end ('post') of the sequences\n            truncating (str): Truncating the beginning ('pre') or the end ('post') of the sequences (if superior to max_sequence_length)\n            tokenizer_filters (str): Filter to use by the tokenizer\n        Raises:\n            ValueError: If the object padding is not a valid choice (['pre', 'post'])\n            ValueError: If the object truncating is not a valid choice (['pre', 'post'])\n        '''\n        if padding not in ['pre', 'post']:\n            raise ValueError(f\"The object padding ({padding}) is not a valid choice (['pre', 'post'])\")\n        if truncating not in ['pre', 'post']:\n            raise ValueError(f\"The object truncating ({truncating}) is not a valid choice (['pre', 'post'])\")\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        self.max_sequence_length = max_sequence_length\n        self.max_words = max_words\n        self.padding = padding\n        self.truncating = truncating\n\n        # Tokenizer set on fit\n        self.tokenizer: Any = None\n        self.tokenizer_filters = tokenizer_filters\n\n    def _prepare_x_train(self, x_train) -&gt; np.ndarray:\n        '''Prepares the input data for the model. Called when fitting the model\n\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Prepared data\n        '''\n        # Get tokenizer &amp; fit on train\n        self.tokenizer = Tokenizer(num_words=self.max_words, filters=self.tokenizer_filters)\n        self.logger.info('Fitting the tokenizer')\n        self.tokenizer.fit_on_texts(x_train)\n        return self._get_sequence(x_train, self.tokenizer, self.max_sequence_length, padding=self.padding, truncating=self.truncating)\n\n    def _prepare_x_test(self, x_test) -&gt; np.ndarray:\n        '''Prepares the input data for the model. Called when fitting the model\n\n        Args:\n            x_test (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Prepared data\n        '''\n        # Get sequences on test (already fitted on train)\n        return self._get_sequence(x_test, self.tokenizer, self.max_sequence_length, padding=self.padding, truncating=self.truncating)\n\n    def _get_model(self, custom_tokenizer=None) -&gt; Any:\n        '''Gets a model structure - returns the instance model instead if already defined\n\n        Kwargs:\n            custom_tokenizer (?): Tokenizer (if different from the one of the class). Permits to manage \"new embeddings\"\n        Returns:\n            (Model): a Keras model\n        '''\n        # Return model if already set\n        if self.model is not None:\n            return self.model\n\n        # Start by getting embedding matrix\n        if custom_tokenizer is not None:\n            embedding_matrix, embedding_size = self._get_embedding_matrix(custom_tokenizer)\n        else:\n            embedding_matrix, embedding_size = self._get_embedding_matrix(self.tokenizer)\n\n        # Get input dim\n        input_dim = embedding_matrix.shape[0]\n\n        # Get model\n        num_classes = len(self.list_classes)\n\n        # Get random_state\n        random_state = np.random.RandomState(self.random_seed)\n        limit = int(1e9)\n\n        # Process\n        LSTM_UNITS = 100\n        words = Input(shape=(self.max_sequence_length,))\n        # trainable=True to finetune the model\n        # words = Input(shape=(None,))\n        # x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n        x = Embedding(input_dim, embedding_size, weights=[embedding_matrix], trainable=False)(words)\n        x = BatchNormalization(momentum=0.9)(x)\n        x = SpatialDropout1D(0.5, seed=random_state.randint(limit))(x)\n        # LSTM and GRU will default to CuDNNLSTM and CuDNNGRU if all conditions are met:\n        # - activation = 'tanh'\n        # - recurrent_activation = 'sigmoid'\n        # - recurrent_dropout = 0\n        # - unroll = False\n        # - use_bias = True\n        # - Inputs, if masked, are strictly right-padded\n        # - reset_after = True (GRU only)\n        # /!\\ https://stackoverflow.com/questions/60468385/is-there-cudnnlstm-or-cudnngru-alternative-in-tensorflow-2-0\n        x = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True, kernel_initializer=GlorotUniform(random_state.randint(limit)), \n                               recurrent_initializer=Orthogonal(seed=random_state.randint(limit))))(x)  # returns a sequence of vectors of dimension 32\n        x = Bidirectional(GRU(LSTM_UNITS, return_sequences=True, kernel_initializer=GlorotUniform(random_state.randint(limit)), \n                               recurrent_initializer=Orthogonal(seed=random_state.randint(limit))))(x)  # returns a sequence of vectors of dimension 32\n\n        att = AttentionWithContext(w_initializer=GlorotUniform(random_state.randint(limit)), b_initializer=GlorotUniform(random_state.randint(limit)),\n                                   u_initializer=GlorotUniform(random_state.randint(limit)))(x)\n        avg_pool1 = GlobalAveragePooling1D()(x)\n        max_pool1 = GlobalMaxPooling1D()(x)\n\n        x = concatenate([att, avg_pool1, max_pool1])\n        # Last layer\n        activation = 'sigmoid' if self.multi_label else 'softmax'\n        out = Dense(num_classes, activation=activation, kernel_initializer=GlorotUniform(random_state.randint(limit)))(x)\n\n        # Compile model\n        model = Model(inputs=words, outputs=[out])\n        lr = self.keras_params['learning_rate'] if 'learning_rate' in self.keras_params.keys() else 0.001\n        decay = self.keras_params['decay'] if 'decay' in self.keras_params.keys() else 0.0\n        self.logger.info(f\"Learning rate: {lr}\")\n        self.logger.info(f\"Decay: {decay}\")\n        optimizer = Adam(lr=lr, decay=decay)\n        loss = utils_deep_keras.f1_loss if self.multi_label else 'categorical_crossentropy'\n        # loss = 'binary_crossentropy' if self.multi_label else 'categorical_crossentropy'  # utils_deep_keras.f1_loss also possible if multi-labels\n        metrics: List[Union[str, Callable]] = ['accuracy'] if not self.multi_label else ['categorical_accuracy', utils_deep_keras.f1, utils_deep_keras.precision, utils_deep_keras.recall]\n        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n        if self.logger.getEffectiveLevel() &lt; logging.ERROR:\n            model.summary()\n\n        # Try to save model as png if level_save &gt; 'LOW'\n        if self.level_save in ['MEDIUM', 'HIGH']:\n            self._save_model_png(model)\n\n        # Return\n        return model\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save configuration JSON\n        if json_data is None:\n            json_data = {}\n\n        # Add specific data\n        json_data['max_sequence_length'] = self.max_sequence_length\n        json_data['max_words'] = self.max_words\n        json_data['padding'] = self.padding\n        json_data['truncating'] = self.truncating\n        json_data['tokenizer_filters'] = self.tokenizer_filters\n\n        # Save tokenizer if not None &amp; level_save &gt; LOW\n        if (self.tokenizer is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n            # Manage paths\n            tokenizer_path = os.path.join(self.model_dir, \"embedding_tokenizer.pkl\")\n            # Save as pickle\n            with open(tokenizer_path, 'wb') as f:\n                pickle.dump(self.tokenizer, f)\n\n        # Save\n        super().save(json_data=json_data)\n\n    @classmethod\n    def _init_new_instance_from_configs(cls, configs):\n        '''Inits a new instance from a set of configurations\n\n        Args:\n            configs: a set of configurations of a model to be reloaded\n        Returns:\n            ModelClass: the newly generated class\n        '''\n        # Call parent\n        model = super()._init_new_instance_from_configs(configs)\n\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['max_sequence_length', 'max_words', 'padding', 'truncating', 'tokenizer_filters']:\n            setattr(model, attribute, configs.get(attribute, getattr(model, attribute)))\n\n        # Return the new model\n        return model\n\n    def _load_standalone_files(self, default_model_dir: Union[str, None] = None,  # type: ignore\n                               tokenizer_path: Union[str, None] = None, *args, **kwargs):\n        '''Loads standalone files for a newly created model via _init_new_instance_from_configs\n\n        Kwargs:\n            default_model_dir (str): a path to look for default file paths\n                                     If None, standalone files path should all be provided\n            tokenizer_path (str): Path to the tokenizer file\n        Raises:\n            ValueError: If the tokenizer file is not specified and can't be inferred\n            FileNotFoundError: If the tokenizer file does not exist\n        '''\n        # Check if we are able to get all needed paths\n        if default_model_dir is None and tokenizer_path is None:\n            raise ValueError(\"The tokenizer file is not specified and can't be inferred\")\n\n        # Call parent\n        super()._load_standalone_files(default_model_dir=default_model_dir, **kwargs)\n\n        # Retrieve file paths\n        if tokenizer_path is None:\n            tokenizer_path = os.path.join(default_model_dir, \"embedding_tokenizer.pkl\")\n\n        # Check paths exists\n        if not os.path.isfile(tokenizer_path):\n            raise FileNotFoundError(f\"Can't find tokenizer file ({tokenizer_path})\")\n\n        # Reload tokenizer\n        with open(tokenizer_path, 'rb') as f:\n            self.tokenizer = pickle.load(f)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm_attention/#template_nlp.models_training.models_tensorflow.model_embedding_lstm_attention.ModelEmbeddingLstmAttention.__init__","title":"<code>__init__(max_sequence_length=200, max_words=100000, padding='pre', truncating='post', tokenizer_filters='\u2019!#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~\\t\\n\\r\\'\"', **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass &amp; ModelKeras for more arguments)</p> Kwargs <p>max_sequence_length (int): Maximum number of words per sequence (ie. sentences) max_words (int): Maximum number of words for tokenization padding (str): Padding (add zeros) at the beginning ('pre') or at the end ('post') of the sequences truncating (str): Truncating the beginning ('pre') or the end ('post') of the sequences (if superior to max_sequence_length) tokenizer_filters (str): Filter to use by the tokenizer</p> <p>Raises:     ValueError: If the object padding is not a valid choice (['pre', 'post'])     ValueError: If the object truncating is not a valid choice (['pre', 'post'])</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_lstm_attention.py</code> <pre><code>def __init__(self, max_sequence_length: int = 200, max_words: int = 100000,\n             padding: str = 'pre', truncating: str = 'post',\n             tokenizer_filters: str = \"\u2019!#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~\\t\\n\\r\\'\\\"\", **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelClass &amp; ModelKeras for more arguments)\n\n    Kwargs:\n        max_sequence_length (int): Maximum number of words per sequence (ie. sentences)\n        max_words (int): Maximum number of words for tokenization\n        padding (str): Padding (add zeros) at the beginning ('pre') or at the end ('post') of the sequences\n        truncating (str): Truncating the beginning ('pre') or the end ('post') of the sequences (if superior to max_sequence_length)\n        tokenizer_filters (str): Filter to use by the tokenizer\n    Raises:\n        ValueError: If the object padding is not a valid choice (['pre', 'post'])\n        ValueError: If the object truncating is not a valid choice (['pre', 'post'])\n    '''\n    if padding not in ['pre', 'post']:\n        raise ValueError(f\"The object padding ({padding}) is not a valid choice (['pre', 'post'])\")\n    if truncating not in ['pre', 'post']:\n        raise ValueError(f\"The object truncating ({truncating}) is not a valid choice (['pre', 'post'])\")\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    self.max_sequence_length = max_sequence_length\n    self.max_words = max_words\n    self.padding = padding\n    self.truncating = truncating\n\n    # Tokenizer set on fit\n    self.tokenizer: Any = None\n    self.tokenizer_filters = tokenizer_filters\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm_attention/#template_nlp.models_training.models_tensorflow.model_embedding_lstm_attention.ModelEmbeddingLstmAttention.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_lstm_attention.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save configuration JSON\n    if json_data is None:\n        json_data = {}\n\n    # Add specific data\n    json_data['max_sequence_length'] = self.max_sequence_length\n    json_data['max_words'] = self.max_words\n    json_data['padding'] = self.padding\n    json_data['truncating'] = self.truncating\n    json_data['tokenizer_filters'] = self.tokenizer_filters\n\n    # Save tokenizer if not None &amp; level_save &gt; LOW\n    if (self.tokenizer is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n        # Manage paths\n        tokenizer_path = os.path.join(self.model_dir, \"embedding_tokenizer.pkl\")\n        # Save as pickle\n        with open(tokenizer_path, 'wb') as f:\n            pickle.dump(self.tokenizer, f)\n\n    # Save\n    super().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm_gru/","title":"Model embedding lstm gru","text":""},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm_gru/#template_nlp.models_training.models_tensorflow.model_embedding_lstm_gru.ModelEmbeddingLstmGru","title":"<code>ModelEmbeddingLstmGru</code>","text":"<p>             Bases: <code>ModelKeras</code></p> <p>Model for predictions via embedding + LSTM/GRU</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_lstm_gru.py</code> <pre><code>class ModelEmbeddingLstmGru(ModelKeras):\n    '''Model for predictions via embedding + LSTM/GRU'''\n\n    _default_name = 'model_embedding_lstm_gru'\n\n    def __init__(self, max_sequence_length: int = 200, max_words: int = 100000,\n                 padding: str = 'pre', truncating: str = 'post',\n                 tokenizer_filters=\"\u2019!#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~\\t\\n\\r\\'\\\"\", **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelClass &amp; ModelKeras for more arguments)\n\n        Kwargs:\n            max_sequence_length (int): Maximum number of words per sequence (ie. sentences)\n            max_words (int): Maximum number of words for tokenization\n            padding (str): Padding (add zeros) at the beginning ('pre') or at the end ('post') of the sequences\n            truncating (str): Truncating the beginning ('pre') or the end ('post') of the sequences (if superior to max_sequence_length)\n            tokenizer_filters (str): Filter to use by the tokenizer\n        Raises:\n            ValueError: If the object padding is not a valid choice (['pre', 'post'])\n            ValueError: If the object truncating is not a valid choice (['pre', 'post'])\n        '''\n        if padding not in ['pre', 'post']:\n            raise ValueError(f\"The object padding ({padding}) is not a valid choice (['pre', 'post'])\")\n        if truncating not in ['pre', 'post']:\n            raise ValueError(f\"The object truncating ({truncating}) is not a valid choice (['pre', 'post'])\")\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        self.max_sequence_length = max_sequence_length\n        self.max_words = max_words\n        self.padding = padding\n        self.truncating = truncating\n\n        # Tokenizer set on fit\n        self.tokenizer: Any = None\n        self.tokenizer_filters = tokenizer_filters\n\n    def _prepare_x_train(self, x_train) -&gt; np.ndarray:\n        '''Prepares the input data for the model. Called when fitting the model\n\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Prepared data\n        '''\n        # Get tokenizer &amp; fit on train\n        self.tokenizer = Tokenizer(num_words=self.max_words, filters=self.tokenizer_filters)\n        self.logger.info('Fitting the tokenizer')\n        self.tokenizer.fit_on_texts(x_train)\n        return self._get_sequence(x_train, self.tokenizer, self.max_sequence_length, padding=self.padding, truncating=self.truncating)\n\n    def _prepare_x_test(self, x_test) -&gt; np.ndarray:\n        '''Prepares the input data for the model. Called when fitting the model\n\n        Args:\n            x_test (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Prepared data\n        '''\n        # Get sequences on test (already fitted on train)\n        return self._get_sequence(x_test, self.tokenizer, self.max_sequence_length, padding=self.padding, truncating=self.truncating)\n\n    def _get_model(self, custom_tokenizer=None) -&gt; Any:\n        '''Gets a model structure - returns the instance model instead if already defined\n\n        Kwargs:\n            custom_tokenizer (?): Tokenizer (if different from the one of the class). Permits to manage \"new embeddings\"\n        Returns:\n            (Model): a Keras model\n        '''\n        # Return model if already set\n        if self.model is not None:\n            return self.model\n\n        # Start by getting embedding matrix\n        if custom_tokenizer is not None:\n            embedding_matrix, embedding_size = self._get_embedding_matrix(custom_tokenizer)\n        else:\n            embedding_matrix, embedding_size = self._get_embedding_matrix(self.tokenizer)\n\n        # Get input dim\n        input_dim = embedding_matrix.shape[0]\n\n        # Get random_state\n        random_state = np.random.RandomState(self.random_seed)\n        limit = int(1e9)\n\n        # Get model\n        num_classes = len(self.list_classes)\n        # Process\n        LSTM_UNITS = 60\n        GRU_UNITS = 120\n        words = Input(shape=(self.max_sequence_length,))\n        x = Embedding(input_dim, embedding_size, weights=[embedding_matrix], trainable=False)(words)\n        x = SpatialDropout1D(0.5, seed=random_state.randint(limit))(x)\n        # LSTM and GRU will default to CuDNNLSTM and CuDNNGRU if all conditions are met:\n        # - activation = 'tanh'\n        # - recurrent_activation = 'sigmoid'\n        # - recurrent_dropout = 0\n        # - unroll = False\n        # - use_bias = True\n        # - Inputs, if masked, are strictly right-padded\n        # - reset_after = True (GRU only)\n        # /!\\ https://stackoverflow.com/questions/60468385/is-there-cudnnlstm-or-cudnngru-alternative-in-tensorflow-2-0\n        x = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True, kernel_initializer=GlorotUniform(random_state.randint(limit)), \n                               recurrent_initializer=Orthogonal(seed=random_state.randint(limit))))(x)\n        x = BatchNormalization(momentum=0.9)(x)\n        x, state_h, state_c = Bidirectional(GRU(GRU_UNITS, return_sequences=True, return_state=True, \n                                                kernel_initializer=GlorotUniform(random_state.randint(limit)), \n                                                recurrent_initializer=Orthogonal(seed=random_state.randint(limit))))(x)\n        x = BatchNormalization(momentum=0.9)(x)\n        state_h = BatchNormalization(momentum=0.9)(state_h)\n        state_c = BatchNormalization(momentum=0.9)(state_c)\n\n        pools = []\n        pools.append(GlobalAveragePooling1D()(x))\n        pools.append(state_h)\n        pools.append(state_c)\n        pools.append(GlobalMaxPooling1D()(x))\n        x = Concatenate()(pools)\n\n        x = Dense(128, activation=None, kernel_initializer=HeUniform(random_state.randint(limit)))(x)\n        x = BatchNormalization(momentum=0.9)(x)\n        x = ELU(alpha=1.0)(x)\n\n        # Last layer\n        activation = 'sigmoid' if self.multi_label else 'softmax'\n        out = Dense(num_classes, activation=activation, kernel_initializer=GlorotUniform(random_state.randint(limit)))(x)\n\n        # Compile model\n        model = Model(inputs=words, outputs=[out])\n        lr = self.keras_params['learning_rate'] if 'learning_rate' in self.keras_params.keys() else 0.01\n        decay = self.keras_params['decay'] if 'decay' in self.keras_params.keys() else 0.0\n        self.logger.info(f\"Learning rate: {lr}\")\n        self.logger.info(f\"Decay: {decay}\")\n        optimizer = Adam(lr=lr, decay=decay)\n        loss = utils_deep_keras.f1_loss if self.multi_label else 'categorical_crossentropy'\n        metrics: List[Union[str, Callable]] = ['accuracy'] if not self.multi_label else ['categorical_accuracy', 'categorical_crossentropy', utils_deep_keras.f1, utils_deep_keras.precision, utils_deep_keras.recall, utils_deep_keras.f1_loss]\n        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n        if self.logger.getEffectiveLevel() &lt; logging.ERROR:\n            model.summary()\n\n        # Try to save model as png if level_save &gt; 'LOW'\n        if self.level_save in ['MEDIUM', 'HIGH']:\n            self._save_model_png(model)\n\n        # Return\n        return model\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save configuration JSON\n        if json_data is None:\n            json_data = {}\n\n        # Add specific data\n        json_data['max_sequence_length'] = self.max_sequence_length\n        json_data['max_words'] = self.max_words\n        json_data['padding'] = self.padding\n        json_data['truncating'] = self.truncating\n        json_data['tokenizer_filters'] = self.tokenizer_filters\n\n        # Save tokenizer if not None &amp; level_save &gt; LOW\n        if (self.tokenizer is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n            # Manage paths\n            tokenizer_path = os.path.join(self.model_dir, \"embedding_tokenizer.pkl\")\n            # Save as pickle\n            with open(tokenizer_path, 'wb') as f:\n                pickle.dump(self.tokenizer, f)\n\n        # Save\n        super().save(json_data=json_data)\n\n    @classmethod\n    def _init_new_instance_from_configs(cls, configs):\n        '''Inits a new instance from a set of configurations\n\n        Args:\n            configs: a set of configurations of a model to be reloaded\n        Returns:\n            ModelClass: the newly generated class\n        '''\n        # Call parent\n        model = super()._init_new_instance_from_configs(configs)\n\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['max_sequence_length', 'max_words', 'padding', 'truncating', 'tokenizer_filters']:\n            setattr(model, attribute, configs.get(attribute, getattr(model, attribute)))\n\n        # Return the new model\n        return model\n\n    def _load_standalone_files(self, default_model_dir: Union[str, None] = None,  # type: ignore\n                               tokenizer_path: Union[str, None] = None, *args, **kwargs):\n        '''Loads standalone files for a newly created model via _init_new_instance_from_configs\n\n        Kwargs:\n            default_model_dir (str): a path to look for default file paths\n                                     If None, standalone files path should all be provided\n            tokenizer_path (str): Path to the tokenizer file\n        Raises:\n            ValueError: If the tokenizer file is not specified and can't be inferred\n            FileNotFoundError: If the tokenizer file does not exist\n        '''\n        # Check if we are able to get all needed paths\n        if default_model_dir is None and tokenizer_path is None:\n            raise ValueError(\"The tokenizer file is not specified and can't be inferred\")\n\n        # Call parent\n        super()._load_standalone_files(default_model_dir=default_model_dir, **kwargs)\n\n        # Retrieve file paths\n        if tokenizer_path is None:\n            tokenizer_path = os.path.join(default_model_dir, \"embedding_tokenizer.pkl\")\n\n        # Check paths exists\n        if not os.path.isfile(tokenizer_path):\n            raise FileNotFoundError(f\"Can't find tokenizer file ({tokenizer_path})\")\n\n        # Reload tokenizer\n        with open(tokenizer_path, 'rb') as f:\n            self.tokenizer = pickle.load(f)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm_gru/#template_nlp.models_training.models_tensorflow.model_embedding_lstm_gru.ModelEmbeddingLstmGru.__init__","title":"<code>__init__(max_sequence_length=200, max_words=100000, padding='pre', truncating='post', tokenizer_filters='\u2019!#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~\\t\\n\\r\\'\"', **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass &amp; ModelKeras for more arguments)</p> Kwargs <p>max_sequence_length (int): Maximum number of words per sequence (ie. sentences) max_words (int): Maximum number of words for tokenization padding (str): Padding (add zeros) at the beginning ('pre') or at the end ('post') of the sequences truncating (str): Truncating the beginning ('pre') or the end ('post') of the sequences (if superior to max_sequence_length) tokenizer_filters (str): Filter to use by the tokenizer</p> <p>Raises:     ValueError: If the object padding is not a valid choice (['pre', 'post'])     ValueError: If the object truncating is not a valid choice (['pre', 'post'])</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_lstm_gru.py</code> <pre><code>def __init__(self, max_sequence_length: int = 200, max_words: int = 100000,\n             padding: str = 'pre', truncating: str = 'post',\n             tokenizer_filters=\"\u2019!#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~\\t\\n\\r\\'\\\"\", **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelClass &amp; ModelKeras for more arguments)\n\n    Kwargs:\n        max_sequence_length (int): Maximum number of words per sequence (ie. sentences)\n        max_words (int): Maximum number of words for tokenization\n        padding (str): Padding (add zeros) at the beginning ('pre') or at the end ('post') of the sequences\n        truncating (str): Truncating the beginning ('pre') or the end ('post') of the sequences (if superior to max_sequence_length)\n        tokenizer_filters (str): Filter to use by the tokenizer\n    Raises:\n        ValueError: If the object padding is not a valid choice (['pre', 'post'])\n        ValueError: If the object truncating is not a valid choice (['pre', 'post'])\n    '''\n    if padding not in ['pre', 'post']:\n        raise ValueError(f\"The object padding ({padding}) is not a valid choice (['pre', 'post'])\")\n    if truncating not in ['pre', 'post']:\n        raise ValueError(f\"The object truncating ({truncating}) is not a valid choice (['pre', 'post'])\")\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    self.max_sequence_length = max_sequence_length\n    self.max_words = max_words\n    self.padding = padding\n    self.truncating = truncating\n\n    # Tokenizer set on fit\n    self.tokenizer: Any = None\n    self.tokenizer_filters = tokenizer_filters\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm_gru/#template_nlp.models_training.models_tensorflow.model_embedding_lstm_gru.ModelEmbeddingLstmGru.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_lstm_gru.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save configuration JSON\n    if json_data is None:\n        json_data = {}\n\n    # Add specific data\n    json_data['max_sequence_length'] = self.max_sequence_length\n    json_data['max_words'] = self.max_words\n    json_data['padding'] = self.padding\n    json_data['truncating'] = self.truncating\n    json_data['tokenizer_filters'] = self.tokenizer_filters\n\n    # Save tokenizer if not None &amp; level_save &gt; LOW\n    if (self.tokenizer is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n        # Manage paths\n        tokenizer_path = os.path.join(self.model_dir, \"embedding_tokenizer.pkl\")\n        # Save as pickle\n        with open(tokenizer_path, 'wb') as f:\n            pickle.dump(self.tokenizer, f)\n\n    # Save\n    super().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm_structured_attention/","title":"Model embedding lstm structured attention","text":""},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm_structured_attention/#template_nlp.models_training.models_tensorflow.model_embedding_lstm_structured_attention.ModelEmbeddingLstmStructuredAttention","title":"<code>ModelEmbeddingLstmStructuredAttention</code>","text":"<p>             Bases: <code>ModelKeras</code></p> <p>Model for predictions via embedding + LSTM + structured attention -&gt; useful to get predictions explanation. Based on Ga\u00eblle JOUIS Thesis. Work in progress.</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_lstm_structured_attention.py</code> <pre><code>class ModelEmbeddingLstmStructuredAttention(ModelKeras):\n    '''Model for predictions via embedding + LSTM + structured attention -&gt; useful to get predictions explanation.\n    Based on Ga\u00eblle JOUIS Thesis.\n    Work in progress.\n    '''\n\n    _default_name = 'model_embedding_lstm_structured_attention'\n\n    def __init__(self, max_sequence_length: int = 200, max_words: int = 100000,\n                 padding: str = 'pre', truncating: str = 'post', oov_token: str = \"oovt\",\n                 tokenizer_filters: str = \"\u2019!#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~\\t\\n\\r\\'\\\"\", **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelClass &amp; ModelKeras for more arguments)\n\n        Kwargs:\n            max_sequence_length (int): Maximum number of words per sequence (ie. sentences)\n            max_words (int): Maximum number of words for tokenization\n            padding (str): Padding (add zeros) at the beginning ('pre') or at the end ('post') of the sequences\n            truncating (str): Truncating the beginning ('pre') or the end ('post') of the sequences (if superior to max_sequence_length)\n            oov_token (str): Out Of Vocabulary token (to be used with the Tokenizer)\n            tokenizer_filters (str): Filter to use by the tokenizer\n        Raises:\n            ValueError: If the object padding is not a valid choice (['pre', 'post'])\n            ValueError: If the object truncating is not a valid choice (['pre', 'post'])\n        '''\n        if padding not in ['pre', 'post']:\n            raise ValueError(f\"The object padding ({padding}) is not a valid choice (['pre', 'post'])\")\n        if truncating not in ['pre', 'post']:\n            raise ValueError(f\"The object truncating ({truncating}) is not a valid choice (['pre', 'post'])\")\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        self.max_sequence_length = max_sequence_length\n        self.max_words = max_words\n        self.padding = padding\n        self.truncating = truncating\n        self.oov_token = oov_token\n\n        # Tokenizer set on fit\n        self.tokenizer: Any = None\n        self.tokenizer_filters = tokenizer_filters\n\n    def _prepare_x_train(self, x_train) -&gt; np.ndarray:\n        '''Prepares the input data for the model. Called when fitting the model\n\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Prepared data\n        '''\n        # Get tokenizer &amp; fit on train\n        self.tokenizer = Tokenizer(num_words=self.max_words, filters=self.tokenizer_filters, oov_token=self.oov_token)\n        self.logger.info('Fitting the tokenizer')\n        self.tokenizer.fit_on_texts(x_train)\n        return self._get_sequence(x_train, self.tokenizer, self.max_sequence_length, padding=self.padding, truncating=self.truncating)\n\n    def _prepare_x_test(self, x_test, max_sequence_length: int = 0) -&gt; Any:\n        '''Prepares the input data for the model. Called when fitting the model\n\n        Args:\n            x_test (?): Array-like, shape = [n_samples, n_features]\n        Kwargs:\n            max_sequence_length (int): Maximum number of words per sequence (ie. sentences)\n                Default to self.max_sequence_length.\n                Useful only with explanations.\n                We don't use 'None' as it is a particular usage for _get_sequence.\n                Hence we backup on default value if max_sequence_length is 0.\n        Returns:\n            (np.ndarray): Prepared data\n        '''\n        if max_sequence_length == 0:\n            max_sequence_length = self.max_sequence_length\n        # Get sequences on test (already fitted on train)\n        return self._get_sequence(x_test, self.tokenizer, max_sequence_length, padding=self.padding, truncating=self.truncating)\n\n    def _get_model(self, custom_tokenizer=None) -&gt; Any:\n        '''Gets a model structure - returns the instance model instead if already defined\n\n        Kwargs:\n            custom_tokenizer (?): Tokenizer (if different from the one of the class). Permits to manage \"new embeddings\"\n        Returns:\n            (Model): a Keras model\n        '''\n        # Return model if already set\n        if self.model is not None:\n            return self.model\n\n        # Get parameters\n        lstm_units = self.keras_params['lstm_units'] if 'lstm_units' in self.keras_params.keys() else 50  # u = 50 in the GIT implementation, 300 in the paper (YELP)\n        dense_size = self.keras_params['dense_size'] if 'dense_size' in self.keras_params.keys() else 300  # d_a = 100 in the GIT implementation, 350 in the paper (YELP)\n        attention_hops = self.keras_params['attention_hops'] if 'attention_hops' in self.keras_params.keys() else 1  # r = 10 in the GIT implementation, 30 in the paper (YELP)\n        lr = self.keras_params['lr'] if 'lr' in self.keras_params.keys() else 0.01\n\n        # Start by getting embedding matrix\n        if custom_tokenizer is not None:\n            embedding_matrix, embedding_size = self._get_embedding_matrix(custom_tokenizer)\n        else:\n            embedding_matrix, embedding_size = self._get_embedding_matrix(self.tokenizer)\n\n        # Get input dim\n        input_dim = embedding_matrix.shape[0]\n\n        # Get model\n        num_classes = len(self.list_classes)\n\n        # Get random_state\n        random_state = np.random.RandomState(self.random_seed)\n        limit = int(1e9)\n\n        # Process\n        words = Input(shape=(self.max_sequence_length,))\n        x = Embedding(input_dim, embedding_size, weights=[embedding_matrix], trainable=False)(words)\n        h = Bidirectional(LSTM(lstm_units, return_sequences=True, kernel_initializer=GlorotUniform(random_state.randint(limit)), \n                               recurrent_initializer=Orthogonal(seed=random_state.randint(limit))))(x)\n        x = Dense(dense_size, activation='tanh', kernel_initializer=GlorotUniform(random_state.randint(limit)))(h)  # tanh(W_{S1}*H^T) , H^T = x (LSTM output), dim = d_a*2u\n        a = Dense(attention_hops, kernel_initializer=GlorotUniform(random_state.randint(limit)))(x)  # softmax(W_{s2}*X) = A\n        a_with_activation = Softmax(axis=1)(a)\n        at = tf.transpose(a_with_activation, perm=[0, 2, 1], name=\"attention_layer\")  # At, used in Kaushalshetty project, output dim = (r,n)\n        # Trick to name the attention layer (does not work with TensorFlow layers)\n        # https://github.com/keras-team/keras/issues/6194#issuecomment-416365112\n        at_identity = Lambda(lambda x: x, name=\"attention_layer\")(at)\n        m = at_identity @ h  # M = AH\n        x = AttentionAverage(attention_hops)(m)\n\n        # Last layer\n        activation = 'sigmoid' if self.multi_label else 'softmax'\n        out = Dense(num_classes, activation=activation, kernel_initializer=GlorotUniform(random_state.randint(limit)))(x)\n\n        # Compile model\n        model = Model(inputs=words, outputs=[out])\n        optimizer = Adam(lr=lr)\n        # optimizer = SGD(lr=0.06, clipnorm=0.5)  # paper: 0.06 YELP\n        loss = utils_deep_keras.f1_loss if self.multi_label else 'categorical_crossentropy'\n        metrics: List[Union[str, Callable]] = ['accuracy'] if not self.multi_label else ['categorical_accuracy', 'categorical_crossentropy', utils_deep_keras.f1, utils_deep_keras.precision, utils_deep_keras.recall, utils_deep_keras.f1_loss]\n        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n        if self.logger.getEffectiveLevel() &lt; logging.ERROR:\n            model.summary()\n\n        # Try to save model as png if level_save &gt; 'LOW'\n        if self.level_save in ['MEDIUM', 'HIGH']:\n            self._save_model_png(model)\n\n        # Return\n        return model\n\n    @utils.data_agnostic_str_to_list\n    @utils.trained_needed\n    def explain(self, x_test, attention_threshold: float = 0.15, fix_index: bool = False) -&gt; list:\n        '''Predictions on test set, with all attentions weights\n        -&gt; explanations on preprocessed words\n\n        This function returns a list of dictionnaries:\n            {\n                index: (word, value)\n            }\n        where:\n            - index: word index in the sequence (after tokenization &amp; padding)\n            - word: the corresponding word after preprocessing\n            - value: the attention value for this word\n\n        Precision: if fix_index is set to True, the output indexes correspond to the word index in the preprocessed sentence (and not in the sequence)\n\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples]\n                WARNING : sentences must be preprocessed here\n        Kwargs:\n            attention_threshold (float): Minimum attention threshold\n            fix_index (bool): If we have to fix sequences index to get word indexes in the preprocessed sentence\n        Returns:\n            list: List of dictionnaries (one entry per sentence) with matched words (i.e. attention &gt; threshold)\n        '''\n        # Cast en pd.Series\n        x_test = pd.Series(x_test)\n\n        # Prepare input\n        x_test_prep = self._prepare_x_test(x_test)\n\n        # Retrieve attention scores\n        intermediate_layer_model = Model(inputs=self.model.input, outputs=self.model.get_layer('attention_layer').output)  # type: ignore\n        intermediate_output = intermediate_layer_model(x_test_prep)\n\n        # Retrieve (word, attention) tuples\n        intermediate_output_reshaped = tf.squeeze(intermediate_output)\n        # Manage cases where x_test has only one element\n        if len(intermediate_output_reshaped.shape) == 1:\n            intermediate_output_reshaped = tf.expand_dims(intermediate_output_reshaped, axis=0)\n        seq_q_attention = tf.stack([x_test_prep, intermediate_output_reshaped], axis=1)\n        seq_q_attention = tf.transpose(seq_q_attention, [0, 2, 1])\n        seq_q_attention = seq_q_attention.numpy()\n        text_w_attention = [[(self.tokenizer.sequences_to_texts([[y[0]]]), y[1])  # type: ignore\n                             if y[0] != 0 else (\"&lt;PAD&gt;\", y[1]) for y in entry]\n                            for entry in seq_q_attention]\n        # Filter words with low attention score\n        selected_words = [{index: (tup[0][0], tup[1]) for index, tup in enumerate(entry) if tup[0] != '&lt;PAD&gt;' and tup[1] &gt;= attention_threshold} for entry in text_w_attention]\n\n        # If wanted, fix indexes\n        if fix_index:\n            # Process sentence per sentence\n            for i, entry in enumerate(selected_words):\n                # Case 1 : padding\n                # Check for padding (at least one 0)\n                nb_padding = len(np.where(x_test_prep[i] == 0)[0])\n                padded = True if nb_padding != 0 else False\n                if self.padding == 'pre':\n                    if nb_padding != 0:\n                        # We shift the sequence (i.e. we remove the padding)\n                        selected_words[i] = {index - nb_padding: val for index, val in entry.items()}\n                else:\n                    pass  # We do nothing (already in correct ordre if post padding)\n                # Case 2 : truncating\n                if not padded:\n                    if self.truncating == 'pre':\n                        # We must reapply get_sequences with max sequence length to retrieve removed words\n                        x_test_full = self._prepare_x_test([x_test[i]], max_sequence_length=None)[0]  # type: ignore\n                        nb_truncating = len(x_test_full) - len(x_test_prep[i])\n                        if nb_truncating != 0:\n                            # We shift the sequence to take truncation in account\n                            selected_words[i] = {index + nb_truncating: val for index, val in entry.items()}\n                    else:\n                        pass  # We do nothing (already in correct ordre if post truncating)\n\n        # Returns\n        return selected_words\n\n    def _pad_text(self, text: list, pad_token: str = '&lt;PAD&gt;') -&gt; list:\n        '''Apply padding on a tokenized text (list)\n\n        Args:\n            text (list): List of tokenized words\n        Kwargs:\n            pad_token (str): Default pad token\n        Returns:\n            list: List of tokenized words, with padding / truncating management\n        '''\n        # If there is too much words, we truncate the sequence\n        if len(text) &gt; self.max_sequence_length:\n            if self.truncating == 'post':\n                text = text[:self.max_sequence_length]\n            else:  # pre\n                text = text[-self.max_sequence_length:]\n        # If there is not enough words, we pad the sequence\n        elif len(text) &lt; self.max_sequence_length:\n            padding_list = [pad_token for i in range(self.max_sequence_length - len(text))]\n            if self.padding == 'pre':\n                text = padding_list + text\n            else:\n                text = text + padding_list\n        # Return\n        return text\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save configuration JSON\n        if json_data is None:\n            json_data = {}\n\n        # Add specific data\n        json_data['max_sequence_length'] = self.max_sequence_length\n        json_data['max_words'] = self.max_words\n        json_data['padding'] = self.padding\n        json_data['truncating'] = self.truncating\n        json_data['oov_token'] = self.oov_token\n        json_data['tokenizer_filters'] = self.tokenizer_filters\n\n        # Save tokenizer if not None &amp; level_save &gt; LOW\n        if (self.tokenizer is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n            # Manage paths\n            tokenizer_path = os.path.join(self.model_dir, \"embedding_tokenizer.pkl\")\n            # Save as pickle\n            with open(tokenizer_path, 'wb') as f:\n                pickle.dump(self.tokenizer, f)\n\n        # Save\n        super().save(json_data=json_data)\n\n    @classmethod\n    def _init_new_instance_from_configs(cls, configs):\n        '''Inits a new instance from a set of configurations\n\n        Args:\n            configs: a set of configurations of a model to be reloaded\n        Returns:\n            ModelClass: the newly generated class\n        '''\n        # Call parent\n        model = super()._init_new_instance_from_configs(configs)\n\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['max_sequence_length', 'max_words', 'padding', 'truncating', 'oov_token', 'tokenizer_filters']:\n            setattr(model, attribute, configs.get(attribute, getattr(model, attribute)))\n\n        # Return the new model\n        return model\n\n    def _load_standalone_files(self, default_model_dir: Union[str, None] = None,  # type: ignore\n                               tokenizer_path: Union[str, None] = None, *args, **kwargs):\n        '''Loads standalone files for a newly created model via _init_new_instance_from_configs\n\n        Kwargs:\n            default_model_dir (str): a path to look for default file paths\n                                     If None, standalone files path should all be provided\n            tokenizer_path (str): Path to the tokenizer file\n        Raises:\n            ValueError: If the tokenizer file is not specified and can't be inferred\n            FileNotFoundError: If the tokenizer file does not exist\n        '''\n        # Check if we are able to get all needed paths\n        if default_model_dir is None and tokenizer_path is None:\n            raise ValueError(\"The tokenizer file is not specified and can't be inferred\")\n\n        # Call parent\n        super()._load_standalone_files(default_model_dir=default_model_dir, **kwargs)\n\n        # Retrieve file paths\n        if tokenizer_path is None:\n            tokenizer_path = os.path.join(default_model_dir, \"embedding_tokenizer.pkl\")\n\n        # Check paths exists\n        if not os.path.isfile(tokenizer_path):\n            raise FileNotFoundError(f\"Can't find tokenizer file ({tokenizer_path})\")\n\n        # Reload tokenizer\n        with open(tokenizer_path, 'rb') as f:\n            self.tokenizer = pickle.load(f)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm_structured_attention/#template_nlp.models_training.models_tensorflow.model_embedding_lstm_structured_attention.ModelEmbeddingLstmStructuredAttention.__init__","title":"<code>__init__(max_sequence_length=200, max_words=100000, padding='pre', truncating='post', oov_token='oovt', tokenizer_filters='\u2019!#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~\\t\\n\\r\\'\"', **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass &amp; ModelKeras for more arguments)</p> Kwargs <p>max_sequence_length (int): Maximum number of words per sequence (ie. sentences) max_words (int): Maximum number of words for tokenization padding (str): Padding (add zeros) at the beginning ('pre') or at the end ('post') of the sequences truncating (str): Truncating the beginning ('pre') or the end ('post') of the sequences (if superior to max_sequence_length) oov_token (str): Out Of Vocabulary token (to be used with the Tokenizer) tokenizer_filters (str): Filter to use by the tokenizer</p> <p>Raises:     ValueError: If the object padding is not a valid choice (['pre', 'post'])     ValueError: If the object truncating is not a valid choice (['pre', 'post'])</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_lstm_structured_attention.py</code> <pre><code>def __init__(self, max_sequence_length: int = 200, max_words: int = 100000,\n             padding: str = 'pre', truncating: str = 'post', oov_token: str = \"oovt\",\n             tokenizer_filters: str = \"\u2019!#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~\\t\\n\\r\\'\\\"\", **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelClass &amp; ModelKeras for more arguments)\n\n    Kwargs:\n        max_sequence_length (int): Maximum number of words per sequence (ie. sentences)\n        max_words (int): Maximum number of words for tokenization\n        padding (str): Padding (add zeros) at the beginning ('pre') or at the end ('post') of the sequences\n        truncating (str): Truncating the beginning ('pre') or the end ('post') of the sequences (if superior to max_sequence_length)\n        oov_token (str): Out Of Vocabulary token (to be used with the Tokenizer)\n        tokenizer_filters (str): Filter to use by the tokenizer\n    Raises:\n        ValueError: If the object padding is not a valid choice (['pre', 'post'])\n        ValueError: If the object truncating is not a valid choice (['pre', 'post'])\n    '''\n    if padding not in ['pre', 'post']:\n        raise ValueError(f\"The object padding ({padding}) is not a valid choice (['pre', 'post'])\")\n    if truncating not in ['pre', 'post']:\n        raise ValueError(f\"The object truncating ({truncating}) is not a valid choice (['pre', 'post'])\")\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    self.max_sequence_length = max_sequence_length\n    self.max_words = max_words\n    self.padding = padding\n    self.truncating = truncating\n    self.oov_token = oov_token\n\n    # Tokenizer set on fit\n    self.tokenizer: Any = None\n    self.tokenizer_filters = tokenizer_filters\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm_structured_attention/#template_nlp.models_training.models_tensorflow.model_embedding_lstm_structured_attention.ModelEmbeddingLstmStructuredAttention.explain","title":"<code>explain(x_test, attention_threshold=0.15, fix_index=False)</code>","text":"<p>Predictions on test set, with all attentions weights -&gt; explanations on preprocessed words</p> This function returns a list of dictionnaries <p>{     index: (word, value) }</p> <p>where:     - index: word index in the sequence (after tokenization &amp; padding)     - word: the corresponding word after preprocessing     - value: the attention value for this word</p> <p>Precision: if fix_index is set to True, the output indexes correspond to the word index in the preprocessed sentence (and not in the sequence)</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples] WARNING : sentences must be preprocessed here</p> required <p>Kwargs:     attention_threshold (float): Minimum attention threshold     fix_index (bool): If we have to fix sequences index to get word indexes in the preprocessed sentence Returns:     list: List of dictionnaries (one entry per sentence) with matched words (i.e. attention &gt; threshold)</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_lstm_structured_attention.py</code> <pre><code>@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef explain(self, x_test, attention_threshold: float = 0.15, fix_index: bool = False) -&gt; list:\n    '''Predictions on test set, with all attentions weights\n    -&gt; explanations on preprocessed words\n\n    This function returns a list of dictionnaries:\n        {\n            index: (word, value)\n        }\n    where:\n        - index: word index in the sequence (after tokenization &amp; padding)\n        - word: the corresponding word after preprocessing\n        - value: the attention value for this word\n\n    Precision: if fix_index is set to True, the output indexes correspond to the word index in the preprocessed sentence (and not in the sequence)\n\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples]\n            WARNING : sentences must be preprocessed here\n    Kwargs:\n        attention_threshold (float): Minimum attention threshold\n        fix_index (bool): If we have to fix sequences index to get word indexes in the preprocessed sentence\n    Returns:\n        list: List of dictionnaries (one entry per sentence) with matched words (i.e. attention &gt; threshold)\n    '''\n    # Cast en pd.Series\n    x_test = pd.Series(x_test)\n\n    # Prepare input\n    x_test_prep = self._prepare_x_test(x_test)\n\n    # Retrieve attention scores\n    intermediate_layer_model = Model(inputs=self.model.input, outputs=self.model.get_layer('attention_layer').output)  # type: ignore\n    intermediate_output = intermediate_layer_model(x_test_prep)\n\n    # Retrieve (word, attention) tuples\n    intermediate_output_reshaped = tf.squeeze(intermediate_output)\n    # Manage cases where x_test has only one element\n    if len(intermediate_output_reshaped.shape) == 1:\n        intermediate_output_reshaped = tf.expand_dims(intermediate_output_reshaped, axis=0)\n    seq_q_attention = tf.stack([x_test_prep, intermediate_output_reshaped], axis=1)\n    seq_q_attention = tf.transpose(seq_q_attention, [0, 2, 1])\n    seq_q_attention = seq_q_attention.numpy()\n    text_w_attention = [[(self.tokenizer.sequences_to_texts([[y[0]]]), y[1])  # type: ignore\n                         if y[0] != 0 else (\"&lt;PAD&gt;\", y[1]) for y in entry]\n                        for entry in seq_q_attention]\n    # Filter words with low attention score\n    selected_words = [{index: (tup[0][0], tup[1]) for index, tup in enumerate(entry) if tup[0] != '&lt;PAD&gt;' and tup[1] &gt;= attention_threshold} for entry in text_w_attention]\n\n    # If wanted, fix indexes\n    if fix_index:\n        # Process sentence per sentence\n        for i, entry in enumerate(selected_words):\n            # Case 1 : padding\n            # Check for padding (at least one 0)\n            nb_padding = len(np.where(x_test_prep[i] == 0)[0])\n            padded = True if nb_padding != 0 else False\n            if self.padding == 'pre':\n                if nb_padding != 0:\n                    # We shift the sequence (i.e. we remove the padding)\n                    selected_words[i] = {index - nb_padding: val for index, val in entry.items()}\n            else:\n                pass  # We do nothing (already in correct ordre if post padding)\n            # Case 2 : truncating\n            if not padded:\n                if self.truncating == 'pre':\n                    # We must reapply get_sequences with max sequence length to retrieve removed words\n                    x_test_full = self._prepare_x_test([x_test[i]], max_sequence_length=None)[0]  # type: ignore\n                    nb_truncating = len(x_test_full) - len(x_test_prep[i])\n                    if nb_truncating != 0:\n                        # We shift the sequence to take truncation in account\n                        selected_words[i] = {index + nb_truncating: val for index, val in entry.items()}\n                else:\n                    pass  # We do nothing (already in correct ordre if post truncating)\n\n    # Returns\n    return selected_words\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm_structured_attention/#template_nlp.models_training.models_tensorflow.model_embedding_lstm_structured_attention.ModelEmbeddingLstmStructuredAttention.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_lstm_structured_attention.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save configuration JSON\n    if json_data is None:\n        json_data = {}\n\n    # Add specific data\n    json_data['max_sequence_length'] = self.max_sequence_length\n    json_data['max_words'] = self.max_words\n    json_data['padding'] = self.padding\n    json_data['truncating'] = self.truncating\n    json_data['oov_token'] = self.oov_token\n    json_data['tokenizer_filters'] = self.tokenizer_filters\n\n    # Save tokenizer if not None &amp; level_save &gt; LOW\n    if (self.tokenizer is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n        # Manage paths\n        tokenizer_path = os.path.join(self.model_dir, \"embedding_tokenizer.pkl\")\n        # Save as pickle\n        with open(tokenizer_path, 'wb') as f:\n            pickle.dump(self.tokenizer, f)\n\n    # Save\n    super().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_keras/","title":"Model keras","text":""},{"location":"reference/template_nlp/models_training/models_tensorflow/model_keras/#template_nlp.models_training.models_tensorflow.model_keras.ModelKeras","title":"<code>ModelKeras</code>","text":"<p>             Bases: <code>ModelClass</code></p> <p>Generic model for Keras NN</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_keras.py</code> <pre><code>class ModelKeras(ModelClass):\n    '''Generic model for Keras NN'''\n\n    _default_name = 'model_keras'\n\n    # Not implemented :\n    # -&gt; _prepare_x_train (prepare data for training)\n    # -&gt; _prepare_x_test (prepare data for testing)\n    # -&gt; _get_model (defines the model structure)\n\n    # Probably need to be overridden, depending on your model :\n    # -&gt; predict_proba (predict on new content - returns probas) -&gt; some pipelines do not provide proba, or may have specificities\n    # -&gt; save (specific save instructions)\n    # -&gt; _init_new_instance_from_configs (loads model attributes - for a newly created model)\n    # -&gt; _load_standalone_files (loads standalone files - for a newly created model) -&gt; add pipeline elements\n\n    def __init__(self, batch_size: int = 64, epochs: int = 99, validation_split: float = 0.2, patience: int = 5,\n                 embedding_name: str = 'cc.fr.300.pkl', keras_params: Union[dict, None] = None, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelClass for more arguments)\n\n        Kwargs:\n            batch_size (int): Batch size\n            epochs (int): Number of epochs\n            validation_split (float): Percentage for the validation set split\n                Only used if no input validation set when fitting\n            patience (int): Early stopping patience\n            embedding_name (str) : The name of the embedding matrix to use\n            keras_params (dict): Parameters used by Keras models.\n                e.g. learning_rate, nb_lstm_units, etc...\n                The purpose of this dictionary is for the user to use it as they wants in the _get_model function\n                This parameter was initially added in order to do an hyperparameters search\n        '''\n        # TODO: learning rate should be an attribute !\n        # Init.\n        super().__init__(**kwargs)\n\n        # Fix tensorflow GPU\n        gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n        for device in gpu_devices:\n            tf.config.experimental.set_memory_growth(device, True)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Param. model\n        self.batch_size = batch_size\n        self.epochs = epochs\n        self.validation_split = validation_split\n        self.patience = patience\n\n        # Model set on fit\n        self.model: Any = None\n\n        # Param embedding (can be None if no embedding)\n        self.embedding_name = embedding_name\n\n        # Keras params\n        if keras_params is None:\n            keras_params = {}\n        self.keras_params = keras_params.copy()\n\n        # Keras custom objects : we get the ones specified in utils_deep_keras\n        self.custom_objects = utils_deep_keras.custom_objects\n\n    def fit(self, x_train, y_train, x_valid=None, y_valid=None, with_shuffle: bool = True, **kwargs) -&gt; None:\n        '''Fits the model\n\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n            y_train (?): Array-like, shape = [n_samples, n_targets]\n        Kwargs:\n            x_valid (?): Array-like, shape = [n_samples, n_features]\n            y_valid (?): Array-like, shape = [n_samples, n_targets]\n            with_shuffle (bool): If x, y must be shuffled before fitting\n                Experimental: We must verify if it works as intended depending on the formats of x and y\n                This should be used if y is not shuffled as the split_validation takes the lines in order.\n                Thus, the validation set might get classes which are not in the train set ...\n        Raises:\n            ValueError: If different classes when comparing an already fitted model and a new dataset\n        '''\n        ##############################################\n        # Manage retrain\n        ##############################################\n        # If a model has already been fitted, we make a new folder in order not to overwrite the existing one !\n        # And we save the old conf\n        if self.trained:\n            # Get src files to save\n            src_files = [os.path.join(self.model_dir, \"configurations.json\")]\n            if self.nb_fit &gt; 1:\n                for i in range(1, self.nb_fit):\n                    src_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n            # Change model dir\n            self.model_dir = self._get_new_model_dir()\n            # Get dst files\n            dst_files = [os.path.join(self.model_dir, f\"configurations_fit_{self.nb_fit}.json\")]\n            if self.nb_fit &gt; 1:\n                for i in range(1, self.nb_fit):\n                    dst_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n            # Copies\n            for src, dst in zip(src_files, dst_files):\n                try:\n                    shutil.copyfile(src, dst)\n                except Exception as e:\n                    self.logger.error(f\"Impossible to copy {src} to {dst}\")\n                    self.logger.error(\"We still continue ...\")\n                    self.logger.error(repr(e))\n\n        ##############################################\n        # Prepare x_train, x_valid, y_train &amp; y_valid\n        # Also extract list of classes\n        ##############################################\n\n        # If not multilabel, transform y_train as dummies (should already be the case for multi-labels)\n        if not self.multi_label:\n            # If len(array.shape)==2, we flatten the array if the second dimension is useless\n            if isinstance(y_train, np.ndarray) and len(y_train.shape) == 2 and y_train.shape[1] == 1:\n                y_train = np.ravel(y_train)\n            if isinstance(y_valid, np.ndarray) and len(y_valid.shape) == 2 and y_valid.shape[1] == 1:\n                y_valid = np.ravel(y_valid)\n            # Transformation dummies\n            y_train_dummies = pd.get_dummies(y_train).astype(int)\n            y_valid_dummies = pd.get_dummies(y_valid).astype(int) if y_valid is not None else None\n            # Important : get_dummies reorder the columns in alphabetical order\n            # Thus, there is no problem if we fit again on a new dataframe with shuffled data\n            list_classes = list(y_train_dummies.columns)\n            # FIX: valid test might miss some classes, hence we need to add them back to y_valid_dummies\n            if y_valid_dummies is not None and y_train_dummies.shape[1] != y_valid_dummies.shape[1]:\n                for cl in list_classes:\n                    # Add missing columns\n                    if cl not in y_valid_dummies.columns:\n                        y_valid_dummies[cl] = 0\n                y_valid_dummies = y_valid_dummies[list_classes]  # Reorder\n        # Else keep it as it is\n        else:\n            y_train_dummies = y_train\n            y_valid_dummies = y_valid\n            if hasattr(y_train_dummies, 'columns'):\n                list_classes = list(y_train_dummies.columns)\n            else:\n                self.logger.warning(\n                    \"Can't read the name of the columns of y_train -&gt; inverse transformation won't be possible\"\n                )\n                # We still create a list of classes in order to be compatible with other functions\n                list_classes = [str(_) for _ in range(pd.DataFrame(y_train_dummies).shape[1])]\n\n        # Set dict_classes based on list classes\n        dict_classes = {i: col for i, col in enumerate(list_classes)}\n\n        # Validate classes if already trained, else set them\n        if self.trained:\n            if self.list_classes != list_classes:\n                raise ValueError(\"Error: the new dataset does not match with the already fitted model\")\n            if self.dict_classes != dict_classes:\n                raise ValueError(\"Error: the new dataset does not match with the already fitted model\")\n        else:\n            self.list_classes = list_classes\n            self.dict_classes = dict_classes\n\n        # Shuffle x, y if wanted\n        # It is advised as validation_split from keras does not shufle the data\n        # Hence we might have classes in the validation data that we never met in the training data\n        if with_shuffle:\n            rng = np.random.RandomState(self.random_seed)\n            p = rng.permutation(len(x_train))\n            x_train = np.array(x_train)[p]\n            y_train_dummies = np.array(y_train_dummies)[p]\n        # Else still transform to numpy array\n        else:\n            x_train = np.array(x_train)\n            y_train_dummies = np.array(y_train_dummies)\n\n        # Also get y_valid_dummies as numpy\n        y_valid_dummies = np.array(y_valid_dummies)\n\n        # Prepare x_train\n        x_train = self._prepare_x_train(x_train)\n\n        # If available, also prepare x_valid &amp; get validation_data (tuple)\n        validation_data: Optional[tuple] = None  # Def. None if y_valid is None\n        if y_valid is not None:\n            x_valid = self._prepare_x_test(x_valid)\n            validation_data = (x_valid, y_valid_dummies)\n        else:\n            x_train, x_valid,  y_train_dummies, y_valid_dummies = train_test_split(x_train, y_train_dummies, \n                                                                                   test_size=self.validation_split,\n                                                                                   random_state=self.random_seed)\n            validation_data = (x_valid, y_valid_dummies)\n\n        if validation_data is None:\n            self.logger.warning(f\"Warning, no validation set. The training set will be splitted (validation fraction = {self.validation_split})\")\n\n        ##############################################\n        # Fit\n        ##############################################\n\n        # Get model (if already fitted, _get_model returns instance model)\n        self.model = self._get_model()\n\n        # Get callbacks (early stopping &amp; checkpoint)\n        callbacks = self._get_callbacks()\n\n        # Create data generator\n        data_train_generator = RandomStateDataGenerator(x_train, y_train_dummies, self.batch_size, self.random_seed)\n        data_val_generator = RandomStateDataGenerator(x_valid, y_valid_dummies, self.batch_size, self.random_seed)\n\n        # Fit\n        # We use a try...except in order to save the model if an error arises\n        # after more than a minute into training\n        start_time = time.time()\n        try:\n            fit_history = self.model.fit(  # type: ignore\n                data_train_generator,\n                epochs=self.epochs,\n                validation_data=data_val_generator,\n                callbacks=callbacks,\n                verbose=1,\n                shuffle=False\n            )\n        except (RuntimeError, SystemError, SystemExit, EnvironmentError, KeyboardInterrupt, tf.errors.ResourceExhaustedError, tf.errors.InternalError,\n                tf.errors.UnavailableError, tf.errors.UnimplementedError, tf.errors.UnknownError, Exception) as e:\n            # Steps:\n            # 1. Display tensorflow error\n            # 2. Check if more than one minute elapsed &amp; existence best.hdf5\n            # 3. Reload best model\n            # 4. We consider that a fit occured (trained = True, nb_fit += 1)\n            # 5. Save &amp; create a warning file\n            # 6. Display error messages\n            # 7. Raise an error\n\n            # 1.\n            self.logger.error(repr(e))\n\n            # 2.\n            best_path = os.path.join(self.model_dir, 'best.hdf5')\n            time_spent = time.time() - start_time\n            if time_spent &gt;= 60 and os.path.exists(best_path):\n                # 3.\n                self.model = load_model_keras(best_path, custom_objects=self.custom_objects)\n                # 4.\n                self.trained = True\n                self.nb_fit += 1\n                # 5.\n                self.save()\n                with open(os.path.join(self.model_dir, \"0_MODEL_INCOMPLETE\"), 'w'):\n                    pass\n                with open(os.path.join(self.model_dir, \"1_TRAINING_NEEDS_TO_BE_RESUMED\"), 'w'):\n                    pass\n                # 6.\n                self.logger.error(\"[EXPERIMENTAL] Error during model training\")\n                self.logger.error(f\"[EXPERIMENTAL] The error happened after {round(time_spent, 2)}s of training\")\n                self.logger.error(\"[EXPERIMENTAL] A saving of the model is done but this model won't be usable as is.\")\n                self.logger.error(f\"[EXPERIMENTAL] In order to resume the training, we have to specify this model ({ntpath.basename(self.model_dir)}) in the file 2_training.py\")\n                self.logger.error(\"[EXPERIMENTAL] Warning, the preprocessing is not saved in the configuration file\")\n                self.logger.error(\"[EXPERIMENTAL] Warning, the best model might be corrupted in some cases\")\n            # 7.\n            raise RuntimeError(\"Error during model training\")\n\n        # Print accuracy &amp; loss if level_save &gt; 'LOW'\n        if self.level_save in ['MEDIUM', 'HIGH']:\n            # Plot accuracy\n            self._plot_metrics_and_loss(fit_history)\n            # Reload best model\n            self.model = load_model_keras(os.path.join(self.model_dir, 'best.hdf5'), custom_objects=self.custom_objects)\n\n        # Set trained\n        self.trained = True\n        self.nb_fit += 1\n\n    @utils.data_agnostic_str_to_list\n    @utils.trained_needed\n    def predict(self, x_test, return_proba: bool = False, inference_batch_size: int = 128, alternative_version: bool = False, **kwargs) -&gt; np.ndarray:\n        '''Predictions on test set\n\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples]\n        Kwargs:\n            return_proba (bool): If the function should return the probabilities instead of the classes\n            inference_batch_size (int): size (approximate) of batches\n            alternative_version (bool): If an alternative predict version (`tf.function` + `model.__call__`) must be used. Should be faster with low nb of inputs.\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n        # Cast in pd.Series\n        x_test = pd.Series(x_test)\n\n        # Predict\n        predicted_proba = self.predict_proba(x_test, inference_batch_size=inference_batch_size, alternative_version=alternative_version)\n\n        # We return the probabilities if wanted\n        if return_proba:\n            return predicted_proba\n\n        # Finally, we get the classes predictions\n        return self.get_classes_from_proba(predicted_proba)\n\n    @utils.data_agnostic_str_to_list\n    @utils.trained_needed\n    def predict_proba(self, x_test, inference_batch_size: int = 128, alternative_version: bool = False, **kwargs) -&gt; np.ndarray:\n        '''Predicts probabilities on the test dataset\n\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n        Kwargs:\n            inference_batch_size (int): size (approximate) of batches\n            alternative_version (bool): If an alternative predict version (`tf.function` + `model.__call__`) must be used. Should be faster with low nb of inputs.\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n        # Prepare input\n        x_test = self._prepare_x_test(x_test)\n        # Process\n        if alternative_version:\n            return self._alternative_predict_proba(x_test, inference_batch_size=inference_batch_size)\n        else:\n            # We advise you to avoid using `model.predict` with newest TensorFlow versions (possible memory leak) in a production environment (e.g. API)\n            # https://github.com/tensorflow/tensorflow/issues/58676\n            # Instead, you can use the alternative version that uses tf.function decorator &amp; model.__call__\n            # However, it should still be better to use `model.predict` for one-shot, batch mode, large input, iterations.\n            return self.model.predict(x_test, batch_size=inference_batch_size, verbose=1)  # type: ignore\n\n    @utils.trained_needed\n    def _alternative_predict_proba(self, x_test, inference_batch_size: int = 128, **kwargs) -&gt; np.ndarray:\n        '''Predicts probabilities on the test dataset - Alternative version\n        Should be faster with low nb of inputs.\n\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples]\n        Kwargs:\n            inference_batch_size (int): size (approximate) of batches\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n        # Assert batch size is &gt;= 1\n        inference_batch_size = max(1, inference_batch_size)\n        # Process by batches - avoid huge memory impact\n        nb_batches = max(1, len(x_test)//inference_batch_size)\n        list_array = []\n        for arr in np.array_split(x_test, nb_batches, axis=0):\n            tmp_results = self._serve(arr).numpy()\n            list_array.append(tmp_results)\n        np_results = np.concatenate(list_array)\n        # Return\n        return np_results\n\n    # We used to use reduce_retracing to avoid retracing and memory leaks (tensors with different shapes)\n    # but it is still experimental and seems to still do some retracing\n    # Hence, we now use input_signature and it seems to work as intended\n    @tf.function(input_signature=(tf.TensorSpec(shape=(None, None), dtype=tf.int32, name='x'), ))\n    def _serve(self, x: np.ndarray):\n        '''Improves predict function using tf.function (cf. https://www.tensorflow.org/guide/function)\n\n        Args:\n            x (np.ndarray): input data\n        Returns:\n            tf.tensor: model's output\n        '''\n        return self.model(x, training=False)\n\n    def _prepare_x_train(self, x_train) -&gt; np.ndarray:\n        '''Prepares the input data for the model\n\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Prepared data\n        '''\n        raise NotImplementedError(\"'_prepare_x_train' needs to be overridden\")\n\n    def _prepare_x_test(self, x_test) -&gt; np.ndarray:\n        '''Prepares the input data for the model\n\n        Args:\n            x_test (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Prepared data\n        '''\n        raise NotImplementedError(\"'_prepare_x_test' needs to be overridden\")\n\n    def _get_embedding_matrix(self, tokenizer) -&gt; Tuple[np.ndarray, int]:\n        '''Get embedding matrix\n\n        Args:\n            tokenizer (?): Tokenizer to use (useful to test with a new matrice embedding)\n        Returns:\n            np.ndarray: Embedding matrix\n            int: Embedding size\n        '''\n        # Get embedding indexes\n        embedding_indexes = utils_models.get_embedding(self.embedding_name)\n        # Get embedding_size\n        embedding_size = len(embedding_indexes[list(embedding_indexes.keys())[0]])\n\n        # Get embedding matrix\n        # The first line of this matrix is a zero vector\n        # The following lines are the projections of the words obtained by the tokenizer (same index)\n\n        # We keep only the max tokens 'num_words'\n        # https://github.com/keras-team/keras/issues/8092\n        if tokenizer.num_words is None:\n            word_index = {e: i for e, i in tokenizer.word_index.items()}\n        else:\n            word_index = {e: i for e, i in tokenizer.word_index.items() if i &lt;= tokenizer.num_words}\n        # Create embedding matrix\n        embedding_matrix = np.zeros((len(word_index) + 1, embedding_size))\n        # Fill it\n        for word, i in word_index.items():\n            embedding_vector = embedding_indexes.get(word)\n            if embedding_vector is not None:\n                # words not found in embedding index will be all-zeros.\n                embedding_matrix[i] = embedding_vector\n        self.logger.info(f\"Size of the embedding matrix (ie. number of matches on the input) : {len(embedding_matrix)}\")\n        return embedding_matrix, embedding_size\n\n    def _get_sequence(self, x_test, tokenizer, maxlen: int, padding: str = 'pre', truncating: str = 'post') -&gt; np.ndarray:\n        '''Transform input of text into sequences. Needs a tokenizer.\n\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n            tokenizer (?): Tokenizer to use (useful to test with a new matrice embedding)\n            maxlen (int): maximum sequence length\n        Kwargs:\n            padding (str): Padding (add zeros) at the beginning ('pre') or at the end ('post') of the sequences\n            truncating (str): Truncating the beginning ('pre') or the end ('post') of the sequences (if superior to max_sequence_length)\n        Raises:\n            ValueError: If the object padding is not a valid choice (['pre', 'post'])\n            ValueError: If the object truncating is not a valid choice (['pre', 'post'])\n        Returns:\n            (np.ndarray): Padded sequence\n        '''\n        if padding not in ['pre', 'post']:\n            raise ValueError(f\"The object padding ({padding}) is not a valid choice (['pre', 'post'])\")\n        if truncating not in ['pre', 'post']:\n            raise ValueError(f\"The object truncating ({truncating}) is not a valid choice (['pre', 'post'])\")\n        # Process\n        sequences = tokenizer.texts_to_sequences(x_test)\n        return pad_sequences(sequences, maxlen=maxlen, padding=padding, truncating=truncating)\n\n    def _get_model(self) -&gt; Model:\n        '''Gets a model structure - returns the instance model instead if already defined\n\n        Returns:\n            (Model): a Keras model\n        '''\n        raise NotImplementedError(\"'_get_model' needs to be overridden\")\n\n    def _get_callbacks(self) -&gt; list:\n        '''Gets model callbacks\n\n        Returns:\n            list: List of callbacks\n        '''\n        # Get classic callbacks\n        callbacks = [EarlyStopping(monitor='val_loss', patience=self.patience, restore_best_weights=True)]\n        if self.level_save in ['MEDIUM', 'HIGH']:\n            callbacks.append(\n                ModelCheckpoint(\n                    filepath=os.path.join(self.model_dir, f'best.hdf5'), monitor='val_loss', save_best_only=True, mode='auto'\n                )\n            )\n        callbacks.append(CSVLogger(filename=os.path.join(self.model_dir, f'logger.csv'), separator=';', append=False))\n        callbacks.append(TerminateOnNaN())\n\n        # Get LearningRateScheduler\n        scheduler = self._get_learning_rate_scheduler()\n        if scheduler is not None:\n            callbacks.append(LearningRateScheduler(scheduler))\n\n        # Manage tensorboard\n        if self.level_save in ['HIGH']:\n            # Get log directory\n            models_path = utils.get_models_path()\n            tensorboard_dir = os.path.join(models_path, 'tensorboard_logs')\n            # We add a prefix so that the function load_model works correctly (it looks for a sub-folder with model name)\n            log_dir = os.path.join(tensorboard_dir, f\"tensorboard_{ntpath.basename(self.model_dir)}\")\n            if not os.path.exists(log_dir):\n                os.makedirs(log_dir)\n\n            # TODO: check if this class does not slow proccesses\n            # -&gt; For now: comment\n            # Create custom class to monitore LR changes\n            # https://stackoverflow.com/questions/49127214/keras-how-to-output-learning-rate-onto-tensorboard\n            # class LRTensorBoard(TensorBoard):\n            #     def __init__(self, log_dir, **kwargs) -&gt; None:  # add other arguments to __init__ if you need\n            #         super().__init__(log_dir=log_dir, **kwargs)\n            #\n            #     def on_epoch_end(self, epoch, logs=None):\n            #         logs.update({'lr': K.eval(self.model.optimizer.lr)})\n            #         super().on_epoch_end(epoch, logs)\n\n            # Append tensorboard callback\n            # TODO: check compatibility tensorflow 2.3\n            # WARNING : https://stackoverflow.com/questions/63619763/model-training-hangs-forever-when-using-a-tensorboard-callback-with-an-lstm-laye\n            # A compatibility problem TensorBoard / TensorFlow 2.3 (cuDNN implementation of LSTM/GRU) can arise\n            # In this case, the training of the model can be \"blocked\" and does not respond anymore\n            # This problem has arisen two times on P\u00f4le Emploi computers (windows 7 &amp; VM Ubuntu on windows 7 host)\n            # No problem on Valeuriad computers (windows 10)\n            # Thus, TensorBoard is deactivated by default for now\n            # While awaiting a possible fix, you are responsible for checking if TensorBoard works on your computer\n            self.logger.warning(\" ###################### \")\n            self.logger.warning(\"TensorBoard deactivated : compatibility problem TensorBoard / TensorFlow 2.3 (cuDNN implementation of LSTM/GRU) can arise\")\n            self.logger.warning(\"https://stackoverflow.com/questions/63619763/model-training-hangs-forever-when-using-a-tensorboard-callback-with-an-lstm-laye\")\n            self.logger.warning(\" In order to activate if, one has to modify the method _get_callbacks of model_keras.py\")\n            self.logger.warning(\" ###################### \")\n            # callbacks.append(TensorBoard(log_dir=log_dir, write_grads=False, write_images=False))\n            # self.logger.info(f\"To start tensorboard: python -m tensorboard.main --logdir {tensorboard_dir}\")\n\n        return callbacks\n\n    def _get_learning_rate_scheduler(self) -&gt; Union[Callable, None]:\n        '''Fonction to define a Learning Rate Scheduler\n           -&gt; if it returns None, no scheduler will be used. (def.)\n           -&gt; This function will be save directly in the model configuration file\n           -&gt; This can be overridden at runing time\n\n        Returns:\n            (Callable | None): A learning rate Scheduler\n        '''\n        # e.g.\n        # def scheduler(epoch):\n        #     lim_epoch = 75\n        #     if epoch &lt; lim_epoch:\n        #         return 0.01\n        #     else:\n        #         return max(0.001, 0.01 * math.exp(0.01 * (lim_epoch - epoch)))\n        scheduler = None\n        return scheduler\n\n    def _plot_metrics_and_loss(self, fit_history) -&gt; None:\n        '''Plots accuracy &amp; loss\n\n        Arguments:\n            fit_history (?) : fit history\n        '''\n        # Manage dir\n        plots_path = os.path.join(self.model_dir, 'plots')\n        if not os.path.exists(plots_path):\n            os.makedirs(plots_path)\n\n        # Get a dictionnary of possible metrics/loss plots\n        metrics_dir = {\n            'acc': ['Accuracy', 'accuracy'],\n            'loss': ['Loss', 'loss'],\n            'categorical_accuracy': ['Categorical accuracy', 'categorical_accuracy'],\n            'f1': ['F1-score', 'f1_score'],\n            'precision': ['Precision', 'precision'],\n            'recall': ['Recall', 'recall'],\n        }\n\n        # Plot each available metric\n        for metric in fit_history.history.keys():\n            if metric in metrics_dir.keys():\n                title = metrics_dir[metric][0]\n                filename = metrics_dir[metric][1]\n                plt.figure(figsize=(10, 8))\n                plt.plot(fit_history.history[metric])\n                plt.plot(fit_history.history[f'val_{metric}'])\n                plt.title(f\"Model {title}\")\n                plt.ylabel(title)\n                plt.xlabel('Epoch')\n                plt.legend(['Train', 'Validation'], loc='upper left')\n                # Save\n                filename = f\"{filename}.jpeg\"\n                plt.savefig(os.path.join(plots_path, filename))\n\n                # Close figures\n                plt.close('all')\n\n    def _save_model_png(self, model) -&gt; None:\n        '''Tries to save the structure of the model in png format\n        Graphviz necessary\n\n        Args:\n            model (?): model to plot\n        '''\n        # Check if graphiz is intalled\n        # TODO : to be improved !\n        graphiz_path = 'C:/Program Files (x86)/Graphviz2.38/bin/'\n        if os.path.isdir(graphiz_path):\n            os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n            img_path = os.path.join(self.model_dir, 'model.png')\n            plot_model(model, to_file=img_path)\n\n    @no_type_check  # We do not check the type, because it is complicated with managing custom_objects_str\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save configuration JSON\n        if json_data is None:\n            json_data = {}\n\n        json_data['librairie'] = 'keras'\n        json_data['batch_size'] = self.batch_size\n        json_data['epochs'] = self.epochs\n        json_data['validation_split'] = self.validation_split\n        json_data['patience'] = self.patience\n        json_data['embedding_name'] = self.embedding_name\n        json_data['keras_params'] = self.keras_params\n\n        if self.model is not None:\n            json_data['keras_model'] = json.loads(self.model.to_json())\n        else:\n            json_data['keras_model'] = None\n\n        # Add _get_model code if not in json_data\n        if '_get_model' not in json_data.keys():\n            json_data['_get_model'] = pickle.source.getsourcelines(self._get_model)[0]\n        # Add _get_learning_rate_scheduler code if not in json_data\n        if '_get_learning_rate_scheduler' not in json_data.keys():\n            json_data['_get_learning_rate_scheduler'] = pickle.source.getsourcelines(self._get_learning_rate_scheduler)[0]\n        # Add custom_objects code if not in json_data\n        if 'custom_objects' not in json_data.keys():\n            custom_objects_str = self.custom_objects.copy()\n            for key in custom_objects_str.keys():\n                if callable(custom_objects_str[key]):\n                    # Nominal case\n                    if not type(custom_objects_str[key]) == functools.partial:\n                        custom_objects_str[key] = pickle.source.getsourcelines(custom_objects_str[key])[0]\n                    # Manage partials\n                    else:\n                        custom_objects_str[key] = {\n                            'type': 'partial',\n                            'args': custom_objects_str[key].args,\n                            'function': pickle.source.getsourcelines(custom_objects_str[key].func)[0],\n                        }\n            json_data['custom_objects'] = custom_objects_str\n\n        # Save strategy :\n        # - best.hdf5 already saved in fit()\n        # - We don't want it in the .pkl as it is heavy &amp; already saved\n        keras_model = self.model\n        self.model = None\n        super().save(json_data=json_data)\n        self.model = keras_model\n\n    def _hook_post_load_model_pkl(self):\n        '''Manages a model specificities post load from a pickle file (i.e. not from standalone files)\n\n        Raises:\n            FileNotFoundError: If the weights file does not exist\n        '''\n        # Paths\n        hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\n\n        # Manage errors\n        if not os.path.isfile(hdf5_path):\n            raise FileNotFoundError(f\"Can't find weights file ({hdf5_path})\")\n\n        # Loading the weights\n        self.model = self._reload_weights(hdf5_path)\n\n    @classmethod\n    def _init_new_instance_from_configs(cls, configs):\n        '''Inits a new instance from a set of configurations\n\n        Args:\n            configs: a set of configurations of a model to be reloaded\n        Returns:\n            ModelClass: the newly generated class\n        '''\n        # Call parent\n        model = super()._init_new_instance_from_configs(configs)\n\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['batch_size', 'epochs', 'validation_split', 'patience',\n                          'embedding_name', 'keras_params']:\n            setattr(model, attribute, configs.get(attribute, getattr(model, attribute)))\n\n        # Return the new model\n        return model\n\n    def _load_standalone_files(self, default_model_dir: Union[str, None] = None,\n                               hdf5_path: Union[str, None] = None, *args, **kwargs):\n        '''Loads standalone files for a newly created model via _init_new_instance_from_configs\n\n        Kwargs:\n            default_model_dir (str): a path to look for default file paths\n                                     If None, standalone files path should all be provided\n            hdf5_path (str): Path to the hdf5 weights file\n        Raises:\n            ValueError: If the hdf5 weights file is not specified and can't be inferred\n            FileNotFoundError: If the hdf5 weights file does not exist\n        '''\n        # Check if we are able to get all needed paths\n        if default_model_dir is None and hdf5_path is None:\n            raise ValueError(\"The hdf5 weights file is not specified and can't be inferred\")\n\n        # Retrieve file paths\n        if hdf5_path is None:\n            hdf5_path = os.path.join(default_model_dir, \"best.hdf5\")\n\n        # Check paths exists\n        if not os.path.isfile(hdf5_path):\n            raise FileNotFoundError(f\"Can't find hdf5 weights file ({hdf5_path})\")\n\n        # Reload model\n        self.model = self._reload_weights(hdf5_path)\n\n        # Save best hdf5 in new folder (as this is skipped in save function)\n        new_hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\n        shutil.copyfile(hdf5_path, new_hdf5_path)\n\n    def _reload_weights(self, hdf5_path: str) -&gt; Any:\n        '''Loads a Keras model from a HDF5 file\n\n        Args:\n            hdf5_path (str): Path to the hdf5 file\n        Returns:\n            ?: Keras model\n        '''\n        # Fix tensorflow GPU if not already done (useful if we reload a model)\n        try:\n            gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n            for device in gpu_devices:\n                tf.config.experimental.set_memory_growth(device, True)\n        except Exception:\n            pass\n\n        # We check if we already have the custom objects\n        if hasattr(self, 'custom_objects') and self.custom_objects is not None:\n            custom_objects = self.custom_objects\n        else:\n            self.logger.warning(\"Can't find the attribute 'custom_objects' in the model to be reloaded\")\n            self.logger.warning(\"Backup on the default custom_objects of utils_deep_keras\")\n            custom_objects = utils_deep_keras.custom_objects\n\n        # Loading of the model\n        keras_model = load_model_keras(hdf5_path, custom_objects=custom_objects)\n\n        # Return\n        return keras_model\n\n    def _is_gpu_activated(self) -&gt; bool:\n        '''Checks if a GPU is used\n\n        Returns:\n            bool: whether GPU is available or not\n        '''\n        # Check for available GPU devices\n        physical_devices = tf.config.list_physical_devices('GPU')\n        if len(physical_devices) &gt; 0:\n            return True\n        else:\n            return False\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_keras/#template_nlp.models_training.models_tensorflow.model_keras.ModelKeras.__init__","title":"<code>__init__(batch_size=64, epochs=99, validation_split=0.2, patience=5, embedding_name='cc.fr.300.pkl', keras_params=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass for more arguments)</p> Kwargs <p>batch_size (int): Batch size epochs (int): Number of epochs validation_split (float): Percentage for the validation set split     Only used if no input validation set when fitting patience (int): Early stopping patience embedding_name (str) : The name of the embedding matrix to use keras_params (dict): Parameters used by Keras models.     e.g. learning_rate, nb_lstm_units, etc...     The purpose of this dictionary is for the user to use it as they wants in the _get_model function     This parameter was initially added in order to do an hyperparameters search</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_keras.py</code> <pre><code>def __init__(self, batch_size: int = 64, epochs: int = 99, validation_split: float = 0.2, patience: int = 5,\n             embedding_name: str = 'cc.fr.300.pkl', keras_params: Union[dict, None] = None, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelClass for more arguments)\n\n    Kwargs:\n        batch_size (int): Batch size\n        epochs (int): Number of epochs\n        validation_split (float): Percentage for the validation set split\n            Only used if no input validation set when fitting\n        patience (int): Early stopping patience\n        embedding_name (str) : The name of the embedding matrix to use\n        keras_params (dict): Parameters used by Keras models.\n            e.g. learning_rate, nb_lstm_units, etc...\n            The purpose of this dictionary is for the user to use it as they wants in the _get_model function\n            This parameter was initially added in order to do an hyperparameters search\n    '''\n    # TODO: learning rate should be an attribute !\n    # Init.\n    super().__init__(**kwargs)\n\n    # Fix tensorflow GPU\n    gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n    for device in gpu_devices:\n        tf.config.experimental.set_memory_growth(device, True)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Param. model\n    self.batch_size = batch_size\n    self.epochs = epochs\n    self.validation_split = validation_split\n    self.patience = patience\n\n    # Model set on fit\n    self.model: Any = None\n\n    # Param embedding (can be None if no embedding)\n    self.embedding_name = embedding_name\n\n    # Keras params\n    if keras_params is None:\n        keras_params = {}\n    self.keras_params = keras_params.copy()\n\n    # Keras custom objects : we get the ones specified in utils_deep_keras\n    self.custom_objects = utils_deep_keras.custom_objects\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_keras/#template_nlp.models_training.models_tensorflow.model_keras.ModelKeras.fit","title":"<code>fit(x_train, y_train, x_valid=None, y_valid=None, with_shuffle=True, **kwargs)</code>","text":"<p>Fits the model</p> <p>Parameters:</p> Name Type Description Default <code>x_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <code>y_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_targets]</p> required <p>Kwargs:     x_valid (?): Array-like, shape = [n_samples, n_features]     y_valid (?): Array-like, shape = [n_samples, n_targets]     with_shuffle (bool): If x, y must be shuffled before fitting         Experimental: We must verify if it works as intended depending on the formats of x and y         This should be used if y is not shuffled as the split_validation takes the lines in order.         Thus, the validation set might get classes which are not in the train set ... Raises:     ValueError: If different classes when comparing an already fitted model and a new dataset</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_keras.py</code> <pre><code>def fit(self, x_train, y_train, x_valid=None, y_valid=None, with_shuffle: bool = True, **kwargs) -&gt; None:\n    '''Fits the model\n\n    Args:\n        x_train (?): Array-like, shape = [n_samples, n_features]\n        y_train (?): Array-like, shape = [n_samples, n_targets]\n    Kwargs:\n        x_valid (?): Array-like, shape = [n_samples, n_features]\n        y_valid (?): Array-like, shape = [n_samples, n_targets]\n        with_shuffle (bool): If x, y must be shuffled before fitting\n            Experimental: We must verify if it works as intended depending on the formats of x and y\n            This should be used if y is not shuffled as the split_validation takes the lines in order.\n            Thus, the validation set might get classes which are not in the train set ...\n    Raises:\n        ValueError: If different classes when comparing an already fitted model and a new dataset\n    '''\n    ##############################################\n    # Manage retrain\n    ##############################################\n    # If a model has already been fitted, we make a new folder in order not to overwrite the existing one !\n    # And we save the old conf\n    if self.trained:\n        # Get src files to save\n        src_files = [os.path.join(self.model_dir, \"configurations.json\")]\n        if self.nb_fit &gt; 1:\n            for i in range(1, self.nb_fit):\n                src_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n        # Change model dir\n        self.model_dir = self._get_new_model_dir()\n        # Get dst files\n        dst_files = [os.path.join(self.model_dir, f\"configurations_fit_{self.nb_fit}.json\")]\n        if self.nb_fit &gt; 1:\n            for i in range(1, self.nb_fit):\n                dst_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n        # Copies\n        for src, dst in zip(src_files, dst_files):\n            try:\n                shutil.copyfile(src, dst)\n            except Exception as e:\n                self.logger.error(f\"Impossible to copy {src} to {dst}\")\n                self.logger.error(\"We still continue ...\")\n                self.logger.error(repr(e))\n\n    ##############################################\n    # Prepare x_train, x_valid, y_train &amp; y_valid\n    # Also extract list of classes\n    ##############################################\n\n    # If not multilabel, transform y_train as dummies (should already be the case for multi-labels)\n    if not self.multi_label:\n        # If len(array.shape)==2, we flatten the array if the second dimension is useless\n        if isinstance(y_train, np.ndarray) and len(y_train.shape) == 2 and y_train.shape[1] == 1:\n            y_train = np.ravel(y_train)\n        if isinstance(y_valid, np.ndarray) and len(y_valid.shape) == 2 and y_valid.shape[1] == 1:\n            y_valid = np.ravel(y_valid)\n        # Transformation dummies\n        y_train_dummies = pd.get_dummies(y_train).astype(int)\n        y_valid_dummies = pd.get_dummies(y_valid).astype(int) if y_valid is not None else None\n        # Important : get_dummies reorder the columns in alphabetical order\n        # Thus, there is no problem if we fit again on a new dataframe with shuffled data\n        list_classes = list(y_train_dummies.columns)\n        # FIX: valid test might miss some classes, hence we need to add them back to y_valid_dummies\n        if y_valid_dummies is not None and y_train_dummies.shape[1] != y_valid_dummies.shape[1]:\n            for cl in list_classes:\n                # Add missing columns\n                if cl not in y_valid_dummies.columns:\n                    y_valid_dummies[cl] = 0\n            y_valid_dummies = y_valid_dummies[list_classes]  # Reorder\n    # Else keep it as it is\n    else:\n        y_train_dummies = y_train\n        y_valid_dummies = y_valid\n        if hasattr(y_train_dummies, 'columns'):\n            list_classes = list(y_train_dummies.columns)\n        else:\n            self.logger.warning(\n                \"Can't read the name of the columns of y_train -&gt; inverse transformation won't be possible\"\n            )\n            # We still create a list of classes in order to be compatible with other functions\n            list_classes = [str(_) for _ in range(pd.DataFrame(y_train_dummies).shape[1])]\n\n    # Set dict_classes based on list classes\n    dict_classes = {i: col for i, col in enumerate(list_classes)}\n\n    # Validate classes if already trained, else set them\n    if self.trained:\n        if self.list_classes != list_classes:\n            raise ValueError(\"Error: the new dataset does not match with the already fitted model\")\n        if self.dict_classes != dict_classes:\n            raise ValueError(\"Error: the new dataset does not match with the already fitted model\")\n    else:\n        self.list_classes = list_classes\n        self.dict_classes = dict_classes\n\n    # Shuffle x, y if wanted\n    # It is advised as validation_split from keras does not shufle the data\n    # Hence we might have classes in the validation data that we never met in the training data\n    if with_shuffle:\n        rng = np.random.RandomState(self.random_seed)\n        p = rng.permutation(len(x_train))\n        x_train = np.array(x_train)[p]\n        y_train_dummies = np.array(y_train_dummies)[p]\n    # Else still transform to numpy array\n    else:\n        x_train = np.array(x_train)\n        y_train_dummies = np.array(y_train_dummies)\n\n    # Also get y_valid_dummies as numpy\n    y_valid_dummies = np.array(y_valid_dummies)\n\n    # Prepare x_train\n    x_train = self._prepare_x_train(x_train)\n\n    # If available, also prepare x_valid &amp; get validation_data (tuple)\n    validation_data: Optional[tuple] = None  # Def. None if y_valid is None\n    if y_valid is not None:\n        x_valid = self._prepare_x_test(x_valid)\n        validation_data = (x_valid, y_valid_dummies)\n    else:\n        x_train, x_valid,  y_train_dummies, y_valid_dummies = train_test_split(x_train, y_train_dummies, \n                                                                               test_size=self.validation_split,\n                                                                               random_state=self.random_seed)\n        validation_data = (x_valid, y_valid_dummies)\n\n    if validation_data is None:\n        self.logger.warning(f\"Warning, no validation set. The training set will be splitted (validation fraction = {self.validation_split})\")\n\n    ##############################################\n    # Fit\n    ##############################################\n\n    # Get model (if already fitted, _get_model returns instance model)\n    self.model = self._get_model()\n\n    # Get callbacks (early stopping &amp; checkpoint)\n    callbacks = self._get_callbacks()\n\n    # Create data generator\n    data_train_generator = RandomStateDataGenerator(x_train, y_train_dummies, self.batch_size, self.random_seed)\n    data_val_generator = RandomStateDataGenerator(x_valid, y_valid_dummies, self.batch_size, self.random_seed)\n\n    # Fit\n    # We use a try...except in order to save the model if an error arises\n    # after more than a minute into training\n    start_time = time.time()\n    try:\n        fit_history = self.model.fit(  # type: ignore\n            data_train_generator,\n            epochs=self.epochs,\n            validation_data=data_val_generator,\n            callbacks=callbacks,\n            verbose=1,\n            shuffle=False\n        )\n    except (RuntimeError, SystemError, SystemExit, EnvironmentError, KeyboardInterrupt, tf.errors.ResourceExhaustedError, tf.errors.InternalError,\n            tf.errors.UnavailableError, tf.errors.UnimplementedError, tf.errors.UnknownError, Exception) as e:\n        # Steps:\n        # 1. Display tensorflow error\n        # 2. Check if more than one minute elapsed &amp; existence best.hdf5\n        # 3. Reload best model\n        # 4. We consider that a fit occured (trained = True, nb_fit += 1)\n        # 5. Save &amp; create a warning file\n        # 6. Display error messages\n        # 7. Raise an error\n\n        # 1.\n        self.logger.error(repr(e))\n\n        # 2.\n        best_path = os.path.join(self.model_dir, 'best.hdf5')\n        time_spent = time.time() - start_time\n        if time_spent &gt;= 60 and os.path.exists(best_path):\n            # 3.\n            self.model = load_model_keras(best_path, custom_objects=self.custom_objects)\n            # 4.\n            self.trained = True\n            self.nb_fit += 1\n            # 5.\n            self.save()\n            with open(os.path.join(self.model_dir, \"0_MODEL_INCOMPLETE\"), 'w'):\n                pass\n            with open(os.path.join(self.model_dir, \"1_TRAINING_NEEDS_TO_BE_RESUMED\"), 'w'):\n                pass\n            # 6.\n            self.logger.error(\"[EXPERIMENTAL] Error during model training\")\n            self.logger.error(f\"[EXPERIMENTAL] The error happened after {round(time_spent, 2)}s of training\")\n            self.logger.error(\"[EXPERIMENTAL] A saving of the model is done but this model won't be usable as is.\")\n            self.logger.error(f\"[EXPERIMENTAL] In order to resume the training, we have to specify this model ({ntpath.basename(self.model_dir)}) in the file 2_training.py\")\n            self.logger.error(\"[EXPERIMENTAL] Warning, the preprocessing is not saved in the configuration file\")\n            self.logger.error(\"[EXPERIMENTAL] Warning, the best model might be corrupted in some cases\")\n        # 7.\n        raise RuntimeError(\"Error during model training\")\n\n    # Print accuracy &amp; loss if level_save &gt; 'LOW'\n    if self.level_save in ['MEDIUM', 'HIGH']:\n        # Plot accuracy\n        self._plot_metrics_and_loss(fit_history)\n        # Reload best model\n        self.model = load_model_keras(os.path.join(self.model_dir, 'best.hdf5'), custom_objects=self.custom_objects)\n\n    # Set trained\n    self.trained = True\n    self.nb_fit += 1\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_keras/#template_nlp.models_training.models_tensorflow.model_keras.ModelKeras.predict","title":"<code>predict(x_test, return_proba=False, inference_batch_size=128, alternative_version=False, **kwargs)</code>","text":"<p>Predictions on test set</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples]</p> required <p>Kwargs:     return_proba (bool): If the function should return the probabilities instead of the classes     inference_batch_size (int): size (approximate) of batches     alternative_version (bool): If an alternative predict version (<code>tf.function</code> + <code>model.__call__</code>) must be used. Should be faster with low nb of inputs. Returns:     (np.ndarray): Array, shape = [n_samples, n_classes]</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_keras.py</code> <pre><code>@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict(self, x_test, return_proba: bool = False, inference_batch_size: int = 128, alternative_version: bool = False, **kwargs) -&gt; np.ndarray:\n    '''Predictions on test set\n\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples]\n    Kwargs:\n        return_proba (bool): If the function should return the probabilities instead of the classes\n        inference_batch_size (int): size (approximate) of batches\n        alternative_version (bool): If an alternative predict version (`tf.function` + `model.__call__`) must be used. Should be faster with low nb of inputs.\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n    # Cast in pd.Series\n    x_test = pd.Series(x_test)\n\n    # Predict\n    predicted_proba = self.predict_proba(x_test, inference_batch_size=inference_batch_size, alternative_version=alternative_version)\n\n    # We return the probabilities if wanted\n    if return_proba:\n        return predicted_proba\n\n    # Finally, we get the classes predictions\n    return self.get_classes_from_proba(predicted_proba)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_keras/#template_nlp.models_training.models_tensorflow.model_keras.ModelKeras.predict_proba","title":"<code>predict_proba(x_test, inference_batch_size=128, alternative_version=False, **kwargs)</code>","text":"<p>Predicts probabilities on the test dataset</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples, n_features]</p> required <p>Kwargs:     inference_batch_size (int): size (approximate) of batches     alternative_version (bool): If an alternative predict version (<code>tf.function</code> + <code>model.__call__</code>) must be used. Should be faster with low nb of inputs. Returns:     (np.ndarray): Array, shape = [n_samples, n_classes]</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_keras.py</code> <pre><code>@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict_proba(self, x_test, inference_batch_size: int = 128, alternative_version: bool = False, **kwargs) -&gt; np.ndarray:\n    '''Predicts probabilities on the test dataset\n\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n    Kwargs:\n        inference_batch_size (int): size (approximate) of batches\n        alternative_version (bool): If an alternative predict version (`tf.function` + `model.__call__`) must be used. Should be faster with low nb of inputs.\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n    # Prepare input\n    x_test = self._prepare_x_test(x_test)\n    # Process\n    if alternative_version:\n        return self._alternative_predict_proba(x_test, inference_batch_size=inference_batch_size)\n    else:\n        # We advise you to avoid using `model.predict` with newest TensorFlow versions (possible memory leak) in a production environment (e.g. API)\n        # https://github.com/tensorflow/tensorflow/issues/58676\n        # Instead, you can use the alternative version that uses tf.function decorator &amp; model.__call__\n        # However, it should still be better to use `model.predict` for one-shot, batch mode, large input, iterations.\n        return self.model.predict(x_test, batch_size=inference_batch_size, verbose=1)  # type: ignore\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_keras/#template_nlp.models_training.models_tensorflow.model_keras.ModelKeras.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_keras.py</code> <pre><code>@no_type_check  # We do not check the type, because it is complicated with managing custom_objects_str\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save configuration JSON\n    if json_data is None:\n        json_data = {}\n\n    json_data['librairie'] = 'keras'\n    json_data['batch_size'] = self.batch_size\n    json_data['epochs'] = self.epochs\n    json_data['validation_split'] = self.validation_split\n    json_data['patience'] = self.patience\n    json_data['embedding_name'] = self.embedding_name\n    json_data['keras_params'] = self.keras_params\n\n    if self.model is not None:\n        json_data['keras_model'] = json.loads(self.model.to_json())\n    else:\n        json_data['keras_model'] = None\n\n    # Add _get_model code if not in json_data\n    if '_get_model' not in json_data.keys():\n        json_data['_get_model'] = pickle.source.getsourcelines(self._get_model)[0]\n    # Add _get_learning_rate_scheduler code if not in json_data\n    if '_get_learning_rate_scheduler' not in json_data.keys():\n        json_data['_get_learning_rate_scheduler'] = pickle.source.getsourcelines(self._get_learning_rate_scheduler)[0]\n    # Add custom_objects code if not in json_data\n    if 'custom_objects' not in json_data.keys():\n        custom_objects_str = self.custom_objects.copy()\n        for key in custom_objects_str.keys():\n            if callable(custom_objects_str[key]):\n                # Nominal case\n                if not type(custom_objects_str[key]) == functools.partial:\n                    custom_objects_str[key] = pickle.source.getsourcelines(custom_objects_str[key])[0]\n                # Manage partials\n                else:\n                    custom_objects_str[key] = {\n                        'type': 'partial',\n                        'args': custom_objects_str[key].args,\n                        'function': pickle.source.getsourcelines(custom_objects_str[key].func)[0],\n                    }\n        json_data['custom_objects'] = custom_objects_str\n\n    # Save strategy :\n    # - best.hdf5 already saved in fit()\n    # - We don't want it in the .pkl as it is heavy &amp; already saved\n    keras_model = self.model\n    self.model = None\n    super().save(json_data=json_data)\n    self.model = keras_model\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_keras/#template_nlp.models_training.models_tensorflow.model_keras.RandomStateDataGenerator","title":"<code>RandomStateDataGenerator</code>","text":"<p>             Bases: <code>Sequence</code></p> <p>Custom data generator to control batch randomness with random_state</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_keras.py</code> <pre><code>class RandomStateDataGenerator(Sequence):\n    '''Custom data generator to control batch randomness with random_state'''\n\n    def __init__(self, x_train: np.ndarray, y_train: np.ndarray, batch_size: int = 32,\n                  random_seed: Union[int, None] = None):\n        '''Initialization of the class\n        Args:\n            x_train (ndarray): training features\n            y_train (ndarray): training outputs\n            batch_size (int): Batch size\n            random_seed (int or None): seed to use for random_state initialization\n        '''\n        self.x = x_train\n        self.y = y_train\n        self.batch_size = batch_size\n        self.random_state = np.random.RandomState(seed=random_seed)\n        self.indices = shuffle(np.arange(len(self.x)), random_state=self.random_state)\n\n\n    def __len__(self):\n        return int(np.ceil(len(self.x) / self.batch_size))\n\n\n    def __getitem__(self, index):\n        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n        batch_x = self.x[batch_indices]\n        batch_y = self.y[batch_indices]\n        return np.array(batch_x), np.array(batch_y)\n\n\n    def on_epoch_end(self):\n        self.indices = shuffle(np.arange(len(self.x)), random_state=self.random_state)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_keras/#template_nlp.models_training.models_tensorflow.model_keras.RandomStateDataGenerator.__init__","title":"<code>__init__(x_train, y_train, batch_size=32, random_seed=None)</code>","text":"<p>Initialization of the class Args:     x_train (ndarray): training features     y_train (ndarray): training outputs     batch_size (int): Batch size     random_seed (int or None): seed to use for random_state initialization</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_keras.py</code> <pre><code>def __init__(self, x_train: np.ndarray, y_train: np.ndarray, batch_size: int = 32,\n              random_seed: Union[int, None] = None):\n    '''Initialization of the class\n    Args:\n        x_train (ndarray): training features\n        y_train (ndarray): training outputs\n        batch_size (int): Batch size\n        random_seed (int or None): seed to use for random_state initialization\n    '''\n    self.x = x_train\n    self.y = y_train\n    self.batch_size = batch_size\n    self.random_state = np.random.RandomState(seed=random_seed)\n    self.indices = shuffle(np.arange(len(self.x)), random_state=self.random_state)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_tfidf_dense/","title":"Model tfidf dense","text":""},{"location":"reference/template_nlp/models_training/models_tensorflow/model_tfidf_dense/#template_nlp.models_training.models_tensorflow.model_tfidf_dense.ModelTfidfDense","title":"<code>ModelTfidfDense</code>","text":"<p>             Bases: <code>ModelKeras</code></p> <p>Model for predictions via TF-IDF + Dense</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_tfidf_dense.py</code> <pre><code>class ModelTfidfDense(ModelKeras):\n    '''Model for predictions via TF-IDF + Dense'''\n\n    _default_name = 'model_tfidf_dense'\n\n    def __init__(self, tfidf_params: Union[dict, None] = None, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelClass &amp; ModelKeras for more arguments).\n\n        Kwargs:\n            tfidf_params (dict) : Parameters for the tfidf\n        '''\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        if tfidf_params is None:\n            tfidf_params = {}\n        self.tfidf = TfidfVectorizer(**tfidf_params)\n\n    def _prepare_x_train(self, x_train) -&gt; np.ndarray:\n        '''Prepares the input data for the model. Called when fitting the model\n\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Prepared data\n        '''\n        # Fit tfidf &amp; return x transformed\n        self.tfidf.fit(x_train)\n        # TODO: Use of todense because tensorflow 2.3 does not support sparse data anymore\n        return self.tfidf.transform(x_train).todense()\n\n    def _prepare_x_test(self, x_test) -&gt; np.ndarray:\n        '''Prepares the input data for the model. Called when fitting the model\n\n        Args:\n            x_test (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Prepared data\n        '''\n        # Get tf-idf &amp; fit on train\n        # TODO: Use of todense because tensorflow 2.3 does not support sparse data anymore\n        return self.tfidf.transform(x_test).todense()\n\n    def _get_model(self) -&gt; Model:\n        '''Gets a model structure - returns the instance model instead if already defined\n\n        Returns:\n            (Model): a Keras model\n        '''\n        # Return model if already set\n        if self.model is not None:\n            return self.model\n\n        # Get input/output dimensions\n        input_dim = len(self.tfidf.get_feature_names_out())\n        num_classes = len(self.list_classes)\n\n        # Get random_state\n        random_state = np.random.RandomState(self.random_seed)\n        limit = int(1e9)\n\n        # Process\n        tfidf_features = Input(shape=(input_dim,))\n        x = Dense(128, activation=None, kernel_initializer=HeUniform(random_state.randint(limit)))(tfidf_features)\n        x = BatchNormalization(momentum=0.9)(x)\n        x = ELU(alpha=1.0)(x)\n        x = Dropout(0.5,seed=random_state.randint(limit))(x)\n\n        x = Dense(64, activation=None, kernel_initializer=HeUniform(random_state.randint(limit)))(x)\n        x = BatchNormalization(momentum=0.9)(x)\n        x = ELU(alpha=1.0)(x)\n        x = Dropout(0.5, seed=random_state.randint(limit))(x)\n\n        x = Dense(32, activation=None, kernel_initializer=HeUniform(random_state.randint(limit)))(x)\n        x = BatchNormalization(momentum=0.9)(x)\n        x = ELU(alpha=1.0)(x)\n        x = Dropout(0.5, seed=random_state.randint(limit))(x)\n\n        # Last layer\n        activation = 'sigmoid' if self.multi_label else 'softmax'\n        out = Dense(num_classes, activation=activation, kernel_initializer=GlorotUniform(random_state.randint(limit)))(x)\n\n        # Compile model\n        model = Model(inputs=tfidf_features, outputs=[out])\n        lr = self.keras_params.get('learning_rate', 0.002)\n        decay = self.keras_params.get('decay', 0.0)\n        self.logger.info(f\"Learning rate: {lr}\")\n        self.logger.info(f\"Decay: {decay}\")\n        optimizer = Adam(lr=lr, decay=decay)\n        # loss = utils_deep_keras.f1_loss if self.multi_label else 'categorical_crossentropy'\n        loss = 'binary_crossentropy' if self.multi_label else 'categorical_crossentropy'  # utils_deep_keras.f1_loss also possible if multi-labels\n        metrics: List[Union[str, Callable]] = ['accuracy'] if not self.multi_label else ['categorical_accuracy', utils_deep_keras.f1, utils_deep_keras.precision, utils_deep_keras.recall]\n        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n        if self.logger.getEffectiveLevel() &lt; logging.ERROR:\n            model.summary()\n\n        # Try to save model as png if level_save &gt; 'LOW'\n        if self.level_save in ['MEDIUM', 'HIGH']:\n            self._save_model_png(model)\n\n        # Return\n        return model\n\n    @tf.function(input_signature=(tf.TensorSpec(shape=(None, None), dtype=tf.float64, name='x'), ))\n    def _serve(self, x: np.ndarray):\n        '''Improves predict function using tf.function (cf. https://www.tensorflow.org/guide/function)\n        Args:\n            x (np.ndarray): input data\n        Returns:\n            tf.tensor: model's output\n        '''\n        return self.model(x, training=False)\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save configuration JSON\n        if json_data is None:\n            json_data = {}\n\n        # Add tfidf params\n        confs = self.tfidf.get_params()\n        # Get rid of some non serializable conf\n        for special_conf in ['dtype', 'base_estimator']:\n            if special_conf in confs.keys():\n                confs[special_conf] = str(confs[special_conf])\n        json_data['tfidf_confs'] = confs\n\n        # Save tfidf if not None &amp; level_save &gt; LOW\n        if (self.tfidf is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n            # Manage paths\n            tfidf_path = os.path.join(self.model_dir, \"tfidf_standalone.pkl\")\n            # Save as pickle\n            with open(tfidf_path, 'wb') as f:\n                pickle.dump(self.tfidf, f)\n\n        # Save\n        super().save(json_data=json_data)\n\n    def _load_standalone_files(self, default_model_dir: Union[str, None] = None,  # type: ignore\n                               tfidf_path: Union[str, None] = None, *args, **kwargs):\n        '''Loads standalone files for a newly created model via _init_new_instance_from_configs\n\n        Kwargs:\n            default_model_dir (str): a path to look for default file paths\n                                     If None, standalone files path should all be provided\n            tfidf_path (str): Path to the TFIDF file\n        Raises:\n            ValueError: If the TFIDF file is not specified and can't be inferred\n            FileNotFoundError: If the TFIDF file does not exist\n        '''\n        # Check if we are able to get all needed paths\n        if default_model_dir is None and tfidf_path is None:\n            raise ValueError(\"The TFIDF file is not specified and can't be inferred\")\n\n        # Call parent\n        super()._load_standalone_files(default_model_dir=default_model_dir, **kwargs)\n\n        # Retrieve file paths\n        if tfidf_path is None:\n            tfidf_path = os.path.join(default_model_dir, \"tfidf_standalone.pkl\")\n\n        # Check paths exists\n        if not os.path.isfile(tfidf_path):\n            raise FileNotFoundError(f\"Can't find the TFIDF file ({tfidf_path})\")\n\n        # Reload tfidf\n        with open(tfidf_path, 'rb') as f:\n            self.tfidf = pickle.load(f)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_tfidf_dense/#template_nlp.models_training.models_tensorflow.model_tfidf_dense.ModelTfidfDense.__init__","title":"<code>__init__(tfidf_params=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass &amp; ModelKeras for more arguments).</p> Kwargs <p>tfidf_params (dict) : Parameters for the tfidf</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_tfidf_dense.py</code> <pre><code>def __init__(self, tfidf_params: Union[dict, None] = None, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelClass &amp; ModelKeras for more arguments).\n\n    Kwargs:\n        tfidf_params (dict) : Parameters for the tfidf\n    '''\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    if tfidf_params is None:\n        tfidf_params = {}\n    self.tfidf = TfidfVectorizer(**tfidf_params)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_tfidf_dense/#template_nlp.models_training.models_tensorflow.model_tfidf_dense.ModelTfidfDense.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_tfidf_dense.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save configuration JSON\n    if json_data is None:\n        json_data = {}\n\n    # Add tfidf params\n    confs = self.tfidf.get_params()\n    # Get rid of some non serializable conf\n    for special_conf in ['dtype', 'base_estimator']:\n        if special_conf in confs.keys():\n            confs[special_conf] = str(confs[special_conf])\n    json_data['tfidf_confs'] = confs\n\n    # Save tfidf if not None &amp; level_save &gt; LOW\n    if (self.tfidf is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n        # Manage paths\n        tfidf_path = os.path.join(self.model_dir, \"tfidf_standalone.pkl\")\n        # Save as pickle\n        with open(tfidf_path, 'wb') as f:\n            pickle.dump(self.tfidf, f)\n\n    # Save\n    super().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/utils_deep_keras/","title":"Utils deep keras","text":""},{"location":"reference/template_nlp/models_training/models_tensorflow/utils_deep_keras/#template_nlp.models_training.models_tensorflow.utils_deep_keras.AttentionAverage","title":"<code>AttentionAverage</code>","text":"<p>             Bases: <code>Layer</code></p> Source code in <code>template_nlp/models_training/models_tensorflow/utils_deep_keras.py</code> <pre><code>class AttentionAverage(Layer):\n    def __init__(self, attention_hops, **kwargs) -&gt; None:\n        self.attention_hops = attention_hops\n        self.applied_axis = 1\n        super(AttentionAverage, self).__init__()\n\n    def get_config(self) -&gt; Any:\n        '''Gets the config'''\n        config = super().get_config().copy()\n        config.update({\n            'attention_hops': self.attention_hops\n        })\n        return config\n\n    def call(self, input) -&gt; Any:\n        return tf.divide(tf.reduce_sum(input, self.applied_axis), self.attention_hops)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/utils_deep_keras/#template_nlp.models_training.models_tensorflow.utils_deep_keras.AttentionAverage.get_config","title":"<code>get_config()</code>","text":"<p>Gets the config</p> Source code in <code>template_nlp/models_training/models_tensorflow/utils_deep_keras.py</code> <pre><code>def get_config(self) -&gt; Any:\n    '''Gets the config'''\n    config = super().get_config().copy()\n    config.update({\n        'attention_hops': self.attention_hops\n    })\n    return config\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/utils_deep_keras/#template_nlp.models_training.models_tensorflow.utils_deep_keras.AttentionWithContext","title":"<code>AttentionWithContext</code>","text":"<p>             Bases: <code>Layer</code></p> <p>Attention operation, with a context/query vector, for temporal data. Supports Masking. Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf] \"Hierarchical Attention Networks for Document Classification\" by using a context vector to assist the attention</p>"},{"location":"reference/template_nlp/models_training/models_tensorflow/utils_deep_keras/#template_nlp.models_training.models_tensorflow.utils_deep_keras.AttentionWithContext--input-shape","title":"Input shape","text":"<pre><code>3D tensor with shape: `(samples, steps, features)`.\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/utils_deep_keras/#template_nlp.models_training.models_tensorflow.utils_deep_keras.AttentionWithContext--output-shape","title":"Output shape","text":"<pre><code>2D tensor with shape: `(samples, features)`.\n</code></pre> <p>:param kwargs: Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True. The dimensions are inferred based on the output shape of the RNN. Example:     model.add(LSTM(64, return_sequences=True))     model.add(AttentionWithContext())</p> Source code in <code>template_nlp/models_training/models_tensorflow/utils_deep_keras.py</code> <pre><code>class AttentionWithContext(Layer):\n    '''Attention operation, with a context/query vector, for temporal data.\n    Supports Masking.\n    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n    \"Hierarchical Attention Networks for Document Classification\"\n    by using a context vector to assist the attention\n    # Input shape\n        3D tensor with shape: `(samples, steps, features)`.\n    # Output shape\n        2D tensor with shape: `(samples, features)`.\n    :param kwargs:\n    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n    The dimensions are inferred based on the output shape of the RNN.\n    Example:\n        model.add(LSTM(64, return_sequences=True))\n        model.add(AttentionWithContext())\n    '''\n    def __init__(self, W_regularizer=None, u_regularizer=None, b_regularizer=None,\n                 W_constraint=None, u_constraint=None, b_constraint=None, bias=True,\n                 return_attention=False, w_initializer=None, b_initializer=None, \n                 u_initializer=None, **kwargs):\n        self.return_attention = return_attention\n        self.bias = bias\n        self.w_initializer = w_initializer\n        self.b_initializer = b_initializer\n        self.u_initializer = u_initializer\n        super(AttentionWithContext, self).__init__(**kwargs)\n\n    def get_config(self) -&gt; Any:\n        config = super().get_config().copy()\n        config.update({\n            'return_attention': self.return_attention,\n            'bias': self.bias\n        })\n        return config\n\n    def build(self, input_shape) -&gt; None:\n        if len(input_shape) != 3:\n            raise ValueError(\"input_shape should be 3\")\n        input_shape_list = input_shape.as_list()\n\n        self.W = self.add_weight(shape=((input_shape_list[-1], input_shape_list[-1])),\n                                 name='{}_W'.format(self.name), initializer=self.w_initializer)\n        if self.bias:\n            self.b = self.add_weight(shape=(input_shape_list[-1],),\n                                     name='{}_b'.format(self.name), initializer=self.b_initializer)\n\n        self.u = self.add_weight(shape=(input_shape_list[-1],),\n                                 name='{}_u'.format(self.name), initializer=self.u_initializer)\n\n        super(AttentionWithContext, self).build(input_shape.as_list())\n\n    def compute_mask(self, input, input_mask=None) -&gt; Any:\n        # do not pass the mask to the next layers\n        return None\n\n    def call(self, x, mask=None) -&gt; Any:\n        uit = tf.tensordot(x, self.W, axes=1)\n        if self.bias:\n            uit += self.b\n        uit = activations.tanh(uit)\n        ait = tf.tensordot(uit, self.u, axes=1)\n        a = activations.exponential(ait)\n        # Apply mask after the exp. will be re-normalized next\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in theano\n            a *= tf.cast(mask, K.floatx())\n        # In some cases especially in the early stages of training the sum may be almost zero\n        # and this results in NaN's. A workaround is to add a very small positive number \u03b5 to the sum.\n        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n        a /= tf.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        result = K.sum(weighted_input, axis=1)\n        if self.return_attention:\n            return [result, a]\n        return result\n\n    def compute_output_shape(self, input_shape) -&gt; Any:\n        if self.return_attention:\n            return [(input_shape[0], input_shape[-1]), (input_shape[0], input_shape[1])]\n        else:\n            return tf.TensorShape([input_shape[0].value, input_shape[-1].value])\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/utils_deep_keras/#template_nlp.models_training.models_tensorflow.utils_deep_keras.compare_keras_models","title":"<code>compare_keras_models(model1, model2)</code>","text":"<p>Checks if all weights of each keras model layer are the same</p> Source code in <code>template_nlp/models_training/models_tensorflow/utils_deep_keras.py</code> <pre><code>def compare_keras_models(model1, model2):\n    ''' Checks if all weights of each keras model layer are the same\n    '''\n    for layer1, layer2 in zip(model1.layers, model2.layers):\n        if layer1.__class__.__name__!=layer2.__class__.__name__:\n            return False\n        l1 = layer1.get_weights()\n        l2 = layer2.get_weights()\n        if not all(np.array_equal(weights1, weights2) for weights1, weights2 in zip(l1, l2)):\n            return False\n    return True\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/utils_deep_keras/#template_nlp.models_training.models_tensorflow.utils_deep_keras.f1","title":"<code>f1(y_true, y_pred)</code>","text":"<p>f1 score, to use as custom metrics</p> <ul> <li>/! To use with a big batch size /! -</li> </ul> From <p>https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth values</p> required <code>y_pred</code> <p>The predicted values</p> required <p>Returns:     float: metric</p> Source code in <code>template_nlp/models_training/models_tensorflow/utils_deep_keras.py</code> <pre><code>def f1(y_true, y_pred) -&gt; float:\n    '''f1 score, to use as custom metrics\n\n    - /!\\\\ To use with a big batch size /!\\\\ -\n\n    From:\n        https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n        https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras\n\n    Args:\n        y_true: Ground truth values\n        y_pred: The predicted values\n    Returns:\n        float: metric\n    '''\n    # Round pred to 0 &amp; 1\n    y_pred = K.round(y_pred)\n    y_true = K.cast(y_true, 'float32')  # Fix : TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.\n\n    ground_positives = K.sum(y_true, axis=0) + K.epsilon()  # We add an epsilon -&gt; manage the case where a class is absent in the batch\n\n    y_pred = K.round(y_pred)\n    tp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n    # tn = K.sum(K.cast((1 - y_true) * (1 - y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\n\n    p = tp / (tp + fp + K.epsilon())\n    r = tp / (tp + fn + K.epsilon())\n\n    f1 = 2 * p * r / (p + r + K.epsilon())\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n\n    weighted_f1 = f1 * ground_positives / K.sum(ground_positives)\n    weighted_f1 = K.sum(weighted_f1)\n\n    return weighted_f1\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/utils_deep_keras/#template_nlp.models_training.models_tensorflow.utils_deep_keras.f1_loss","title":"<code>f1_loss(y_true, y_pred)</code>","text":"<p>f1 loss, to use as custom loss</p> <ul> <li>/! To use with a big batch size /! -</li> </ul> From <p>https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth values</p> required <code>y_pred</code> <p>The predicted values</p> required <p>Returns:     float: metric</p> Source code in <code>template_nlp/models_training/models_tensorflow/utils_deep_keras.py</code> <pre><code>def f1_loss(y_true, y_pred) -&gt; float:\n    '''f1 loss, to use as custom loss\n\n    - /!\\\\ To use with a big batch size /!\\\\ -\n\n    From:\n        https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n        https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras\n\n    Args:\n        y_true: Ground truth values\n        y_pred: The predicted values\n    Returns:\n        float: metric\n    '''\n    # TODO : Find a mean of rounding y_pred\n    # TODO : Problem : models will quickly converge on probabilities 1.0 &amp; 0.0 to optimize this loss....\n    # We can't round here :(\n    # Please make sure that all of your ops have a gradient defined (i.e. are differentiable).\n    # Common ops without gradient: K.argmax, K.round, K.eval.\n    y_true = K.cast(y_true, 'float32')  # Fix : TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.\n\n    ground_positives = K.sum(y_true, axis=0) + K.epsilon()  # We add an epsilon -&gt; manage the case where a class is absent in the batch\n\n    tp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n    # tn = K.sum(K.cast((1 - y_true) * (1 - y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\n\n    p = tp / (tp + fp + K.epsilon())\n    r = tp / (tp + fn + K.epsilon())\n\n    f1 = 2 * p * r / (p + r + K.epsilon())\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n\n    weighted_f1 = f1 * ground_positives / K.sum(ground_positives)\n    weighted_f1 = K.sum(weighted_f1)\n\n    return 1 - weighted_f1\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/utils_deep_keras/#template_nlp.models_training.models_tensorflow.utils_deep_keras.fb_loss","title":"<code>fb_loss(b, y_true, y_pred)</code>","text":"<p>fB loss, to use as custom loss</p> <ul> <li>/! To use with a big batch size /! -</li> </ul> From <p>https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>float</code> <p>importance recall in the calculation of the fB score</p> required <code>y_true</code> <p>Ground truth values</p> required <code>y_pred</code> <p>The predicted values</p> required <p>Returns:     float: metric</p> Source code in <code>template_nlp/models_training/models_tensorflow/utils_deep_keras.py</code> <pre><code>def fb_loss(b: float, y_true, y_pred) -&gt; float:\n    '''fB loss, to use as custom loss\n\n    - /!\\\\ To use with a big batch size /!\\\\ -\n\n    From:\n        https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n        https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras\n\n    Args:\n        b (float): importance recall in the calculation of the fB score\n        y_true: Ground truth values\n        y_pred: The predicted values\n    Returns:\n        float: metric\n    '''\n    # TODO : Find a mean of rounding y_pred\n    # TODO : Problem : models will quickly converge on probabilities 1.0 &amp; 0.0 to optimize this loss....\n    # We can't round here :(\n    # Please make sure that all of your ops have a gradient defined (i.e. are differentiable).\n    # Common ops without gradient: K.argmax, K.round, K.eval.\n    y_true = K.cast(y_true, 'float32')  # Fix : TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.\n\n    ground_positives = K.sum(y_true, axis=0) + K.epsilon()  # We add an epsilon -&gt; manage the case where a class is absent in the batch\n\n    tp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n    # tn = K.sum(K.cast((1 - y_true) * (1 - y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\n\n    p = tp / (tp + fp + K.epsilon())\n    r = tp / (tp + fn + K.epsilon())\n\n    fb = (1 + b**2) * p * r / ((p * b**2) + r + K.epsilon())\n    fb = tf.where(tf.math.is_nan(fb), tf.zeros_like(fb), fb)\n\n    weighted_fb = fb * ground_positives / K.sum(ground_positives)\n    weighted_fb = K.sum(weighted_fb)\n\n    return 1 - weighted_fb\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/utils_deep_keras/#template_nlp.models_training.models_tensorflow.utils_deep_keras.get_fb_loss","title":"<code>get_fb_loss(b=2.0)</code>","text":"<p>Gets a fB-score loss</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>float</code> <p>importance recall in the calculation of the fB score</p> <code>2.0</code> <p>Returns:     Callable: fb_loss</p> Source code in <code>template_nlp/models_training/models_tensorflow/utils_deep_keras.py</code> <pre><code>def get_fb_loss(b: float = 2.0) -&gt; Callable:\n    ''' Gets a fB-score loss\n\n    Args:\n        b (float): importance recall in the calculation of the fB score\n    Returns:\n        Callable: fb_loss\n    '''\n    # - /!\\ Utilisation partial obligatoire pour pouvoir pickle des fonctions dynamiques ! /!\\ -\n    fn = partial(fb_loss, b)\n    # FIX:  AttributeError: 'functools.partial' object has no attribute '__name__'\n    fn.__name__ = 'fb_loss'  # type: ignore\n    return fn\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/utils_deep_keras/#template_nlp.models_training.models_tensorflow.utils_deep_keras.get_weighted_binary_crossentropy","title":"<code>get_weighted_binary_crossentropy(pos_weight=10.0)</code>","text":"<p>Gets a \"weighted binary crossentropy\" loss From https://stats.stackexchange.com/questions/261128/neural-network-for-multi-label-classification-with-large-number-of-classes-outpu TO BE ADDED IN custom_objects : 'weighted_binary_crossentropy': utils_deep_keras.get_weighted_binary_crossentropy(pos_weight=...)</p> <p>Parameters:</p> Name Type Description Default <code>pos_weight</code> <code>float</code> <p>Weight of the positive class, to be tuned</p> <code>10.0</code> <p>Returns:     Callable: Weighted binary crossentropy loss</p> Source code in <code>template_nlp/models_training/models_tensorflow/utils_deep_keras.py</code> <pre><code>def get_weighted_binary_crossentropy(pos_weight: float = 10.0) -&gt; Callable:\n    ''' Gets a \"weighted binary crossentropy\" loss\n    From https://stats.stackexchange.com/questions/261128/neural-network-for-multi-label-classification-with-large-number-of-classes-outpu\n    TO BE ADDED IN custom_objects : 'weighted_binary_crossentropy': utils_deep_keras.get_weighted_binary_crossentropy(pos_weight=...)\n\n    Args:\n        pos_weight (float): Weight of the positive class, to be tuned\n    Returns:\n        Callable: Weighted binary crossentropy loss\n    '''\n    # - /!\\ Use of partial mandatory in order to be able to pickle dynamical functions ! /!\\ -\n    fn = partial(weighted_binary_crossentropy, pos_weight)\n    # FIX:  AttributeError: 'functools.partial' object has no attribute '__name__'\n    fn.__name__ = 'weighted_binary_crossentropy'  # type: ignore\n    return fn\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/utils_deep_keras/#template_nlp.models_training.models_tensorflow.utils_deep_keras.precision","title":"<code>precision(y_true, y_pred)</code>","text":"<p>Precision, to use as custom metrics</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth values</p> required <code>y_pred</code> <p>The predicted values</p> required <p>Returns:     float: metric</p> Source code in <code>template_nlp/models_training/models_tensorflow/utils_deep_keras.py</code> <pre><code>def precision(y_true, y_pred) -&gt; float:\n    '''Precision, to use as custom metrics\n\n    Args:\n        y_true: Ground truth values\n        y_pred: The predicted values\n    Returns:\n        float: metric\n    '''\n    y_pred = K.round(y_pred)\n    y_true = K.cast(y_true, 'float32')  # Fix : TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.\n\n    ground_positives = K.sum(y_true, axis=0) + K.epsilon()  # We add an epsilon -&gt; manage the case where a class is absent in the batch\n\n    tp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n    fp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\n\n    precision = tp / (tp + fp + K.epsilon())\n    precision = tf.where(tf.math.is_nan(precision), tf.zeros_like(precision), precision)\n\n    weighted_precision = precision * ground_positives / K.sum(ground_positives)\n    weighted_precision = K.sum(weighted_precision)\n\n    return weighted_precision\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/utils_deep_keras/#template_nlp.models_training.models_tensorflow.utils_deep_keras.recall","title":"<code>recall(y_true, y_pred)</code>","text":"<p>Recall to use as a custom metrics</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth values</p> required <code>y_pred</code> <p>The predicted values</p> required <p>Returns:     float: metric</p> Source code in <code>template_nlp/models_training/models_tensorflow/utils_deep_keras.py</code> <pre><code>def recall(y_true, y_pred) -&gt; float:\n    '''Recall to use as a custom metrics\n\n    Args:\n        y_true: Ground truth values\n        y_pred: The predicted values\n    Returns:\n        float: metric\n    '''\n    y_pred = K.round(y_pred)\n    y_true = K.cast(y_true, 'float32')  # Fix : TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.\n\n    ground_positives = K.sum(y_true, axis=0) + K.epsilon()  # We add an epsilon -&gt; manage the case where a class is absent in the batch\n\n    tp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\n\n    recall = tp / (tp + fn + K.epsilon())\n    recall = tf.where(tf.math.is_nan(recall), tf.zeros_like(recall), recall)\n\n    weighted_recall = recall * ground_positives / K.sum(ground_positives)\n    weighted_recall = K.sum(weighted_recall)\n\n    return weighted_recall\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/utils_deep_keras/#template_nlp.models_training.models_tensorflow.utils_deep_keras.weighted_binary_crossentropy","title":"<code>weighted_binary_crossentropy(pos_weight, target, output)</code>","text":"<p>Weighted binary crossentropy between an output tensor and a target tensor. pos_weight is used as a multiplier for the positive targets.</p> <p>Combination of the following functions: * keras.losses.binary_crossentropy * keras.backend.tensorflow_backend.binary_crossentropy * tf.nn.weighted_cross_entropy_with_logits</p> <p>Parameters:</p> Name Type Description Default <code>pos_weight</code> <code>float</code> <p>poid classe positive, to be tuned</p> required <code>target</code> <p>Target tensor</p> required <code>output</code> <p>Output tensor</p> required <p>Returns:     float: metric</p> Source code in <code>template_nlp/models_training/models_tensorflow/utils_deep_keras.py</code> <pre><code>def weighted_binary_crossentropy(pos_weight: float, target, output) -&gt; float:\n    '''Weighted binary crossentropy between an output tensor\n    and a target tensor. pos_weight is used as a multiplier\n    for the positive targets.\n\n    Combination of the following functions:\n    * keras.losses.binary_crossentropy\n    * keras.backend.tensorflow_backend.binary_crossentropy\n    * tf.nn.weighted_cross_entropy_with_logits\n\n    Args:\n        pos_weight (float): poid classe positive, to be tuned\n        target: Target tensor\n        output: Output tensor\n    Returns:\n        float: metric\n    '''\n    target = K.cast(target, 'float32')\n    output = K.cast(output, 'float32')\n    # transform back to logits\n    _epsilon = tf.convert_to_tensor(K.epsilon(), output.dtype.base_dtype)\n    output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n    output = tf.math.log(output / (1 - output))\n    # compute weighted loss\n    loss = tf.nn.weighted_cross_entropy_with_logits(target, output, pos_weight=pos_weight)\n    return tf.reduce_mean(loss, axis=-1)\n</code></pre>"},{"location":"reference/template_nlp/monitoring/","title":"Monitoring","text":""},{"location":"reference/template_nlp/monitoring/mlflow_logger/","title":"Mlflow logger","text":""},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger","title":"<code>MLflowLogger</code>","text":"<p>Abstracts how MlFlow works</p> Source code in <code>template_nlp/monitoring/mlflow_logger.py</code> <pre><code>class MLflowLogger:\n    '''Abstracts how MlFlow works'''\n\n    def __init__(self, experiment_name: str, tracking_uri: str = '', artifact_uri: str = '') -&gt; None:\n        '''Class initialization\n        Args:\n            experiment_name (str):  Name of the experiment to activate\n        Kwargs:\n            tracking_uri (str): URI of the tracking server\n            artifact_uri (str): URI where to store artifacts\n        '''\n        # Get logger\n        self.logger = logging.getLogger(__name__)\n\n        # Backup to local save if no uri (i.e. empty string)\n        if not tracking_uri:\n            tracking_uri = pathlib.Path(os.path.join(utils.get_data_path(), 'experiments', 'mlruns')).as_uri()\n        # Add \"file\" scheme if no scheme in the tracking_uri\n        elif not urlparse(tracking_uri).scheme:\n            tracking_uri = pathlib.Path(tracking_uri).resolve().as_uri()\n\n        # If no artifact_uri and tracking_uri scheme is \"file\", we set a default artifact_uri in experiments folder\n        # Otherwise we suppose artifact_uri is configured by the system\n        if not artifact_uri and urlparse(tracking_uri).scheme == \"file\":\n            artifact_uri = pathlib.Path(os.path.join(utils.get_data_path(), 'experiments', 'mlruns_artifacts')).as_uri()\n\n        # Set tracking URI &amp; experiment name\n        self.tracking_uri = tracking_uri\n\n        # Get the experiment if it exists and check if there is a connection error by doing it\n        try:\n            experiment = mlflow.get_experiment_by_name(experiment_name)\n        except Exception as e:\n            self.logger.error(repr(e))\n            raise ConnectionError(f\"Can't reach MLflow at {self.tracking_uri}. Please check the URI.\")\n\n        # If the experiment exists, we recover experiment id and artifact_uri (which is link to the experiment)\n        if experiment:\n            experiment_id = experiment.experiment_id\n            artifact_uri = experiment.artifact_location\n        # Otherwise we create a new experiment with the provided artifact_uri\n        else:\n            experiment_id = mlflow.create_experiment(experiment_name, artifact_location=artifact_uri)\n            experiment = mlflow.get_experiment_by_name(experiment_name)\n            artifact_uri = experiment.artifact_location\n\n        mlflow.set_experiment(experiment_id=experiment_id)\n\n        self.__experiment_id = experiment_id\n        self.__experiment_name = experiment_name\n        self.__artifact_uri = artifact_uri\n\n        self.logger.info(f'MLflow running. Metrics available @ {self.tracking_uri}. Experiment artifacts availaible @ {self.artifact_uri}')\n\n    @property\n    def tracking_uri(self) -&gt; str:\n        '''Current tracking uri'''\n        return mlflow.get_tracking_uri()\n\n    @tracking_uri.setter\n    def tracking_uri(self, uri:str) -&gt; None:\n        '''Set tracking uri'''\n        mlflow.set_tracking_uri(uri)\n\n    @property\n    def experiment_id(self) -&gt; str:\n        '''Experiment id. It can not be changed.'''\n        return self.__experiment_id\n\n    @property\n    def experiment_name(self) -&gt; str:\n        '''Experiment name. It can not be changed.'''\n        return self.__experiment_name\n\n    @property\n    def artifact_uri(self) -&gt; str:\n        '''Experiment artifact URI. It can not be changed.'''\n        return self.__artifact_uri\n\n    def end_run(self) -&gt; None:\n        '''Stops an MLflow run'''\n        try:\n            mlflow.end_run()\n        except Exception:\n            self.logger.error(\"Can't stop mlflow run\")\n\n    def log_metric(self, key: str, value, step: Union[int, None] = None) -&gt; None:\n        '''Logs a metric on mlflow\n\n        Args:\n            key (str): Name of the metric\n            value (float, ?): Value of the metric\n        Kwargs:\n            step (int): Step of the metric\n        '''\n        # Check for None\n        if value is None:\n            value = math.nan\n        # Log metric\n        mlflow.log_metric(key, value, step)\n\n    def log_metrics(self, metrics: dict, step: Union[int, None] = None) -&gt; None:\n        '''Logs a set of metrics in mlflow\n\n        Args:\n            metrics (dict): Metrics to log\n        Kwargs:\n            step (int): Step of the metric\n        '''\n        # Check for Nones\n        for k, v in metrics.items():\n            if v is None:\n                metrics[k] = math.nan\n        # Log metrics\n        mlflow.log_metrics(metrics, step)\n\n    def log_param(self, key: str, value) -&gt; None:\n        '''Logs a parameter in mlflow\n\n        Args:\n            key (str): Name of the parameter\n            value (str, ?): Value of the parameter (which will be cast to str if not already of type str)\n        '''\n        if value is None:\n            value = 'None'\n        # Log parameter\n        mlflow.log_param(key, value)\n\n    def log_params(self, params: dict) -&gt; None:\n        '''Logs a set of parameters in mlflow\n\n        Args:\n            params (dict): Name and value of each parameter\n        '''\n        # Check for Nones\n        for k, v in params.items():\n            if v is None:\n                params[k] = 'None'\n        # Log parameters\n        mlflow.log_params(params)\n\n    def set_tag(self, key: str, value) -&gt; None:\n        '''Logs a tag in mlflow\n\n        Args:\n            key (str): Name of the tag\n            value (str, ?): Value of the tag (which will be cast to str if not already of type str)\n        Raises:\n            ValueError: If the object value is None\n        '''\n        if value is None:\n            raise ValueError('value must not be None')\n        # Log tag\n        mlflow.set_tag(key, value)\n\n    def set_tags(self, tags: dict) -&gt; None:\n        '''Logs a set of tags in mlflow\n\n        Args:\n            tags (dict): Name and value of each tag\n        '''\n        # Log tags\n        mlflow.set_tags(tags)\n\n    def valid_name(self, key: str) -&gt; bool:\n        '''Validates key names\n\n        Args:\n            key (str): Key to check\n        Returns:\n            bool: If key is a valid mlflow key\n        '''\n        if mlflow.utils.validation._VALID_PARAM_AND_METRIC_NAMES.match(key):\n            return True\n        else:\n            return False\n\n    def log_df_stats(self, df_stats: pd.DataFrame, label_col: str = 'Label') -&gt; None:\n        '''Log a dataframe containing metrics from a training\n\n        Args:\n            df_stats (pd.Dataframe): Dataframe containing metrics from a training\n        Kwargs:\n            label_col (str): default labelc column name\n        '''\n        if label_col not in df_stats.columns:\n            raise ValueError(f\"The provided label column name ({label_col}) not found in df_stats' columns.\")\n\n        # Get metrics columns\n        metrics_columns = [col for col in df_stats.columns if col != label_col]\n\n        # Log labels\n        labels = df_stats[label_col].values\n        for i, label in enumerate(labels):  # type: ignore\n            self.log_param(f'Label {i}', label)\n\n        # Log metrics\n        ml_flow_metrics = {}\n        for i, row in df_stats.iterrows():\n            for j, col in enumerate(metrics_columns):\n                metric_key = f\"{row[label_col]} --- {col}\"\n                # Check that mlflow accepts the key, otherwise, replace it\n                # TODO: could be improved ...\n                if not self.valid_name(metric_key):\n                    metric_key = f\"Label {i} --- {col}\"\n                if not self.valid_name(metric_key):\n                    metric_key = f\"{row[label_col]} --- Col {j}\"\n                if not self.valid_name(metric_key):\n                    metric_key = f\"Label {i} --- Col {j}\"\n                ml_flow_metrics[metric_key] = row[col]\n\n        # Log metrics\n        self.log_metrics(ml_flow_metrics)\n\n    def log_dict(self, dictionary: dict, artifact_file: str) -&gt; None:\n        '''Logs a dictionary as an artifact in MLflow\n\n        Args:\n            dictionary (dict): A dictionary\n            artifact_file (str): The run-relative artifact file path in posixpath format to which the dictionary is saved\n        '''\n        mlflow.log_dict(dictionary=dictionary, artifact_file=artifact_file)\n\n    def log_text(self, text: str, artifact_file: str) -&gt; None:\n        '''Logs a text as an artifact in MLflow\n\n        Args:\n            text (str): A text\n            artifact_file (str): The run-relative artifact file path in posixpath format to which the dictionary is saved\n        '''\n        mlflow.log_text(text=text, artifact_file=artifact_file)\n\n    def log_figure(self, figure: Figure, artifact_file: str) -&gt; None:\n        '''Logs a text as an artifact in MLflow\n\n        Args:\n            figure (matplotlib.figure.Figure): A matplotlib figure\n            artifact_file (str): The run-relative artifact file path in posixpath format to which the figure is saved\n        '''\n        mlflow.log_figure(figure=figure, artifact_file=artifact_file)\n</code></pre>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.artifact_uri","title":"<code>artifact_uri: str</code>  <code>property</code>","text":"<p>Experiment artifact URI. It can not be changed.</p>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.experiment_id","title":"<code>experiment_id: str</code>  <code>property</code>","text":"<p>Experiment id. It can not be changed.</p>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.experiment_name","title":"<code>experiment_name: str</code>  <code>property</code>","text":"<p>Experiment name. It can not be changed.</p>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.tracking_uri","title":"<code>tracking_uri: str</code>  <code>property</code> <code>writable</code>","text":"<p>Current tracking uri</p>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.__init__","title":"<code>__init__(experiment_name, tracking_uri='', artifact_uri='')</code>","text":"<p>Class initialization Args:     experiment_name (str):  Name of the experiment to activate Kwargs:     tracking_uri (str): URI of the tracking server     artifact_uri (str): URI where to store artifacts</p> Source code in <code>template_nlp/monitoring/mlflow_logger.py</code> <pre><code>def __init__(self, experiment_name: str, tracking_uri: str = '', artifact_uri: str = '') -&gt; None:\n    '''Class initialization\n    Args:\n        experiment_name (str):  Name of the experiment to activate\n    Kwargs:\n        tracking_uri (str): URI of the tracking server\n        artifact_uri (str): URI where to store artifacts\n    '''\n    # Get logger\n    self.logger = logging.getLogger(__name__)\n\n    # Backup to local save if no uri (i.e. empty string)\n    if not tracking_uri:\n        tracking_uri = pathlib.Path(os.path.join(utils.get_data_path(), 'experiments', 'mlruns')).as_uri()\n    # Add \"file\" scheme if no scheme in the tracking_uri\n    elif not urlparse(tracking_uri).scheme:\n        tracking_uri = pathlib.Path(tracking_uri).resolve().as_uri()\n\n    # If no artifact_uri and tracking_uri scheme is \"file\", we set a default artifact_uri in experiments folder\n    # Otherwise we suppose artifact_uri is configured by the system\n    if not artifact_uri and urlparse(tracking_uri).scheme == \"file\":\n        artifact_uri = pathlib.Path(os.path.join(utils.get_data_path(), 'experiments', 'mlruns_artifacts')).as_uri()\n\n    # Set tracking URI &amp; experiment name\n    self.tracking_uri = tracking_uri\n\n    # Get the experiment if it exists and check if there is a connection error by doing it\n    try:\n        experiment = mlflow.get_experiment_by_name(experiment_name)\n    except Exception as e:\n        self.logger.error(repr(e))\n        raise ConnectionError(f\"Can't reach MLflow at {self.tracking_uri}. Please check the URI.\")\n\n    # If the experiment exists, we recover experiment id and artifact_uri (which is link to the experiment)\n    if experiment:\n        experiment_id = experiment.experiment_id\n        artifact_uri = experiment.artifact_location\n    # Otherwise we create a new experiment with the provided artifact_uri\n    else:\n        experiment_id = mlflow.create_experiment(experiment_name, artifact_location=artifact_uri)\n        experiment = mlflow.get_experiment_by_name(experiment_name)\n        artifact_uri = experiment.artifact_location\n\n    mlflow.set_experiment(experiment_id=experiment_id)\n\n    self.__experiment_id = experiment_id\n    self.__experiment_name = experiment_name\n    self.__artifact_uri = artifact_uri\n\n    self.logger.info(f'MLflow running. Metrics available @ {self.tracking_uri}. Experiment artifacts availaible @ {self.artifact_uri}')\n</code></pre>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.end_run","title":"<code>end_run()</code>","text":"<p>Stops an MLflow run</p> Source code in <code>template_nlp/monitoring/mlflow_logger.py</code> <pre><code>def end_run(self) -&gt; None:\n    '''Stops an MLflow run'''\n    try:\n        mlflow.end_run()\n    except Exception:\n        self.logger.error(\"Can't stop mlflow run\")\n</code></pre>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.log_df_stats","title":"<code>log_df_stats(df_stats, label_col='Label')</code>","text":"<p>Log a dataframe containing metrics from a training</p> <p>Parameters:</p> Name Type Description Default <code>df_stats</code> <code>Dataframe</code> <p>Dataframe containing metrics from a training</p> required <p>Kwargs:     label_col (str): default labelc column name</p> Source code in <code>template_nlp/monitoring/mlflow_logger.py</code> <pre><code>def log_df_stats(self, df_stats: pd.DataFrame, label_col: str = 'Label') -&gt; None:\n    '''Log a dataframe containing metrics from a training\n\n    Args:\n        df_stats (pd.Dataframe): Dataframe containing metrics from a training\n    Kwargs:\n        label_col (str): default labelc column name\n    '''\n    if label_col not in df_stats.columns:\n        raise ValueError(f\"The provided label column name ({label_col}) not found in df_stats' columns.\")\n\n    # Get metrics columns\n    metrics_columns = [col for col in df_stats.columns if col != label_col]\n\n    # Log labels\n    labels = df_stats[label_col].values\n    for i, label in enumerate(labels):  # type: ignore\n        self.log_param(f'Label {i}', label)\n\n    # Log metrics\n    ml_flow_metrics = {}\n    for i, row in df_stats.iterrows():\n        for j, col in enumerate(metrics_columns):\n            metric_key = f\"{row[label_col]} --- {col}\"\n            # Check that mlflow accepts the key, otherwise, replace it\n            # TODO: could be improved ...\n            if not self.valid_name(metric_key):\n                metric_key = f\"Label {i} --- {col}\"\n            if not self.valid_name(metric_key):\n                metric_key = f\"{row[label_col]} --- Col {j}\"\n            if not self.valid_name(metric_key):\n                metric_key = f\"Label {i} --- Col {j}\"\n            ml_flow_metrics[metric_key] = row[col]\n\n    # Log metrics\n    self.log_metrics(ml_flow_metrics)\n</code></pre>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.log_dict","title":"<code>log_dict(dictionary, artifact_file)</code>","text":"<p>Logs a dictionary as an artifact in MLflow</p> <p>Parameters:</p> Name Type Description Default <code>dictionary</code> <code>dict</code> <p>A dictionary</p> required <code>artifact_file</code> <code>str</code> <p>The run-relative artifact file path in posixpath format to which the dictionary is saved</p> required Source code in <code>template_nlp/monitoring/mlflow_logger.py</code> <pre><code>def log_dict(self, dictionary: dict, artifact_file: str) -&gt; None:\n    '''Logs a dictionary as an artifact in MLflow\n\n    Args:\n        dictionary (dict): A dictionary\n        artifact_file (str): The run-relative artifact file path in posixpath format to which the dictionary is saved\n    '''\n    mlflow.log_dict(dictionary=dictionary, artifact_file=artifact_file)\n</code></pre>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.log_figure","title":"<code>log_figure(figure, artifact_file)</code>","text":"<p>Logs a text as an artifact in MLflow</p> <p>Parameters:</p> Name Type Description Default <code>figure</code> <code>Figure</code> <p>A matplotlib figure</p> required <code>artifact_file</code> <code>str</code> <p>The run-relative artifact file path in posixpath format to which the figure is saved</p> required Source code in <code>template_nlp/monitoring/mlflow_logger.py</code> <pre><code>def log_figure(self, figure: Figure, artifact_file: str) -&gt; None:\n    '''Logs a text as an artifact in MLflow\n\n    Args:\n        figure (matplotlib.figure.Figure): A matplotlib figure\n        artifact_file (str): The run-relative artifact file path in posixpath format to which the figure is saved\n    '''\n    mlflow.log_figure(figure=figure, artifact_file=artifact_file)\n</code></pre>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.log_metric","title":"<code>log_metric(key, value, step=None)</code>","text":"<p>Logs a metric on mlflow</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of the metric</p> required <code>value</code> <code>float, ?</code> <p>Value of the metric</p> required <p>Kwargs:     step (int): Step of the metric</p> Source code in <code>template_nlp/monitoring/mlflow_logger.py</code> <pre><code>def log_metric(self, key: str, value, step: Union[int, None] = None) -&gt; None:\n    '''Logs a metric on mlflow\n\n    Args:\n        key (str): Name of the metric\n        value (float, ?): Value of the metric\n    Kwargs:\n        step (int): Step of the metric\n    '''\n    # Check for None\n    if value is None:\n        value = math.nan\n    # Log metric\n    mlflow.log_metric(key, value, step)\n</code></pre>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.log_metrics","title":"<code>log_metrics(metrics, step=None)</code>","text":"<p>Logs a set of metrics in mlflow</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>dict</code> <p>Metrics to log</p> required <p>Kwargs:     step (int): Step of the metric</p> Source code in <code>template_nlp/monitoring/mlflow_logger.py</code> <pre><code>def log_metrics(self, metrics: dict, step: Union[int, None] = None) -&gt; None:\n    '''Logs a set of metrics in mlflow\n\n    Args:\n        metrics (dict): Metrics to log\n    Kwargs:\n        step (int): Step of the metric\n    '''\n    # Check for Nones\n    for k, v in metrics.items():\n        if v is None:\n            metrics[k] = math.nan\n    # Log metrics\n    mlflow.log_metrics(metrics, step)\n</code></pre>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.log_param","title":"<code>log_param(key, value)</code>","text":"<p>Logs a parameter in mlflow</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of the parameter</p> required <code>value</code> <code>str, ?</code> <p>Value of the parameter (which will be cast to str if not already of type str)</p> required Source code in <code>template_nlp/monitoring/mlflow_logger.py</code> <pre><code>def log_param(self, key: str, value) -&gt; None:\n    '''Logs a parameter in mlflow\n\n    Args:\n        key (str): Name of the parameter\n        value (str, ?): Value of the parameter (which will be cast to str if not already of type str)\n    '''\n    if value is None:\n        value = 'None'\n    # Log parameter\n    mlflow.log_param(key, value)\n</code></pre>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.log_params","title":"<code>log_params(params)</code>","text":"<p>Logs a set of parameters in mlflow</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Name and value of each parameter</p> required Source code in <code>template_nlp/monitoring/mlflow_logger.py</code> <pre><code>def log_params(self, params: dict) -&gt; None:\n    '''Logs a set of parameters in mlflow\n\n    Args:\n        params (dict): Name and value of each parameter\n    '''\n    # Check for Nones\n    for k, v in params.items():\n        if v is None:\n            params[k] = 'None'\n    # Log parameters\n    mlflow.log_params(params)\n</code></pre>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.log_text","title":"<code>log_text(text, artifact_file)</code>","text":"<p>Logs a text as an artifact in MLflow</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>A text</p> required <code>artifact_file</code> <code>str</code> <p>The run-relative artifact file path in posixpath format to which the dictionary is saved</p> required Source code in <code>template_nlp/monitoring/mlflow_logger.py</code> <pre><code>def log_text(self, text: str, artifact_file: str) -&gt; None:\n    '''Logs a text as an artifact in MLflow\n\n    Args:\n        text (str): A text\n        artifact_file (str): The run-relative artifact file path in posixpath format to which the dictionary is saved\n    '''\n    mlflow.log_text(text=text, artifact_file=artifact_file)\n</code></pre>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.set_tag","title":"<code>set_tag(key, value)</code>","text":"<p>Logs a tag in mlflow</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of the tag</p> required <code>value</code> <code>str, ?</code> <p>Value of the tag (which will be cast to str if not already of type str)</p> required <p>Raises:     ValueError: If the object value is None</p> Source code in <code>template_nlp/monitoring/mlflow_logger.py</code> <pre><code>def set_tag(self, key: str, value) -&gt; None:\n    '''Logs a tag in mlflow\n\n    Args:\n        key (str): Name of the tag\n        value (str, ?): Value of the tag (which will be cast to str if not already of type str)\n    Raises:\n        ValueError: If the object value is None\n    '''\n    if value is None:\n        raise ValueError('value must not be None')\n    # Log tag\n    mlflow.set_tag(key, value)\n</code></pre>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.set_tags","title":"<code>set_tags(tags)</code>","text":"<p>Logs a set of tags in mlflow</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>dict</code> <p>Name and value of each tag</p> required Source code in <code>template_nlp/monitoring/mlflow_logger.py</code> <pre><code>def set_tags(self, tags: dict) -&gt; None:\n    '''Logs a set of tags in mlflow\n\n    Args:\n        tags (dict): Name and value of each tag\n    '''\n    # Log tags\n    mlflow.set_tags(tags)\n</code></pre>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.valid_name","title":"<code>valid_name(key)</code>","text":"<p>Validates key names</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Key to check</p> required <p>Returns:     bool: If key is a valid mlflow key</p> Source code in <code>template_nlp/monitoring/mlflow_logger.py</code> <pre><code>def valid_name(self, key: str) -&gt; bool:\n    '''Validates key names\n\n    Args:\n        key (str): Key to check\n    Returns:\n        bool: If key is a valid mlflow key\n    '''\n    if mlflow.utils.validation._VALID_PARAM_AND_METRIC_NAMES.match(key):\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"reference/template_nlp/monitoring/model_explainer/","title":"Model explainer","text":""},{"location":"reference/template_nlp/monitoring/model_explainer/#template_nlp.monitoring.model_explainer.Explainer","title":"<code>Explainer</code>","text":"<p>Parent class for the explainers</p> Source code in <code>template_nlp/monitoring/model_explainer.py</code> <pre><code>class Explainer:\n    '''Parent class for the explainers'''\n\n    def __init__(self, *args, **kwargs) -&gt; None:\n        '''Initialization of the parent class'''\n        self.logger = logging.getLogger(__name__)\n\n    def explain_instance(self, content: str, **kwargs) -&gt; Any:\n        '''Explains a prediction\n\n        Args:\n            content (str): Text to be explained\n        Returns:\n            (?): An explanation object\n        '''\n        raise NotImplementedError(\"'explain_instance' needs to be overridden\")\n\n    def explain_instance_as_html(self, content: str, **kwargs) -&gt; str:\n        '''Explains a prediction - returns an HTML object\n\n        Args:\n            content (str): Text to be explained\n        Returns:\n            str: An HTML code with the explanation\n        '''\n        raise NotImplementedError(\"'explain_instance_as_html' needs to be overridden\")\n\n    def explain_instance_as_json(self, content: str, **kwargs) -&gt; Union[dict, list]:\n        '''Explains a prediction - returns an JSON serializable object\n\n        Args:\n            content (str): Text to be explained\n        Returns:\n            str: A JSON serializable object with the explanation\n        '''\n        raise NotImplementedError(\"'explain_instance_as_json' needs to be overridden\")\n\n    def explain_instance_as_list(self, content: str, **kwargs) -&gt; list:\n        '''Explains a prediction - returns a list object\n\n        Args:\n            content (str): Text to be explained\n        Returns:\n            list: List of tuples with words and corresponding weights\n        '''\n        raise NotImplementedError(\"'explain_instance_as_list' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_nlp/monitoring/model_explainer/#template_nlp.monitoring.model_explainer.Explainer.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialization of the parent class</p> Source code in <code>template_nlp/monitoring/model_explainer.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n    '''Initialization of the parent class'''\n    self.logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/template_nlp/monitoring/model_explainer/#template_nlp.monitoring.model_explainer.Explainer.explain_instance","title":"<code>explain_instance(content, **kwargs)</code>","text":"<p>Explains a prediction</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Text to be explained</p> required <p>Returns:     (?): An explanation object</p> Source code in <code>template_nlp/monitoring/model_explainer.py</code> <pre><code>def explain_instance(self, content: str, **kwargs) -&gt; Any:\n    '''Explains a prediction\n\n    Args:\n        content (str): Text to be explained\n    Returns:\n        (?): An explanation object\n    '''\n    raise NotImplementedError(\"'explain_instance' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_nlp/monitoring/model_explainer/#template_nlp.monitoring.model_explainer.Explainer.explain_instance_as_html","title":"<code>explain_instance_as_html(content, **kwargs)</code>","text":"<p>Explains a prediction - returns an HTML object</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Text to be explained</p> required <p>Returns:     str: An HTML code with the explanation</p> Source code in <code>template_nlp/monitoring/model_explainer.py</code> <pre><code>def explain_instance_as_html(self, content: str, **kwargs) -&gt; str:\n    '''Explains a prediction - returns an HTML object\n\n    Args:\n        content (str): Text to be explained\n    Returns:\n        str: An HTML code with the explanation\n    '''\n    raise NotImplementedError(\"'explain_instance_as_html' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_nlp/monitoring/model_explainer/#template_nlp.monitoring.model_explainer.Explainer.explain_instance_as_json","title":"<code>explain_instance_as_json(content, **kwargs)</code>","text":"<p>Explains a prediction - returns an JSON serializable object</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Text to be explained</p> required <p>Returns:     str: A JSON serializable object with the explanation</p> Source code in <code>template_nlp/monitoring/model_explainer.py</code> <pre><code>def explain_instance_as_json(self, content: str, **kwargs) -&gt; Union[dict, list]:\n    '''Explains a prediction - returns an JSON serializable object\n\n    Args:\n        content (str): Text to be explained\n    Returns:\n        str: A JSON serializable object with the explanation\n    '''\n    raise NotImplementedError(\"'explain_instance_as_json' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_nlp/monitoring/model_explainer/#template_nlp.monitoring.model_explainer.Explainer.explain_instance_as_list","title":"<code>explain_instance_as_list(content, **kwargs)</code>","text":"<p>Explains a prediction - returns a list object</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Text to be explained</p> required <p>Returns:     list: List of tuples with words and corresponding weights</p> Source code in <code>template_nlp/monitoring/model_explainer.py</code> <pre><code>def explain_instance_as_list(self, content: str, **kwargs) -&gt; list:\n    '''Explains a prediction - returns a list object\n\n    Args:\n        content (str): Text to be explained\n    Returns:\n        list: List of tuples with words and corresponding weights\n    '''\n    raise NotImplementedError(\"'explain_instance_as_list' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_nlp/monitoring/model_explainer/#template_nlp.monitoring.model_explainer.LimeExplainer","title":"<code>LimeExplainer</code>","text":"<p>             Bases: <code>Explainer</code></p> <p>Lime Explainer wrapper class</p> Source code in <code>template_nlp/monitoring/model_explainer.py</code> <pre><code>class LimeExplainer(Explainer):\n    '''Lime Explainer wrapper class'''\n\n    def __init__(self, model: Type[ModelClass], model_conf: dict) -&gt; None:\n        ''' Initialization\n\n        Args:\n            model: A model instance with predict &amp; predict_proba functions, and list_classes attribute\n            model_conf (dict): The model's configuration\n        Raises:\n            TypeError: If the provided model does not implement a `predict_proba` function\n            TypeError: If the provided model does not have a `list_classes` attribute\n        '''\n        super().__init__()\n        pred_proba_op = getattr(model, \"predict_proba\", None)\n\n        if pred_proba_op is None or not callable(pred_proba_op):\n            raise TypeError(\"The supplied model must implement a predict_proba() function\")\n        if getattr(model, \"list_classes\", None) is None:\n            raise TypeError(\"The supplied model must have a list_classes attribute\")\n\n        self.model = model\n        self.model_conf = model_conf\n        self.class_names = self.model.list_classes\n\n        # Create the explainer\n        self.explainer = LimeTextExplainer(class_names=self.class_names)\n\n    def classifier_fn(self, content_list: list) -&gt; np.ndarray:\n        '''Function to get probabilities from a list of (not preprocessed) texts\n\n        Args:\n            content_list (list): texts to be considered\n        Returns:\n            np.array: probabilities\n        '''\n        # Get preprocessor\n        if 'preprocess_str' in self.model_conf.keys():\n            preprocess_str = self.model_conf['preprocess_str']\n        else:\n            preprocess_str = 'no_preprocess'\n        preprocessor = preprocess.get_preprocessor(preprocess_str)\n        # Preprocess\n        content_prep = preprocessor(content_list)\n        # Get probabilities\n        return self.model.predict_proba(content_prep)\n\n    def explain_instance(self, content: str, class_or_label_index: Union[None, int, Iterable] = None,\n                         max_features: int = 15, **kwargs) -&gt; Explanation:\n        '''Explains a prediction\n\n        This function calls the Lime module. It creates a linear model around the input text to evaluate\n        the weight of each word in the final prediction.\n\n        Args:\n            content (str): Text to be explained\n        Kwargs:\n            class_or_label_index (Union[None, int, Iterable]): for classification only. Class or label index to be considered.\n            max_features (int): Maximum number of features (cf. Lime documentation)\n        Returns:\n            (?): An explanation object\n        '''\n        # If a numpy object is passed as class_or_label_index we convert it to an object of a builtin type\n        if is_ndarray_convertable(class_or_label_index):\n            class_or_label_index = ndarray_to_builtin_object(class_or_label_index)\n\n        if class_or_label_index is None:\n            probas = self.classifier_fn([content])[0]\n            class_or_label_index = (i for i, p in enumerate(probas) if p &gt;= 0.5)\n\n        elif isinstance(class_or_label_index, int):\n            class_or_label_index = (class_or_label_index, )\n\n        # Get explanations\n        return self.explainer.explain_instance(content, self.classifier_fn, labels=class_or_label_index, num_features=max_features)\n\n    def explain_instance_as_html(self, content: str, class_or_label_index: Union[int, None] = None,\n                                 max_features: int = 15, **kwargs) -&gt; str:\n        '''Explains a prediction - returns an HTML object\n\n        Args:\n            content (str): Text to be explained\n        Kwargs:\n            class_or_label_index (int): for classification only. Class or label index to be considered.\n            max_features (int): Maximum number of features (cf. Lime documentation)\n        Returns:\n            str: An HTML code with the explanation\n        '''\n        return self.explain_instance(content, class_or_label_index=class_or_label_index, max_features=max_features, **kwargs).as_html()\n\n    def explain_instance_as_list(self, content: str, class_or_label_index: Union[int, None] = None,\n                                 max_features: int = 15, **kwargs) -&gt; list:\n        '''Explains a prediction - returns a list object\n\n        Args:\n            content (str): Text to be explained\n        Kwargs:\n            class_or_label_index (int): for classification only. Class or label index to be considered.\n            max_features (int): Maximum number of features (cf. Lime documentation)\n        Returns:\n            list: List of tuples with words and corresponding weights\n        '''\n        explanation = self.explain_instance(content, class_or_label_index=class_or_label_index, max_features=max_features, **kwargs)\n\n        # as_list can only return lime weight for a single label so in case no class_or_label_index\n        # was specified we return only the weight for the class with the max probabilty\n        if class_or_label_index is None:\n            class_or_label_index = explanation.predict_proba.argmax()\n\n        # Return as list for selected class or label\n        return explanation.as_list(label=class_or_label_index)\n\n    def explain_instance_as_json(self, content: str, class_or_label_index: Union[None, int, Iterable] = None,\n                                 max_features: int = 15, **kwargs) -&gt; Union[dict, list]:\n        '''Explains a prediction - returns a JSON serializable object\n\n        Args:\n            content (str): Text to be explained\n        Kwargs:\n            class_or_label_index (Union[None, int, Iterable]): for classification only. Class or label index to be considered.\n            max_features (int): Maximum number of features (cf. Lime documentation)\n        Returns:\n            Union[list, dict]: JSON serializable object containing a list of tuples with words and corresponding weights\n        '''\n        explanation = self.explain_instance(content, class_or_label_index=class_or_label_index, max_features=max_features, **kwargs)\n\n        return explanation.as_map()\n</code></pre>"},{"location":"reference/template_nlp/monitoring/model_explainer/#template_nlp.monitoring.model_explainer.LimeExplainer.__init__","title":"<code>__init__(model, model_conf)</code>","text":"<p>Initialization</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Type[ModelClass]</code> <p>A model instance with predict &amp; predict_proba functions, and list_classes attribute</p> required <code>model_conf</code> <code>dict</code> <p>The model's configuration</p> required <p>Raises:     TypeError: If the provided model does not implement a <code>predict_proba</code> function     TypeError: If the provided model does not have a <code>list_classes</code> attribute</p> Source code in <code>template_nlp/monitoring/model_explainer.py</code> <pre><code>def __init__(self, model: Type[ModelClass], model_conf: dict) -&gt; None:\n    ''' Initialization\n\n    Args:\n        model: A model instance with predict &amp; predict_proba functions, and list_classes attribute\n        model_conf (dict): The model's configuration\n    Raises:\n        TypeError: If the provided model does not implement a `predict_proba` function\n        TypeError: If the provided model does not have a `list_classes` attribute\n    '''\n    super().__init__()\n    pred_proba_op = getattr(model, \"predict_proba\", None)\n\n    if pred_proba_op is None or not callable(pred_proba_op):\n        raise TypeError(\"The supplied model must implement a predict_proba() function\")\n    if getattr(model, \"list_classes\", None) is None:\n        raise TypeError(\"The supplied model must have a list_classes attribute\")\n\n    self.model = model\n    self.model_conf = model_conf\n    self.class_names = self.model.list_classes\n\n    # Create the explainer\n    self.explainer = LimeTextExplainer(class_names=self.class_names)\n</code></pre>"},{"location":"reference/template_nlp/monitoring/model_explainer/#template_nlp.monitoring.model_explainer.LimeExplainer.classifier_fn","title":"<code>classifier_fn(content_list)</code>","text":"<p>Function to get probabilities from a list of (not preprocessed) texts</p> <p>Parameters:</p> Name Type Description Default <code>content_list</code> <code>list</code> <p>texts to be considered</p> required <p>Returns:     np.array: probabilities</p> Source code in <code>template_nlp/monitoring/model_explainer.py</code> <pre><code>def classifier_fn(self, content_list: list) -&gt; np.ndarray:\n    '''Function to get probabilities from a list of (not preprocessed) texts\n\n    Args:\n        content_list (list): texts to be considered\n    Returns:\n        np.array: probabilities\n    '''\n    # Get preprocessor\n    if 'preprocess_str' in self.model_conf.keys():\n        preprocess_str = self.model_conf['preprocess_str']\n    else:\n        preprocess_str = 'no_preprocess'\n    preprocessor = preprocess.get_preprocessor(preprocess_str)\n    # Preprocess\n    content_prep = preprocessor(content_list)\n    # Get probabilities\n    return self.model.predict_proba(content_prep)\n</code></pre>"},{"location":"reference/template_nlp/monitoring/model_explainer/#template_nlp.monitoring.model_explainer.LimeExplainer.explain_instance","title":"<code>explain_instance(content, class_or_label_index=None, max_features=15, **kwargs)</code>","text":"<p>Explains a prediction</p> <p>This function calls the Lime module. It creates a linear model around the input text to evaluate the weight of each word in the final prediction.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Text to be explained</p> required <p>Kwargs:     class_or_label_index (Union[None, int, Iterable]): for classification only. Class or label index to be considered.     max_features (int): Maximum number of features (cf. Lime documentation) Returns:     (?): An explanation object</p> Source code in <code>template_nlp/monitoring/model_explainer.py</code> <pre><code>def explain_instance(self, content: str, class_or_label_index: Union[None, int, Iterable] = None,\n                     max_features: int = 15, **kwargs) -&gt; Explanation:\n    '''Explains a prediction\n\n    This function calls the Lime module. It creates a linear model around the input text to evaluate\n    the weight of each word in the final prediction.\n\n    Args:\n        content (str): Text to be explained\n    Kwargs:\n        class_or_label_index (Union[None, int, Iterable]): for classification only. Class or label index to be considered.\n        max_features (int): Maximum number of features (cf. Lime documentation)\n    Returns:\n        (?): An explanation object\n    '''\n    # If a numpy object is passed as class_or_label_index we convert it to an object of a builtin type\n    if is_ndarray_convertable(class_or_label_index):\n        class_or_label_index = ndarray_to_builtin_object(class_or_label_index)\n\n    if class_or_label_index is None:\n        probas = self.classifier_fn([content])[0]\n        class_or_label_index = (i for i, p in enumerate(probas) if p &gt;= 0.5)\n\n    elif isinstance(class_or_label_index, int):\n        class_or_label_index = (class_or_label_index, )\n\n    # Get explanations\n    return self.explainer.explain_instance(content, self.classifier_fn, labels=class_or_label_index, num_features=max_features)\n</code></pre>"},{"location":"reference/template_nlp/monitoring/model_explainer/#template_nlp.monitoring.model_explainer.LimeExplainer.explain_instance_as_html","title":"<code>explain_instance_as_html(content, class_or_label_index=None, max_features=15, **kwargs)</code>","text":"<p>Explains a prediction - returns an HTML object</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Text to be explained</p> required <p>Kwargs:     class_or_label_index (int): for classification only. Class or label index to be considered.     max_features (int): Maximum number of features (cf. Lime documentation) Returns:     str: An HTML code with the explanation</p> Source code in <code>template_nlp/monitoring/model_explainer.py</code> <pre><code>def explain_instance_as_html(self, content: str, class_or_label_index: Union[int, None] = None,\n                             max_features: int = 15, **kwargs) -&gt; str:\n    '''Explains a prediction - returns an HTML object\n\n    Args:\n        content (str): Text to be explained\n    Kwargs:\n        class_or_label_index (int): for classification only. Class or label index to be considered.\n        max_features (int): Maximum number of features (cf. Lime documentation)\n    Returns:\n        str: An HTML code with the explanation\n    '''\n    return self.explain_instance(content, class_or_label_index=class_or_label_index, max_features=max_features, **kwargs).as_html()\n</code></pre>"},{"location":"reference/template_nlp/monitoring/model_explainer/#template_nlp.monitoring.model_explainer.LimeExplainer.explain_instance_as_json","title":"<code>explain_instance_as_json(content, class_or_label_index=None, max_features=15, **kwargs)</code>","text":"<p>Explains a prediction - returns a JSON serializable object</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Text to be explained</p> required <p>Kwargs:     class_or_label_index (Union[None, int, Iterable]): for classification only. Class or label index to be considered.     max_features (int): Maximum number of features (cf. Lime documentation) Returns:     Union[list, dict]: JSON serializable object containing a list of tuples with words and corresponding weights</p> Source code in <code>template_nlp/monitoring/model_explainer.py</code> <pre><code>def explain_instance_as_json(self, content: str, class_or_label_index: Union[None, int, Iterable] = None,\n                             max_features: int = 15, **kwargs) -&gt; Union[dict, list]:\n    '''Explains a prediction - returns a JSON serializable object\n\n    Args:\n        content (str): Text to be explained\n    Kwargs:\n        class_or_label_index (Union[None, int, Iterable]): for classification only. Class or label index to be considered.\n        max_features (int): Maximum number of features (cf. Lime documentation)\n    Returns:\n        Union[list, dict]: JSON serializable object containing a list of tuples with words and corresponding weights\n    '''\n    explanation = self.explain_instance(content, class_or_label_index=class_or_label_index, max_features=max_features, **kwargs)\n\n    return explanation.as_map()\n</code></pre>"},{"location":"reference/template_nlp/monitoring/model_explainer/#template_nlp.monitoring.model_explainer.LimeExplainer.explain_instance_as_list","title":"<code>explain_instance_as_list(content, class_or_label_index=None, max_features=15, **kwargs)</code>","text":"<p>Explains a prediction - returns a list object</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Text to be explained</p> required <p>Kwargs:     class_or_label_index (int): for classification only. Class or label index to be considered.     max_features (int): Maximum number of features (cf. Lime documentation) Returns:     list: List of tuples with words and corresponding weights</p> Source code in <code>template_nlp/monitoring/model_explainer.py</code> <pre><code>def explain_instance_as_list(self, content: str, class_or_label_index: Union[int, None] = None,\n                             max_features: int = 15, **kwargs) -&gt; list:\n    '''Explains a prediction - returns a list object\n\n    Args:\n        content (str): Text to be explained\n    Kwargs:\n        class_or_label_index (int): for classification only. Class or label index to be considered.\n        max_features (int): Maximum number of features (cf. Lime documentation)\n    Returns:\n        list: List of tuples with words and corresponding weights\n    '''\n    explanation = self.explain_instance(content, class_or_label_index=class_or_label_index, max_features=max_features, **kwargs)\n\n    # as_list can only return lime weight for a single label so in case no class_or_label_index\n    # was specified we return only the weight for the class with the max probabilty\n    if class_or_label_index is None:\n        class_or_label_index = explanation.predict_proba.argmax()\n\n    # Return as list for selected class or label\n    return explanation.as_list(label=class_or_label_index)\n</code></pre>"},{"location":"reference/template_nlp/preprocessing/","title":"Preprocessing","text":""},{"location":"reference/template_nlp/preprocessing/preprocess/","title":"Preprocess","text":""},{"location":"reference/template_nlp/preprocessing/preprocess/#template_nlp.preprocessing.preprocess.get_preprocessor","title":"<code>get_preprocessor(preprocess_str)</code>","text":"<p>Gets a preprocessing (function) from its name</p> <p>Parameters:</p> Name Type Description Default <code>preprocess_str</code> <code>str</code> <p>Name of the preprocess</p> required <p>Raises:     ValueError: If the name of the preprocess is not known Returns:     Callable: Function to be used for the preprocessing</p> Source code in <code>template_nlp/preprocessing/preprocess.py</code> <pre><code>def get_preprocessor(preprocess_str: str) -&gt; Callable:\n    '''Gets a preprocessing (function) from its name\n\n    Args:\n        preprocess_str (str): Name of the preprocess\n    Raises:\n        ValueError: If the name of the preprocess is not known\n    Returns:\n        Callable: Function to be used for the preprocessing\n    '''\n    # Process\n    preprocessors_dict = get_preprocessors_dict()\n    if preprocess_str not in preprocessors_dict.keys():\n        raise ValueError(f\"The preprocess {preprocess_str} is not known.\")\n    # Get preprocessor\n    preprocessor = preprocessors_dict[preprocess_str]\n    # Return\n    return preprocessor\n</code></pre>"},{"location":"reference/template_nlp/preprocessing/preprocess/#template_nlp.preprocessing.preprocess.get_preprocessors_dict","title":"<code>get_preprocessors_dict()</code>","text":"<p>Gets a dictionary of available preprocessing</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary of preprocessing</p> Source code in <code>template_nlp/preprocessing/preprocess.py</code> <pre><code>def get_preprocessors_dict() -&gt; dict:\n    '''Gets a dictionary of available preprocessing\n\n    Returns:\n        dict: Dictionary of preprocessing\n    '''\n    preprocessors_dict = {\n        'no_preprocess': lambda x: x,  # - /!\\ DO NOT DELETE -&gt; necessary for compatibility /!\\ -\n        'preprocess_P1': preprocess_sentence_P1,  # Example of a preprocessing\n        #  'preprocess_P2': preprocess_sentence_P2 , ETC ...\n    }\n    return preprocessors_dict\n</code></pre>"},{"location":"reference/template_nlp/preprocessing/preprocess/#template_nlp.preprocessing.preprocess.preprocess_sentence_P1","title":"<code>preprocess_sentence_P1(docs)</code>","text":"<p>Applies \"default\" preprocess to a list of documents (text)</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Series</code> <p>Documents to be preprocessed</p> required <p>Returns:     pd.Series: Preprocessed documents</p> Source code in <code>template_nlp/preprocessing/preprocess.py</code> <pre><code>@wnf_utils.data_agnostic\n@wnf_utils.regroup_data_series\ndef preprocess_sentence_P1(docs: pd.Series) -&gt; pd.Series:\n    '''Applies \"default\" preprocess to a list of documents (text)\n\n    Args:\n        docs (pd.Series): Documents to be preprocessed\n    Returns:\n        pd.Series: Preprocessed documents\n    '''\n    pipeline = ['remove_non_string', 'get_true_spaces', 'remove_punct', 'to_lower', 'trim_string',\n                'remove_leading_and_ending_spaces']\n    return api.preprocess_pipeline(docs, pipeline=pipeline, chunksize=100000)\n</code></pre>"},{"location":"reference/template_num/","title":"Template num","text":""},{"location":"reference/template_num/utils/","title":"Utils","text":""},{"location":"reference/template_num/utils/#template_num.utils.NpEncoder","title":"<code>NpEncoder</code>","text":"<p>             Bases: <code>JSONEncoder</code></p> <p>JSON encoder to manage numpy objects</p> Source code in <code>template_num/utils.py</code> <pre><code>class NpEncoder(json.JSONEncoder):\n    '''JSON encoder to manage numpy objects'''\n    def default(self, obj) -&gt; Any:\n        if is_ndarray_convertable(obj):\n            return ndarray_to_builtin_object(obj)\n        elif isinstance(obj, set):\n            return list(obj)\n        else:\n            return super(NpEncoder, self).default(obj)\n</code></pre>"},{"location":"reference/template_num/utils/#template_num.utils.display_shape","title":"<code>display_shape(df)</code>","text":"<p>Displays the number of line and of column of a table.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Table to parse</p> required Source code in <code>template_num/utils.py</code> <pre><code>def display_shape(df: pd.DataFrame) -&gt; None:\n    '''Displays the number of line and of column of a table.\n\n    Args:\n        df (pd.DataFrame): Table to parse\n    '''\n    # Display\n    logger.info(f\"Number of lines : {df.shape[0]}. Number of columns : {df.shape[1]}.\")\n</code></pre>"},{"location":"reference/template_num/utils/#template_num.utils.find_folder_path","title":"<code>find_folder_path(folder_name, base_folder=None)</code>","text":"<p>Find a folder in a base folder and its subfolders. If base_folder is None, considers folder_name as a path and check it exists</p> <p>i.e., with the following structure : - C:/     - base_folder/         - folderA/             - folderB/         - folderC/ find_folder_path(folderA, C:/base_folder) == C:/base_folder/folderA find_folder_path(folderB, C:/base_folder) == C:/base_folder/folderA/folderB find_folder_path(C:/base_folder/folderC, None) == C:/base_folder/folderC find_folder_path(folderB, None) raises an error</p> <p>Parameters:</p> Name Type Description Default <code>folder_name</code> <code>str</code> <p>name of the folder to find. If base_folder is None, consider a path instead.</p> required <p>Kwargs:     base_folder (str): path of the base folder. If None, consider folder_name as a path. Raises:     FileNotFoundError: If we can't find folder_name in base_folder     FileNotFoundError: If folder_name is not a valid path (case where base_folder is None) Returns:     str: path to the wanted folder</p> Source code in <code>template_num/utils.py</code> <pre><code>def find_folder_path(folder_name: str, base_folder: Union[str, None] = None) -&gt; str:\n    '''Find a folder in a base folder and its subfolders.\n    If base_folder is None, considers folder_name as a path and check it exists\n\n    i.e., with the following structure :\n    - C:/\n        - base_folder/\n            - folderA/\n                - folderB/\n            - folderC/\n    find_folder_path(folderA, C:/base_folder) == C:/base_folder/folderA\n    find_folder_path(folderB, C:/base_folder) == C:/base_folder/folderA/folderB\n    find_folder_path(C:/base_folder/folderC, None) == C:/base_folder/folderC\n    find_folder_path(folderB, None) raises an error\n\n    Args:\n        folder_name (str): name of the folder to find. If base_folder is None, consider a path instead.\n    Kwargs:\n        base_folder (str): path of the base folder. If None, consider folder_name as a path.\n    Raises:\n        FileNotFoundError: If we can't find folder_name in base_folder\n        FileNotFoundError: If folder_name is not a valid path (case where base_folder is None)\n    Returns:\n        str: path to the wanted folder\n    '''\n    if base_folder is not None:\n        folder_path = None\n        for path, subdirs, files in os.walk(base_folder):\n            for name in subdirs:\n                if name == folder_name:\n                    folder_path = os.path.join(path, name)\n        if folder_path is None:\n            raise FileNotFoundError(f\"Can't find folder {folder_name} inside {base_folder} and its subfolders\")\n    else:\n        folder_path = folder_name\n        if not os.path.exists(folder_path):\n            raise FileNotFoundError(f\"Can't find folder {folder_path} (considered as a path)\")\n    return folder_path\n</code></pre>"},{"location":"reference/template_num/utils/#template_num.utils.flatten","title":"<code>flatten(my_list)</code>","text":"<p>Flattens a list of mixed elements (ie. some iterable, some not) e.g. [[1, 2], 3, [4]] -&gt; [1, 2, 3, 4] From : https://stackoverflow.com/questions/2158395/flatten-an-irregular-list-of-lists</p> <p>Parameters:</p> Name Type Description Default <code>my_list</code> <code>Iterable</code> <p>List to consider</p> required <p>Results:     generator: Flattened list (generator format)</p> Source code in <code>template_num/utils.py</code> <pre><code>def flatten(my_list: Iterable) -&gt; Generator:\n    '''Flattens a list of mixed elements (ie. some iterable, some not)\n    e.g. [[1, 2], 3, [4]] -&gt; [1, 2, 3, 4]\n    From : https://stackoverflow.com/questions/2158395/flatten-an-irregular-list-of-lists\n\n    Args:\n        my_list (Iterable): List to consider\n    Results:\n        generator: Flattened list (generator format)\n    '''\n    for el in my_list:\n        if isinstance(el, Iterable) and not isinstance(el, (str, bytes)):\n            yield from flatten(el)\n        else:\n            yield el\n</code></pre>"},{"location":"reference/template_num/utils/#template_num.utils.get_chunk_limits","title":"<code>get_chunk_limits(x, chunksize=10000)</code>","text":"<p>Gets chunk limits from a pandas series or dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series or DataFrame</code> <p>Documents to consider</p> required <p>Kwargs:     chunksize (int): The chunk size Raises:     ValueError: If the chunk size is negative Returns:     list: the chunk limits Source code in <code>template_num/utils.py</code> <pre><code>def get_chunk_limits(x: Union[pd.DataFrame, pd.Series], chunksize: int = 10000) -&gt; List[Tuple[int]]:\n    '''Gets chunk limits from a pandas series or dataframe.\n\n    Args:\n        x (pd.Series or pd.DataFrame): Documents to consider\n    Kwargs:\n        chunksize (int): The chunk size\n    Raises:\n        ValueError: If the chunk size is negative\n    Returns:\n        list&lt;tuple&gt;: the chunk limits\n    '''\n    if chunksize &lt; 0:\n        raise ValueError('The object chunksize must not be negative.')\n    # Processs\n    if chunksize == 0 or chunksize &gt;= x.shape[0]:\n        chunks_limits = [(0, x.shape[0])]\n    else:\n        chunks_limits = [(i * chunksize, min((i + 1) * chunksize, x.shape[0]))\n                         for i in range(1 + ((x.shape[0] - 1) // chunksize))]\n    return chunks_limits  # type: ignore\n</code></pre>"},{"location":"reference/template_num/utils/#template_num.utils.get_data_path","title":"<code>get_data_path()</code>","text":"<p>Returns the path to the data folder</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path of the data folder</p> Source code in <code>template_num/utils.py</code> <pre><code>def get_data_path() -&gt; str:\n    '''Returns the path to the data folder\n\n    Returns:\n        str: Path of the data folder\n    '''\n    if DIR_PATH is None:\n        dir_path = os.path.join(os.path.dirname(os.path.dirname(os.path.realpath(__file__))), 'template_num-data')\n    else:\n        dir_path = os.path.join(os.path.abspath(DIR_PATH), 'template_num-data')\n    if not os.path.isdir(dir_path):\n        os.mkdir(dir_path)\n    return os.path.abspath(dir_path)\n</code></pre>"},{"location":"reference/template_num/utils/#template_num.utils.get_models_path","title":"<code>get_models_path()</code>","text":"<p>Returns the path to the models folder</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path of the models folder</p> Source code in <code>template_num/utils.py</code> <pre><code>def get_models_path() -&gt; str:\n    '''Returns the path to the models folder\n\n    Returns:\n        str: Path of the models folder\n    '''\n    if DIR_PATH is None:\n        dir_path = os.path.join(os.path.dirname(os.path.dirname(os.path.realpath(__file__))), 'template_num-models')\n    else:\n        dir_path = os.path.join(os.path.abspath(DIR_PATH), 'template_num-models')\n    if not os.path.isdir(dir_path):\n        os.mkdir(dir_path)\n    return os.path.abspath(dir_path)\n</code></pre>"},{"location":"reference/template_num/utils/#template_num.utils.get_new_column_name","title":"<code>get_new_column_name(column_list, wanted_name)</code>","text":"<p>Gets a new column name from a list of existing ones &amp; a wanted name</p> <p>If the wanted name does not exists, return it. Otherwise get a new column prefixed by the wanted name.</p> <p>Parameters:</p> Name Type Description Default <code>column_list</code> <code>list</code> <p>List of existing columns</p> required <code>wanted_name</code> <code>str</code> <p>Wanted name</p> required Source code in <code>template_num/utils.py</code> <pre><code>def get_new_column_name(column_list: list, wanted_name: str) -&gt; str:\n    '''Gets a new column name from a list of existing ones &amp; a wanted name\n\n    If the wanted name does not exists, return it.\n    Otherwise get a new column prefixed by the wanted name.\n\n    Args:\n        column_list (list): List of existing columns\n        wanted_name (str): Wanted name\n    '''\n    if wanted_name not in column_list:\n        return wanted_name\n    else:\n        new_name = f'{wanted_name}_{str(uuid.uuid4())[:8]}'\n        # It should not happen, but we still check if new_name is available (bad luck ?)\n        return get_new_column_name(column_list, new_name)\n</code></pre>"},{"location":"reference/template_num/utils/#template_num.utils.get_package_version","title":"<code>get_package_version()</code>","text":"<p>Returns the current version of the package</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>version of the package</p> Source code in <code>template_num/utils.py</code> <pre><code>def get_package_version() -&gt; str:\n    '''Returns the current version of the package\n\n    Returns:\n        str: version of the package\n    '''\n    version = importlib.metadata.version('template_num')\n    return version\n</code></pre>"},{"location":"reference/template_num/utils/#template_num.utils.get_pipelines_path","title":"<code>get_pipelines_path()</code>","text":"<p>Returns the path to the pipelines folder</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path of the pipelines folder</p> Source code in <code>template_num/utils.py</code> <pre><code>def get_pipelines_path() -&gt; str:\n    '''Returns the path to the pipelines folder\n\n    Returns:\n        str: Path of the pipelines folder\n    '''\n    if DIR_PATH is None:\n        dir_path = os.path.join(os.path.dirname(os.path.dirname(os.path.realpath(__file__))), 'template_num-pipelines')\n    else:\n        dir_path = os.path.join(os.path.abspath(DIR_PATH), 'template_num-pipelines')\n    if not os.path.isdir(dir_path):\n        os.mkdir(dir_path)\n    return os.path.abspath(dir_path)\n</code></pre>"},{"location":"reference/template_num/utils/#template_num.utils.get_ressources_path","title":"<code>get_ressources_path()</code>","text":"<p>Returns the path to the ressources folder</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path of the ressources folder</p> Source code in <code>template_num/utils.py</code> <pre><code>def get_ressources_path() -&gt; str:\n    '''Returns the path to the ressources folder\n\n    Returns:\n        str: Path of the ressources folder\n    '''\n    dir_path = os.path.join(os.path.dirname(os.path.dirname(os.path.realpath(__file__))), 'template_num-ressources')\n    if not os.path.isdir(dir_path):\n        os.mkdir(dir_path)\n    return os.path.abspath(dir_path)\n</code></pre>"},{"location":"reference/template_num/utils/#template_num.utils.is_ndarray_convertable","title":"<code>is_ndarray_convertable(obj)</code>","text":"<p>Returns True if the object is covertable to a builtin type in the same way a np.ndarray is</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>an object to test</p> required <p>Returns:     bool: True if the object is covertable to a list as a np.ndarray is</p> Source code in <code>template_num/utils.py</code> <pre><code>def is_ndarray_convertable(obj: Any) -&gt; bool:\n    '''Returns True if the object is covertable to a builtin type in the same way a np.ndarray is\n\n    Args:\n        obj (Any): an object to test\n    Returns:\n        bool: True if the object is covertable to a list as a np.ndarray is\n    '''\n    return hasattr(obj, \"dtype\") and hasattr(obj, \"astype\") and hasattr(obj, \"tolist\")\n</code></pre>"},{"location":"reference/template_num/utils/#template_num.utils.ndarray_to_builtin_object","title":"<code>ndarray_to_builtin_object(obj)</code>","text":"<p>Transform a numpy.ndarray like object to a builtin type like int, float or list</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>An object</p> required <p>Raises:     ValueError: Raise a ValueError when obj is not ndarray convertable Returns:     Any: The object converted to a builtin type like int, float or list</p> Source code in <code>template_num/utils.py</code> <pre><code>def ndarray_to_builtin_object(obj: Any) -&gt; Any:\n    '''Transform a numpy.ndarray like object to a builtin type like int, float or list\n\n    Args:\n        obj (Any): An object\n    Raises:\n        ValueError: Raise a ValueError when obj is not ndarray convertable\n    Returns:\n        Any: The object converted to a builtin type like int, float or list\n    '''\n    if is_ndarray_convertable(obj):\n        if np.issubdtype(obj.dtype, np.integer):\n            return obj.astype(int).tolist()\n        elif np.issubdtype(obj.dtype, np.number):\n            return obj.astype(float).tolist()\n        else:\n            return obj.tolist()\n    else:\n        raise ValueError(f\"{obj} is not ndarray convertable\")\n</code></pre>"},{"location":"reference/template_num/utils/#template_num.utils.read_csv","title":"<code>read_csv(file_path, sep=';', encoding='utf-8', **kwargs)</code>","text":"<p>Reads a .csv file and parses the first line.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the .csv file containing the data</p> required <p>Kwargs:     sep (str): Separator of the data file     encoding (str): Encoding of the data file     kwargs: Pandas' kwargs Raises:     FileNotFoundError: If the file_path object does not point to an existing file Returns:     pd.DataFrame: Data     str: First line of the .csv (None if not beginning with #) and with no line break</p> Source code in <code>template_num/utils.py</code> <pre><code>def read_csv(file_path: str, sep: str = ';', encoding: str = 'utf-8', **kwargs) -&gt; Tuple[pd.DataFrame, Union[str, None]]:\n    '''Reads a .csv file and parses the first line.\n\n    Args:\n        file_path (str): Path to the .csv file containing the data\n    Kwargs:\n        sep (str): Separator of the data file\n        encoding (str): Encoding of the data file\n        kwargs: Pandas' kwargs\n    Raises:\n        FileNotFoundError: If the file_path object does not point to an existing file\n    Returns:\n        pd.DataFrame: Data\n        str: First line of the .csv (None if not beginning with #) and with no line break\n    '''\n    if not os.path.isfile(file_path):\n        raise FileNotFoundError(f\"The file {file_path} does not exist\")\n\n    # We get the first line\n    with open(file_path, 'r', encoding=encoding) as f:\n        first_line = f.readline()\n    # We check if the first line contains metadata\n    has_metada = True if first_line.startswith('#') else False\n    # We load the dataset\n    if has_metada:\n        df = pd.read_csv(file_path, sep=sep, encoding=encoding, skiprows=1, **kwargs)\n    else:\n        df = pd.read_csv(file_path, sep=sep, encoding=encoding, **kwargs)\n\n    # If no metadata, return only the dataframe\n    if not has_metada:\n        return df, None\n    # Else process the first_line\n    else:\n        # Deletion of the line break\n        if first_line is not None and first_line.endswith('\\n'):\n            first_line = first_line[:-1]\n        # Deletion of the return carriage\n        if first_line is not None and first_line.endswith('\\r'):\n            first_line = first_line[:-1]\n        # Return\n        return df, first_line\n</code></pre>"},{"location":"reference/template_num/utils/#template_num.utils.to_csv","title":"<code>to_csv(df, file_path, first_line=None, sep=';', encoding='utf-8', **kwargs)</code>","text":"<p>Writes a .csv and manages the first line.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Data to write</p> required <code>file_path</code> <code>str</code> <p>Path to the file to create</p> required <p>Kwargs:     first_line (str): First line to write (without line break which is done in this function)     sep (str): Separator for the data file     encoding (str): Encoding of the data file     kwargs: pandas' kwargs</p> Source code in <code>template_num/utils.py</code> <pre><code>def to_csv(df: pd.DataFrame, file_path: str, first_line: Union[str, None] = None, sep: str = ';',\n           encoding: str = 'utf-8', **kwargs) -&gt; None:\n    '''Writes a .csv and manages the first line.\n\n    Args:\n        df (pd.DataFrame): Data to write\n        file_path (str): Path to the file to create\n    Kwargs:\n        first_line (str): First line to write (without line break which is done in this function)\n        sep (str): Separator for the data file\n        encoding (str): Encoding of the data file\n        kwargs: pandas' kwargs\n    '''\n    # We get the first line\n    with open(file_path, 'w', encoding=encoding) as f:\n        if first_line is not None:\n            f.write(first_line + '\\n')  # We add the first line if metadata are present\n        df.to_csv(f, sep=sep, encoding=encoding, index=None, **kwargs)\n</code></pre>"},{"location":"reference/template_num/utils/#template_num.utils.trained_needed","title":"<code>trained_needed(function)</code>","text":"<p>Decorator to ensure that a model has been trained.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>func</code> <p>Function to decorate</p> required <p>Returns:     function: The decorated function</p> Source code in <code>template_num/utils.py</code> <pre><code>def trained_needed(function: Callable) -&gt; Callable:\n    '''Decorator to ensure that a model has been trained.\n\n    Args:\n        function (func): Function to decorate\n    Returns:\n        function: The decorated function\n    '''\n    # Get wrapper\n    def wrapper(self, *args, **kwargs):\n        '''Wrapper'''\n        if not self.trained:\n            raise AttributeError(f\"The function {function.__name__} can't be called as long as the model hasn't been fitted\")\n        else:\n            return function(self, *args, **kwargs)\n    return wrapper\n</code></pre>"},{"location":"reference/template_num/models_training/","title":"Models training","text":""},{"location":"reference/template_num/models_training/model_class/","title":"Model class","text":""},{"location":"reference/template_num/models_training/model_class/#template_num.models_training.model_class.ModelClass","title":"<code>ModelClass</code>","text":"<p>Parent class for the models</p> Source code in <code>template_num/models_training/model_class.py</code> <pre><code>class ModelClass:\n    '''Parent class for the models'''\n\n    _default_name = 'none'\n    # Variable annotation : https://www.python.org/dev/peps/pep-0526/\n    # Solves lots of typing errors, cf mypy\n    multi_label: Union[bool, None]\n    list_classes: Union[list, None]\n    dict_classes: Union[dict, None]\n\n    # Not implemented :\n    # -&gt; fit\n    # -&gt; predict\n    # -&gt; predict_proba\n    # -&gt; inverse_transform\n    # -&gt; get_and_save_metrics\n\n    def __init__(self, model_dir: Union[str, None] = None, model_name: Union[str, None] = None,\n                 x_col: Union[list, None] = None, y_col: Union[str, int, list, None] = None, random_seed: Union[int, None] = None,\n                 preprocess_pipeline: Union[ColumnTransformer, None] = None, level_save: str = 'HIGH', **kwargs) -&gt; None:\n        '''Initialization of the parent class.\n\n        Kwargs:\n            model_dir (str): Folder where to save the model\n                If None, creates a directory based on the model's name and the date (most common usage)\n            model_name (str): The name of the model\n            x_col (list): Names of the columns used for the training - x\n            y_col (str or int or list if multi-labels): Name of the model's target column(s) - y\n            random_seed (int): Seed to use for packages randomness\n            preprocess_pipeline (ColumnTransformer): The pipeline used for preprocessing. If None -&gt; no preprocessing !\n            level_save (str): Level of saving\n                LOW: stats + configurations + logger keras - /!\\\\ The model can't be reused /!\\\\ -\n                MEDIUM: LOW + hdf5 + pkl + plots\n                HIGH: MEDIUM + predictions\n        Raises:\n            ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\n            NotADirectoryError: If a provided model directory is not a directory (i.e. it's a file)\n        '''\n        if level_save not in ['LOW', 'MEDIUM', 'HIGH']:\n            raise ValueError(f\"The object level_save ({level_save}) is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\")\n\n        # Get logger\n        self.logger = logging.getLogger(__name__)\n\n        # Model type -&gt; 'classifier' or 'regressor' depending on the model\n        self.model_type = None\n\n        # Model name\n        self.model_name = self._default_name if model_name is None else model_name\n\n        # Names of the columns used\n        self.x_col = x_col\n        self.y_col = y_col\n\n        # Random seed\n        self.random_seed = random_seed\n\n        # Can be None if reloading a model\n        if x_col is None:\n            self.logger.warning(\"Warning, the attribute x_col is not given! The model might not work as intended.\")\n        if y_col is None:\n            self.logger.warning(\"Warning, the attribute y_col is not given! The model might not work as intended.\")\n\n        # Model folder\n        if model_dir is None:\n            self.model_dir = self._get_new_model_dir()\n        else:\n            if not os.path.exists(model_dir):\n                os.makedirs(model_dir)\n            if not os.path.isdir(model_dir):\n                raise NotADirectoryError(f\"{model_dir} is not a valid directory\")\n            self.model_dir = os.path.abspath(model_dir)\n\n        # Preprocessing pipeline\n        self.preprocess_pipeline = preprocess_pipeline\n        if self.preprocess_pipeline is not None:\n            try:\n                check_is_fitted(self.preprocess_pipeline)\n            except NotFittedError as e:\n                self.logger.error(\"The preprocessing pipeline hasn't been fitted !\")\n                self.logger.error(repr(e))\n                raise NotFittedError()\n            # We get the associated columns (and a check if there has been a fit is done)\n            self.columns_in, self.mandatory_columns = utils_models.get_columns_pipeline(self.preprocess_pipeline)\n        else:\n            # We can't define a \"no_preprocess\" pipeline since we should fit it\n            # So we take care of that at the first fit\n            self.logger.warning(\"Warning, no preprocessing pipeline given !\")\n            self.columns_in, self.mandatory_columns = None, None\n\n        # Other options\n        self.level_save = level_save\n\n        # is trained ?\n        self.trained = False\n        self.nb_fit = 0\n\n        # Configuration dict. to be logged. Set on save.\n        self.json_dict: Dict[Any, Any] = {}\n\n    def fit(self, x_train, y_train, **kwargs) -&gt; None:\n        '''Trains the model\n\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n            y_train (?): Array-like, shape = [n_samples, n_targets]\n        '''\n        raise NotImplementedError(\"'fit' needs to be overridden\")\n\n    def predict(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n        '''Predictions on the test set\n\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n        raise NotImplementedError(\"'predict' needs to be overridden\")\n\n    def predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n        '''Predicts probabilities on the test dataset\n\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n        raise NotImplementedError(\"'predict_proba' needs to be overridden\")\n\n    def inverse_transform(self, y: Union[list, np.ndarray]) -&gt; Union[list, tuple]:\n        '''Gets the final format of prediction\n            - Classification : classes from predictions\n            - Regression : values (identity function)\n\n        Args:\n            y (list | np.ndarray): Array-like, shape = [n_samples,]\n                   OR 1D array shape = [n_classes] (only one prediction)\n        Returns:\n            (?): Array, shape = [n_samples, ?]\n        '''\n        raise NotImplementedError(\"'inverse_transform' needs to be overridden\")\n\n    def get_and_save_metrics(self, y_true, y_pred, df_x: Union[pd.DataFrame, None] = None,\n                             series_to_add: Union[List[pd.Series], None] = None,\n                             type_data: str = '') -&gt; pd.DataFrame:\n        '''Gets and saves the metrics of a model\n\n        Args:\n            y_true (?): Array-like, shape = [n_samples, n_targets]\n            y_pred (?): Array-like, shape = [n_samples, n_targets]\n        Kwargs:\n            df_x (pd.DataFrame or None): Input dataFrame used for the prediction\n            series_to_add (list): List of pd.Series to add to the dataframe\n            type_data (str): Type of dataset (validation, test, ...)\n        Returns:\n            pd.DataFrame: The dataframe containing the statistics\n        '''\n        raise NotImplementedError(\"'get_and_save_metrics' needs to be overridden\")\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n\n        # Manage paths\n        pkl_path = os.path.join(self.model_dir, f\"{self.model_name}.pkl\")\n        preprocess_pipeline_path = os.path.join(self.model_dir, \"preprocess_pipeline.pkl\")\n        conf_path = os.path.join(self.model_dir, \"configurations.json\")\n\n        # Save the model &amp; preprocessing pipeline if level_save &gt; 'LOW'\n        if self.level_save in ['MEDIUM', 'HIGH']:\n            with open(pkl_path, 'wb') as f:\n                pickle.dump(self, f)\n            # Useful for reload_from_standalone, otherwise, saved as a class attribute\n            with open(preprocess_pipeline_path, 'wb') as f:\n                pickle.dump(self.preprocess_pipeline, f)\n\n        # Saving JSON configuration\n        json_dict = {\n            'maintainers': 'Agence DataServices',\n            'gabarit_version': '1.3.4.dev0+local',\n            'date': datetime.now().strftime(\"%d/%m/%Y - %H:%M:%S\"),  # Not the same as the folder's name\n            'package_version': utils.get_package_version(),\n            'model_name': self.model_name,\n            'model_dir': self.model_dir,\n            'model_type': self.model_type,\n            'trained': self.trained,\n            'nb_fit': self.nb_fit,\n            'x_col': self.x_col,\n            'y_col': self.y_col,\n            'columns_in': self.columns_in,\n            'mandatory_columns': self.mandatory_columns,\n            'random_seed': self.random_seed,\n            'level_save': self.level_save,\n            'librairie': None,\n        }\n        # Merge json_data if not None\n        if json_data is not None:\n            # Priority given to json_data !\n            json_dict = {**json_dict, **json_data}\n\n        # Add conf to attributes\n        self.json_dict = json_dict\n\n        # Save conf\n        with open(conf_path, 'w', encoding='utf-8') as json_file:\n            json.dump(json_dict, json_file, indent=4, cls=utils.NpEncoder)\n\n        # Now, save a properties file for the model upload\n        self._save_upload_properties(json_dict)\n\n    def _save_upload_properties(self, json_dict: Union[dict, None] = None) -&gt; None:\n        '''Prepares a configuration file for a future export (e.g on an artifactory)\n\n        Kwargs:\n            json_dict: Configurations to save\n        '''\n        if json_dict is None:\n            json_dict = {}\n\n        # Manage paths\n        properties_path = os.path.join(self.model_dir, \"properties.json\")\n        vanilla_model_upload_instructions = os.path.join(utils.get_ressources_path(), 'model_upload_instructions.md')\n        specific_model_upload_instructions = os.path.join(self.model_dir, \"model_upload_instructions.md\")\n\n        # First, we define a list of \"allowed\" properties\n        allowed_properties = [\"maintainers\", \"gabarit_version\", \"date\", \"package_version\", \"model_name\", \"list_classes\",\n                              \"librairie\", \"fit_time\"]\n        # Now we filter these properties\n        final_dict = {k: v for k, v in json_dict.items() if k in allowed_properties}\n        # Save\n        with open(properties_path, 'w', encoding='utf-8') as f:\n            json.dump(final_dict, f, indent=4, cls=utils.NpEncoder)\n\n        # Add instructions to upload a model to a storage solution (e.g. Artifactory)\n        with open(vanilla_model_upload_instructions, 'r', encoding='utf-8') as f:\n            content = f.read()\n        # TODO: to be improved\n        new_content = content.replace('model_dir_path_identifier', os.path.abspath(self.model_dir))\n        with open(specific_model_upload_instructions, 'w', encoding='utf-8') as f:\n            f.write(new_content)\n\n    def _get_new_model_dir(self) -&gt; str:\n        '''Gets a folder where to save the model\n\n        Returns:\n            str: Path to the folder\n        '''\n        models_dir = utils.get_models_path()\n        subfolder = os.path.join(models_dir, self.model_name)\n        folder_name = datetime.now().strftime(f\"{self.model_name}_%Y_%m_%d-%H_%M_%S\")\n        model_dir = os.path.join(subfolder, folder_name)\n        if os.path.isdir(model_dir):\n            time.sleep(1)  # Wait 1 second so that the 'date' changes...\n            return self._get_new_model_dir()  # Get new directory name\n        else:\n            os.makedirs(model_dir)\n        return model_dir\n\n    def _check_input_format(self, x_input: Union[pd.DataFrame, np.ndarray], y_input: Union[pd.DataFrame, pd.Series, np.ndarray, None] = None,\n                            fit_function: bool = False) -&gt; Tuple[Union[pd.DataFrame, np.ndarray], Union[pd.DataFrame, pd.Series, np.ndarray, None]]:\n        '''Checks the inputs of a function. We check the number of columns and the ordering.\n\n        Strategy :\n            - If fit function, set preprocessing pipeline, columns_in, mandatory_columns, x_col (if not set), y_col (if not set) with input data\n            - Then, for both x &amp; y\n                - If input data has a column attribute\n                    - If we can find all needed columns, reorder the dataset using only the needed columns (so it works if we have more columns)\n                    - Else, raise an error if length do not match (otherwise log a warning)\n                - Else, raise an error if length do not match (otherwise log a warning)\n\n        We also set the pipeline to a passthrough pipeline if None\n\n        Args:\n            x_input (pd.DataFrame, np.ndarray): Array-like, shape = [n_samples, n_features]\n        Kwargs:\n            y_input (pd.DataFrame, pd.Series, np.ndarray): Array-like, shape = [n_samples, n_target]\n                Mandatory if fit_function\n            fit_function (bool): If it is a fit function\n        Raises:\n            AttributeError: If fit_function == True, but y_input is None\n            ValueError: If one of the inputs hasn't the right number of columns\n        Returns:\n            (pd.DataFrame, np.ndarray): x_input, may be reordered if needed\n            (pd.DataFrame, pd.Series, np.ndarray): y_input, may be reordered if needed\n        '''\n        # Getting some info first\n        x_input_shape = x_input.shape[-1] if len(x_input.shape) &gt; 1 else 1\n        if y_input is not None:\n            y_input_shape = y_input.shape[-1] if len(y_input.shape) &gt; 1 else 1\n        else:\n            y_input_shape = 0  # not used\n\n        # Manage fit_function = True\n        if fit_function:\n            if y_input is None:\n                raise AttributeError(\"The argument y_input is mandatory if fit_function == True\")\n            # Set x_col if not set yet\n            if self.x_col is None:\n                self.logger.warning(\"Warning, the attribute x_col was not given when creating the model\")\n                self.logger.warning(\"We set it now with the input data of the fit function\")\n                if hasattr(x_input, 'columns'):\n                    # TODO : tmp mypy fix https://github.com/python/mypy/pull/13544\n                    self.x_col = list(x_input.columns)  # type: ignore\n                else:\n                    self.x_col = [_ for _ in range(x_input_shape)]\n            # Same thing for y_col\n            if self.y_col is None:\n                self.logger.warning(\"Warning, the attribute y_col was not given when creating the model\")\n                self.logger.warning(\"We set it now with the input data of the fit function\")\n                if hasattr(y_input, 'columns'):\n                    # TODO : tmp mypy fix https://github.com/python/mypy/pull/13544\n                    self.y_col = list(y_input.columns)  # type: ignore\n                else:\n                    self.y_col = [_ for _ in range(y_input_shape)]\n                # If there is only one element, we get rid of the list\n                if y_input_shape == 1:\n                    self.y_col = self.y_col[0]\n            # If pipeline, columns_in or mandatory_columns is None, sets it\n            if self.preprocess_pipeline is None:  # ie no pipeline given when initializing the class\n                preprocess_str = \"no_preprocess\"\n                preprocess_pipeline = preprocess.get_pipeline(preprocess_str)  # Warning, the pipeline must be fitted\n                preprocess_pipeline.fit(x_input)  # We fit the pipeline to set the necessary columns for the pipeline\n                # Set attributes\n                self.preprocess_pipeline = preprocess_pipeline\n                self.columns_in, self.mandatory_columns = utils_models.get_columns_pipeline(self.preprocess_pipeline)\n\n        # Checking x_input\n        if self.x_col is None:\n            self.logger.warning(\"Can't check the input format (x) because x_col is not set...\")\n        else:\n            x_col_len = len(self.x_col)\n            # We check the presence of the columns\n            if hasattr(x_input, 'columns'):\n                can_reorder = True\n                for col in self.x_col:\n                    # TODO : tmp mypy fix https://github.com/python/mypy/pull/13544\n                    if col not in x_input.columns:  # type: ignore\n                        can_reorder = False\n                        self.logger.warning(f\"The column {col} is missing from the input (x)\")\n                # If we can't reorder :\n                # 1. Exact number of columns : we write a warning message and continue with columns renamed\n                # 2. Not the correct number of column : raise an error\n                if not can_reorder:\n                    if x_input_shape != x_col_len:\n                        raise ValueError(f\"Input data (x) is not in the right format ({x_input_shape} != {x_col_len})\")\n                    self.logger.warning(\"The names of the columns (x) do not match. The process continues since there is the right number of columns\")\n                    x_input = x_input.copy()  # needs a copy as we wil change columns names\n                    x_input.columns = self.x_col  # type: ignore\n                # If we can reorder :\n                # 1. Same number of inputs but not the same order -&gt; we just reorder\n                # 2. More columns ? -&gt; we just take the needed subset + log a warning message\n                else:\n                    # TODO : tmp mypy fix https://github.com/python/mypy/pull/13544\n                    if list(x_input.columns) != self.x_col:  # type: ignore\n                        if x_input_shape == x_col_len:\n                            self.logger.warning(\"The input columns (x) are not in the right order -&gt; automatic reordering !\")\n                        else:\n                            self.logger.warning(\"More columns in input (x) than needed, but we can find the needed columns -&gt; only considering the needed columns\")\n                        x_input = x_input[self.x_col]\n            else:\n                if x_input_shape != len(self.x_col):\n                    raise ValueError(f\"Input data (x) is not in the right format ({x_input_shape} != {x_col_len})\")\n                self.logger.warning(\"The input (x) does not have the 'columns' attribute -&gt; can't check the ordering of the columns\")\n\n        # Checking y_input\n        if y_input is not None:\n            if self.y_col is None:\n                self.logger.warning(\"Can't check the input format (y) because y_col is not set...\")\n            else:\n                # Checking y_input format\n                y_col_len = len(self.y_col) if type(self.y_col) == list else 1\n                # We check the presence of the columns\n                if hasattr(y_input, 'columns'):\n                    can_reorder = True\n                    target_cols = self.y_col if type(self.y_col) == list else [self.y_col]\n                    for col in target_cols:\n                        # TODO : tmp mypy fix https://github.com/python/mypy/pull/13544\n                        if col not in y_input.columns:  # type: ignore\n                            can_reorder = False\n                            self.logger.warning(f\"The column {col} is missing from the input (y)\")\n                    # If we can't reorder :\n                    # 1. Exact number of columns : we write a warning message and continue with columns renamed\n                    # 2. Not the correct number of column : raise an error\n                    if not can_reorder:\n                        if y_input_shape != y_col_len:\n                            raise ValueError(f\"Input data (y) is not in the right format ({y_input_shape} != {y_col_len})\")\n                        self.logger.warning(\"The names of the columns (y) do not match. The process continues since there is the right number of columns\")\n                        y_input = y_input.copy()  # needs a copy as we wil change columns names\n                        y_input.columns = self.y_col  # type: ignore\n                    # If we can reorder :\n                    # 1. Same number of inputs but not the same order -&gt; we just reorder\n                    # 2. More columns ? -&gt; we just take the needed subset + log a warning message\n                    else:\n                        # TODO : tmp mypy fix https://github.com/python/mypy/pull/13544\n                        if list(y_input.columns) != target_cols:  # type: ignore\n                            if y_input_shape == y_col_len:\n                                self.logger.warning(\"The input columns (y) are not in the right order -&gt; automatic reordering !\")\n                            else:\n                                self.logger.warning(\"More columns in input (y) than needed, but we can find the needed columns -&gt; only considering the needed columns\")\n                            y_input = y_input[target_cols]\n                else:\n                    if y_input_shape != y_col_len:\n                        raise ValueError(f\"Input data (y) is not in the right format ({y_input_shape} != {y_col_len})\")\n                    self.logger.warning(\"The input (y) does not have the 'columns' attribute -&gt; can't check the ordering of the columns\")\n\n        # Return\n        return x_input, y_input\n\n    def display_if_gpu_activated(self) -&gt; None:\n        '''Displays if a GPU is being used'''\n        if self._is_gpu_activated():\n            self.logger.info(\"GPU activated\")\n\n    def _is_gpu_activated(self) -&gt; bool:\n        '''Checks if we use a GPU\n\n        Returns:\n            bool: whether GPU is available or not\n        '''\n        # By default, no GPU\n        return False\n</code></pre>"},{"location":"reference/template_num/models_training/model_class/#template_num.models_training.model_class.ModelClass.__init__","title":"<code>__init__(model_dir=None, model_name=None, x_col=None, y_col=None, random_seed=None, preprocess_pipeline=None, level_save='HIGH', **kwargs)</code>","text":"<p>Initialization of the parent class.</p> Kwargs <p>model_dir (str): Folder where to save the model     If None, creates a directory based on the model's name and the date (most common usage) model_name (str): The name of the model x_col (list): Names of the columns used for the training - x y_col (str or int or list if multi-labels): Name of the model's target column(s) - y random_seed (int): Seed to use for packages randomness preprocess_pipeline (ColumnTransformer): The pipeline used for preprocessing. If None -&gt; no preprocessing ! level_save (str): Level of saving     LOW: stats + configurations + logger keras - /! The model can't be reused /! -     MEDIUM: LOW + hdf5 + pkl + plots     HIGH: MEDIUM + predictions</p> <p>Raises:     ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])     NotADirectoryError: If a provided model directory is not a directory (i.e. it's a file)</p> Source code in <code>template_num/models_training/model_class.py</code> <pre><code>def __init__(self, model_dir: Union[str, None] = None, model_name: Union[str, None] = None,\n             x_col: Union[list, None] = None, y_col: Union[str, int, list, None] = None, random_seed: Union[int, None] = None,\n             preprocess_pipeline: Union[ColumnTransformer, None] = None, level_save: str = 'HIGH', **kwargs) -&gt; None:\n    '''Initialization of the parent class.\n\n    Kwargs:\n        model_dir (str): Folder where to save the model\n            If None, creates a directory based on the model's name and the date (most common usage)\n        model_name (str): The name of the model\n        x_col (list): Names of the columns used for the training - x\n        y_col (str or int or list if multi-labels): Name of the model's target column(s) - y\n        random_seed (int): Seed to use for packages randomness\n        preprocess_pipeline (ColumnTransformer): The pipeline used for preprocessing. If None -&gt; no preprocessing !\n        level_save (str): Level of saving\n            LOW: stats + configurations + logger keras - /!\\\\ The model can't be reused /!\\\\ -\n            MEDIUM: LOW + hdf5 + pkl + plots\n            HIGH: MEDIUM + predictions\n    Raises:\n        ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\n        NotADirectoryError: If a provided model directory is not a directory (i.e. it's a file)\n    '''\n    if level_save not in ['LOW', 'MEDIUM', 'HIGH']:\n        raise ValueError(f\"The object level_save ({level_save}) is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\")\n\n    # Get logger\n    self.logger = logging.getLogger(__name__)\n\n    # Model type -&gt; 'classifier' or 'regressor' depending on the model\n    self.model_type = None\n\n    # Model name\n    self.model_name = self._default_name if model_name is None else model_name\n\n    # Names of the columns used\n    self.x_col = x_col\n    self.y_col = y_col\n\n    # Random seed\n    self.random_seed = random_seed\n\n    # Can be None if reloading a model\n    if x_col is None:\n        self.logger.warning(\"Warning, the attribute x_col is not given! The model might not work as intended.\")\n    if y_col is None:\n        self.logger.warning(\"Warning, the attribute y_col is not given! The model might not work as intended.\")\n\n    # Model folder\n    if model_dir is None:\n        self.model_dir = self._get_new_model_dir()\n    else:\n        if not os.path.exists(model_dir):\n            os.makedirs(model_dir)\n        if not os.path.isdir(model_dir):\n            raise NotADirectoryError(f\"{model_dir} is not a valid directory\")\n        self.model_dir = os.path.abspath(model_dir)\n\n    # Preprocessing pipeline\n    self.preprocess_pipeline = preprocess_pipeline\n    if self.preprocess_pipeline is not None:\n        try:\n            check_is_fitted(self.preprocess_pipeline)\n        except NotFittedError as e:\n            self.logger.error(\"The preprocessing pipeline hasn't been fitted !\")\n            self.logger.error(repr(e))\n            raise NotFittedError()\n        # We get the associated columns (and a check if there has been a fit is done)\n        self.columns_in, self.mandatory_columns = utils_models.get_columns_pipeline(self.preprocess_pipeline)\n    else:\n        # We can't define a \"no_preprocess\" pipeline since we should fit it\n        # So we take care of that at the first fit\n        self.logger.warning(\"Warning, no preprocessing pipeline given !\")\n        self.columns_in, self.mandatory_columns = None, None\n\n    # Other options\n    self.level_save = level_save\n\n    # is trained ?\n    self.trained = False\n    self.nb_fit = 0\n\n    # Configuration dict. to be logged. Set on save.\n    self.json_dict: Dict[Any, Any] = {}\n</code></pre>"},{"location":"reference/template_num/models_training/model_class/#template_num.models_training.model_class.ModelClass.display_if_gpu_activated","title":"<code>display_if_gpu_activated()</code>","text":"<p>Displays if a GPU is being used</p> Source code in <code>template_num/models_training/model_class.py</code> <pre><code>def display_if_gpu_activated(self) -&gt; None:\n    '''Displays if a GPU is being used'''\n    if self._is_gpu_activated():\n        self.logger.info(\"GPU activated\")\n</code></pre>"},{"location":"reference/template_num/models_training/model_class/#template_num.models_training.model_class.ModelClass.fit","title":"<code>fit(x_train, y_train, **kwargs)</code>","text":"<p>Trains the model</p> <p>Parameters:</p> Name Type Description Default <code>x_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <code>y_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_targets]</p> required Source code in <code>template_num/models_training/model_class.py</code> <pre><code>def fit(self, x_train, y_train, **kwargs) -&gt; None:\n    '''Trains the model\n\n    Args:\n        x_train (?): Array-like, shape = [n_samples, n_features]\n        y_train (?): Array-like, shape = [n_samples, n_targets]\n    '''\n    raise NotImplementedError(\"'fit' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_num/models_training/model_class/#template_num.models_training.model_class.ModelClass.get_and_save_metrics","title":"<code>get_and_save_metrics(y_true, y_pred, df_x=None, series_to_add=None, type_data='')</code>","text":"<p>Gets and saves the metrics of a model</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>?</code> <p>Array-like, shape = [n_samples, n_targets]</p> required <code>y_pred</code> <code>?</code> <p>Array-like, shape = [n_samples, n_targets]</p> required <p>Kwargs:     df_x (pd.DataFrame or None): Input dataFrame used for the prediction     series_to_add (list): List of pd.Series to add to the dataframe     type_data (str): Type of dataset (validation, test, ...) Returns:     pd.DataFrame: The dataframe containing the statistics</p> Source code in <code>template_num/models_training/model_class.py</code> <pre><code>def get_and_save_metrics(self, y_true, y_pred, df_x: Union[pd.DataFrame, None] = None,\n                         series_to_add: Union[List[pd.Series], None] = None,\n                         type_data: str = '') -&gt; pd.DataFrame:\n    '''Gets and saves the metrics of a model\n\n    Args:\n        y_true (?): Array-like, shape = [n_samples, n_targets]\n        y_pred (?): Array-like, shape = [n_samples, n_targets]\n    Kwargs:\n        df_x (pd.DataFrame or None): Input dataFrame used for the prediction\n        series_to_add (list): List of pd.Series to add to the dataframe\n        type_data (str): Type of dataset (validation, test, ...)\n    Returns:\n        pd.DataFrame: The dataframe containing the statistics\n    '''\n    raise NotImplementedError(\"'get_and_save_metrics' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_num/models_training/model_class/#template_num.models_training.model_class.ModelClass.inverse_transform","title":"<code>inverse_transform(y)</code>","text":"<p>Gets the final format of prediction     - Classification : classes from predictions     - Regression : values (identity function)</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>list | ndarray</code> <p>Array-like, shape = [n_samples,]    OR 1D array shape = [n_classes] (only one prediction)</p> required <p>Returns:     (?): Array, shape = [n_samples, ?]</p> Source code in <code>template_num/models_training/model_class.py</code> <pre><code>def inverse_transform(self, y: Union[list, np.ndarray]) -&gt; Union[list, tuple]:\n    '''Gets the final format of prediction\n        - Classification : classes from predictions\n        - Regression : values (identity function)\n\n    Args:\n        y (list | np.ndarray): Array-like, shape = [n_samples,]\n               OR 1D array shape = [n_classes] (only one prediction)\n    Returns:\n        (?): Array, shape = [n_samples, ?]\n    '''\n    raise NotImplementedError(\"'inverse_transform' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_num/models_training/model_class/#template_num.models_training.model_class.ModelClass.predict","title":"<code>predict(x_test, **kwargs)</code>","text":"<p>Predictions on the test set</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>DataFrame</code> <p>DataFrame with the test data to be predicted</p> required <p>Returns:     (np.ndarray): Array, shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/model_class.py</code> <pre><code>def predict(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n    '''Predictions on the test set\n\n    Args:\n        x_test (pd.DataFrame): DataFrame with the test data to be predicted\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n    raise NotImplementedError(\"'predict' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_num/models_training/model_class/#template_num.models_training.model_class.ModelClass.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts probabilities on the test dataset</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>DataFrame</code> <p>DataFrame with the test data to be predicted</p> required <p>Returns:     (np.ndarray): Array, shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/model_class.py</code> <pre><code>def predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n    '''Predicts probabilities on the test dataset\n\n    Args:\n        x_test (pd.DataFrame): DataFrame with the test data to be predicted\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n    raise NotImplementedError(\"'predict_proba' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_num/models_training/model_class/#template_num.models_training.model_class.ModelClass.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/model_class.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n\n    # Manage paths\n    pkl_path = os.path.join(self.model_dir, f\"{self.model_name}.pkl\")\n    preprocess_pipeline_path = os.path.join(self.model_dir, \"preprocess_pipeline.pkl\")\n    conf_path = os.path.join(self.model_dir, \"configurations.json\")\n\n    # Save the model &amp; preprocessing pipeline if level_save &gt; 'LOW'\n    if self.level_save in ['MEDIUM', 'HIGH']:\n        with open(pkl_path, 'wb') as f:\n            pickle.dump(self, f)\n        # Useful for reload_from_standalone, otherwise, saved as a class attribute\n        with open(preprocess_pipeline_path, 'wb') as f:\n            pickle.dump(self.preprocess_pipeline, f)\n\n    # Saving JSON configuration\n    json_dict = {\n        'maintainers': 'Agence DataServices',\n        'gabarit_version': '1.3.4.dev0+local',\n        'date': datetime.now().strftime(\"%d/%m/%Y - %H:%M:%S\"),  # Not the same as the folder's name\n        'package_version': utils.get_package_version(),\n        'model_name': self.model_name,\n        'model_dir': self.model_dir,\n        'model_type': self.model_type,\n        'trained': self.trained,\n        'nb_fit': self.nb_fit,\n        'x_col': self.x_col,\n        'y_col': self.y_col,\n        'columns_in': self.columns_in,\n        'mandatory_columns': self.mandatory_columns,\n        'random_seed': self.random_seed,\n        'level_save': self.level_save,\n        'librairie': None,\n    }\n    # Merge json_data if not None\n    if json_data is not None:\n        # Priority given to json_data !\n        json_dict = {**json_dict, **json_data}\n\n    # Add conf to attributes\n    self.json_dict = json_dict\n\n    # Save conf\n    with open(conf_path, 'w', encoding='utf-8') as json_file:\n        json.dump(json_dict, json_file, indent=4, cls=utils.NpEncoder)\n\n    # Now, save a properties file for the model upload\n    self._save_upload_properties(json_dict)\n</code></pre>"},{"location":"reference/template_num/models_training/model_keras/","title":"Model keras","text":""},{"location":"reference/template_num/models_training/model_keras/#template_num.models_training.model_keras.ModelKeras","title":"<code>ModelKeras</code>","text":"<p>             Bases: <code>ModelClass</code></p> <p>Generic model for Keras NN</p> Source code in <code>template_num/models_training/model_keras.py</code> <pre><code>class ModelKeras(ModelClass):\n    '''Generic model for Keras NN'''\n\n    _default_name = 'model_keras'\n\n    # Not implemented :\n    # -&gt; _get_model\n    # -&gt; reload_from_standalone\n\n    def __init__(self, batch_size: int = 64, epochs: int = 99, validation_split: float = 0.2,\n                 patience: int = 5, keras_params: Union[dict, None] = None, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelClass for more arguments)\n\n        Kwargs:\n            batch_size (int): Batch size\n            epochs (int): Number of epochs\n            validation_split (float): Percentage for the validation set split\n                Only used if no input validation set when fitting\n            patience (int): Early stopping patience\n            keras_params (dict): Parameters used by Keras models.\n                e.g. learning_rate, nb_lstm_units, etc...\n                The purpose of this dictionary is for the user to use it as they wants in the _get_model function\n                This parameter was initially added in order to do an hyperparameters search\n        '''\n        # TODO: learning rate should be an attribute !\n        # Init.\n        super().__init__(**kwargs)\n\n        # Fix tensorflow GPU\n        gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n        for device in gpu_devices:\n            tf.config.experimental.set_memory_growth(device, True)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Param. model\n        self.batch_size = batch_size\n        self.epochs = epochs\n        self.validation_split = validation_split\n        self.patience = patience\n\n        # Model set on fit\n        self.model: Any = None\n\n        # Keras params\n        if keras_params is None:\n            keras_params = {}\n        self.keras_params = keras_params.copy()\n\n        # Keras custom objects : we get the ones specified in utils_deep_keras\n        self.custom_objects = utils_deep_keras.custom_objects\n\n    def fit(self, x_train, y_train, x_valid=None, y_valid=None, with_shuffle: bool = True, **kwargs) -&gt; None:\n        '''Fits the model\n\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n            y_train (?): Array-like, shape = [n_samples, n_targets]\n        Kwargs:\n            x_valid (?): Array-like, shape = [n_samples, n_features]\n            y_valid (?): Array-like, shape = [n_samples, n_targets]\n            with_shuffle (bool): If x, y must be shuffled before fitting\n                This should be used if y is not shuffled as the split_validation takes the lines in order.\n                Thus, the validation set might get classes which are not in the train set ...\n        Raises:\n            ValueError: If different classes when comparing an already fitted model and a new dataset\n        '''\n\n        ##############################################\n        # Manage retrain\n        ##############################################\n\n        # If a model has already been fitted, we make a new folder in order not to overwrite the existing one !\n        # And we save the old conf\n        if self.trained:\n            # Get src files to save\n            src_files = [os.path.join(self.model_dir, \"configurations.json\")]\n            if self.nb_fit &gt; 1:\n                for i in range(1, self.nb_fit):\n                    src_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n            # Change model dir\n            self.model_dir = self._get_new_model_dir()\n            # Get dst files\n            dst_files = [os.path.join(self.model_dir, f\"configurations_fit_{self.nb_fit}.json\")]\n            if self.nb_fit &gt; 1:\n                for i in range(1, self.nb_fit):\n                    dst_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n            # Copies\n            for src, dst in zip(src_files, dst_files):\n                try:\n                    shutil.copyfile(src, dst)\n                except Exception as e:\n                    self.logger.error(f\"Unable to copy {src} to {dst}\")\n                    self.logger.error(\"We still go on\")\n                    self.logger.error(repr(e))\n\n        ##############################################\n        # Prepare x_train, x_valid, y_train &amp; y_valid\n        # Also extract list of classes if classification\n        ##############################################\n\n        # Checking input formats\n        x_train, y_train = self._check_input_format(x_train, y_train, fit_function=True)\n        # If the validation set is present, we check its format (but with fit_function=False)\n        if y_valid is not None:\n            x_valid, y_valid = self._check_input_format(x_valid, y_valid, fit_function=False)\n\n        # If classification, we need to transform y\n        if self.model_type == 'classifier':\n            # if not multilabel, transform y_train as dummies (should already be the case for multi-labels)\n            if not self.multi_label:\n                # If len(array.shape)==2, we flatten the array if the second dimension is useless\n                if isinstance(y_train, np.ndarray) and len(y_train.shape) == 2 and y_train.shape[1] == 1:\n                    y_train = np.ravel(y_train)\n                if isinstance(y_valid, np.ndarray) and len(y_valid.shape) == 2 and y_valid.shape[1] == 1:\n                    y_valid = np.ravel(y_valid)\n                # Dummies transformation\n                y_train = pd.get_dummies(y_train).astype(int)\n                y_valid = pd.get_dummies(y_valid).astype(int) if y_valid is not None else None\n                # Important : get_dummies reorder the columns in alphabetical order\n                # Thus, there is no problem if we fit again on a new dataframe with shuffled data\n                list_classes = list(y_train.columns)\n                # FIX: valid test might miss some classes, hence we need to add them back to y_valid\n                if y_valid is not None and y_train.shape[1] != y_valid.shape[1]:\n                    for cl in list_classes:\n                        # Add missing columns\n                        if cl not in y_valid.columns:\n                            y_valid[cl] = 0\n                    y_valid = y_valid[list_classes]  # Reorder\n            # Else keep it as it is\n            else:\n                y_train = y_train\n                y_valid = y_valid\n                if hasattr(y_train, 'columns'):\n                    # TODO : tmp mypy fix https://github.com/python/mypy/pull/13544\n                    list_classes = list(y_train.columns)  # type: ignore\n                else:\n                    self.logger.warning(\n                        \"Can't read the name of the columns of y_train -&gt; inverse transformation won't be possible\"\n                    )\n                    # We still create a list of classes in order to be compatible with other functions\n                    list_classes = [str(_) for _ in range(pd.DataFrame(y_train).shape[1])]\n\n            # Set dict_classes based on list classes\n            dict_classes = {i: col for i, col in enumerate(list_classes)}\n\n            # Validate classes if already trained, else set them\n            if self.trained:\n                if self.list_classes != list_classes:\n                    raise ValueError(\"Error: the new dataset does not match with the already fitted model\")\n                if self.dict_classes != dict_classes:\n                    raise ValueError(\"Error: the new dataset does not match with the already fitted model\")\n            else:\n                self.list_classes = list_classes\n                self.dict_classes = dict_classes\n\n        # Shuffle x, y if wanted\n        # It is advised as validation_split from keras does not shufle the data\n        # Hence, for classificationt task, we might have classes in the validation data that we never met in the training data\n        if with_shuffle:\n            rng = np.random.RandomState(self.random_seed)\n            p = rng.permutation(len(x_train))\n            x_train = np.array(x_train)[p]\n            y_train = np.array(y_train)[p]\n        # Else still transform to numpy array\n        else:\n            x_train = np.array(x_train)\n            y_train = np.array(y_train)\n\n        # Also get y_valid as numpy &amp; get validation_data (tuple) if available\n        validation_data: Optional[tuple] = None  # Def. None if y_valid is None\n        if y_valid is not None:\n            x_valid = np.array(x_valid)\n            y_valid = np.array(y_valid)\n            validation_data = (x_valid, y_valid)\n\n        else:\n            x_train, x_valid,  y_train, y_valid = train_test_split(x_train, y_train, test_size=self.validation_split, \n                                                                   random_state=self.random_seed)\n            validation_data = (x_valid, y_valid)\n\n        if validation_data is None:\n            self.logger.warning(f\"Warning, no validation set. The training set will be splitted (validation fraction = {self.validation_split})\")\n        ##############################################\n        # Fit\n        ##############################################\n\n        # Get model (if already fitted, _get_model returns instance model)\n        self.model = self._get_model()\n\n        # Get callbacks (early stopping &amp; checkpoint)\n        callbacks = self._get_callbacks()\n\n        # Create data generator\n        data_train_generator = RandomStateDataGenerator(x_train, y_train, self.batch_size, self.random_seed)\n        data_val_generator = RandomStateDataGenerator(x_valid, y_valid, self.batch_size, self.random_seed)\n\n        # Fit\n        # We use a try...except in order to save the model if an error arises\n        # after more than a minute into training\n        start_time = time.time()\n        try:\n            fit_history = self.model.fit(  # type: ignore\n                data_train_generator,\n                epochs=self.epochs,\n                validation_data=data_val_generator,\n                callbacks=callbacks,\n                verbose=1,\n                shuffle=False\n            )\n        except (RuntimeError, SystemError, SystemExit, EnvironmentError, KeyboardInterrupt, tf.errors.ResourceExhaustedError, tf.errors.InternalError,\n                tf.errors.UnavailableError, tf.errors.UnimplementedError, tf.errors.UnknownError, Exception) as e:\n            # Steps:\n            # 1. Display tensor flow error\n            # 2. Check if more than one minute elapsed &amp; existence best.hdf5\n            # 3. Reload best model\n            # 4. We consider that a fit occured (trained = True, nb_fit += 1)\n            # 5. Save &amp; create a warning file\n            # 6. Display error messages\n            # 7. Raise an error\n\n            # 1.\n            self.logger.error(repr(e))\n\n            # 2.\n            best_path = os.path.join(self.model_dir, 'best.hdf5')\n            time_spent = time.time() - start_time\n            if time_spent &gt;= 60 and os.path.exists(best_path):\n                # 3.\n                self.model = load_model(best_path, custom_objects=self.custom_objects)\n                # 4.\n                self.trained = True\n                self.nb_fit += 1\n                # 5.\n                self.save()\n                with open(os.path.join(self.model_dir, \"0_MODEL_INCOMPLETE\"), 'w'):\n                    pass\n                with open(os.path.join(self.model_dir, \"1_TRAINING_NEEDS_TO_BE_RESUMED\"), 'w'):\n                    pass\n                # 6.\n                self.logger.error(\"[EXPERIMENTAL] Error during model training\")\n                self.logger.error(f\"[EXPERIMENTAL] The error happened after {round(time_spent, 2)}s of training\")\n                self.logger.error(\"[EXPERIMENTAL] A saving of the model is done but this model won't be usable as is.\")\n                self.logger.error(f\"[EXPERIMENTAL] In order to resume the training, we have to specify this model ({ntpath.basename(self.model_dir)}) in the file 2_training.py\")\n                self.logger.error(\"[EXPERIMENTAL] Warning, the preprocessing is not saved in the configuration file\")\n                self.logger.error(\"[EXPERIMENTAL] Warning, the best model might be corrupted in some cases\")\n            # 7.\n            raise RuntimeError(\"Error during model training\")\n\n        # Print accuracy &amp; loss if level_save &gt; 'LOW'\n        if self.level_save in ['MEDIUM', 'HIGH']:\n            self._plot_metrics_and_loss(fit_history)\n            # Reload best model\n            self.model = load_model(\n                os.path.join(self.model_dir, 'best.hdf5'),\n                custom_objects=self.custom_objects\n            )\n\n        # Set trained\n        self.trained = True\n        self.nb_fit += 1\n\n    @utils.trained_needed\n    def predict(self, x_test: pd.DataFrame, return_proba: bool = False, inference_batch_size: int = 128,\n                alternative_version: bool = False, **kwargs) -&gt; np.ndarray:\n        '''Predictions on test set\n\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Kwargs:\n            return_proba (bool): If the function should return the probabilities instead of the classes\n            inference_batch_size (int): size (approximate) of batches\n            alternative_version (bool): If an alternative predict version (`tf.function` + `model.__call__`) must be used. Should be faster with low nb of inputs.\n        Raises:\n            ValueError: If the model is not classifier and return_proba=True\n            ValueError: If the model is neither a classifier nor a regressor\n        Returns:\n            (np.ndarray): Array\n                # If not return_proba, shape = [n_samples,] or [n_samples, n_classes]\n                # Else, shape = [n_samples, n_classes]\n        '''\n        # Manage errors\n        if return_proba and self.model_type != 'classifier':\n            raise ValueError(f\"Models of the type {self.model_type} can't handle probabilities\")\n\n        # We check input format\n        x_test, _ = self._check_input_format(x_test)\n\n        # Predict depends on model type\n        if self.model_type == 'classifier':\n            return self._predict_classifier(x_test, return_proba=return_proba, inference_batch_size=inference_batch_size,\n                                            alternative_version=alternative_version)\n        elif self.model_type == 'regressor':\n            return self._predict_regressor(x_test, inference_batch_size=inference_batch_size, alternative_version=alternative_version)\n        else:\n            raise ValueError(f\"The model type ({self.model_type}) must be 'classifier' or 'regressor'\")\n\n    @utils.trained_needed\n    def _predict_classifier(self, x_test: pd.DataFrame, return_proba: bool = False, inference_batch_size: int = 128,\n                            alternative_version: bool = False) -&gt; np.ndarray:\n        '''Predictions on test\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Kwargs:\n            return_proba (boolean): If the function should return the probabilities instead of the classes\n            inference_batch_size (int): size (approximate) of batches\n            alternative_version (bool): If an alternative predict version (`tf.function` + `model.__call__`) must be used. Should be faster with low nb of inputs.\n        Raises:\n            ValueError: If the model is not of classifier type\n        Returns:\n            (np.ndarray): Array\n                # If not return_proba, shape = [n_samples,] or [n_samples, n_classes]\n                # Else, shape = [n_samples, n_classes]\n        '''\n        if self.model_type != 'classifier':\n            raise ValueError(f\"Models of type {self.model_type} do not implement the method predict_classifier\")\n\n        # Getting the predictions\n        if alternative_version:\n            predicted_proba = self._alternative_predict_proba(x_test, inference_batch_size=inference_batch_size)\n        else:\n            # We advise you to avoid using model.predict with newest TensorFlow versions (possible memory leak) in a production environment (e.g. API)\n            # https://github.com/tensorflow/tensorflow/issues/58676\n            # Instead, you can use the alternative version that uses tf.function decorator &amp; model.__call__\n            # However, it should still be better to use `model.predict` for one-shot, batch mode, large input, iterations.\n            predicted_proba = self.model.predict(x_test, batch_size=inference_batch_size, verbose=1)  # type: ignore\n\n        # We return the probabilities if wanted\n        if return_proba:\n            return predicted_proba\n\n        # Finally, we get the classes predictions\n        return self.get_classes_from_proba(predicted_proba)  # type: ignore\n\n    @utils.trained_needed\n    def _predict_regressor(self, x_test, inference_batch_size: int = 128, alternative_version: bool = False) -&gt; np.ndarray:\n        '''Predictions on test\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Kwargs:\n            inference_batch_size (int): size (approximate) of batches\n            alternative_version (bool): If an alternative predict version (`tf.function` + `model.__call__`) must be used. Should be faster with low nb of inputs.\n        Raises:\n            ValueError: If the model is not of regressor type\n        Returns:\n            (np.ndarray): Array, shape = [n_samples]\n        '''\n        if self.model_type != 'regressor':\n            raise ValueError(f\"Models of type {self.model_type} do not implement the method predict_regressor\")\n\n        # Getting the predictions\n        if alternative_version:\n            predictions = self._alternative_predict_proba(x_test, inference_batch_size=inference_batch_size)\n        else:\n            # We advise you to avoid using model.predict with newest TensorFlow versions (possible memory leak) in a production environment (e.g. API)\n            # https://github.com/tensorflow/tensorflow/issues/58676\n            # Instead, you can use the alternative version that uses tf.function decorator &amp; model.__call__\n            predictions = self.model.predict(x_test, batch_size=inference_batch_size, verbose=1)  # type: ignore\n\n        # Finally, we get the final format\n        # TODO : should certainly be changed for multi-output\n        # TODO : create an equivalent of get_classes_from_proba for regression ?\n        return np.array([pred[0] for pred in predictions])\n\n    @utils.trained_needed\n    def predict_proba(self, x_test: pd.DataFrame, alternative_version: bool = False, **kwargs) -&gt; np.ndarray:\n        '''Predicts the probabilities on the test set\n\n        Args:\n            x_test (pd.DataFrame): Array-like, shape = [n_samples, n_features]\n        Kwargs:\n            alternative_version (bool): If an alternative predict version (`tf.function` + `model.__call__`) must be used. Should be faster with low nb of inputs.\n        Raises:\n            ValueError: If model not classifier\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n        if self.model_type != 'classifier':\n            raise ValueError(f\"Models of type {self.model_type} do not implement the method predict_proba\")\n\n        # We check input format\n        x_test, _ = self._check_input_format(x_test)\n\n        # We use predict again\n        return self.predict(x_test, return_proba=True, alternative_version=alternative_version)\n\n    @utils.trained_needed\n    def _alternative_predict_proba(self, x_test: pd.DataFrame, inference_batch_size: int = 128) -&gt; np.ndarray:\n        '''Predicts probabilities on the test dataset - Alternative version\n        Should be faster with low nb of inputs.\n\n        Args:\n            x_test (pd.DataFrame): Array-like, shape = [n_samples]\n        Kwargs:\n            inference_batch_size (int): size (approximate) of batches\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n        # Assert batch size is &gt;= 1\n        inference_batch_size = max(1, inference_batch_size)\n        # Process by batches - avoid huge memory impact\n        nb_batches = max(1, len(x_test)//inference_batch_size)\n        list_array = []\n        # We also cast our dataframe to a numpy array\n        for arr in np.array_split(x_test.to_numpy(), nb_batches, axis=0):\n            tmp_results = self._serve(arr).numpy()\n            list_array.append(tmp_results)\n        np_results = np.concatenate(list_array)\n        # Return\n        return np_results\n\n    # We used to use reduce_retracing to avoid retracing and memory leaks (tensors with different shapes)\n    # but it is still experimental and seems to still do some retracing\n    # Hence, we now use input_signature and it seems to work as intended\n    @tf.function(input_signature=(tf.TensorSpec(shape=(None, None,), dtype=tf.float64, name='x'), ))\n    def _serve(self, x: np.ndarray):\n        '''Improves predict function using tf.function (cf. https://www.tensorflow.org/guide/function)\n\n        Args:\n            x (np.array): input data\n        Returns:\n            tf.tensor: model's output\n        '''\n        return self.model(x, training=False)\n\n    def _get_model(self) -&gt; Model:\n        '''Gets a model structure - returns the instance model instead if already defined\n\n        Returns:\n            (Model): a Keras model\n        '''\n        raise NotImplementedError(\"'_get_model' needs to be overridden\")\n\n    def _get_callbacks(self) -&gt; list:\n        '''Gets model callbacks\n\n        Returns:\n            list: List of callbacks\n        '''\n        # Get classic callbacks\n        callbacks = [EarlyStopping(monitor='val_loss', patience=self.patience, restore_best_weights=True)]\n        if self.level_save in ['MEDIUM', 'HIGH']:\n            callbacks.append(\n                ModelCheckpoint(\n                    filepath=os.path.join(self.model_dir, f'best.hdf5'), monitor='val_loss', save_best_only=True, mode='auto'\n                )\n            )\n        callbacks.append(CSVLogger(filename=os.path.join(self.model_dir, f'logger.csv'), separator=';', append=False))\n        callbacks.append(TerminateOnNaN())\n\n        # Get LearningRateScheduler\n        scheduler = self._get_learning_rate_scheduler()\n        if scheduler is not None:\n            callbacks.append(LearningRateScheduler(scheduler))\n\n        # Manage tensorboard\n        if self.level_save in ['HIGH']:\n            # Get log directory\n            models_path = utils.get_models_path()\n            tensorboard_dir = os.path.join(models_path, 'tensorboard_logs')\n            # We add a prefix so that the function load_model works correctly (it looks for a sub-folder with model name)\n            log_dir = os.path.join(tensorboard_dir, f\"tensorboard_{ntpath.basename(self.model_dir)}\")\n            if not os.path.exists(log_dir):\n                os.makedirs(log_dir)\n\n            # TODO: check if this class does not slow proccesses\n            # -&gt; For now: comment\n            # Create custom class to monitore LR changes\n            # https://stackoverflow.com/questions/49127214/keras-how-to-output-learning-rate-onto-tensorboard\n            # class LRTensorBoard(TensorBoard):\n            #     def __init__(self, log_dir, **kwargs) -&gt; None:  # add other arguments to __init__ if you need\n            #         super().__init__(log_dir=log_dir, **kwargs)\n            #\n            #     def on_epoch_end(self, epoch, logs=None):\n            #         logs.update({'lr': K.eval(self.model.optimizer.lr)})\n            #         super().on_epoch_end(epoch, logs)\n\n            # Append tensorboard callback\n            # TODO: check compatibility tensorflow 2.3\n            # WARNING : https://stackoverflow.com/questions/63619763/model-training-hangs-forever-when-using-a-tensorboard-callback-with-an-lstm-laye\n            # A compatibility problem TensorBoard / TensorFlow 2.3 (cuDNN implementation of LSTM/GRU) can arise\n            # In this case, the training of the model can be \"blocked\" and does not respond anymore\n            # This problem has arisen two times on P\u00f4le Emploi computers (windows 7 &amp; VM Ubuntu on windows 7 host)\n            # No problem on Valeuriad computers (windows 10)\n            # Thus, TensorBoard is deactivated by default for now\n            # While awaiting a possible fix, you are responsible for checking if TensorBoard works on your computer\n            self.logger.warning(\" ###################### \")\n            self.logger.warning(\"TensorBoard deactivated : compatibility problem TensorBoard / TensorFlow 2.3 (cuDNN implementation of LSTM/GRU) can arise\")\n            self.logger.warning(\"https://stackoverflow.com/questions/63619763/model-training-hangs-forever-when-using-a-tensorboard-callback-with-an-lstm-laye\")\n            self.logger.warning(\" In order to activate if, one has to modify the method _get_callbacks of model_keras.py\")\n            self.logger.warning(\" ###################### \")\n            # callbacks.append(TensorBoard(log_dir=log_dir, write_grads=False, write_images=False))\n            # self.logger.info(f\"To start tensorboard: python -m tensorboard.main --logdir {tensorboard_dir}\")\n\n        return callbacks\n\n    def _get_learning_rate_scheduler(self) -&gt; Union[Callable, None]:\n        '''Defines a Learning Rate Scheduler\n           -&gt; if it returns None, no scheduler will be used. (def.)\n           -&gt; This function will be save directly in the model configuration file\n           -&gt; This can be overridden at runing time\n\n        Returns:\n            (Callable | None): A learning rate Scheduler\n        '''\n        # e.g.\n        # def scheduler(epoch):\n        #     lim_epoch = 75\n        #     if epoch &lt; lim_epoch:\n        #         return 0.01\n        #     else:\n        #         return max(0.001, 0.01 * math.exp(0.01 * (lim_epoch - epoch)))\n        scheduler = None\n        return scheduler\n\n    def _plot_metrics_and_loss(self, fit_history) -&gt; None:\n        '''Plots some metrics &amp; loss\n\n        Arguments:\n            fit_history (?) : fit history\n        '''\n        # Manage dir\n        plots_path = os.path.join(self.model_dir, 'plots')\n        if not os.path.exists(plots_path):\n            os.makedirs(plots_path)\n\n        # Get a dictionnary of possible metrics/loss plots\n        metrics_dir = {\n            'acc': ['Accuracy', 'accuracy'],\n            'loss': ['Loss', 'loss'],\n            'categorical_accuracy': ['Categorical accuracy', 'categorical_accuracy'],\n            'f1': ['F1-score', 'f1_score'],\n            'precision': ['Precision', 'precision'],\n            'recall': ['Recall', 'recall'],\n            'mean_absolute_error': ['MAE', 'mae'],\n            'mae': ['MAE', 'mae'],\n            'mean_squared_error': ['MSE', 'mse'],\n            'mse': ['MSE', 'mse'],\n            'root_mean_squared_error': ['RMSE', 'rmse'],\n            'rmse': ['RMSE', 'rmse'],\n        }\n\n        # Plot each available metric\n        for metric in fit_history.history.keys():\n            if metric in metrics_dir.keys():\n                title = metrics_dir[metric][0]\n                filename = metrics_dir[metric][1]\n                plt.figure(figsize=(10, 8))\n                plt.plot(fit_history.history[metric])\n                plt.plot(fit_history.history[f'val_{metric}'])\n                plt.title(f\"Model {title}\")\n                plt.ylabel(title)\n                plt.xlabel('Epoch')\n                plt.legend(['Train', 'Validation'], loc='upper left')\n                # Save\n                filename = f\"{filename}.jpeg\"\n                plt.savefig(os.path.join(plots_path, filename))\n\n                # Close figures\n                plt.close('all')\n\n    def _save_model_png(self, model) -&gt; None:\n        '''Tries to save the structure of the model in png format\n        Graphviz necessary\n\n        Args:\n            model (?): model to plot\n        '''\n        # Check if graphiz is intalled\n        # TODO : to be improved !\n        graphiz_path = 'C:/Program Files (x86)/Graphviz2.38/bin/'\n        if os.path.isdir(graphiz_path):\n            os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n            img_path = os.path.join(self.model_dir, 'model.png')\n            plot_model(model, to_file=img_path)\n\n    @no_type_check  # We do not check the type, because it is complicated with managing custom_objects_str\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save configuration JSON\n        if json_data is None:\n            json_data = {}\n\n        json_data['librairie'] = 'keras'\n        json_data['batch_size'] = self.batch_size\n        json_data['epochs'] = self.epochs\n        json_data['validation_split'] = self.validation_split\n        json_data['patience'] = self.patience\n        json_data['keras_params'] = self.keras_params\n        if self.model is not None:\n            json_data['keras_model'] = json.loads(self.model.to_json())\n        else:\n            json_data['keras_model'] = None\n\n        # Add _get_model code if not in json_data\n        if '_get_model' not in json_data.keys():\n            json_data['_get_model'] = pickle.source.getsourcelines(self._get_model)[0]\n        # Add _get_learning_rate_scheduler code if not in json_data\n        if '_get_learning_rate_scheduler' not in json_data.keys():\n            json_data['_get_learning_rate_scheduler'] = pickle.source.getsourcelines(self._get_learning_rate_scheduler)[0]\n        # Add custom_objects code if not in json_data\n        if 'custom_objects' not in json_data.keys():\n            custom_objects_str = self.custom_objects.copy()\n            for key in custom_objects_str.keys():\n                if callable(custom_objects_str[key]):\n                    # Nominal case\n                    if not isinstance(custom_objects_str[key], functools.partial):\n                        custom_objects_str[key] = pickle.source.getsourcelines(custom_objects_str[key])[0]\n                    # Manage partials\n                    else:\n                        custom_objects_str[key] = {\n                            'type': 'partial',\n                            'args': custom_objects_str[key].args,\n                            'function': pickle.source.getsourcelines(custom_objects_str[key].func)[0],\n                        }\n            json_data['custom_objects'] = custom_objects_str\n\n        # Save strategy :\n        # - best.hdf5 already saved in fit()\n        # - can't pickle keras model, so we drop it, save, and reload it\n        keras_model = self.model\n        self.model = None\n        super().save(json_data=json_data)\n        self.model = keras_model\n\n    def reload_model(self, hdf5_path: str) -&gt; Any:\n        '''Loads a Keras model from a HDF5 file\n\n        Args:\n            hdf5_path (str): Path to the hdf5 file\n        Returns:\n            ?: Keras model\n        '''\n        # Fix tensorflow GPU if not already done (useful if we reload a model)\n        try:\n            gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n            for device in gpu_devices:\n                tf.config.experimental.set_memory_growth(device, True)\n        except Exception:\n            pass\n\n        # We check if we already have the custom objects\n        if hasattr(self, 'custom_objects') and self.custom_objects is not None:\n            custom_objects = self.custom_objects\n        else:\n            self.logger.warning(\"Can't find the attribute 'custom_objects' in the model to be reloaded\")\n            self.logger.warning(\"Backup on the default custom_objects of utils_deep_keras\")\n            custom_objects = utils_deep_keras.custom_objects\n\n        # Loading of the model\n        keras_model = load_model(hdf5_path, custom_objects=custom_objects)\n\n        # Set trained to true if not already true\n        if not self.trained:\n            self.trained = True\n            self.nb_fit = 1\n\n        # Return\n        return keras_model\n\n    def reload_from_standalone(self, **kwargs) -&gt; None:\n        '''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Needs to be overridden /!\\\\ -\n        '''\n        raise NotImplementedError(\"'reload_from_standalone' needs to be overridden\")\n\n    def _is_gpu_activated(self) -&gt; bool:\n        '''Checks if a GPU is used\n\n        Returns:\n            bool: whether GPU is available or not\n        '''\n        # Checks for available GPU devices\n        physical_devices = tf.config.list_physical_devices('GPU')\n        if len(physical_devices) &gt; 0:\n            return True\n        else:\n            return False\n</code></pre>"},{"location":"reference/template_num/models_training/model_keras/#template_num.models_training.model_keras.ModelKeras.__init__","title":"<code>__init__(batch_size=64, epochs=99, validation_split=0.2, patience=5, keras_params=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass for more arguments)</p> Kwargs <p>batch_size (int): Batch size epochs (int): Number of epochs validation_split (float): Percentage for the validation set split     Only used if no input validation set when fitting patience (int): Early stopping patience keras_params (dict): Parameters used by Keras models.     e.g. learning_rate, nb_lstm_units, etc...     The purpose of this dictionary is for the user to use it as they wants in the _get_model function     This parameter was initially added in order to do an hyperparameters search</p> Source code in <code>template_num/models_training/model_keras.py</code> <pre><code>def __init__(self, batch_size: int = 64, epochs: int = 99, validation_split: float = 0.2,\n             patience: int = 5, keras_params: Union[dict, None] = None, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelClass for more arguments)\n\n    Kwargs:\n        batch_size (int): Batch size\n        epochs (int): Number of epochs\n        validation_split (float): Percentage for the validation set split\n            Only used if no input validation set when fitting\n        patience (int): Early stopping patience\n        keras_params (dict): Parameters used by Keras models.\n            e.g. learning_rate, nb_lstm_units, etc...\n            The purpose of this dictionary is for the user to use it as they wants in the _get_model function\n            This parameter was initially added in order to do an hyperparameters search\n    '''\n    # TODO: learning rate should be an attribute !\n    # Init.\n    super().__init__(**kwargs)\n\n    # Fix tensorflow GPU\n    gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n    for device in gpu_devices:\n        tf.config.experimental.set_memory_growth(device, True)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Param. model\n    self.batch_size = batch_size\n    self.epochs = epochs\n    self.validation_split = validation_split\n    self.patience = patience\n\n    # Model set on fit\n    self.model: Any = None\n\n    # Keras params\n    if keras_params is None:\n        keras_params = {}\n    self.keras_params = keras_params.copy()\n\n    # Keras custom objects : we get the ones specified in utils_deep_keras\n    self.custom_objects = utils_deep_keras.custom_objects\n</code></pre>"},{"location":"reference/template_num/models_training/model_keras/#template_num.models_training.model_keras.ModelKeras.fit","title":"<code>fit(x_train, y_train, x_valid=None, y_valid=None, with_shuffle=True, **kwargs)</code>","text":"<p>Fits the model</p> <p>Parameters:</p> Name Type Description Default <code>x_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <code>y_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_targets]</p> required <p>Kwargs:     x_valid (?): Array-like, shape = [n_samples, n_features]     y_valid (?): Array-like, shape = [n_samples, n_targets]     with_shuffle (bool): If x, y must be shuffled before fitting         This should be used if y is not shuffled as the split_validation takes the lines in order.         Thus, the validation set might get classes which are not in the train set ... Raises:     ValueError: If different classes when comparing an already fitted model and a new dataset</p> Source code in <code>template_num/models_training/model_keras.py</code> <pre><code>def fit(self, x_train, y_train, x_valid=None, y_valid=None, with_shuffle: bool = True, **kwargs) -&gt; None:\n    '''Fits the model\n\n    Args:\n        x_train (?): Array-like, shape = [n_samples, n_features]\n        y_train (?): Array-like, shape = [n_samples, n_targets]\n    Kwargs:\n        x_valid (?): Array-like, shape = [n_samples, n_features]\n        y_valid (?): Array-like, shape = [n_samples, n_targets]\n        with_shuffle (bool): If x, y must be shuffled before fitting\n            This should be used if y is not shuffled as the split_validation takes the lines in order.\n            Thus, the validation set might get classes which are not in the train set ...\n    Raises:\n        ValueError: If different classes when comparing an already fitted model and a new dataset\n    '''\n\n    ##############################################\n    # Manage retrain\n    ##############################################\n\n    # If a model has already been fitted, we make a new folder in order not to overwrite the existing one !\n    # And we save the old conf\n    if self.trained:\n        # Get src files to save\n        src_files = [os.path.join(self.model_dir, \"configurations.json\")]\n        if self.nb_fit &gt; 1:\n            for i in range(1, self.nb_fit):\n                src_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n        # Change model dir\n        self.model_dir = self._get_new_model_dir()\n        # Get dst files\n        dst_files = [os.path.join(self.model_dir, f\"configurations_fit_{self.nb_fit}.json\")]\n        if self.nb_fit &gt; 1:\n            for i in range(1, self.nb_fit):\n                dst_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n        # Copies\n        for src, dst in zip(src_files, dst_files):\n            try:\n                shutil.copyfile(src, dst)\n            except Exception as e:\n                self.logger.error(f\"Unable to copy {src} to {dst}\")\n                self.logger.error(\"We still go on\")\n                self.logger.error(repr(e))\n\n    ##############################################\n    # Prepare x_train, x_valid, y_train &amp; y_valid\n    # Also extract list of classes if classification\n    ##############################################\n\n    # Checking input formats\n    x_train, y_train = self._check_input_format(x_train, y_train, fit_function=True)\n    # If the validation set is present, we check its format (but with fit_function=False)\n    if y_valid is not None:\n        x_valid, y_valid = self._check_input_format(x_valid, y_valid, fit_function=False)\n\n    # If classification, we need to transform y\n    if self.model_type == 'classifier':\n        # if not multilabel, transform y_train as dummies (should already be the case for multi-labels)\n        if not self.multi_label:\n            # If len(array.shape)==2, we flatten the array if the second dimension is useless\n            if isinstance(y_train, np.ndarray) and len(y_train.shape) == 2 and y_train.shape[1] == 1:\n                y_train = np.ravel(y_train)\n            if isinstance(y_valid, np.ndarray) and len(y_valid.shape) == 2 and y_valid.shape[1] == 1:\n                y_valid = np.ravel(y_valid)\n            # Dummies transformation\n            y_train = pd.get_dummies(y_train).astype(int)\n            y_valid = pd.get_dummies(y_valid).astype(int) if y_valid is not None else None\n            # Important : get_dummies reorder the columns in alphabetical order\n            # Thus, there is no problem if we fit again on a new dataframe with shuffled data\n            list_classes = list(y_train.columns)\n            # FIX: valid test might miss some classes, hence we need to add them back to y_valid\n            if y_valid is not None and y_train.shape[1] != y_valid.shape[1]:\n                for cl in list_classes:\n                    # Add missing columns\n                    if cl not in y_valid.columns:\n                        y_valid[cl] = 0\n                y_valid = y_valid[list_classes]  # Reorder\n        # Else keep it as it is\n        else:\n            y_train = y_train\n            y_valid = y_valid\n            if hasattr(y_train, 'columns'):\n                # TODO : tmp mypy fix https://github.com/python/mypy/pull/13544\n                list_classes = list(y_train.columns)  # type: ignore\n            else:\n                self.logger.warning(\n                    \"Can't read the name of the columns of y_train -&gt; inverse transformation won't be possible\"\n                )\n                # We still create a list of classes in order to be compatible with other functions\n                list_classes = [str(_) for _ in range(pd.DataFrame(y_train).shape[1])]\n\n        # Set dict_classes based on list classes\n        dict_classes = {i: col for i, col in enumerate(list_classes)}\n\n        # Validate classes if already trained, else set them\n        if self.trained:\n            if self.list_classes != list_classes:\n                raise ValueError(\"Error: the new dataset does not match with the already fitted model\")\n            if self.dict_classes != dict_classes:\n                raise ValueError(\"Error: the new dataset does not match with the already fitted model\")\n        else:\n            self.list_classes = list_classes\n            self.dict_classes = dict_classes\n\n    # Shuffle x, y if wanted\n    # It is advised as validation_split from keras does not shufle the data\n    # Hence, for classificationt task, we might have classes in the validation data that we never met in the training data\n    if with_shuffle:\n        rng = np.random.RandomState(self.random_seed)\n        p = rng.permutation(len(x_train))\n        x_train = np.array(x_train)[p]\n        y_train = np.array(y_train)[p]\n    # Else still transform to numpy array\n    else:\n        x_train = np.array(x_train)\n        y_train = np.array(y_train)\n\n    # Also get y_valid as numpy &amp; get validation_data (tuple) if available\n    validation_data: Optional[tuple] = None  # Def. None if y_valid is None\n    if y_valid is not None:\n        x_valid = np.array(x_valid)\n        y_valid = np.array(y_valid)\n        validation_data = (x_valid, y_valid)\n\n    else:\n        x_train, x_valid,  y_train, y_valid = train_test_split(x_train, y_train, test_size=self.validation_split, \n                                                               random_state=self.random_seed)\n        validation_data = (x_valid, y_valid)\n\n    if validation_data is None:\n        self.logger.warning(f\"Warning, no validation set. The training set will be splitted (validation fraction = {self.validation_split})\")\n    ##############################################\n    # Fit\n    ##############################################\n\n    # Get model (if already fitted, _get_model returns instance model)\n    self.model = self._get_model()\n\n    # Get callbacks (early stopping &amp; checkpoint)\n    callbacks = self._get_callbacks()\n\n    # Create data generator\n    data_train_generator = RandomStateDataGenerator(x_train, y_train, self.batch_size, self.random_seed)\n    data_val_generator = RandomStateDataGenerator(x_valid, y_valid, self.batch_size, self.random_seed)\n\n    # Fit\n    # We use a try...except in order to save the model if an error arises\n    # after more than a minute into training\n    start_time = time.time()\n    try:\n        fit_history = self.model.fit(  # type: ignore\n            data_train_generator,\n            epochs=self.epochs,\n            validation_data=data_val_generator,\n            callbacks=callbacks,\n            verbose=1,\n            shuffle=False\n        )\n    except (RuntimeError, SystemError, SystemExit, EnvironmentError, KeyboardInterrupt, tf.errors.ResourceExhaustedError, tf.errors.InternalError,\n            tf.errors.UnavailableError, tf.errors.UnimplementedError, tf.errors.UnknownError, Exception) as e:\n        # Steps:\n        # 1. Display tensor flow error\n        # 2. Check if more than one minute elapsed &amp; existence best.hdf5\n        # 3. Reload best model\n        # 4. We consider that a fit occured (trained = True, nb_fit += 1)\n        # 5. Save &amp; create a warning file\n        # 6. Display error messages\n        # 7. Raise an error\n\n        # 1.\n        self.logger.error(repr(e))\n\n        # 2.\n        best_path = os.path.join(self.model_dir, 'best.hdf5')\n        time_spent = time.time() - start_time\n        if time_spent &gt;= 60 and os.path.exists(best_path):\n            # 3.\n            self.model = load_model(best_path, custom_objects=self.custom_objects)\n            # 4.\n            self.trained = True\n            self.nb_fit += 1\n            # 5.\n            self.save()\n            with open(os.path.join(self.model_dir, \"0_MODEL_INCOMPLETE\"), 'w'):\n                pass\n            with open(os.path.join(self.model_dir, \"1_TRAINING_NEEDS_TO_BE_RESUMED\"), 'w'):\n                pass\n            # 6.\n            self.logger.error(\"[EXPERIMENTAL] Error during model training\")\n            self.logger.error(f\"[EXPERIMENTAL] The error happened after {round(time_spent, 2)}s of training\")\n            self.logger.error(\"[EXPERIMENTAL] A saving of the model is done but this model won't be usable as is.\")\n            self.logger.error(f\"[EXPERIMENTAL] In order to resume the training, we have to specify this model ({ntpath.basename(self.model_dir)}) in the file 2_training.py\")\n            self.logger.error(\"[EXPERIMENTAL] Warning, the preprocessing is not saved in the configuration file\")\n            self.logger.error(\"[EXPERIMENTAL] Warning, the best model might be corrupted in some cases\")\n        # 7.\n        raise RuntimeError(\"Error during model training\")\n\n    # Print accuracy &amp; loss if level_save &gt; 'LOW'\n    if self.level_save in ['MEDIUM', 'HIGH']:\n        self._plot_metrics_and_loss(fit_history)\n        # Reload best model\n        self.model = load_model(\n            os.path.join(self.model_dir, 'best.hdf5'),\n            custom_objects=self.custom_objects\n        )\n\n    # Set trained\n    self.trained = True\n    self.nb_fit += 1\n</code></pre>"},{"location":"reference/template_num/models_training/model_keras/#template_num.models_training.model_keras.ModelKeras.predict","title":"<code>predict(x_test, return_proba=False, inference_batch_size=128, alternative_version=False, **kwargs)</code>","text":"<p>Predictions on test set</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>DataFrame</code> <p>DataFrame with the test data to be predicted</p> required <p>Kwargs:     return_proba (bool): If the function should return the probabilities instead of the classes     inference_batch_size (int): size (approximate) of batches     alternative_version (bool): If an alternative predict version (<code>tf.function</code> + <code>model.__call__</code>) must be used. Should be faster with low nb of inputs. Raises:     ValueError: If the model is not classifier and return_proba=True     ValueError: If the model is neither a classifier nor a regressor Returns:     (np.ndarray): Array         # If not return_proba, shape = [n_samples,] or [n_samples, n_classes]         # Else, shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/model_keras.py</code> <pre><code>@utils.trained_needed\ndef predict(self, x_test: pd.DataFrame, return_proba: bool = False, inference_batch_size: int = 128,\n            alternative_version: bool = False, **kwargs) -&gt; np.ndarray:\n    '''Predictions on test set\n\n    Args:\n        x_test (pd.DataFrame): DataFrame with the test data to be predicted\n    Kwargs:\n        return_proba (bool): If the function should return the probabilities instead of the classes\n        inference_batch_size (int): size (approximate) of batches\n        alternative_version (bool): If an alternative predict version (`tf.function` + `model.__call__`) must be used. Should be faster with low nb of inputs.\n    Raises:\n        ValueError: If the model is not classifier and return_proba=True\n        ValueError: If the model is neither a classifier nor a regressor\n    Returns:\n        (np.ndarray): Array\n            # If not return_proba, shape = [n_samples,] or [n_samples, n_classes]\n            # Else, shape = [n_samples, n_classes]\n    '''\n    # Manage errors\n    if return_proba and self.model_type != 'classifier':\n        raise ValueError(f\"Models of the type {self.model_type} can't handle probabilities\")\n\n    # We check input format\n    x_test, _ = self._check_input_format(x_test)\n\n    # Predict depends on model type\n    if self.model_type == 'classifier':\n        return self._predict_classifier(x_test, return_proba=return_proba, inference_batch_size=inference_batch_size,\n                                        alternative_version=alternative_version)\n    elif self.model_type == 'regressor':\n        return self._predict_regressor(x_test, inference_batch_size=inference_batch_size, alternative_version=alternative_version)\n    else:\n        raise ValueError(f\"The model type ({self.model_type}) must be 'classifier' or 'regressor'\")\n</code></pre>"},{"location":"reference/template_num/models_training/model_keras/#template_num.models_training.model_keras.ModelKeras.predict_proba","title":"<code>predict_proba(x_test, alternative_version=False, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>DataFrame</code> <p>Array-like, shape = [n_samples, n_features]</p> required <p>Kwargs:     alternative_version (bool): If an alternative predict version (<code>tf.function</code> + <code>model.__call__</code>) must be used. Should be faster with low nb of inputs. Raises:     ValueError: If model not classifier Returns:     (np.ndarray): Array, shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/model_keras.py</code> <pre><code>@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, alternative_version: bool = False, **kwargs) -&gt; np.ndarray:\n    '''Predicts the probabilities on the test set\n\n    Args:\n        x_test (pd.DataFrame): Array-like, shape = [n_samples, n_features]\n    Kwargs:\n        alternative_version (bool): If an alternative predict version (`tf.function` + `model.__call__`) must be used. Should be faster with low nb of inputs.\n    Raises:\n        ValueError: If model not classifier\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n    if self.model_type != 'classifier':\n        raise ValueError(f\"Models of type {self.model_type} do not implement the method predict_proba\")\n\n    # We check input format\n    x_test, _ = self._check_input_format(x_test)\n\n    # We use predict again\n    return self.predict(x_test, return_proba=True, alternative_version=alternative_version)\n</code></pre>"},{"location":"reference/template_num/models_training/model_keras/#template_num.models_training.model_keras.ModelKeras.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Needs to be overridden /! -</p> Source code in <code>template_num/models_training/model_keras.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n    '''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Needs to be overridden /!\\\\ -\n    '''\n    raise NotImplementedError(\"'reload_from_standalone' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_num/models_training/model_keras/#template_num.models_training.model_keras.ModelKeras.reload_model","title":"<code>reload_model(hdf5_path)</code>","text":"<p>Loads a Keras model from a HDF5 file</p> <p>Parameters:</p> Name Type Description Default <code>hdf5_path</code> <code>str</code> <p>Path to the hdf5 file</p> required <p>Returns:     ?: Keras model</p> Source code in <code>template_num/models_training/model_keras.py</code> <pre><code>def reload_model(self, hdf5_path: str) -&gt; Any:\n    '''Loads a Keras model from a HDF5 file\n\n    Args:\n        hdf5_path (str): Path to the hdf5 file\n    Returns:\n        ?: Keras model\n    '''\n    # Fix tensorflow GPU if not already done (useful if we reload a model)\n    try:\n        gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n        for device in gpu_devices:\n            tf.config.experimental.set_memory_growth(device, True)\n    except Exception:\n        pass\n\n    # We check if we already have the custom objects\n    if hasattr(self, 'custom_objects') and self.custom_objects is not None:\n        custom_objects = self.custom_objects\n    else:\n        self.logger.warning(\"Can't find the attribute 'custom_objects' in the model to be reloaded\")\n        self.logger.warning(\"Backup on the default custom_objects of utils_deep_keras\")\n        custom_objects = utils_deep_keras.custom_objects\n\n    # Loading of the model\n    keras_model = load_model(hdf5_path, custom_objects=custom_objects)\n\n    # Set trained to true if not already true\n    if not self.trained:\n        self.trained = True\n        self.nb_fit = 1\n\n    # Return\n    return keras_model\n</code></pre>"},{"location":"reference/template_num/models_training/model_keras/#template_num.models_training.model_keras.ModelKeras.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/model_keras.py</code> <pre><code>@no_type_check  # We do not check the type, because it is complicated with managing custom_objects_str\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save configuration JSON\n    if json_data is None:\n        json_data = {}\n\n    json_data['librairie'] = 'keras'\n    json_data['batch_size'] = self.batch_size\n    json_data['epochs'] = self.epochs\n    json_data['validation_split'] = self.validation_split\n    json_data['patience'] = self.patience\n    json_data['keras_params'] = self.keras_params\n    if self.model is not None:\n        json_data['keras_model'] = json.loads(self.model.to_json())\n    else:\n        json_data['keras_model'] = None\n\n    # Add _get_model code if not in json_data\n    if '_get_model' not in json_data.keys():\n        json_data['_get_model'] = pickle.source.getsourcelines(self._get_model)[0]\n    # Add _get_learning_rate_scheduler code if not in json_data\n    if '_get_learning_rate_scheduler' not in json_data.keys():\n        json_data['_get_learning_rate_scheduler'] = pickle.source.getsourcelines(self._get_learning_rate_scheduler)[0]\n    # Add custom_objects code if not in json_data\n    if 'custom_objects' not in json_data.keys():\n        custom_objects_str = self.custom_objects.copy()\n        for key in custom_objects_str.keys():\n            if callable(custom_objects_str[key]):\n                # Nominal case\n                if not isinstance(custom_objects_str[key], functools.partial):\n                    custom_objects_str[key] = pickle.source.getsourcelines(custom_objects_str[key])[0]\n                # Manage partials\n                else:\n                    custom_objects_str[key] = {\n                        'type': 'partial',\n                        'args': custom_objects_str[key].args,\n                        'function': pickle.source.getsourcelines(custom_objects_str[key].func)[0],\n                    }\n        json_data['custom_objects'] = custom_objects_str\n\n    # Save strategy :\n    # - best.hdf5 already saved in fit()\n    # - can't pickle keras model, so we drop it, save, and reload it\n    keras_model = self.model\n    self.model = None\n    super().save(json_data=json_data)\n    self.model = keras_model\n</code></pre>"},{"location":"reference/template_num/models_training/model_keras/#template_num.models_training.model_keras.RandomStateDataGenerator","title":"<code>RandomStateDataGenerator</code>","text":"<p>             Bases: <code>Sequence</code></p> <p>Custom data generator to control batch randomness with random_state</p> Source code in <code>template_num/models_training/model_keras.py</code> <pre><code>class RandomStateDataGenerator(Sequence):\n    '''Custom data generator to control batch randomness with random_state'''\n\n    def __init__(self, x_train: np.ndarray, y_train: np.ndarray, batch_size: int = 32,\n                  random_seed: Union[int, None] = None):\n        '''Initialization of the class\n        Args:\n            x_train (ndarray): training features\n            y_train (ndarray): training outputs\n            batch_size (int): Batch size\n            random_seed (int or None): seed to use for random_state initialization\n        '''\n        self.x = x_train\n        self.y = y_train\n        self.batch_size = batch_size\n        self.random_state = np.random.RandomState(seed=random_seed)\n        self.indices = shuffle(np.arange(len(self.x)), random_state=self.random_state)\n\n\n    def __len__(self):\n        return int(np.ceil(len(self.x) / self.batch_size))\n\n\n    def __getitem__(self, index):\n        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n        batch_x = self.x[batch_indices]\n        batch_y = self.y[batch_indices]\n        return np.array(batch_x), np.array(batch_y)\n\n\n    def on_epoch_end(self):\n        self.indices = shuffle(np.arange(len(self.x)), random_state=self.random_state)\n</code></pre>"},{"location":"reference/template_num/models_training/model_keras/#template_num.models_training.model_keras.RandomStateDataGenerator.__init__","title":"<code>__init__(x_train, y_train, batch_size=32, random_seed=None)</code>","text":"<p>Initialization of the class Args:     x_train (ndarray): training features     y_train (ndarray): training outputs     batch_size (int): Batch size     random_seed (int or None): seed to use for random_state initialization</p> Source code in <code>template_num/models_training/model_keras.py</code> <pre><code>def __init__(self, x_train: np.ndarray, y_train: np.ndarray, batch_size: int = 32,\n              random_seed: Union[int, None] = None):\n    '''Initialization of the class\n    Args:\n        x_train (ndarray): training features\n        y_train (ndarray): training outputs\n        batch_size (int): Batch size\n        random_seed (int or None): seed to use for random_state initialization\n    '''\n    self.x = x_train\n    self.y = y_train\n    self.batch_size = batch_size\n    self.random_state = np.random.RandomState(seed=random_seed)\n    self.indices = shuffle(np.arange(len(self.x)), random_state=self.random_state)\n</code></pre>"},{"location":"reference/template_num/models_training/model_pipeline/","title":"Model pipeline","text":""},{"location":"reference/template_num/models_training/model_pipeline/#template_num.models_training.model_pipeline.ModelPipeline","title":"<code>ModelPipeline</code>","text":"<p>             Bases: <code>ModelClass</code></p> <p>Generic model for sklearn pipeline</p> Source code in <code>template_num/models_training/model_pipeline.py</code> <pre><code>class ModelPipeline(ModelClass):\n    '''Generic model for sklearn pipeline'''\n\n    _default_name = 'model_pipeline'\n\n    # Not implemented :\n    # -&gt; reload_from_standalone\n\n    def __init__(self, pipeline: Union[Pipeline, None] = None, **kwargs) -&gt; None:\n        '''Class initialization (see ModelClass for more arguments)\n\n        Kwargs:\n            pipeline (Pipeline): Pipeline to use\n        '''\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Manage model (to implement for children class)\n        self.pipeline = pipeline\n\n    def fit(self, x_train, y_train, **kwargs) -&gt; None:\n        '''Trains the model\n           **kwargs permits compatibility with Keras model\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n            y_train (?): Array-like, shape = [n_samples, n_targets]\n        Raises:\n            RuntimeError: If the model is already fitted\n            ValueError: If the type of model is not classifier or regressor\n        '''\n        if self.trained:\n            self.logger.error(\"We can't train again a pipeline sklearn model\")\n            self.logger.error(\"Please train a new model\")\n            raise RuntimeError(\"We can't train again a pipeline sklearn model\")\n\n        # We check input format\n        x_train, y_train = self._check_input_format(x_train, y_train, fit_function=True)\n\n        if self.model_type == 'classifier':\n            self._fit_classifier(x_train, y_train, **kwargs)\n        elif self.model_type == 'regressor':\n            self._fit_regressor(x_train, y_train, **kwargs)\n        else:\n            raise ValueError(f\"The model type ({self.model_type}) must be 'classifier' or 'regressor'\")\n\n    def _fit_classifier(self, x_train, y_train, **kwargs) -&gt; None:\n        '''Training of a model of model type classifier\n           **kwargs permits compatibility with Keras model\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n            y_train (?): Array-like, shape = [n_samples, n_targets]\n        '''\n        # We \"only\" check if no multi-classes multi-labels (which can't be managed by most SKLEARN pipelines)\n        if self.multi_label:\n            df_tmp = pd.DataFrame(y_train)\n            for col in df_tmp:\n                uniques = df_tmp[col].unique()\n                if len(uniques) &gt; 2:\n                    self.logger.warning(' - /!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\ - ')\n                    self.logger.warning(\"Most sklearn pipelines can't manage multi-classes/multi-labels\")\n                    self.logger.warning(' - /!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\ - ')\n                    # We \"let\" the process crash by itself\n                    break\n\n        # Fit pipeline\n        self.pipeline.fit(x_train, y_train)\n\n        # Set list classes\n        if not self.multi_label:\n            self.list_classes = list(self.pipeline.classes_)\n        else:\n            if hasattr(y_train, 'columns'):\n                self.list_classes = list(y_train.columns)\n            else:\n                self.logger.warning(\n                    \"Can't read the name of the columns of y_train -&gt; inverse transformation won't be possible\"\n                )\n                # We still create a list of classes in order to be compatible with other functions\n                self.list_classes = [str(_) for _ in range(pd.DataFrame(y_train).shape[1])]\n\n        # Set dict_classes based on list classes\n        self.dict_classes = {i: col for i, col in enumerate(self.list_classes)}\n\n        # Set trained\n        self.trained = True\n        self.nb_fit += 1\n\n    def _fit_regressor(self, x_train, y_train, **kwargs) -&gt; None:\n        '''Training of a regressor model\n           **kwargs permits compatibility with Keras model\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n            y_train (?): Array-like, shape = [n_samples, n_targets]\n        '''\n        # Fit pipeline\n        self.pipeline.fit(x_train, y_train)\n\n        # Set trained\n        self.trained = True\n        self.nb_fit += 1\n\n    @utils.trained_needed\n    def predict(self, x_test: pd.DataFrame, return_proba: bool = False, **kwargs) -&gt; np.ndarray:\n        '''Predictions on test set\n\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Kwargs:\n            return_proba (bool): If the function should return the probabilities instead of the classes (Keras compatibility)\n        Raises:\n            ValueError: If the model is not classifier and return_proba=True\n        Returns:\n            (np.ndarray): Array\n                # If not return_proba, shape = [n_samples,] or [n_samples, n_classes]\n                # Else, shape = [n_samples, n_classes]\n        '''\n        # Manage errors\n        if return_proba is True and self.model_type != 'classifier':\n            raise ValueError(f\"Models of the type {self.model_type} can't handle probabilities\")\n\n        # We check input format\n        x_test, _ = self._check_input_format(x_test)\n\n        #\n        if not return_proba:\n            return np.array(self.pipeline.predict(x_test))\n        else:\n            return self.predict_proba(x_test)\n\n    @utils.trained_needed\n    def predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n        '''Predicts the probabilities on the test set - Classifier only\n\n        Args:\n            x_test (pd.DataFrame): Array-like, shape = [n_samples, n_features]\n        Raises:\n            ValueError: If model not classifier\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n        if self.model_type != 'classifier':\n            raise ValueError(f\"Models of type {self.model_type} do not implement the method predict_proba\")\n\n        # We check input format\n        x_test, _ = self._check_input_format(x_test)\n\n        #\n        probas = np.array(self.pipeline.predict_proba(x_test))\n        # Very specific fix: in some cases, with OvR, strategy, all estimators return 0, which generates a division per 0 when normalizing\n        # Hence, we replace NaNs with 1 / nb_classes\n        if not np.isnan(probas).any():\n            probas = np.nan_to_num(probas, nan=1/len(self.list_classes))\n\n        # If use of MultiOutputClassifier -&gt;  returns probabilities of 0 and 1 for all elements and all classes\n        # Same thing for some base models\n        # Correction in case where we detect a shape of length &gt; 2 (ie. equals to 3)\n        # Reminder : we do not manage multi-labels/multi-classes\n        if len(probas.shape) &gt; 2:\n            probas = np.swapaxes(probas[:, :, 1], 0, 1)\n        return probas\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save model\n        if json_data is None:\n            json_data = {}\n\n        json_data['librairie'] = 'scikit-learn'\n\n        # Add each pipeline steps' conf\n        if self.pipeline is not None:\n            for step in self.pipeline.steps:\n                name = step[0]\n                confs = step[1].get_params()\n                # Get rid of some non serializable conf\n                for special_conf in ['dtype', 'base_estimator', 'estimator', 'estimator__base_estimator',\n                                     'estimator__estimator', 'estimator__estimator__base_estimator']:\n                    if special_conf in confs.keys():\n                        confs[special_conf] = str(confs[special_conf])\n                json_data[f'{name}_confs'] = confs\n\n        # Save\n        super().save(json_data=json_data)\n\n        # Save model standalone if wanted &amp; pipeline is not None &amp; level_save &gt; 'LOW'\n        if self.pipeline is not None and self.level_save in ['MEDIUM', 'HIGH']:\n            pkl_path = os.path.join(self.model_dir, \"sklearn_pipeline_standalone.pkl\")\n            # Save model\n            with open(pkl_path, 'wb') as f:\n                pickle.dump(self.pipeline, f)\n\n    def reload_from_standalone(self, **kwargs) -&gt; None:\n        '''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Needs to be overridden /!\\\\ -\n        '''\n        raise NotImplementedError(\"'reload_from_standalone' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_num/models_training/model_pipeline/#template_num.models_training.model_pipeline.ModelPipeline.__init__","title":"<code>__init__(pipeline=None, **kwargs)</code>","text":"<p>Class initialization (see ModelClass for more arguments)</p> Kwargs <p>pipeline (Pipeline): Pipeline to use</p> Source code in <code>template_num/models_training/model_pipeline.py</code> <pre><code>def __init__(self, pipeline: Union[Pipeline, None] = None, **kwargs) -&gt; None:\n    '''Class initialization (see ModelClass for more arguments)\n\n    Kwargs:\n        pipeline (Pipeline): Pipeline to use\n    '''\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Manage model (to implement for children class)\n    self.pipeline = pipeline\n</code></pre>"},{"location":"reference/template_num/models_training/model_pipeline/#template_num.models_training.model_pipeline.ModelPipeline.fit","title":"<code>fit(x_train, y_train, **kwargs)</code>","text":"<p>Trains the model    **kwargs permits compatibility with Keras model Args:     x_train (?): Array-like, shape = [n_samples, n_features]     y_train (?): Array-like, shape = [n_samples, n_targets] Raises:     RuntimeError: If the model is already fitted     ValueError: If the type of model is not classifier or regressor</p> Source code in <code>template_num/models_training/model_pipeline.py</code> <pre><code>def fit(self, x_train, y_train, **kwargs) -&gt; None:\n    '''Trains the model\n       **kwargs permits compatibility with Keras model\n    Args:\n        x_train (?): Array-like, shape = [n_samples, n_features]\n        y_train (?): Array-like, shape = [n_samples, n_targets]\n    Raises:\n        RuntimeError: If the model is already fitted\n        ValueError: If the type of model is not classifier or regressor\n    '''\n    if self.trained:\n        self.logger.error(\"We can't train again a pipeline sklearn model\")\n        self.logger.error(\"Please train a new model\")\n        raise RuntimeError(\"We can't train again a pipeline sklearn model\")\n\n    # We check input format\n    x_train, y_train = self._check_input_format(x_train, y_train, fit_function=True)\n\n    if self.model_type == 'classifier':\n        self._fit_classifier(x_train, y_train, **kwargs)\n    elif self.model_type == 'regressor':\n        self._fit_regressor(x_train, y_train, **kwargs)\n    else:\n        raise ValueError(f\"The model type ({self.model_type}) must be 'classifier' or 'regressor'\")\n</code></pre>"},{"location":"reference/template_num/models_training/model_pipeline/#template_num.models_training.model_pipeline.ModelPipeline.predict","title":"<code>predict(x_test, return_proba=False, **kwargs)</code>","text":"<p>Predictions on test set</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>DataFrame</code> <p>DataFrame with the test data to be predicted</p> required <p>Kwargs:     return_proba (bool): If the function should return the probabilities instead of the classes (Keras compatibility) Raises:     ValueError: If the model is not classifier and return_proba=True Returns:     (np.ndarray): Array         # If not return_proba, shape = [n_samples,] or [n_samples, n_classes]         # Else, shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/model_pipeline.py</code> <pre><code>@utils.trained_needed\ndef predict(self, x_test: pd.DataFrame, return_proba: bool = False, **kwargs) -&gt; np.ndarray:\n    '''Predictions on test set\n\n    Args:\n        x_test (pd.DataFrame): DataFrame with the test data to be predicted\n    Kwargs:\n        return_proba (bool): If the function should return the probabilities instead of the classes (Keras compatibility)\n    Raises:\n        ValueError: If the model is not classifier and return_proba=True\n    Returns:\n        (np.ndarray): Array\n            # If not return_proba, shape = [n_samples,] or [n_samples, n_classes]\n            # Else, shape = [n_samples, n_classes]\n    '''\n    # Manage errors\n    if return_proba is True and self.model_type != 'classifier':\n        raise ValueError(f\"Models of the type {self.model_type} can't handle probabilities\")\n\n    # We check input format\n    x_test, _ = self._check_input_format(x_test)\n\n    #\n    if not return_proba:\n        return np.array(self.pipeline.predict(x_test))\n    else:\n        return self.predict_proba(x_test)\n</code></pre>"},{"location":"reference/template_num/models_training/model_pipeline/#template_num.models_training.model_pipeline.ModelPipeline.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set - Classifier only</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>DataFrame</code> <p>Array-like, shape = [n_samples, n_features]</p> required <p>Raises:     ValueError: If model not classifier Returns:     (np.ndarray): Array, shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/model_pipeline.py</code> <pre><code>@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n    '''Predicts the probabilities on the test set - Classifier only\n\n    Args:\n        x_test (pd.DataFrame): Array-like, shape = [n_samples, n_features]\n    Raises:\n        ValueError: If model not classifier\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n    if self.model_type != 'classifier':\n        raise ValueError(f\"Models of type {self.model_type} do not implement the method predict_proba\")\n\n    # We check input format\n    x_test, _ = self._check_input_format(x_test)\n\n    #\n    probas = np.array(self.pipeline.predict_proba(x_test))\n    # Very specific fix: in some cases, with OvR, strategy, all estimators return 0, which generates a division per 0 when normalizing\n    # Hence, we replace NaNs with 1 / nb_classes\n    if not np.isnan(probas).any():\n        probas = np.nan_to_num(probas, nan=1/len(self.list_classes))\n\n    # If use of MultiOutputClassifier -&gt;  returns probabilities of 0 and 1 for all elements and all classes\n    # Same thing for some base models\n    # Correction in case where we detect a shape of length &gt; 2 (ie. equals to 3)\n    # Reminder : we do not manage multi-labels/multi-classes\n    if len(probas.shape) &gt; 2:\n        probas = np.swapaxes(probas[:, :, 1], 0, 1)\n    return probas\n</code></pre>"},{"location":"reference/template_num/models_training/model_pipeline/#template_num.models_training.model_pipeline.ModelPipeline.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Needs to be overridden /! -</p> Source code in <code>template_num/models_training/model_pipeline.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n    '''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Needs to be overridden /!\\\\ -\n    '''\n    raise NotImplementedError(\"'reload_from_standalone' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_num/models_training/model_pipeline/#template_num.models_training.model_pipeline.ModelPipeline.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/model_pipeline.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save model\n    if json_data is None:\n        json_data = {}\n\n    json_data['librairie'] = 'scikit-learn'\n\n    # Add each pipeline steps' conf\n    if self.pipeline is not None:\n        for step in self.pipeline.steps:\n            name = step[0]\n            confs = step[1].get_params()\n            # Get rid of some non serializable conf\n            for special_conf in ['dtype', 'base_estimator', 'estimator', 'estimator__base_estimator',\n                                 'estimator__estimator', 'estimator__estimator__base_estimator']:\n                if special_conf in confs.keys():\n                    confs[special_conf] = str(confs[special_conf])\n            json_data[f'{name}_confs'] = confs\n\n    # Save\n    super().save(json_data=json_data)\n\n    # Save model standalone if wanted &amp; pipeline is not None &amp; level_save &gt; 'LOW'\n    if self.pipeline is not None and self.level_save in ['MEDIUM', 'HIGH']:\n        pkl_path = os.path.join(self.model_dir, \"sklearn_pipeline_standalone.pkl\")\n        # Save model\n        with open(pkl_path, 'wb') as f:\n            pickle.dump(self.pipeline, f)\n</code></pre>"},{"location":"reference/template_num/models_training/utils_deep_keras/","title":"Utils deep keras","text":""},{"location":"reference/template_num/models_training/utils_deep_keras/#template_num.models_training.utils_deep_keras.compare_keras_models","title":"<code>compare_keras_models(model1, model2)</code>","text":"<p>Checks if all weights of each keras model layer are the same</p> Source code in <code>template_num/models_training/utils_deep_keras.py</code> <pre><code>def compare_keras_models(model1, model2):\n    ''' Checks if all weights of each keras model layer are the same\n    '''\n    for layer1, layer2 in zip(model1.layers, model2.layers):\n        l1 = layer1.get_weights()\n        l2 = layer2.get_weights()\n        if not all(np.array_equal(weights1, weights2) for weights1, weights2 in zip(l1, l2)):\n            return False\n    return True\n</code></pre>"},{"location":"reference/template_num/models_training/utils_deep_keras/#template_num.models_training.utils_deep_keras.f1","title":"<code>f1(y_true, y_pred)</code>","text":"<p>f1 score, to use as custom metrics</p> <ul> <li>/! To use with a big batch size /! -</li> </ul> From <p>https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth values</p> required <code>y_pred</code> <p>The predicted values</p> required <p>Returns:     float: metric</p> Source code in <code>template_num/models_training/utils_deep_keras.py</code> <pre><code>def f1(y_true, y_pred) -&gt; float:\n    '''f1 score, to use as custom metrics\n\n    - /!\\\\ To use with a big batch size /!\\\\ -\n\n    From:\n        https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n        https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras\n\n    Args:\n        y_true: Ground truth values\n        y_pred: The predicted values\n    Returns:\n        float: metric\n    '''\n    # Round pred to 0 &amp; 1\n    y_pred = K.round(y_pred)\n    y_true = K.cast(y_true, 'float32')  # Fix : TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.\n\n    ground_positives = K.sum(y_true, axis=0) + K.epsilon()  # We add an epsilon -&gt; manage the case where a class is absent in the batch\n\n    y_pred = K.round(y_pred)\n    tp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n    # tn = K.sum(K.cast((1 - y_true) * (1 - y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\n\n    p = tp / (tp + fp + K.epsilon())\n    r = tp / (tp + fn + K.epsilon())\n\n    f1 = 2 * p * r / (p + r + K.epsilon())\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n\n    weighted_f1 = f1 * ground_positives / K.sum(ground_positives)\n    weighted_f1 = K.sum(weighted_f1)\n\n    return weighted_f1\n</code></pre>"},{"location":"reference/template_num/models_training/utils_deep_keras/#template_num.models_training.utils_deep_keras.f1_loss","title":"<code>f1_loss(y_true, y_pred)</code>","text":"<p>f1 loss, to use as custom loss</p> <ul> <li>/! To use with a big batch size /! -</li> </ul> From <p>https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth values</p> required <code>y_pred</code> <p>The predicted values</p> required <p>Returns:     float: metric</p> Source code in <code>template_num/models_training/utils_deep_keras.py</code> <pre><code>def f1_loss(y_true, y_pred) -&gt; float:\n    '''f1 loss, to use as custom loss\n\n    - /!\\\\ To use with a big batch size /!\\\\ -\n\n    From:\n        https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n        https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras\n\n    Args:\n        y_true: Ground truth values\n        y_pred: The predicted values\n    Returns:\n        float: metric\n    '''\n    # TODO : Find a mean of rounding y_pred\n    # TODO : Problem : models will quickly converge on probabilities 1.0 &amp; 0.0 to optimize this loss....\n    # We can't round here :(\n    # Please make sure that all of your ops have a gradient defined (i.e. are differentiable).\n    # Common ops without gradient: K.argmax, K.round, K.eval.\n    y_true = K.cast(y_true, 'float32')  # Fix : TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.\n\n    ground_positives = K.sum(y_true, axis=0) + K.epsilon()  # We add an epsilon -&gt; manage the case where a class is absent in the batch\n\n    tp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n    # tn = K.sum(K.cast((1 - y_true) * (1 - y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\n\n    p = tp / (tp + fp + K.epsilon())\n    r = tp / (tp + fn + K.epsilon())\n\n    f1 = 2 * p * r / (p + r + K.epsilon())\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n\n    weighted_f1 = f1 * ground_positives / K.sum(ground_positives)\n    weighted_f1 = K.sum(weighted_f1)\n\n    return 1 - weighted_f1\n</code></pre>"},{"location":"reference/template_num/models_training/utils_deep_keras/#template_num.models_training.utils_deep_keras.fb_loss","title":"<code>fb_loss(b, y_true, y_pred)</code>","text":"<p>fB loss, to use as custom loss</p> <ul> <li>/! To use with a big batch size /! -</li> </ul> From <p>https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>float</code> <p>importance recall in the calculation of the fB score</p> required <code>y_true</code> <p>Ground truth values</p> required <code>y_pred</code> <p>The predicted values</p> required <p>Returns:     float: metric</p> Source code in <code>template_num/models_training/utils_deep_keras.py</code> <pre><code>def fb_loss(b: float, y_true, y_pred) -&gt; float:\n    '''fB loss, to use as custom loss\n\n    - /!\\\\ To use with a big batch size /!\\\\ -\n\n    From:\n        https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n        https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras\n\n    Args:\n        b (float): importance recall in the calculation of the fB score\n        y_true: Ground truth values\n        y_pred: The predicted values\n    Returns:\n        float: metric\n    '''\n    # TODO : Find a mean of rounding y_pred\n    # TODO : Problem : models will quickly converge on probabilities 1.0 &amp; 0.0 to optimize this loss....\n    # We can't round here :(\n    # Please make sure that all of your ops have a gradient defined (i.e. are differentiable).\n    # Common ops without gradient: K.argmax, K.round, K.eval.\n    y_true = K.cast(y_true, 'float32')  # Fix : TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.\n\n    ground_positives = K.sum(y_true, axis=0) + K.epsilon()  # We add an epsilon -&gt; manage the case where a class is absent in the batch\n\n    tp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n    # tn = K.sum(K.cast((1 - y_true) * (1 - y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\n\n    p = tp / (tp + fp + K.epsilon())\n    r = tp / (tp + fn + K.epsilon())\n\n    fb = (1 + b**2) * p * r / ((p * b**2) + r + K.epsilon())\n    fb = tf.where(tf.math.is_nan(fb), tf.zeros_like(fb), fb)\n\n    weighted_fb = fb * ground_positives / K.sum(ground_positives)\n    weighted_fb = K.sum(weighted_fb)\n\n    return 1 - weighted_fb\n</code></pre>"},{"location":"reference/template_num/models_training/utils_deep_keras/#template_num.models_training.utils_deep_keras.get_fb_loss","title":"<code>get_fb_loss(b=2.0)</code>","text":"<p>Gets a fB-score loss</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>float</code> <p>importance recall in the calculation of the fB score</p> <code>2.0</code> <p>Returns:     Callable: fb_loss</p> Source code in <code>template_num/models_training/utils_deep_keras.py</code> <pre><code>def get_fb_loss(b: float = 2.0) -&gt; Callable:\n    ''' Gets a fB-score loss\n\n    Args:\n        b (float): importance recall in the calculation of the fB score\n    Returns:\n        Callable: fb_loss\n    '''\n    # - /!\\ Use of partial mandatory in order to be able to pickle dynamical functions ! /!\\ -\n    fn = partial(fb_loss, b)\n    # FIX:  AttributeError: 'functools.partial' object has no attribute '__name__'\n    fn.__name__ = 'fb_loss'  # type: ignore\n    return fn\n</code></pre>"},{"location":"reference/template_num/models_training/utils_deep_keras/#template_num.models_training.utils_deep_keras.get_weighted_binary_crossentropy","title":"<code>get_weighted_binary_crossentropy(pos_weight=10.0)</code>","text":"<p>Gets a \"weighted binary crossentropy\" loss From https://stats.stackexchange.com/questions/261128/neural-network-for-multi-label-classification-with-large-number-of-classes-outpu TO BE ADDED IN custom_objects : 'weighted_binary_crossentropy': utils_deep_keras.get_weighted_binary_crossentropy(pos_weight=...)</p> <p>Parameters:</p> Name Type Description Default <code>pos_weight</code> <code>float</code> <p>Weight of the positive class, to be tuned</p> <code>10.0</code> <p>Returns:     Callable: Weighted binary crossentropy loss</p> Source code in <code>template_num/models_training/utils_deep_keras.py</code> <pre><code>def get_weighted_binary_crossentropy(pos_weight: float = 10.0) -&gt; Callable:\n    ''' Gets a \"weighted binary crossentropy\" loss\n    From https://stats.stackexchange.com/questions/261128/neural-network-for-multi-label-classification-with-large-number-of-classes-outpu\n    TO BE ADDED IN custom_objects : 'weighted_binary_crossentropy': utils_deep_keras.get_weighted_binary_crossentropy(pos_weight=...)\n\n    Args:\n        pos_weight (float): Weight of the positive class, to be tuned\n    Returns:\n        Callable: Weighted binary crossentropy loss\n    '''\n    # - /!\\ Use of partial mandatory in order to be able to pickle dynamical functions ! /!\\ -\n    fn = partial(weighted_binary_crossentropy, pos_weight)\n    # FIX:  AttributeError: 'functools.partial' object has no attribute '__name__'\n    fn.__name__ = 'weighted_binary_crossentropy'  # type: ignore\n    return fn\n</code></pre>"},{"location":"reference/template_num/models_training/utils_deep_keras/#template_num.models_training.utils_deep_keras.precision","title":"<code>precision(y_true, y_pred)</code>","text":"<p>Precision, to use as custom metrics</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth values</p> required <code>y_pred</code> <p>The predicted values</p> required <p>Returns:     float: metric</p> Source code in <code>template_num/models_training/utils_deep_keras.py</code> <pre><code>def precision(y_true, y_pred) -&gt; float:\n    '''Precision, to use as custom metrics\n\n    Args:\n        y_true: Ground truth values\n        y_pred: The predicted values\n    Returns:\n        float: metric\n    '''\n    y_pred = K.round(y_pred)\n    y_true = K.cast(y_true, 'float32')  # Fix : TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.\n\n    ground_positives = K.sum(y_true, axis=0) + K.epsilon()  # We add an epsilon -&gt; manage the case where a class is absent in the batch\n\n    tp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n    fp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\n\n    precision = tp / (tp + fp + K.epsilon())\n    precision = tf.where(tf.math.is_nan(precision), tf.zeros_like(precision), precision)\n\n    weighted_precision = precision * ground_positives / K.sum(ground_positives)\n    weighted_precision = K.sum(weighted_precision)\n\n    return weighted_precision\n</code></pre>"},{"location":"reference/template_num/models_training/utils_deep_keras/#template_num.models_training.utils_deep_keras.recall","title":"<code>recall(y_true, y_pred)</code>","text":"<p>Recall to use as a custom metrics</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth values</p> required <code>y_pred</code> <p>The predicted values</p> required <p>Returns:     float: metric</p> Source code in <code>template_num/models_training/utils_deep_keras.py</code> <pre><code>def recall(y_true, y_pred) -&gt; float:\n    '''Recall to use as a custom metrics\n\n    Args:\n        y_true: Ground truth values\n        y_pred: The predicted values\n    Returns:\n        float: metric\n    '''\n    y_pred = K.round(y_pred)\n    y_true = K.cast(y_true, 'float32')  # Fix : TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.\n\n    ground_positives = K.sum(y_true, axis=0) + K.epsilon()  # We add an epsilon -&gt; manage the case where a class is absent in the batch\n\n    tp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\n\n    recall = tp / (tp + fn + K.epsilon())\n    recall = tf.where(tf.math.is_nan(recall), tf.zeros_like(recall), recall)\n\n    weighted_recall = recall * ground_positives / K.sum(ground_positives)\n    weighted_recall = K.sum(weighted_recall)\n\n    return weighted_recall\n</code></pre>"},{"location":"reference/template_num/models_training/utils_deep_keras/#template_num.models_training.utils_deep_keras.root_mean_squared_error","title":"<code>root_mean_squared_error(y_true, y_pred)</code>","text":"<p>RMSE, to use as custom metrics</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth values</p> required <code>y_pred</code> <p>The predicted values</p> required <p>Returns:     float: metric</p> Source code in <code>template_num/models_training/utils_deep_keras.py</code> <pre><code>def root_mean_squared_error(y_true, y_pred) -&gt; float:\n    '''RMSE, to use as custom metrics\n\n    Args:\n        y_true: Ground truth values\n        y_pred: The predicted values\n    Returns:\n        float: metric\n    '''\n    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n</code></pre>"},{"location":"reference/template_num/models_training/utils_deep_keras/#template_num.models_training.utils_deep_keras.weighted_binary_crossentropy","title":"<code>weighted_binary_crossentropy(pos_weight, target, output)</code>","text":"<p>Weighted binary crossentropy between an output tensor and a target tensor. pos_weight is used as a multiplier for the positive targets.</p> <p>Combination of the following functions: * keras.losses.binary_crossentropy * keras.backend.tensorflow_backend.binary_crossentropy * tf.nn.weighted_cross_entropy_with_logits</p> <p>Parameters:</p> Name Type Description Default <code>pos_weight</code> <code>float</code> <p>poid classe positive, to be tuned</p> required <code>target</code> <p>Target tensor</p> required <code>output</code> <p>Output tensor</p> required <p>Returns:     float: metric</p> Source code in <code>template_num/models_training/utils_deep_keras.py</code> <pre><code>def weighted_binary_crossentropy(pos_weight: float, target, output) -&gt; float:\n    '''\n    Weighted binary crossentropy between an output tensor\n    and a target tensor. pos_weight is used as a multiplier\n    for the positive targets.\n\n    Combination of the following functions:\n    * keras.losses.binary_crossentropy\n    * keras.backend.tensorflow_backend.binary_crossentropy\n    * tf.nn.weighted_cross_entropy_with_logits\n\n    Args:\n        pos_weight (float): poid classe positive, to be tuned\n        target: Target tensor\n        output: Output tensor\n    Returns:\n        float: metric\n    '''\n    target = K.cast(target, 'float32')\n    output = K.cast(output, 'float32')\n    # transform back to logits\n    _epsilon = tf.convert_to_tensor(K.epsilon(), output.dtype.base_dtype)\n    output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n    output = tf.math.log(output / (1 - output))\n    # compute weighted loss\n    loss = tf.nn.weighted_cross_entropy_with_logits(target, output, pos_weight=pos_weight)\n    return tf.reduce_mean(loss, axis=-1)\n</code></pre>"},{"location":"reference/template_num/models_training/utils_models/","title":"Utils models","text":""},{"location":"reference/template_num/models_training/utils_models/#template_num.models_training.utils_models.apply_pipeline","title":"<code>apply_pipeline(df, preprocess_pipeline)</code>","text":"<p>Applies a fitted pipeline to a dataframe</p> Problem <p>The pipeline expects as input the same columns and in the same order even if some columns are then dropped (and so useless)</p> <p>Solution (experimental 14/04/2021):     We add the \"useless\" columns as columns filled with NaNs</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe to preprocess</p> required <code>preprocess_pipeline</code> <code>ColumnTransformer</code> <p>Pipeline to use</p> required <p>Raises:     ValueError: If some mandatory columns are missing Returns:     pd.DataFrame: Preprocessed dataFrame</p> Source code in <code>template_num/models_training/utils_models.py</code> <pre><code>def apply_pipeline(df: pd.DataFrame, preprocess_pipeline: ColumnTransformer) -&gt; pd.DataFrame:\n    '''Applies a fitted pipeline to a dataframe\n\n    Problem :\n        The pipeline expects as input the same columns and in the same order\n        even if some columns are then dropped (and so useless)\n    Solution (experimental 14/04/2021):\n        We add the \"useless\" columns as columns filled with NaNs\n\n    Args:\n        df (pd.DataFrame): Dataframe to preprocess\n        preprocess_pipeline (ColumnTransformer): Pipeline to use\n    Raises:\n        ValueError: If some mandatory columns are missing\n    Returns:\n        pd.DataFrame: Preprocessed dataFrame\n    '''\n    columns_in, mandatory_columns = get_columns_pipeline(preprocess_pipeline)\n\n    # Removes the useless columns\n    df = df[[col for col in df.columns if col in columns_in]]\n    optionals_columns = [col for col in columns_in if col not in mandatory_columns]\n\n    # Checks if all the mandatory columns are present\n    missing_mandatory_columns = [col for col in mandatory_columns if col not in df.columns]\n    if len(missing_mandatory_columns) &gt; 0:\n        for missing_col in missing_mandatory_columns:\n            logger.error(f\"Missing column in your dataset : {missing_col}\")\n        raise ValueError(f\"There are some missing mandatory columns in order to preprocess the dataset : {missing_mandatory_columns}\")\n\n    # We add the non mandatory columns if they are not already in df\n    # Note : only relevant in the case remainder = \"drop\" (nominal case)\n    missing_optionals_columns = [col for col in optionals_columns if col not in df.columns]\n    for col in missing_optionals_columns:\n        logger.warning(f'The column {col} is missing in order to apply the preprocessing.')\n        logger.warning('Experimental : it should be useless -&gt; creation of an empty column')\n        df[col] = np.nan\n\n    # Apply transform on reordered columns\n    preprocessed_x = preprocess_pipeline.transform(df[columns_in])\n    # Reconstruct dataframe &amp; return\n    preprocessed_df = pd.DataFrame(preprocessed_x)\n    preprocessed_df = preprocess.retrieve_columns_from_pipeline(preprocessed_df, preprocess_pipeline)\n    return preprocessed_df\n</code></pre>"},{"location":"reference/template_num/models_training/utils_models/#template_num.models_training.utils_models.display_train_test_shape","title":"<code>display_train_test_shape(df_train, df_test, df_shape=None)</code>","text":"<p>Displays the size of a train/test split</p> <p>Parameters:</p> Name Type Description Default <code>df_train</code> <code>DataFrame</code> <p>Train dataset</p> required <code>df_test</code> <code>DataFrame</code> <p>Test dataset</p> required <p>Kwargs:     df_shape (int): Size of the initial dataset Raises:     ValueError: If the object df_shape is not positive</p> Source code in <code>template_num/models_training/utils_models.py</code> <pre><code>def display_train_test_shape(df_train: pd.DataFrame, df_test: pd.DataFrame, df_shape: Union[int, None] = None) -&gt; None:\n    '''Displays the size of a train/test split\n\n    Args:\n        df_train (pd.DataFrame): Train dataset\n        df_test (pd.DataFrame): Test dataset\n    Kwargs:\n        df_shape (int): Size of the initial dataset\n    Raises:\n        ValueError: If the object df_shape is not positive\n    '''\n    if df_shape is not None and df_shape &lt; 1:\n        raise ValueError(\"The object df_shape must be positive\")\n\n    # Process\n    if df_shape is None:\n        df_shape = df_train.shape[0] + df_test.shape[0]\n    logger.info(f\"There are {df_train.shape[0]} lines in the train dataset and {df_test.shape[0]} in the test dataset.\")\n    logger.info(f\"{round(100 * df_train.shape[0] / df_shape, 2)}% of data are in the train set\")\n    logger.info(f\"{round(100 * df_test.shape[0] / df_shape, 2)}% of data are in the test set\")\n</code></pre>"},{"location":"reference/template_num/models_training/utils_models/#template_num.models_training.utils_models.get_columns_pipeline","title":"<code>get_columns_pipeline(preprocess_pipeline)</code>","text":"<p>Retrieves a pipeline wanted columns, and mandatory ones</p> <p>Parameters:</p> Name Type Description Default <code>preprocess_pipeline</code> <code>ColumnTransformer</code> <p>Preprocessing pipeline</p> required <p>Returns:     list: List of columns in     list: List of mandatory ones</p> Source code in <code>template_num/models_training/utils_models.py</code> <pre><code>def get_columns_pipeline(preprocess_pipeline: ColumnTransformer) -&gt; Tuple[list, list]:\n    '''Retrieves a pipeline wanted columns, and mandatory ones\n\n    Args:\n        preprocess_pipeline (ColumnTransformer): Preprocessing pipeline\n    Returns:\n        list: List of columns in\n        list: List of mandatory ones\n    '''\n    # Checks if the pipeline is fitted\n    check_is_fitted(preprocess_pipeline)\n    # Gets the names of input columns\n    columns_in = preprocess_pipeline.feature_names_in_.tolist()\n    # Gets the names of the \"mandatory\" columns\n    if preprocess_pipeline._remainder[1] == 'drop':\n        # If drop, we get from _columns\n        mandatory_columns = list(utils.flatten(preprocess_pipeline._columns))\n    else:\n        # Otherwise, we need all the columns\n        mandatory_columns = columns_in\n    # Returns\n    return columns_in, mandatory_columns\n</code></pre>"},{"location":"reference/template_num/models_training/utils_models/#template_num.models_training.utils_models.load_model","title":"<code>load_model(model_dir, is_path=False)</code>","text":"<p>Loads a model from a path</p> <p>Parameters:</p> Name Type Description Default <code>model_dir</code> <code>str</code> <p>Name of the folder containing the model (e.g. model_autres_2019_11_07-13_43_19)</p> required <p>Kwargs:     is_path (bool): If folder path instead of name (permits to load model from elsewhere) Returns:     ?: Model     dict: Model configurations</p> Source code in <code>template_num/models_training/utils_models.py</code> <pre><code>def load_model(model_dir: str, is_path: bool = False) -&gt; Tuple[Any, dict]:\n    '''Loads a model from a path\n\n    Args:\n        model_dir (str): Name of the folder containing the model (e.g. model_autres_2019_11_07-13_43_19)\n    Kwargs:\n        is_path (bool): If folder path instead of name (permits to load model from elsewhere)\n    Returns:\n        ?: Model\n        dict: Model configurations\n    '''\n    # Find model path\n    base_folder = None if is_path else utils.get_models_path()\n    model_path = utils.find_folder_path(model_dir, base_folder)\n\n    # Get configs\n    configuration_path = os.path.join(model_path, 'configurations.json')\n    with open(configuration_path, 'r', encoding='utf-8') as f:\n        configs = json.load(f)\n    # Can't set int as keys in json, so need to cast it after reloading\n    # dict_classes keys are always ints\n    if 'dict_classes' in configs.keys() and configs['dict_classes'] is not None:\n        configs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n\n    # Load model\n    pkl_path = os.path.join(model_path, f\"{configs['model_name']}.pkl\")\n    with open(pkl_path, 'rb') as f:\n        model = pickle.load(f)\n\n    # Change model_dir if diff\n    if model_path != model.model_dir:\n        model.model_dir = model_path\n        configs['model_dir'] = model_path\n\n    # Load specifics\n    hdf5_path = os.path.join(model_path, 'best.hdf5')\n\n    # TODO : we should probably have a single function `load_self` and let the model manage it's reload\n    # Check for keras model\n    if os.path.exists(hdf5_path):\n        model.model = model.reload_model(hdf5_path)\n\n    # Display if GPU is being used\n    model.display_if_gpu_activated()\n\n    # Return model &amp; configs\n    return model, configs\n</code></pre>"},{"location":"reference/template_num/models_training/utils_models/#template_num.models_training.utils_models.load_pipeline","title":"<code>load_pipeline(pipeline_dir, is_path=False)</code>","text":"<p>Loads a pipeline from the pipelines folder</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_dir</code> <code>str</code> <p>Name of the folder containing the pipeline to get. If None, backups on \"no_preprocess\"</p> required <p>Kwargs:     is_path (bool): If path to the folder instead of the name (permits the loading from elsewhere) Returns:     Pipeline: Reloaded pipeline     str: Name of the preprocessing used</p> Source code in <code>template_num/models_training/utils_models.py</code> <pre><code>def load_pipeline(pipeline_dir: Union[str, None], is_path: bool = False) -&gt; Tuple[Pipeline, str]:\n    '''Loads a pipeline from the pipelines folder\n\n    Args:\n        pipeline_dir (str): Name of the folder containing the pipeline to get. If None,\n            backups on \"no_preprocess\"\n    Kwargs:\n        is_path (bool): If path to the folder instead of the name (permits the loading from elsewhere)\n    Returns:\n        Pipeline: Reloaded pipeline\n        str: Name of the preprocessing used\n    '''\n    # If pipeline_dir is None, backups on \"no_preprocess\"\n    if pipeline_dir is None:\n        logger.warning(\"The folder of the pipeline is None. Backups on 'no_preprocess'\")\n        preprocess_str = \"no_preprocess\"\n        preprocess_pipeline = preprocess.get_pipeline(preprocess_str)  # Warning, must be fitted\n        return preprocess_pipeline, preprocess_str\n\n    # Otherwise, nominal case\n    # Find pipeline path\n    base_folder = None if is_path else utils.get_pipelines_path()\n    pipeline_path = utils.find_folder_path(pipeline_dir, base_folder)\n\n    # Get pipeline\n    pipeline_path = os.path.join(pipeline_path, 'pipeline.pkl')\n    with open(pipeline_path, 'rb') as f:\n        pipeline_dict = pickle.load(f)\n\n    # Return\n    return pipeline_dict['preprocess_pipeline'], pipeline_dict['preprocess_str']\n</code></pre>"},{"location":"reference/template_num/models_training/utils_models/#template_num.models_training.utils_models.normal_split","title":"<code>normal_split(df, test_size=0.25, seed=None)</code>","text":"<p>Splits a DataFrame into train and test sets</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe containing the data</p> required <p>Kwargs:     test_size (float): Proportion representing the size of the expected test set     seed (int): random seed Raises:     ValueError: If the object test_size is not between 0 and 1 Returns:     DataFrame: Train dataframe     DataFrame: Test dataframe</p> Source code in <code>template_num/models_training/utils_models.py</code> <pre><code>def normal_split(df: pd.DataFrame, test_size: float = 0.25, seed: Union[int, None] = None) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    '''Splits a DataFrame into train and test sets\n\n    Args:\n        df (pd.DataFrame): Dataframe containing the data\n    Kwargs:\n        test_size (float): Proportion representing the size of the expected test set\n        seed (int): random seed\n    Raises:\n        ValueError: If the object test_size is not between 0 and 1\n    Returns:\n        DataFrame: Train dataframe\n        DataFrame: Test dataframe\n    '''\n    if not 0 &lt;= test_size &lt;= 1:\n        raise ValueError('The object test_size must be between 0 and 1')\n\n    # Normal split\n    logger.info(\"Normal split\")\n    df_train, df_test = train_test_split(df, test_size=test_size, random_state=seed)\n\n    # Display\n    display_train_test_shape(df_train, df_test, df_shape=df.shape[0])\n\n    # Return\n    return df_train, df_test\n</code></pre>"},{"location":"reference/template_num/models_training/utils_models/#template_num.models_training.utils_models.predict","title":"<code>predict(content, model, inference_batch_size=128, alternative_version=False, **kwargs)</code>","text":"<p>Gets predictions of a model on a dataset</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>DataFrame</code> <p>New dataset to be predicted</p> required <code>model</code> <code>ModelClass</code> <p>Model to use</p> required <p>Kwargs:     inference_batch_size (int): size (approximate) of batches     alternative_version (bool): If an alternative version (<code>tf.function</code> + <code>model.__call__</code>) must be used.         Should be faster with low nb of inputs. Only useful for Keras models.         We advise you to set <code>alternative_version</code> to True for APIs to avoid possible memory leaks with <code>model.predict</code> on newest TensorFlow.         https://github.com/tensorflow/tensorflow/issues/58676         Inferences will probably be way faster too. Returns:     REGRESSION :         float: prediction     MONO-LABEL CLASSIFICATION:         str: prediction     MULTI-LABELS CLASSIFICATION:         tuple: predictions</p> <pre><code>If several elements -&gt; list\n</code></pre> Source code in <code>template_num/models_training/utils_models.py</code> <pre><code>def predict(content: pd.DataFrame, model, inference_batch_size: int = 128, alternative_version: bool = False, **kwargs) -&gt; list:\n    '''Gets predictions of a model on a dataset\n\n    Args:\n        content (pd.DataFrame): New dataset to be predicted\n        model (ModelClass): Model to use\n    Kwargs:\n        inference_batch_size (int): size (approximate) of batches\n        alternative_version (bool): If an alternative version (`tf.function` + `model.__call__`) must be used.\n            Should be faster with low nb of inputs. Only useful for Keras models.\n            We advise you to set `alternative_version` to True for APIs to avoid possible memory leaks with `model.predict` on newest TensorFlow.\n            https://github.com/tensorflow/tensorflow/issues/58676\n            Inferences will probably be way faster too.\n    Returns:\n        REGRESSION :\n            float: prediction\n        MONO-LABEL CLASSIFICATION:\n            str: prediction\n        MULTI-LABELS CLASSIFICATION:\n            tuple: predictions\n\n        If several elements -&gt; list\n    '''\n    # Apply preprocessing\n    if model.preprocess_pipeline is not None:\n        df_prep = apply_pipeline(content, model.preprocess_pipeline)\n    else:\n        df_prep = content.copy()\n        logger.warning(\"No preprocessing pipeline found - we consider no preprocessing, but it should not be so !\")\n\n    # Get predictions\n    predictions = model.predict(df_prep, inference_batch_size=inference_batch_size, alternative_version=alternative_version)\n\n    # Inverse transform (needed for classification)\n    predictions = model.inverse_transform(predictions)\n\n    # Return\n    return predictions\n</code></pre>"},{"location":"reference/template_num/models_training/utils_models/#template_num.models_training.utils_models.predict_with_proba","title":"<code>predict_with_proba(content, model, inference_batch_size=128, alternative_version=False, **kwargs)</code>","text":"<p>Gets probabilities predictions of a model on a dataset</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>DataFrame</code> <p>New dataset to be predicted</p> required <code>model</code> <code>ModelClass</code> <p>Model to use</p> required <p>Kwargs:     inference_batch_size (int): size (approximate) of batches     alternative_version (bool): If an alternative version (<code>tf.function</code> + <code>model.__call__</code>) must be used.         Should be faster with low nb of inputs. Only useful for Keras models.         We advise you to set <code>alternative_version</code> to True for APIs to avoid possible memory leaks with <code>model.predict</code> on newest TensorFlow.         https://github.com/tensorflow/tensorflow/issues/58676         Inferences will probably be way faster too. Raises:     ValueError: If the model type is not classifier Returns:     MONO-LABEL CLASSIFICATION:         List[str]: predictions         List[float]: probabilities     MULTI-LABELS CLASSIFICATION:         List[tuple]: predictions         List[tuple]: probabilities</p> Source code in <code>template_num/models_training/utils_models.py</code> <pre><code>def predict_with_proba(content: pd.DataFrame, model, inference_batch_size: int = 128, alternative_version: bool = False,\n                       **kwargs) -&gt; Union[Tuple[List[str], List[float]], Tuple[List[tuple], List[tuple]]]:\n    '''Gets probabilities predictions of a model on a dataset\n\n    Args:\n        content (pd.DataFrame): New dataset to be predicted\n        model (ModelClass): Model to use\n    Kwargs:\n        inference_batch_size (int): size (approximate) of batches\n        alternative_version (bool): If an alternative version (`tf.function` + `model.__call__`) must be used.\n            Should be faster with low nb of inputs. Only useful for Keras models.\n            We advise you to set `alternative_version` to True for APIs to avoid possible memory leaks with `model.predict` on newest TensorFlow.\n            https://github.com/tensorflow/tensorflow/issues/58676\n            Inferences will probably be way faster too.\n    Raises:\n        ValueError: If the model type is not classifier\n    Returns:\n        MONO-LABEL CLASSIFICATION:\n            List[str]: predictions\n            List[float]: probabilities\n        MULTI-LABELS CLASSIFICATION:\n            List[tuple]: predictions\n            List[tuple]: probabilities\n    '''\n    # Regressions\n    if not model.model_type == 'classifier':\n        raise ValueError(f\"The model type ({model.model_type}) is not supported by the method predict_with_proba\")\n\n    # Apply preprocessing\n    if model.preprocess_pipeline is not None:\n        df_prep = apply_pipeline(content, model.preprocess_pipeline)\n    else:\n        df_prep = content.copy()\n        logger.warning(\"No preprocessing pipeline found - we consider no preprocessing, but it should not be so !\")\n\n    # Get predictions\n    predictions, probas = model.predict_with_proba(df_prep, inference_batch_size=inference_batch_size, alternative_version=alternative_version)\n\n    # Rework format\n    if not model.multi_label:\n        predictions = model.inverse_transform(predictions)\n        probas = list(probas.max(axis=1))\n    else:\n        probas = [tuple(np.array(probas[i]).compress(indicators)) for i, indicators in enumerate(predictions)]\n        predictions = [tuple(np.array(model.list_classes).compress(indicators)) for indicators in predictions]\n\n    # Return predictions &amp; probas\n    return predictions, probas\n</code></pre>"},{"location":"reference/template_num/models_training/utils_models/#template_num.models_training.utils_models.preprocess_model_multilabel","title":"<code>preprocess_model_multilabel(df, y_col, classes=None)</code>","text":"<p>Prepares a dataframe for a multi-labels classification</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Training dataset This dataset must be preprocessed. Example:     # Group by &amp; apply tuple to y_col     x_cols = [col for col in list(df.columns) if col != y_col]     df = pd.DataFrame(df.groupby(x_cols)[y_col].apply(tuple))</p> required <code>y_col</code> <code>str or int</code> <p>Name of the column to be used for training - y</p> required <p>Kwargs:     classes (list): List of classes to consider Returns:     DataFrame: Dataframe for training     list: List of 'y' columns</p> Source code in <code>template_num/models_training/utils_models.py</code> <pre><code>def preprocess_model_multilabel(df: pd.DataFrame, y_col: Union[str, int], classes: Union[list, None] = None) -&gt; Tuple[pd.DataFrame, list]:\n    '''Prepares a dataframe for a multi-labels classification\n\n    Args:\n        df (pd.DataFrame): Training dataset\n            This dataset must be preprocessed.\n            Example:\n                # Group by &amp; apply tuple to y_col\n                x_cols = [col for col in list(df.columns) if col != y_col]\n                df = pd.DataFrame(df.groupby(x_cols)[y_col].apply(tuple))\n        y_col (str or int): Name of the column to be used for training - y\n    Kwargs:\n        classes (list): List of classes to consider\n    Returns:\n        DataFrame: Dataframe for training\n        list: List of 'y' columns\n    '''\n    # TODO: add possibility to have sparse output\n    logger.info(\"Preprocess dataframe for multi-labels model\")\n    # Process\n    logger.info(\"Preparing dataset for multi-labels format. Might take several minutes.\")\n    # /!\\ The reset_index is compulsory in order to have the same indexes between df, and MLB transformed values\n    df = df.reset_index(drop=True)\n    # Apply MLB\n    mlb = MultiLabelBinarizer(classes=classes)\n    df = df.assign(**pd.DataFrame(mlb.fit_transform(df[y_col]), columns=mlb.classes_))\n    # Return dataframe &amp; y_cols (i.e. classes)\n    return df, list(mlb.classes_)\n</code></pre>"},{"location":"reference/template_num/models_training/utils_models/#template_num.models_training.utils_models.remove_small_classes","title":"<code>remove_small_classes(df, col, min_rows=2)</code>","text":"<p>Deletes the classes with small numbers of elements</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe containing the data</p> required <code>col</code> <code>str | int</code> <p>Columns containing the classes</p> required <p>Kwargs:     min_rows (int): Minimal number of lines in the training set (default: 2) Raises:     ValueError: If the object min_rows is not positive Returns:     pd.DataFrame: New dataset</p> Source code in <code>template_num/models_training/utils_models.py</code> <pre><code>def remove_small_classes(df: pd.DataFrame, col: Union[str, int], min_rows: int = 2) -&gt; pd.DataFrame:\n    '''Deletes the classes with small numbers of elements\n\n    Args:\n        df (pd.DataFrame): Dataframe containing the data\n        col (str | int): Columns containing the classes\n    Kwargs:\n        min_rows (int): Minimal number of lines in the training set (default: 2)\n    Raises:\n        ValueError: If the object min_rows is not positive\n    Returns:\n        pd.DataFrame: New dataset\n    '''\n    if min_rows &lt; 1:\n        raise ValueError(\"The object min_rows must be positive\")\n\n    # Looking for classes with less than min_rows lines\n    v_count = df[col].value_counts()\n    classes_to_remove = list(v_count[v_count &lt; min_rows].index.values)\n    for cl in classes_to_remove:\n        logger.warning(f\"/!\\\\ /!\\\\ /!\\\\ Class {cl} has less than {min_rows} lines in the training set.\")\n        logger.warning(\"/!\\\\ /!\\\\ /!\\\\ This class is automatically removed from the dataset.\")\n    return df[~df[col].isin(classes_to_remove)]\n</code></pre>"},{"location":"reference/template_num/models_training/utils_models/#template_num.models_training.utils_models.search_hp_cv","title":"<code>search_hp_cv(model_cls, model_params, hp_params, scoring_fn, kwargs_fit, n_splits=5)</code>","text":"<p>Searches for hyperparameters - works only with classifiers !</p> <p>Parameters:</p> Name Type Description Default <code>model_cls</code> <code>?</code> <p>Class of models on which to do a hyperparameters search</p> required <code>model_params</code> <code>dict</code> <p>Set of \"fixed\" parameters of the model (e.g. x_col, y_col). Must contain 'multi_label'.</p> required <code>hp_params</code> <code>dict</code> <p>Set of \"variable\" parameters on which to do a hyperparameters search</p> required <code>scoring_fn</code> <code>str or func</code> <p>Scoring function to maximize This function must take as input a dictionary containing metrics e.g. {'F1-Score': 0.85, 'Accuracy': 0.57, 'Precision': 0.64, 'Recall': 0.90}</p> required <code>kwargs_fit</code> <code>dict</code> <p>Set of kwargs to input in the fit function Must contain 'x_train' and 'y_train'</p> required <p>Kwargs:     n_splits (int): Number of folds to use Raises:     ValueError: If scoring_fn is not a known string     ValueError: If multi_label is not a key in model_params     ValueError: If x_train is not a key in kwargs_fit     ValueError: If y_train is not a key in kwargs_fit     ValueError: If model_params and hp_params share some keys     ValueError: If hp_params values are not the same length     ValueError: If the number of crossvalidation split is less or equal to 1 Returns:     ModelClass: best model to be \"fitted\" on the dataset</p> Source code in <code>template_num/models_training/utils_models.py</code> <pre><code>def search_hp_cv(model_cls, model_params: dict, hp_params: dict, scoring_fn: Union[str, Callable],\n                 kwargs_fit: dict, n_splits: int = 5):\n    '''Searches for hyperparameters - works only with classifiers !\n\n    Args:\n        model_cls (?): Class of models on which to do a hyperparameters search\n        model_params (dict): Set of \"fixed\" parameters of the model (e.g. x_col, y_col).\n            Must contain 'multi_label'.\n        hp_params (dict): Set of \"variable\" parameters on which to do a hyperparameters search\n        scoring_fn (str or func): Scoring function to maximize\n            This function must take as input a dictionary containing metrics\n            e.g. {'F1-Score': 0.85, 'Accuracy': 0.57, 'Precision': 0.64, 'Recall': 0.90}\n        kwargs_fit (dict): Set of kwargs to input in the fit function\n            Must contain 'x_train' and 'y_train'\n    Kwargs:\n        n_splits (int): Number of folds to use\n    Raises:\n        ValueError: If scoring_fn is not a known string\n        ValueError: If multi_label is not a key in model_params\n        ValueError: If x_train is not a key in kwargs_fit\n        ValueError: If y_train is not a key in kwargs_fit\n        ValueError: If model_params and hp_params share some keys\n        ValueError: If hp_params values are not the same length\n        ValueError: If the number of crossvalidation split is less or equal to 1\n    Returns:\n        ModelClass: best model to be \"fitted\" on the dataset\n    '''\n    list_known_scoring = ['accuracy', 'f1', 'precision', 'recall']\n\n    # We can't really check if classifier ...\n\n    #################\n    # Manage errors\n    #################\n\n    if isinstance(scoring_fn, str) and scoring_fn not in list_known_scoring:\n        raise ValueError(f\"The input {scoring_fn} is not a known value for scoring_fn (known values : {list_known_scoring})\")\n\n    if 'multi_label' not in model_params.keys():\n        raise ValueError(\"The key 'multi_label' must be present in the dictionary model_params\")\n\n    if 'x_train' not in kwargs_fit.keys():\n        raise ValueError(\"The key 'x_train' must be present in the dictionary kwargs_fit\")\n\n    if 'y_train' not in kwargs_fit.keys():\n        raise ValueError(\"The key 'y_train' must be present in the dictionary kwargs_fit\")\n\n    if any([k in hp_params.keys() for k in model_params.keys()]):\n        # A key can't be \"fixed\" and \"variable\"\n        raise ValueError(\"The dictionaries model_params and hp_params share at least one key\")\n\n    if len(set([len(_) for _ in hp_params.values()])) != 1:\n        raise ValueError(\"The values of hp_params must have the same length\")\n\n    if n_splits &lt;= 1:\n        raise ValueError(f\"The number of crossvalidation splits ({n_splits}) must be more than 1\")\n\n    #################\n    # Manage scoring\n    #################\n\n    # Get scoring functions\n    if scoring_fn == 'accuracy':\n        scoring_fn = lambda x: x['Accuracy']\n    elif scoring_fn == 'f1':\n        scoring_fn = lambda x: x['F1-Score']\n    elif scoring_fn == 'precision':\n        scoring_fn = lambda x: x['Precision']\n    elif scoring_fn == 'recall':\n        scoring_fn = lambda x: x['Recall']\n\n    #################\n    # Manage x_train &amp; y_train format\n    #################\n\n    if not isinstance(kwargs_fit['x_train'], (pd.DataFrame, pd.Series)):\n        kwargs_fit['x_train'] = pd.Series(kwargs_fit['x_train'].copy())\n\n    if not isinstance(kwargs_fit['y_train'], (pd.DataFrame, pd.Series)):\n        kwargs_fit['y_train'] = pd.Series(kwargs_fit['y_train'].copy())\n\n    #################\n    # Process\n    #################\n\n    # Loop on hyperparameters\n    nb_search = len(list(hp_params.values())[0])\n    logger.info(\"Beginning of hyperparameters search\")\n    logger.info(f\"Number of model fits : {nb_search} (search number) x {n_splits} (CV splits number) = {nb_search * n_splits}\")\n\n    # DataFrame for stocking metrics :\n    metrics_df = pd.DataFrame(columns=['index_params', 'index_fold', 'Score', 'Accuracy', 'F1-Score', 'Precision', 'Recall'])\n    for i in range(nb_search):\n\n        # Display informations\n        logger.info(f\"Search n\u00b0{i + 1}\")\n        tmp_hp_params = {k: v[i] for k, v in hp_params.items()}\n        logger.info(\"Tested hyperparameters : \")\n        logger.info(pprint.pformat(tmp_hp_params))\n\n        # Get folds (shuffle recommended since the classes could be ordered)\n        if model_params['multi_label']:\n            k_fold = KFold(n_splits=n_splits, shuffle=True)  # Can't stratify on multi-labels\n        else:\n            k_fold = StratifiedKFold(n_splits=n_splits, shuffle=True)\n\n        # Process each fold\n        for j, (train_index, valid_index) in enumerate(k_fold.split(kwargs_fit['x_train'], kwargs_fit['y_train'])):\n            logger.info(f\"Search n\u00b0{i + 1}/{nb_search} - fit n\u00b0{j + 1}/{n_splits}\")\n            # get tmp x, y\n            x_train, x_valid = kwargs_fit['x_train'].iloc[train_index], kwargs_fit['x_train'].iloc[valid_index]\n            y_train, y_valid = kwargs_fit['y_train'].iloc[train_index], kwargs_fit['y_train'].iloc[valid_index]\n            # Get tmp model\n            # Manage model_dir\n            tmp_model_dir = os.path.join(utils.get_models_path(), datetime.now().strftime(\"tmp_%Y_%m_%d-%H_%M_%S\"))\n            # The next line prioritize the last dictionary\n            # We force a temporary folder and a low save level (we only want the metrics)\n            model_tmp = model_cls(**{**model_params, **tmp_hp_params, **{'model_dir': tmp_model_dir, 'level_save': 'LOW'}})\n            # Setting log level to ERROR\n            model_tmp.logger.setLevel(logging.ERROR)\n            # Let's fit ! (priority to the last dictionary)\n            model_tmp.fit(**{**kwargs_fit, **{'x_train': x_train, 'y_train': y_train, 'x_valid': x_valid, 'y_valid': y_valid}})\n            # Let's predict !\n            y_pred = model_tmp.predict(x_valid)\n            # Get metrics !\n            metrics_func = model_tmp.get_metrics_simple_multilabel if model_tmp.multi_label else model_tmp.get_metrics_simple_monolabel\n            metrics_tmp = metrics_func(y_valid, y_pred)\n            metrics_tmp = metrics_tmp[metrics_tmp.Label == \"All\"].copy()  # Add .copy() to avoid pandas settingwithcopy\n            metrics_tmp[\"Score\"] = scoring_fn(metrics_tmp.iloc[0].to_dict())  # type: ignore\n            metrics_tmp[\"index_params\"] = i\n            metrics_tmp[\"index_fold\"] = j\n            metrics_tmp = metrics_tmp[metrics_df.columns]  # Keeping only the necessary columns\n            metrics_df = pd.concat([metrics_df, metrics_tmp], ignore_index=True)\n            # Delete the temporary model : the final model must be refitted on the whole dataset\n            del model_tmp\n            gc.collect()\n            shutil.rmtree(tmp_model_dir)\n        # Display score\n        logger.info(f\"Score for search n\u00b0{i + 1}: {metrics_df[metrics_df['index_params'] == i]['Score'].mean()}\")\n\n    # Metric agregation for all the folds\n    metrics_df = metrics_df.join(metrics_df[['index_params', 'Score']].groupby('index_params').mean().rename({'Score': 'mean_score'}, axis=1), on='index_params', how='left')\n\n    # Select the set of parameters with the best mean score (on the folds)\n    best_index = metrics_df[metrics_df.mean_score == metrics_df.mean_score.max()][\"index_params\"].values[0]\n    best_params = {k: v[best_index] for k, v in hp_params.items()}\n    logger.info(f\"Best results for the set of parameter n\u00b0{best_index + 1}: {pprint.pformat(best_params)}\")\n\n    # Instanciation of a model with the best parameters\n    best_model = model_cls(**{**model_params, **best_params})\n\n    # Save the metrics report of the hyperparameters search and the tested parameters\n    csv_path = os.path.join(best_model.model_dir, \"hyper_params_results.csv\")\n    metrics_df.to_csv(csv_path, sep=';', index=False, encoding='utf-8')\n    json_data = {\n        'model_params': model_params,\n        'scoring_fn': pickle.source.getsourcelines(scoring_fn)[0],\n        'n_splits': n_splits,\n        'hp_params_set': {i: {k: v[i] for k, v in hp_params.items()} for i in range(nb_search)},\n    }\n    json_path = os.path.join(best_model.model_dir, \"hyper_params_tested.json\")\n    with open(json_path, 'w', encoding='utf-8') as f:\n        json.dump(json_data, f, indent=4, cls=utils.NpEncoder)\n\n    # TODO: We are forced to reset the logging level which is linked to the class\n    best_model.logger.setLevel(logging.getLogger('template_num').getEffectiveLevel())\n\n    # Return model to be fitted\n    return best_model\n</code></pre>"},{"location":"reference/template_num/models_training/utils_models/#template_num.models_training.utils_models.stratified_split","title":"<code>stratified_split(df, col, test_size=0.25, seed=None)</code>","text":"<p>Splits a DataFrame into train and test sets - Stratified strategy</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe containing the data</p> required <code>col</code> <code>str or int</code> <p>column on which to do the stratified split</p> required <p>Kwargs:     test_size (float): Proportion representing the size of the expected test set     seed (int): Random seed Raises:     ValueError: If the object test_size is not between 0 and 1 Returns:     DataFrame: Train dataframe     DataFrame: Test dataframe</p> Source code in <code>template_num/models_training/utils_models.py</code> <pre><code>def stratified_split(df: pd.DataFrame, col: Union[str, int], test_size: float = 0.25, seed: int = None) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    '''Splits a DataFrame into train and test sets - Stratified strategy\n\n    Args:\n        df (pd.DataFrame): Dataframe containing the data\n        col (str or int): column on which to do the stratified split\n    Kwargs:\n        test_size (float): Proportion representing the size of the expected test set\n        seed (int): Random seed\n    Raises:\n        ValueError: If the object test_size is not between 0 and 1\n    Returns:\n        DataFrame: Train dataframe\n        DataFrame: Test dataframe\n    '''\n    if not 0 &lt;= test_size &lt;= 1:\n        raise ValueError('The object test_size must be between 0 and 1')\n\n    # Stratified split\n    logger.info(\"Stratified split\")\n    df = remove_small_classes(df, col, min_rows=math.ceil(1 / test_size))  # minimum lines number per category to split\n    df_train, df_test = train_test_split(df, stratify=df[col], test_size=test_size, random_state=seed)\n\n    # Display\n    display_train_test_shape(df_train, df_test, df_shape=df.shape[0])\n\n    # Return\n    return df_train, df_test\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/","title":"Classifiers","text":""},{"location":"reference/template_num/models_training/classifiers/model_aggregation_classifier/","title":"Model aggregation classifier","text":""},{"location":"reference/template_num/models_training/classifiers/model_aggregation_classifier/#template_num.models_training.classifiers.model_aggregation_classifier.ModelAggregationClassifier","title":"<code>ModelAggregationClassifier</code>","text":"<p>             Bases: <code>ModelClassifierMixin</code>, <code>ModelClass</code></p> <p>Model for aggregating several classifier models</p> Source code in <code>template_num/models_training/classifiers/model_aggregation_classifier.py</code> <pre><code>class ModelAggregationClassifier(ModelClassifierMixin, ModelClass):\n    '''Model for aggregating several classifier models'''\n    _default_name = 'model_aggregation_classifier'\n\n    _dict_aggregation_function = {'majority_vote': {'aggregation_function': majority_vote, 'using_proba': False, 'multi_label': False},\n                                  'proba_argmax': {'aggregation_function': proba_argmax, 'using_proba': True, 'multi_label': False},\n                                  'all_predictions': {'aggregation_function': all_predictions, 'using_proba': False, 'multi_label': True},\n                                  'vote_labels': {'aggregation_function': vote_labels, 'using_proba': False, 'multi_label': True}}\n\n    def __init__(self, list_models: Union[list, None] = None, aggregation_function: Union[Callable, str] = 'majority_vote',\n                 using_proba: bool = False, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelClass for more arguments)\n        This model will aggregate the predictions of several model. The user can choose an aggregation function (with **kwargs if not using a list_classes arg)\n        from existing ones, or create its own. All models must be either mono label or multi label, we do not accept mixes.\n        However, we accept models that do not have the same class / labels. We will consider a meta model with joined classes / labels.\n\n        Kwargs:\n            list_models (list) : The list of models to be aggregated (can be None if reloading from standalones)\n            aggregation_function (Callable or str) : The aggregation function used (custom function must use **kwargs if not using a list_classes arg)\n            using_proba (bool) : Which object is being aggregated (the probabilities or the predictions).\n        Raises:\n            ValueError: All the aggregated sub_models have not the same multi_label attributes\n            ValueError: The multi_label attributes of the aggregated models are inconsistent with multi_label\n        '''\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Set attributes\n        self.using_proba = using_proba\n        self.aggregation_function = aggregation_function\n\n        # Manage submodels\n        self.sub_models = list_models  # Transform the list into a list of dictionnaries [{'name': xxx, 'model': xxx}, ...]\n\n        # Check if only classifiers are present\n        if False in [isinstance(sub_model['model'], ModelClassifierMixin) for sub_model in self.sub_models]:\n            raise ValueError(f\"model_aggregation_classifier only accepts classifier models\")\n\n        # Check for multi-labels inconsistencies\n        set_multi_label = {sub_model['model'].multi_label for sub_model in self.sub_models}\n        if len(set_multi_label) &gt; 1:\n            raise ValueError(f\"All the aggregated sub_models have not the same multi_label attribute\")\n        if len(set_multi_label.union({self.multi_label})) &gt; 1:\n            raise ValueError(f\"The multi_label attributes of the aggregated models are inconsistent with self.multi_label = {self.multi_label}.\")\n\n        # Set trained &amp; classes info from submodels\n        self.trained, self.list_classes, self.dict_classes = self._check_trained()\n        # Set nb_fit to 1 if already trained\n        if self.trained:\n            self.nb_fit = 1\n\n    @property\n    def aggregation_function(self):\n        '''Getter for aggregation_function'''\n        return self._aggregation_function\n\n    @aggregation_function.setter\n    def aggregation_function(self, agg_function: Union[Callable, str]):\n        '''Setter for aggregation_function\n        If a string, try to match a predefined function\n\n        Raises:\n            ValueError: If the object aggregation_function is a str but not found in the dictionary of predefined aggregation functions\n            ValueError: If the object aggregation_function is incompatible with multi_label\n        '''\n        # Retrieve aggregation function from dict if a string\n        if isinstance(agg_function, str):\n            # Get infos\n            if agg_function not in self._dict_aggregation_function.keys():\n                raise ValueError(f\"The aggregation_function ({agg_function}) is not a valid option (must be chosen in {self._dict_aggregation_function.keys()})\")\n            using_proba = self._dict_aggregation_function[agg_function]['using_proba']\n            multi_label = self._dict_aggregation_function[agg_function]['multi_label']\n            agg_function = self._dict_aggregation_function[agg_function]['aggregation_function']  # type: ignore\n            # Apply checks\n            if self.using_proba != using_proba:\n                self.logger.warning(f\"using_proba {self.using_proba} is incompatible with the selected aggregation function '{agg_function}'. We force using_proba to {using_proba}.\")\n                self.using_proba = using_proba  # type: ignore\n            if self.multi_label != multi_label:\n                raise ValueError(f\"multi_label {self.multi_label} is incompatible with the selected aggregation function '{agg_function}'.\")\n        self._aggregation_function = agg_function\n\n    @aggregation_function.deleter\n    def aggregation_function(self):\n        '''Deleter for aggregation_function'''\n        self._aggregation_function = None\n\n    @property\n    def sub_models(self):\n        '''Getter for sub_models'''\n        return self._sub_models\n\n    @sub_models.setter\n    def sub_models(self, list_models: Union[list, None] = None):\n        '''Setter for sub_models\n\n        Kwargs:\n            list_models (list) : The list of models to be aggregated\n        '''\n        list_models = [] if list_models is None else list_models\n        sub_models = []  # Init list of models\n        for model in list_models:\n            # If a string (a model name), reload it\n            if isinstance(model, str):\n                real_model, _ = utils_models.load_model(model)\n                dict_model = {'name': model, 'model': real_model}\n            else:\n                dict_model = {'name': os.path.split(model.model_dir)[-1], 'model': model}\n            sub_models.append(dict_model.copy())\n        self._sub_models = sub_models.copy()\n\n    @sub_models.deleter\n    def sub_models(self):\n        '''Deleter for sub_models'''\n        self._sub_models = None\n\n    def _check_trained(self) -&gt; Tuple[bool, list, dict]:\n        '''Checks and sets various attributes related to the fitting of underlying models\n\n        Returns:\n            bool: is the aggregation model is considered fitted\n            list: list of classes\n            dict: dict of classes\n        '''\n        # Check fitted\n        models_trained = {sub_model['model'].trained for sub_model in self.sub_models}\n        if len(models_trained) &gt; 0 and all(models_trained):\n            # All models trained\n            trained = True\n            # Set list_classes\n            list_classes = list({label for sub_model in self.sub_models for label in sub_model['model'].list_classes})\n            list_classes.sort()\n            # Set dict_classes based on self.list_classes\n            dict_classes = {i: col for i, col in enumerate(list_classes)}\n        # No model or not fitted\n        else:\n            trained, list_classes, dict_classes = False, [], {}\n        return trained, list_classes, dict_classes\n\n    def fit(self, x_train, y_train, x_valid=None, y_valid=None, with_shuffle: bool = True, **kwargs) -&gt; None:\n        '''Fits the model\n\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n            y_train (?): Array-like, shape = [n_samples, n_targets]\n        Kwargs:\n            x_valid (?): Array-like, shape = [n_samples, n_features] - not used by sklearn models\n            y_valid (?): Array-like, shape = [n_samples, n_targets] - not used by sklearn models\n            with_shuffle (bool): If x, y must be shuffled before fitting - not used by sklearn models\n        '''\n        # Fit each model\n        for sub_model in self.sub_models:\n            model = sub_model['model']\n            if not model.trained:\n                model.fit(x_train, y_train, x_valid=x_valid, y_valid=y_valid, with_shuffle=True, **kwargs)\n\n        # Set nb_fit to 1 if not already trained\n        if not self.trained:\n            self.nb_fit = 1\n\n        # Update attributes\n        self.trained, self.list_classes, self.dict_classes = self._check_trained()\n\n    @utils.trained_needed\n    def predict(self, x_test, return_proba: bool = False, alternative_version: bool = False, **kwargs) -&gt; np.ndarray:\n        '''Prediction\n\n        Args:\n            x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n            return_proba (bool): If the function should return the probabilities instead of the classes\n        Kwargs:\n            alternative_version (bool): If an alternative version (`tf.function` + `model.__call__`) must be used for Keras models. Should be faster with low nb of inputs.\n        Returns:\n            np.ndarray: array of shape = [n_samples]\n        '''\n        # We decide whether to rely on each model's probas or their predictions\n        if return_proba:\n            return self.predict_proba(x_test, alternative_version=alternative_version)\n        else:\n            # Get what we want (probas or preds) and use the aggregation function\n            if self.using_proba:\n                preds_or_probas = self._predict_probas_sub_models(x_test, alternative_version=alternative_version, **kwargs)\n            else:\n                preds_or_probas = self._predict_sub_models(x_test, alternative_version=alternative_version, **kwargs)\n            return np.array([self.aggregation_function(array, list_classes=self.list_classes) for array in preds_or_probas])  # type: ignore\n\n    @utils.trained_needed\n    def predict_proba(self, x_test, alternative_version: bool = False, **kwargs) -&gt; np.ndarray:\n        '''Predicts the probabilities on the test set\n\n        Args:\n            x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n        Kwargs:\n            alternative_version (bool): If an alternative version (`tf.function` + `model.__call__`) must be used for Keras models. Should be faster with low nb of inputs.\n        Returns:\n            np.ndarray: array of shape = [n_samples, n_classes]\n        '''\n        probas_sub_models = self._predict_probas_sub_models(x_test, alternative_version=alternative_version, **kwargs)\n        # The probas of all models are averaged\n        return np.sum(probas_sub_models, axis=1) / probas_sub_models.shape[1]\n\n    @utils.trained_needed\n    def _predict_sub_models(self, x_test, alternative_version: bool = False, **kwargs) -&gt; np.ndarray:\n        '''Recover the predictions of each model being aggregated\n\n        Args:\n            x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n        Kwargs:\n            alternative_version (bool): If an alternative version (`tf.function` + `model.__call__`) must be used for Keras models. Should be faster with low nb of inputs.\n        Returns:\n            np.ndarray: not multi_label : array of shape = [n_samples, nb_model]\n                        multi_label : array of shape = [n_samples, nb_model, n_classes]\n        '''\n        if self.multi_label:\n            array_predict = np.array([self._predict_full_list_classes(sub_model['model'], x_test, return_proba=False, alternative_version=alternative_version)\n                                      for sub_model in self.sub_models])\n            array_predict = np.transpose(array_predict, (1, 0, 2))\n        else:\n            array_predict = np.array([sub_model['model'].predict(x_test, alternative_version=alternative_version) for sub_model in self.sub_models])\n            array_predict = np.transpose(array_predict, (1, 0))\n        return array_predict\n\n    @utils.trained_needed\n    def _predict_probas_sub_models(self, x_test, alternative_version: bool = False, **kwargs) -&gt; np.ndarray:\n        '''Recover the probabilities of each model being aggregated\n\n        Args:\n            x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n        Kwargs:\n            alternative_version (bool): If an alternative version (`tf.function` + `model.__call__`) must be used for Keras models. Should be faster with low nb of inputs.\n        Returns:\n            np.ndarray: array of shape = [n_samples, nb_model, nb_classes]\n        '''\n        array_probas = np.array([self._predict_full_list_classes(sub_model['model'], x_test, return_proba=True, alternative_version=alternative_version)\n                                 for sub_model in self.sub_models])\n        array_probas = np.transpose(array_probas, (1, 0, 2))\n        return array_probas\n\n    def _predict_full_list_classes(self, model: Type[ModelClass], x_test, return_proba: bool = False, alternative_version: bool = False, **kwargs) -&gt; np.ndarray:\n        '''For multi_label: adds missing columns in the prediction of model (class missing in their list_classes)\n        Or, if return_proba, adds a proba of zero to the missing classes in their list_classes\n\n        Args:\n            model (ModelClass): Model to use\n            x_test (?): Array-like or sparse matrix of shape = [n_samples, n_features]\n            return_proba (bool): If the function should return the probabilities instead of the classes\n        Kwargs:\n            alternative_version (bool): If an alternative version (`tf.function` + `model.__call__`) must be used for Keras models. Should be faster with low nb of inputs.\n        Returns:\n            np.ndarray: The array with the missing columns added\n        '''\n        # Get predictions or probas\n        preds_or_probas = model.predict(x_test, return_proba=return_proba, alternative_version=alternative_version) # type: ignore\n\n        # Manage each cases. Reorder predictions or probas according to aggregation model list_classes\n        # Multi label, proba = True\n        # Multi label, proba = False\n        # Mono label, proba = True\n        if model.multi_label or return_proba:\n            df_all = pd.DataFrame(np.zeros((len(preds_or_probas), len(self.list_classes))), columns=self.list_classes)  # type: ignore\n            df_model = pd.DataFrame(preds_or_probas, columns=model.list_classes)\n            for col in model.list_classes:\n                df_all[col] = df_model[col]\n            return df_all.to_numpy()\n        # Mono label, proba = False\n        else:\n            return preds_or_probas\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        if json_data is None:\n            json_data = {}\n        # Specific aggregation - save some wanted entries\n        train_keys = ['filename', 'filename_valid', 'preprocess_str']\n        default_json_data = {key: json_data.get(key, None) for key in train_keys}\n        default_json_data['aggregator_dir'] = self.model_dir\n        # Save each trained and unsaved model\n        for sub_model in self.sub_models:\n            path_config = os.path.join(sub_model['model'].model_dir, 'configurations.json')\n            if os.path.exists(path_config):\n                with open(path_config, 'r', encoding='utf-8') as f:\n                    configs = json.load(f)\n                    trained = configs.get('trained', False)\n                    if not trained:\n                        sub_model['model'].save(default_json_data)\n            else:\n                sub_model['model'].save(default_json_data)\n\n        # Add some specific information\n        json_data['list_models_name'] = [sub_model['name'] for sub_model in self.sub_models]\n        json_data['using_proba'] = self.using_proba\n\n        # Save aggregation_function if not None &amp; level_save &gt; LOW\n        if (self.aggregation_function is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n            # Manage paths\n            aggregation_function_path = os.path.join(self.model_dir, \"aggregation_function.pkl\")\n            # Save as pickle\n            with open(aggregation_function_path, 'wb') as f:\n                pickle.dump(self.aggregation_function, f)\n\n        # Save\n        models_list = [sub_model['name'] for sub_model in self.sub_models]\n        aggregation_function = self.aggregation_function\n        delattr(self, \"sub_models\")\n        delattr(self, \"aggregation_function\")\n        super().save(json_data=json_data)\n        setattr(self, \"aggregation_function\", aggregation_function)\n        setattr(self, \"sub_models\", models_list)  # Setter needs list of models, not sub_models itself\n\n        # Add message in model_upload_instructions.md\n        md_path = os.path.join(self.model_dir, f\"model_upload_instructions.md\")\n        line = \"/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\   The aggregation model is a special model, please ensure that all sub-models and the aggregation model are manually saved together in order to be able to load it .  /!\\\\/!\\\\/!\\\\/!\\\\/!\\\\ \\n\"\n        self.prepend_line(md_path, line)\n\n    def prepend_line(self, file_name: str, line: str) -&gt; None:\n        ''' Insert given string as a new line at the beginning of a file\n\n        Kwargs:\n            file_name (str): Path to file\n            line (str): line to insert\n        '''\n        with open(file_name, 'r+') as f:\n            lines = f.readlines()\n            lines.insert(0, line)\n            f.seek(0)\n            f.writelines(lines)\n\n    def reload_from_standalone(self, **kwargs) -&gt; None:\n        '''Reloads a model aggregation from its configuration and \"standalones\" files\n           Reloads the sub_models from their files\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n            aggregation_function_path (str): Path to aggregation_function_path\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If preprocess_pipeline_path is None\n            ValueError: If aggregation_function_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n            FileNotFoundError: If the object aggregation_function_path is not an existing file\n        '''\n        # Retrieve args\n        configuration_path = kwargs.get('configuration_path', None)\n        preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n        aggregation_function_path = kwargs.get('aggregation_function_path', None)\n\n        # Checks\n        if configuration_path is None:\n            raise ValueError(\"The argument configuration_path can't be None\")\n        if preprocess_pipeline_path is None:\n            raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n        if aggregation_function_path is None:\n            raise ValueError(\"The argument aggregation_function_path can't be None\")\n        if not os.path.exists(configuration_path):\n            raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n        if not os.path.exists(preprocess_pipeline_path):\n            raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n        if not os.path.exists(aggregation_function_path):\n            raise FileNotFoundError(f\"The file {aggregation_function_path} does not exist\")\n\n        # Load confs\n        with open(configuration_path, 'r', encoding='utf-8') as f:\n            configs = json.load(f)\n        # Can't set int as keys in json, so need to cast it after reloading\n        # dict_classes keys are always ints\n        if 'dict_classes' in configs.keys():\n            configs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n        elif 'list_classes' in configs.keys():\n            configs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n\n        # Reload pipeline preprocessing\n        with open(preprocess_pipeline_path, 'rb') as f:\n            self.preprocess_pipeline = pickle.load(f)\n\n        # Reload aggregation_function_path\n        with open(aggregation_function_path, 'rb') as f:\n            self.aggregation_function = pickle.load(f)\n\n        # Set class vars\n        # self.model_name = # Keep the created name\n        # self.model_dir = # Keep the created folder\n        self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n        self.trained = configs.get('trained', True)  # Consider trained by default\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        self.sub_models = configs.get('list_models_name', [])  # Transform the list into a list of dictionnaries [{'name': xxx, 'model': xxx}, ...]\n        for attribute in ['x_col', 'y_col', 'list_classes', 'dict_classes', 'multi_label', 'level_save',\n                          'using_proba']:\n            setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_aggregation_classifier/#template_num.models_training.classifiers.model_aggregation_classifier.ModelAggregationClassifier.aggregation_function","title":"<code>aggregation_function</code>  <code>deletable</code> <code>property</code> <code>writable</code>","text":"<p>Getter for aggregation_function</p>"},{"location":"reference/template_num/models_training/classifiers/model_aggregation_classifier/#template_num.models_training.classifiers.model_aggregation_classifier.ModelAggregationClassifier.sub_models","title":"<code>sub_models</code>  <code>deletable</code> <code>property</code> <code>writable</code>","text":"<p>Getter for sub_models</p>"},{"location":"reference/template_num/models_training/classifiers/model_aggregation_classifier/#template_num.models_training.classifiers.model_aggregation_classifier.ModelAggregationClassifier.__init__","title":"<code>__init__(list_models=None, aggregation_function='majority_vote', using_proba=False, **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass for more arguments) This model will aggregate the predictions of several model. The user can choose an aggregation function (with **kwargs if not using a list_classes arg) from existing ones, or create its own. All models must be either mono label or multi label, we do not accept mixes. However, we accept models that do not have the same class / labels. We will consider a meta model with joined classes / labels.</p> Kwargs <p>list_models (list) : The list of models to be aggregated (can be None if reloading from standalones) aggregation_function (Callable or str) : The aggregation function used (custom function must use **kwargs if not using a list_classes arg) using_proba (bool) : Which object is being aggregated (the probabilities or the predictions).</p> <p>Raises:     ValueError: All the aggregated sub_models have not the same multi_label attributes     ValueError: The multi_label attributes of the aggregated models are inconsistent with multi_label</p> Source code in <code>template_num/models_training/classifiers/model_aggregation_classifier.py</code> <pre><code>def __init__(self, list_models: Union[list, None] = None, aggregation_function: Union[Callable, str] = 'majority_vote',\n             using_proba: bool = False, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelClass for more arguments)\n    This model will aggregate the predictions of several model. The user can choose an aggregation function (with **kwargs if not using a list_classes arg)\n    from existing ones, or create its own. All models must be either mono label or multi label, we do not accept mixes.\n    However, we accept models that do not have the same class / labels. We will consider a meta model with joined classes / labels.\n\n    Kwargs:\n        list_models (list) : The list of models to be aggregated (can be None if reloading from standalones)\n        aggregation_function (Callable or str) : The aggregation function used (custom function must use **kwargs if not using a list_classes arg)\n        using_proba (bool) : Which object is being aggregated (the probabilities or the predictions).\n    Raises:\n        ValueError: All the aggregated sub_models have not the same multi_label attributes\n        ValueError: The multi_label attributes of the aggregated models are inconsistent with multi_label\n    '''\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Set attributes\n    self.using_proba = using_proba\n    self.aggregation_function = aggregation_function\n\n    # Manage submodels\n    self.sub_models = list_models  # Transform the list into a list of dictionnaries [{'name': xxx, 'model': xxx}, ...]\n\n    # Check if only classifiers are present\n    if False in [isinstance(sub_model['model'], ModelClassifierMixin) for sub_model in self.sub_models]:\n        raise ValueError(f\"model_aggregation_classifier only accepts classifier models\")\n\n    # Check for multi-labels inconsistencies\n    set_multi_label = {sub_model['model'].multi_label for sub_model in self.sub_models}\n    if len(set_multi_label) &gt; 1:\n        raise ValueError(f\"All the aggregated sub_models have not the same multi_label attribute\")\n    if len(set_multi_label.union({self.multi_label})) &gt; 1:\n        raise ValueError(f\"The multi_label attributes of the aggregated models are inconsistent with self.multi_label = {self.multi_label}.\")\n\n    # Set trained &amp; classes info from submodels\n    self.trained, self.list_classes, self.dict_classes = self._check_trained()\n    # Set nb_fit to 1 if already trained\n    if self.trained:\n        self.nb_fit = 1\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_aggregation_classifier/#template_num.models_training.classifiers.model_aggregation_classifier.ModelAggregationClassifier.fit","title":"<code>fit(x_train, y_train, x_valid=None, y_valid=None, with_shuffle=True, **kwargs)</code>","text":"<p>Fits the model</p> <p>Parameters:</p> Name Type Description Default <code>x_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <code>y_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_targets]</p> required <p>Kwargs:     x_valid (?): Array-like, shape = [n_samples, n_features] - not used by sklearn models     y_valid (?): Array-like, shape = [n_samples, n_targets] - not used by sklearn models     with_shuffle (bool): If x, y must be shuffled before fitting - not used by sklearn models</p> Source code in <code>template_num/models_training/classifiers/model_aggregation_classifier.py</code> <pre><code>def fit(self, x_train, y_train, x_valid=None, y_valid=None, with_shuffle: bool = True, **kwargs) -&gt; None:\n    '''Fits the model\n\n    Args:\n        x_train (?): Array-like, shape = [n_samples, n_features]\n        y_train (?): Array-like, shape = [n_samples, n_targets]\n    Kwargs:\n        x_valid (?): Array-like, shape = [n_samples, n_features] - not used by sklearn models\n        y_valid (?): Array-like, shape = [n_samples, n_targets] - not used by sklearn models\n        with_shuffle (bool): If x, y must be shuffled before fitting - not used by sklearn models\n    '''\n    # Fit each model\n    for sub_model in self.sub_models:\n        model = sub_model['model']\n        if not model.trained:\n            model.fit(x_train, y_train, x_valid=x_valid, y_valid=y_valid, with_shuffle=True, **kwargs)\n\n    # Set nb_fit to 1 if not already trained\n    if not self.trained:\n        self.nb_fit = 1\n\n    # Update attributes\n    self.trained, self.list_classes, self.dict_classes = self._check_trained()\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_aggregation_classifier/#template_num.models_training.classifiers.model_aggregation_classifier.ModelAggregationClassifier.predict","title":"<code>predict(x_test, return_proba=False, alternative_version=False, **kwargs)</code>","text":"<p>Prediction</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>array-like or sparse matrix of shape = [n_samples, n_features]</p> required <code>return_proba</code> <code>bool</code> <p>If the function should return the probabilities instead of the classes</p> <code>False</code> <p>Kwargs:     alternative_version (bool): If an alternative version (<code>tf.function</code> + <code>model.__call__</code>) must be used for Keras models. Should be faster with low nb of inputs. Returns:     np.ndarray: array of shape = [n_samples]</p> Source code in <code>template_num/models_training/classifiers/model_aggregation_classifier.py</code> <pre><code>@utils.trained_needed\ndef predict(self, x_test, return_proba: bool = False, alternative_version: bool = False, **kwargs) -&gt; np.ndarray:\n    '''Prediction\n\n    Args:\n        x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n        return_proba (bool): If the function should return the probabilities instead of the classes\n    Kwargs:\n        alternative_version (bool): If an alternative version (`tf.function` + `model.__call__`) must be used for Keras models. Should be faster with low nb of inputs.\n    Returns:\n        np.ndarray: array of shape = [n_samples]\n    '''\n    # We decide whether to rely on each model's probas or their predictions\n    if return_proba:\n        return self.predict_proba(x_test, alternative_version=alternative_version)\n    else:\n        # Get what we want (probas or preds) and use the aggregation function\n        if self.using_proba:\n            preds_or_probas = self._predict_probas_sub_models(x_test, alternative_version=alternative_version, **kwargs)\n        else:\n            preds_or_probas = self._predict_sub_models(x_test, alternative_version=alternative_version, **kwargs)\n        return np.array([self.aggregation_function(array, list_classes=self.list_classes) for array in preds_or_probas])  # type: ignore\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_aggregation_classifier/#template_num.models_training.classifiers.model_aggregation_classifier.ModelAggregationClassifier.predict_proba","title":"<code>predict_proba(x_test, alternative_version=False, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>array-like or sparse matrix of shape = [n_samples, n_features]</p> required <p>Kwargs:     alternative_version (bool): If an alternative version (<code>tf.function</code> + <code>model.__call__</code>) must be used for Keras models. Should be faster with low nb of inputs. Returns:     np.ndarray: array of shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/classifiers/model_aggregation_classifier.py</code> <pre><code>@utils.trained_needed\ndef predict_proba(self, x_test, alternative_version: bool = False, **kwargs) -&gt; np.ndarray:\n    '''Predicts the probabilities on the test set\n\n    Args:\n        x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n    Kwargs:\n        alternative_version (bool): If an alternative version (`tf.function` + `model.__call__`) must be used for Keras models. Should be faster with low nb of inputs.\n    Returns:\n        np.ndarray: array of shape = [n_samples, n_classes]\n    '''\n    probas_sub_models = self._predict_probas_sub_models(x_test, alternative_version=alternative_version, **kwargs)\n    # The probas of all models are averaged\n    return np.sum(probas_sub_models, axis=1) / probas_sub_models.shape[1]\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_aggregation_classifier/#template_num.models_training.classifiers.model_aggregation_classifier.ModelAggregationClassifier.prepend_line","title":"<code>prepend_line(file_name, line)</code>","text":"<p>Insert given string as a new line at the beginning of a file</p> Kwargs <p>file_name (str): Path to file line (str): line to insert</p> Source code in <code>template_num/models_training/classifiers/model_aggregation_classifier.py</code> <pre><code>def prepend_line(self, file_name: str, line: str) -&gt; None:\n    ''' Insert given string as a new line at the beginning of a file\n\n    Kwargs:\n        file_name (str): Path to file\n        line (str): line to insert\n    '''\n    with open(file_name, 'r+') as f:\n        lines = f.readlines()\n        lines.insert(0, line)\n        f.seek(0)\n        f.writelines(lines)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_aggregation_classifier/#template_num.models_training.classifiers.model_aggregation_classifier.ModelAggregationClassifier.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model aggregation from its configuration and \"standalones\" files    Reloads the sub_models from their files Kwargs:     configuration_path (str): Path to configuration file     preprocess_pipeline_path (str): Path to preprocess pipeline     aggregation_function_path (str): Path to aggregation_function_path Raises:     ValueError: If configuration_path is None     ValueError: If preprocess_pipeline_path is None     ValueError: If aggregation_function_path is None     FileNotFoundError: If the object configuration_path is not an existing file     FileNotFoundError: If the object preprocess_pipeline_path is not an existing file     FileNotFoundError: If the object aggregation_function_path is not an existing file</p> Source code in <code>template_num/models_training/classifiers/model_aggregation_classifier.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n    '''Reloads a model aggregation from its configuration and \"standalones\" files\n       Reloads the sub_models from their files\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n        aggregation_function_path (str): Path to aggregation_function_path\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If preprocess_pipeline_path is None\n        ValueError: If aggregation_function_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        FileNotFoundError: If the object aggregation_function_path is not an existing file\n    '''\n    # Retrieve args\n    configuration_path = kwargs.get('configuration_path', None)\n    preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n    aggregation_function_path = kwargs.get('aggregation_function_path', None)\n\n    # Checks\n    if configuration_path is None:\n        raise ValueError(\"The argument configuration_path can't be None\")\n    if preprocess_pipeline_path is None:\n        raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n    if aggregation_function_path is None:\n        raise ValueError(\"The argument aggregation_function_path can't be None\")\n    if not os.path.exists(configuration_path):\n        raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n    if not os.path.exists(preprocess_pipeline_path):\n        raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n    if not os.path.exists(aggregation_function_path):\n        raise FileNotFoundError(f\"The file {aggregation_function_path} does not exist\")\n\n    # Load confs\n    with open(configuration_path, 'r', encoding='utf-8') as f:\n        configs = json.load(f)\n    # Can't set int as keys in json, so need to cast it after reloading\n    # dict_classes keys are always ints\n    if 'dict_classes' in configs.keys():\n        configs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n    elif 'list_classes' in configs.keys():\n        configs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n\n    # Reload pipeline preprocessing\n    with open(preprocess_pipeline_path, 'rb') as f:\n        self.preprocess_pipeline = pickle.load(f)\n\n    # Reload aggregation_function_path\n    with open(aggregation_function_path, 'rb') as f:\n        self.aggregation_function = pickle.load(f)\n\n    # Set class vars\n    # self.model_name = # Keep the created name\n    # self.model_dir = # Keep the created folder\n    self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n    self.trained = configs.get('trained', True)  # Consider trained by default\n    # Try to read the following attributes from configs and, if absent, keep the current one\n    self.sub_models = configs.get('list_models_name', [])  # Transform the list into a list of dictionnaries [{'name': xxx, 'model': xxx}, ...]\n    for attribute in ['x_col', 'y_col', 'list_classes', 'dict_classes', 'multi_label', 'level_save',\n                      'using_proba']:\n        setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_aggregation_classifier/#template_num.models_training.classifiers.model_aggregation_classifier.ModelAggregationClassifier.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/classifiers/model_aggregation_classifier.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    if json_data is None:\n        json_data = {}\n    # Specific aggregation - save some wanted entries\n    train_keys = ['filename', 'filename_valid', 'preprocess_str']\n    default_json_data = {key: json_data.get(key, None) for key in train_keys}\n    default_json_data['aggregator_dir'] = self.model_dir\n    # Save each trained and unsaved model\n    for sub_model in self.sub_models:\n        path_config = os.path.join(sub_model['model'].model_dir, 'configurations.json')\n        if os.path.exists(path_config):\n            with open(path_config, 'r', encoding='utf-8') as f:\n                configs = json.load(f)\n                trained = configs.get('trained', False)\n                if not trained:\n                    sub_model['model'].save(default_json_data)\n        else:\n            sub_model['model'].save(default_json_data)\n\n    # Add some specific information\n    json_data['list_models_name'] = [sub_model['name'] for sub_model in self.sub_models]\n    json_data['using_proba'] = self.using_proba\n\n    # Save aggregation_function if not None &amp; level_save &gt; LOW\n    if (self.aggregation_function is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n        # Manage paths\n        aggregation_function_path = os.path.join(self.model_dir, \"aggregation_function.pkl\")\n        # Save as pickle\n        with open(aggregation_function_path, 'wb') as f:\n            pickle.dump(self.aggregation_function, f)\n\n    # Save\n    models_list = [sub_model['name'] for sub_model in self.sub_models]\n    aggregation_function = self.aggregation_function\n    delattr(self, \"sub_models\")\n    delattr(self, \"aggregation_function\")\n    super().save(json_data=json_data)\n    setattr(self, \"aggregation_function\", aggregation_function)\n    setattr(self, \"sub_models\", models_list)  # Setter needs list of models, not sub_models itself\n\n    # Add message in model_upload_instructions.md\n    md_path = os.path.join(self.model_dir, f\"model_upload_instructions.md\")\n    line = \"/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\   The aggregation model is a special model, please ensure that all sub-models and the aggregation model are manually saved together in order to be able to load it .  /!\\\\/!\\\\/!\\\\/!\\\\/!\\\\ \\n\"\n    self.prepend_line(md_path, line)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_aggregation_classifier/#template_num.models_training.classifiers.model_aggregation_classifier.all_predictions","title":"<code>all_predictions(predictions, **kwargs)</code>","text":"<p>Calculates the sum of the arrays along axis 0 casts it to bool and then to int. Expects a numpy array containing only zeroes and ones. When used as an aggregation function, keeps all the prediction of each model (multi-labels)</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>np.ndarray) </code> <p>Array of shape : (n_models, n_classes)</p> required <p>Return:     np.ndarray: The prediction</p> Source code in <code>template_num/models_training/classifiers/model_aggregation_classifier.py</code> <pre><code>def all_predictions(predictions: np.ndarray, **kwargs) -&gt; np.ndarray:\n    '''Calculates the sum of the arrays along axis 0 casts it to bool and then to int.\n    Expects a numpy array containing only zeroes and ones.\n    When used as an aggregation function, keeps all the prediction of each model (multi-labels)\n\n    Args:\n        predictions (np.ndarray) : Array of shape : (n_models, n_classes)\n    Return:\n        np.ndarray: The prediction\n    '''\n    return np.sum(predictions, axis=0, dtype=bool).astype(int)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_aggregation_classifier/#template_num.models_training.classifiers.model_aggregation_classifier.majority_vote","title":"<code>majority_vote(predictions, **kwargs)</code>","text":"<p>Gives the class corresponding to the most present prediction in the given predictions. In case of a tie, gives the prediction of the first model involved in the tie Args:     predictions (np.ndarray): The array containing the predictions of each model (shape (n_models)) Returns:     The prediction</p> Source code in <code>template_num/models_training/classifiers/model_aggregation_classifier.py</code> <pre><code>def majority_vote(predictions: np.ndarray, **kwargs):\n    '''Gives the class corresponding to the most present prediction in the given predictions.\n    In case of a tie, gives the prediction of the first model involved in the tie\n    Args:\n        predictions (np.ndarray): The array containing the predictions of each model (shape (n_models))\n    Returns:\n        The prediction\n    '''\n    labels, counts = np.unique(predictions, return_counts=True)\n    votes = [(label, count) for label, count in zip(labels, counts)]\n    votes = sorted(votes, key=lambda x: x[1], reverse=True)\n    possible_classes = {vote[0] for vote in votes if vote[1]==votes[0][1]}\n    return [prediction for prediction in predictions if prediction in possible_classes][0]\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_aggregation_classifier/#template_num.models_training.classifiers.model_aggregation_classifier.proba_argmax","title":"<code>proba_argmax(proba, list_classes, **kwargs)</code>","text":"<p>Gives the class corresponding to the argmax of the average of the given probabilities</p> <p>Parameters:</p> Name Type Description Default <code>proba</code> <code>ndarray</code> <p>The probabilities of each model for each class, array of shape (nb_models, nb_classes)</p> required <code>list_classes</code> <code>list</code> <p>List of classes</p> required <p>Returns:     The prediction</p> Source code in <code>template_num/models_training/classifiers/model_aggregation_classifier.py</code> <pre><code>def proba_argmax(proba: np.ndarray, list_classes: list, **kwargs):\n    '''Gives the class corresponding to the argmax of the average of the given probabilities\n\n    Args:\n        proba (np.ndarray): The probabilities of each model for each class, array of shape (nb_models, nb_classes)\n        list_classes (list): List of classes\n    Returns:\n        The prediction\n    '''\n    proba_average = np.sum(proba, axis=0) / proba.shape[0]\n    index_class = np.argmax(proba_average)\n    return list_classes[index_class]\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_aggregation_classifier/#template_num.models_training.classifiers.model_aggregation_classifier.vote_labels","title":"<code>vote_labels(predictions, **kwargs)</code>","text":"<p>Gives the result of majority_vote applied on the second axis. When used as an aggregation_function, for each class, performs a majority vote for the aggregated models. It gives a multi-labels result</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>ndarray</code> <p>array of shape : (n_models, n_classes)</p> required <p>Return:     np.ndarray: prediction</p> Source code in <code>template_num/models_training/classifiers/model_aggregation_classifier.py</code> <pre><code>def vote_labels(predictions: np.ndarray, **kwargs) -&gt; np.ndarray:\n    '''Gives the result of majority_vote applied on the second axis.\n    When used as an aggregation_function, for each class, performs a majority vote for the aggregated models.\n    It gives a multi-labels result\n\n    Args:\n        predictions (np.ndarray): array of shape : (n_models, n_classes)\n    Return:\n        np.ndarray: prediction\n    '''\n    return np.apply_along_axis(majority_vote, 0, predictions)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_classifier/","title":"Model classifier","text":""},{"location":"reference/template_num/models_training/classifiers/model_classifier/#template_num.models_training.classifiers.model_classifier.ModelClassifierMixin","title":"<code>ModelClassifierMixin</code>","text":"<p>Parent class (Mixin) for classifier models</p> Source code in <code>template_num/models_training/classifiers/model_classifier.py</code> <pre><code>class ModelClassifierMixin:\n    '''Parent class (Mixin) for classifier models'''\n\n    # Not implemented :\n    # -&gt; predict : To be implementd by the parent class when using this mixin\n\n    def __init__(self, level_save: str = 'HIGH', multi_label: bool = False, **kwargs) -&gt; None:\n        '''Initialization of the class\n\n        Kwargs:\n            level_save (str): Level of saving\n                LOW: stats + configurations + logger keras - /!\\\\ The model can't be reused /!\\\\ -\n                MEDIUM: LOWlevel_save + hdf5 + pkl + plots\n                HIGH: MEDIUM + predictions\n            multi_label (bool): If the classification must be multi-labels\n        Raises:\n            ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\n        '''\n        super().__init__(level_save=level_save, **kwargs)  # forwards level_save &amp; all unused arguments\n\n        if level_save not in ['LOW', 'MEDIUM', 'HIGH']:\n            raise ValueError(f\"The object level_save ({level_save}) is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\")\n\n        # Get logger\n        self.logger = logging.getLogger(__name__)\n\n        # Model type\n        self.model_type = 'classifier'\n\n        # Multi-labels ?\n        self.multi_label = multi_label\n\n        # Classes list to use (set on fit)\n        self.list_classes = None\n        self.dict_classes = None\n\n        # Other options\n        self.level_save = level_save\n\n    @utils.trained_needed\n    def predict_with_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; Tuple[np.ndarray, np.ndarray]:\n        '''Predictions on test with probabilities\n\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Returns:\n            predicted_class (np.ndarray): The predicted classes, shape = [n_samples, n_classes] if multi-labels, shape = [n_samples, 1] otherwise\n            predicted_proba (np.ndarray): The predicted probabilities for each class, shape = [n_samples, n_classes]\n        '''\n        # Process\n        predicted_proba = self.predict(x_test, return_proba=True, **kwargs)\n        predicted_class = self.get_classes_from_proba(predicted_proba)\n        return predicted_class, predicted_proba\n\n    @utils.trained_needed\n    def get_predict_position(self, x_test: pd.DataFrame, y_true) -&gt; np.ndarray:\n        '''Gets the order of predictions of y_true.\n        Positions start at 1 (not 0)\n\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n            y_true (?): Array-like, shape = [n_samples, n_targets]\n        Raises:\n            ValueError: Can't use this method with multi-labels tasks\n        Returns:\n            predict_positions (np.ndarray): The order of prediction of y_true shape = [n_samples, ]\n        '''\n        if self.multi_label:\n            raise ValueError(\"The method 'get_predict_position' is unavailable with multi-labels tasks\")\n        # Process\n        # Cast as pd.Series\n        y_true = pd.Series(y_true)\n        # Get predicted probabilities\n        predicted_proba = self.predict(x_test, return_proba=True)\n        # Get position\n        order = predicted_proba.argsort()\n        ranks = len(self.list_classes) - order.argsort()\n        df_probas = pd.DataFrame(ranks, columns=self.list_classes)\n        predict_positions = np.array([df_probas.loc[i, cl] if cl in df_probas.columns else -1 for i, cl in enumerate(y_true)])\n        return predict_positions\n\n    def get_classes_from_proba(self, predicted_proba: np.ndarray) -&gt; np.ndarray:\n        '''Gets the classes from probabilities\n\n        Args:\n            predicted_proba (np.ndarray): The probabilities predicted by the model, shape = [n_samples, n_classes]\n        Returns:\n            predicted_class (np.ndarray): Shape = [n_samples, n_classes] if multi-labels, shape = [n_samples] otherwise\n        '''\n        if not self.multi_label:\n            predicted_class = np.vectorize(lambda x: self.dict_classes[x])(predicted_proba.argmax(axis=-1))\n        else:\n            # If multi-labels, returns a list of 0 and 1\n            predicted_class = np.vectorize(lambda x: 1 if x &gt;= 0.5 else 0)(predicted_proba)\n        return predicted_class\n\n    def get_top_n_from_proba(self, predicted_proba: np.ndarray, n: int = 5) -&gt; Tuple[list, list]:\n        '''Gets the Top n predictions from probabilities\n\n        Args:\n            predicted_proba (np.ndarray): Predicted probabilities = [n_samples, n_classes]\n        kwargs:\n            n (int): Number of classes to return\n        Raises:\n            ValueError: If the number of classes to return is greater than the number of classes of the model\n        Returns:\n            top_n (list): Top n predicted classes\n            top_n_proba (list): Top n probabilities (corresponding to the top_n list of classes)\n        '''\n        # TODO: Make this method available with multi-labels tasks\n        if self.multi_label:\n            raise ValueError(\"The method 'get_top_n_from_proba' is unavailable with multi-labels tasks\")\n        if self.list_classes is not None and n &gt; len(self.list_classes):\n            raise ValueError(\"The number of classes to return is greater than the number of classes of the model\")\n        # Process\n        idx = predicted_proba.argsort()[:, -n:][:, ::-1]\n        top_n_proba = list(np.take_along_axis(predicted_proba, idx, axis=1))\n        top_n = list(np.vectorize(lambda x: self.dict_classes[x])(idx))\n        return top_n, top_n_proba\n\n    def inverse_transform(self, y: Union[list, np.ndarray]) -&gt; Union[list, tuple]:\n        '''Gets a list of classes from the predictions (mainly useful for multi-labels)\n\n        Args:\n            y (list | np.ndarray): Array-like, shape = [n_samples, n_classes], arrays of 0s and 1s\n                   OR 1D array shape = [n_classes] (only one prediction)\n        Raises:\n            ValueError: If the size of y does not correspond to the number of classes of the model\n        Returns:\n            List of tuple if multi-labels and several predictions\n            Tuple if multi-labels and one prediction\n            List of classes if mono-label\n        '''\n        # If multi-labels, get classes in tuple\n        if self.multi_label:\n            if y.shape[-1] != len(self.list_classes):  # We consider \"-1\" in order to take care of the case where y is 1D\n                raise ValueError(f\"The size of y ({y.shape[-1]}) does not correspond\"\n                                 f\" to the number of classes of the model : ({len(self.list_classes)})\")\n            # Manage 1D array (only one prediction)\n            if len(y.shape) == 1:\n                return tuple(np.array(self.list_classes).compress(y))\n            # Several predictions\n            else:\n                return [tuple(np.array(self.list_classes).compress(indicators)) for indicators in y]\n        # If mono-label, just cast in list if y is np array\n        else:\n            return list(y) if isinstance(y, np.ndarray) else y\n\n    def get_and_save_metrics(self, y_true, y_pred, df_x: Union[pd.DataFrame, None] = None,\n                             series_to_add: Union[List[pd.Series], None] = None,\n                             type_data: str = '') -&gt; pd.DataFrame:\n        '''Gets and saves the metrics of a model\n\n        Args:\n            y_true (?): Array-like, shape = [n_samples, n_targets]\n            y_pred (?): Array-like, shape = [n_samples, n_targets]\n        Kwargs:\n            df_x (pd.DataFrame or None): Input dataFrame used for the prediction\n            series_to_add (list&lt;pd.Series&gt;): List of pd.Series to add to the dataframe\n            type_data (str): Type of dataset (validation, test, ...)\n        Returns:\n            pd.DataFrame: The dataframe containing the statistics\n        '''\n        # Cast to np.array\n        y_true = np.array(y_true)\n        y_pred = np.array(y_pred)\n\n        # Check shapes\n        if not self.multi_label:\n            if len(y_true.shape) == 2 and y_true.shape[1] == 1:\n                y_true = np.ravel(y_true)\n            if len(y_pred.shape) == 2 and y_pred.shape[1] == 1:\n                y_pred = np.ravel(y_pred)\n\n        # Save a predictionn file if wanted\n        if self.level_save == 'HIGH':\n            # Inverse transform\n            y_true_df = list(self.inverse_transform(y_true))\n            y_pred_df = list(self.inverse_transform(y_pred))\n\n            # Concat in a dataframe\n            if df_x is not None:\n                df = df_x.copy()\n                df['y_true'] = y_true_df\n                df['y_pred'] = y_pred_df\n            else:\n                df = pd.DataFrame({'y_true': y_true_df, 'y_pred': y_pred_df})\n            # Add a matched column\n            df['matched'] = (df['y_true'] == df['y_pred']).astype(int)\n            # Add some more columns\n            if series_to_add is not None:\n                for ser in series_to_add:\n                    df[ser.name] = ser.reset_index(drop=True).reindex(index=df.index)  # Reindex\n\n            # Save predictions\n            file_path = os.path.join(self.model_dir, f\"predictions{'_' + type_data if len(type_data) &gt; 0 else ''}.csv\")\n            df.sort_values('matched', ascending=True).to_csv(file_path, sep=';', index=None, encoding='utf-8')\n\n        # Gets global f1 score / acc_tot / trues / falses / precision / recall / support\n        if self.multi_label:\n            f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n            trues = np.sum(np.all(np.equal(y_true, y_pred), axis=1))\n            falses = len(y_true) - trues\n            acc_tot = trues / len(y_true)\n            precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n            recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n            support = list(pd.DataFrame(y_true).sum().values)\n            support = [_ / sum(support) for _ in support] + [1.0]\n        else:\n            # We use 'weighted' even in the mono-label case since there can be several classes !\n            f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n            trues = np.sum(y_true == y_pred)\n            falses = np.sum(y_true != y_pred)\n            acc_tot = accuracy_score(y_true, y_pred)\n            precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n            recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n            labels_tmp, counts_tmp = np.unique(y_true, return_counts=True)\n            support = [0.] * len(self.list_classes) + [1.0]\n            for i, cl in enumerate(self.list_classes):\n                if cl in labels_tmp:\n                    idx_tmp = list(labels_tmp).index(cl)\n                    support[i] = counts_tmp[idx_tmp] / y_pred.shape[0]\n\n        # Global Statistics\n        self.logger.info('-- * * * * * * * * * * * * * * --')\n        self.logger.info(f\"Statistics f1-score{' ' + type_data if len(type_data) &gt; 0 else ''}\")\n        self.logger.info('--------------------------------')\n        self.logger.info(f\"Total accuracy : {round(acc_tot * 100, 2)}% \\t Trues: {trues} \\t Falses: {falses}\")\n        self.logger.info(f\"F1-score (weighted) : {round(f1_weighted, 5)}\")\n        self.logger.info(f\"Precision (weighted) : {round(precision_weighted, 5)}\")\n        self.logger.info(f\"Recall (weighted) : {round(recall_weighted, 5)}\")\n        self.logger.info('--------------------------------')\n\n        # Metrics file\n        dict_df_stats = {}\n\n        # Add metrics depending on mono/multi labels &amp; manage confusion matrices\n        labels = self.list_classes\n        log_stats = len(labels) &lt; 50\n        if self.multi_label:\n            # Details per category\n            mcm = multilabel_confusion_matrix(y_true, y_pred)\n            for i, label in enumerate(labels):\n                c_mat = mcm[i]\n                dict_df_stats[i] = self._update_info_from_c_mat(c_mat, label, log_info=log_stats)\n                # Plot individual confusion matrix if level_save &gt; LOW\n                if self.level_save in ['MEDIUM', 'HIGH']:\n                    none_class = 'not_' + label\n                    tmp_label = re.sub(r',|:|\\s', '_', label)\n                    self._plot_confusion_matrix(c_mat, [none_class, label], type_data=f\"{tmp_label}_{type_data}\",\n                                                normalized=False, subdir=type_data)\n                    self._plot_confusion_matrix(c_mat, [none_class, label], type_data=f\"{tmp_label}_{type_data}\",\n                                                normalized=True, subdir=type_data)\n        else:\n            # Plot confusion matrices if level_save &gt; LOW\n            if self.level_save in ['MEDIUM', 'HIGH']:\n                if len(labels) &gt; 50:\n                    self.logger.warning(\n                        f\"Warning, there are {len(labels)} categories to plot in the confusion matrix.\\n\"\n                        \"Heavy chances of slowness/display bugs/crashes...\\n\"\n                        \"SKIP the plots\"\n                    )\n                else:\n                    # Global statistics\n                    c_mat = confusion_matrix(y_true, y_pred, labels=labels)\n                    self._plot_confusion_matrix(c_mat, labels, type_data=type_data, normalized=False)\n                    self._plot_confusion_matrix(c_mat, labels, type_data=type_data, normalized=True)\n\n            # Get statistics per class\n            for i, label in enumerate(labels):\n                label_str = str(label)  # Fix : If label is an int, can cause some problems (e.g. only zeroes in the confusion matrix)\n                none_class = 'None' if label_str != 'None' else 'others'  # Check that the class is not already 'None'\n                y_true_tmp = [label_str if _ == label else none_class for _ in y_true]\n                y_pred_tmp = [label_str if _ == label else none_class for _ in y_pred]\n                c_mat_tmp = confusion_matrix(y_true_tmp, y_pred_tmp, labels=[none_class, label_str])\n                dict_df_stats[i] = self._update_info_from_c_mat(c_mat_tmp, label, log_info=False)\n\n        # Add global statistics\n        dict_df_stats[i+1] = {\n            'Label': 'All',\n            'F1-Score': f1_weighted,\n            'Accuracy': acc_tot,\n            'Precision': precision_weighted,\n            'Recall': recall_weighted,\n            'Trues': trues,\n            'Falses': falses,\n            'True positive': None,\n            'True negative': None,\n            'False positive': None,\n            'False negative': None,\n            'Condition positive': None,\n            'Condition negative': None,\n            'Predicted positive': None,\n            'Predicted negative': None,\n        }\n        df_stats = pd.DataFrame.from_dict(dict_df_stats, orient='index')\n\n        # Add support\n        df_stats['Support'] = support\n\n        # Save .csv\n        file_path = os.path.join(self.model_dir, f\"f1{'_' + type_data if len(type_data) &gt; 0 else ''}@{f1_weighted}.csv\")\n        df_stats.to_csv(file_path, sep=';', index=False, encoding='utf-8')\n\n        # Save accuracy\n        acc_path = os.path.join(self.model_dir, f\"acc{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(acc_tot, 5)}\")\n        with open(acc_path, 'w'):\n            pass\n\n        return df_stats\n\n    def get_metrics_simple_monolabel(self, y_true, y_pred) -&gt; pd.DataFrame:\n        '''Gets metrics on mono-label predictions\n        Same as the method get_and_save_metrics but without all the fluff (save, etc.)\n\n        Args:\n            y_true (?): Array-like, shape = [n_samples,]\n            y_pred (?): Array-like, shape = [n_samples,]\n        Raises:\n            ValueError: If not in mono-label mode\n        Returns:\n            pd.DataFrame: The dataframe containing statistics\n        '''\n        if self.multi_label:\n            raise ValueError(\"The method get_metrics_simple_monolabel only works for the mono-label case\")\n\n        # Cast to np.array\n        y_true = np.array(y_true)\n        y_pred = np.array(y_pred)\n\n        # Check shapes\n        if len(y_true.shape) == 2 and y_true.shape[1] == 1:\n            y_true = np.ravel(y_true)\n        if len(y_pred.shape) == 2 and y_pred.shape[1] == 1:\n            y_pred = np.ravel(y_pred)\n\n        # Gets global f1 score / acc_tot / trues / falses / precision / recall / support\n        f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n        trues = np.sum(y_true == y_pred)\n        falses = np.sum(y_true != y_pred)\n        acc_tot = accuracy_score(y_true, y_pred)\n        precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n        recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n        labels_tmp, counts_tmp = np.unique(y_true, return_counts=True)\n        support = [0.] * len(self.list_classes) + [1.0]\n        for i, cl in enumerate(self.list_classes):\n            if cl in labels_tmp:\n                idx_tmp = list(labels_tmp).index(cl)\n                support[i] = counts_tmp[idx_tmp] / y_pred.shape[0]\n\n        # DataFrame metrics\n        dict_df_stats = {}\n\n        # Get statistics per class\n        labels = self.list_classes\n        for i, label in enumerate(labels):\n            label_str = str(label)  # Fix : If label is an int, can cause some problems (e.g. only zeroes in the confusion matrix)\n            none_class = 'None' if label_str != 'None' else 'others'  # Check that the class is not already 'None'\n            y_true_tmp = [label_str if _ == label else none_class for _ in y_true]\n            y_pred_tmp = [label_str if _ == label else none_class for _ in y_pred]\n            c_mat_tmp = confusion_matrix(y_true_tmp, y_pred_tmp, labels=[none_class, label_str])\n            dict_df_stats[i] = self._update_info_from_c_mat(c_mat_tmp, label, log_info=False)\n\n        # Add global statistics\n        dict_df_stats[i+1] = {\n            'Label': 'All',\n            'F1-Score': f1_weighted,\n            'Accuracy': acc_tot,\n            'Precision': precision_weighted,\n            'Recall': recall_weighted,\n            'Trues': trues,\n            'Falses': falses,\n            'True positive': None,\n            'True negative': None,\n            'False positive': None,\n            'False negative': None,\n            'Condition positive': None,\n            'Condition negative': None,\n            'Predicted positive': None,\n            'Predicted negative': None,\n        }\n        df_stats = pd.DataFrame.from_dict(dict_df_stats, orient='index')\n\n        # Add support\n        df_stats['Support'] = support\n\n        # Return dataframe\n        return df_stats\n\n    def get_metrics_simple_multilabel(self, y_true, y_pred) -&gt; pd.DataFrame:\n        '''Gets metrics on multi-label predictions\n        Same as the method get_and_save_metrics but without all the fluff (save, etc.)\n\n        Args:\n            y_true (?): Array-like, shape = [n_samples, n_targets]\n            y_pred (?): Array-like, shape = [n_samples, n_targets]\n        Raises:\n            ValueError: If not with multi-labels tasks\n        Returns:\n            pd.DataFrame: The dataframe containing statistics\n        '''\n        if not self.multi_label:\n            raise ValueError(\"The method get_metrics_simple_multilabel only works for multi-labels cases\")\n\n        # Cast to np.array\n        y_true = np.array(y_true)\n        y_pred = np.array(y_pred)\n\n        # Gets global f1 score / acc_tot / trues / falses / precision / recall / support\n        f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n        trues = np.sum(np.all(np.equal(y_true, y_pred), axis=1))\n        falses = len(y_true) - trues\n        acc_tot = trues / len(y_true)\n        precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n        recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n        support = list(pd.DataFrame(y_true).sum().values)\n        support = [_ / sum(support) for _ in support] + [1.0]\n\n        # DataFrame metrics\n        dict_df_stats = {}\n\n        # Add metrics\n        labels = self.list_classes\n        # Details per category\n        mcm = multilabel_confusion_matrix(y_true, y_pred)\n        for i, label in enumerate(labels):\n            c_mat = mcm[i]\n            dict_df_stats[i] = self._update_info_from_c_mat(c_mat, label, log_info=False)\n\n        # Add global statistics\n        dict_df_stats[i+1] = {\n            'Label': 'All',\n            'F1-Score': f1_weighted,\n            'Accuracy': acc_tot,\n            'Precision': precision_weighted,\n            'Recall': recall_weighted,\n            'Trues': trues,\n            'Falses': falses,\n            'True positive': None,\n            'True negative': None,\n            'False positive': None,\n            'False negative': None,\n            'Condition positive': None,\n            'Condition negative': None,\n            'Predicted positive': None,\n            'Predicted negative': None,\n        }\n        df_stats = pd.DataFrame.from_dict(dict_df_stats, orient='index')\n\n        # Add support\n        df_stats['Support'] = support\n\n        # Return dataframe\n        return df_stats\n\n    def _update_info_from_c_mat(self, c_mat: np.ndarray, label: str, log_info: bool = True) -&gt; dict:\n        '''Updates a dataframe for the method get_and_save_metrics, given a confusion matrix\n\n        Args:\n            c_mat (np.ndarray): Confusion matrix\n            label (str): Label to use\n        Kwargs:\n            log_info (bool): If the statistics must be logged\n        Returns:\n            dict: Dictionary with the information for the update of the dataframe\n        '''\n        # Extract all needed info from c_mat\n        true_negative = c_mat[0][0]\n        true_positive = c_mat[1][1]\n        false_negative = c_mat[1][0]\n        false_positive = c_mat[0][1]\n        condition_positive = false_negative + true_positive\n        condition_negative = false_positive + true_negative\n        predicted_positive = false_positive + true_positive\n        predicted_negative = false_negative + true_negative\n        trues_cat = true_negative + true_positive\n        falses_cat = false_negative + false_positive\n        accuracy = (true_negative + true_positive) / (true_negative + true_positive + false_negative + false_positive)\n        precision = 0 if predicted_positive == 0 else true_positive / predicted_positive\n        recall = 0 if condition_positive == 0 else true_positive / condition_positive\n        f1 = 0 if precision + recall == 0 else 2 * precision * recall / (precision + recall)\n\n        # Display some info\n        if log_info:\n            self.logger.info(\n                f\"F1-score: {round(f1, 5)}  \\t Precision: {round(100 * precision, 2)}% \\t\"\n                f\"Recall: {round(100 * recall, 2)}% \\t Trues: {trues_cat} \\t Falses: {falses_cat} \\t\\t --- {label} \"\n            )\n\n        # Return result\n        return {\n            'Label': f'{label}',\n            'F1-Score': f1,\n            'Accuracy': accuracy,\n            'Precision': precision,\n            'Recall': recall,\n            'Trues': trues_cat,\n            'Falses': falses_cat,\n            'True positive': true_positive,\n            'True negative': true_negative,\n            'False positive': false_positive,\n            'False negative': false_negative,\n            'Condition positive': condition_positive,\n            'Condition negative': condition_negative,\n            'Predicted positive': predicted_positive,\n            'Predicted negative': predicted_negative,\n        }\n\n    def _plot_confusion_matrix(self, c_mat: np.ndarray, labels: list, type_data: str = '',\n                               normalized: bool = False, subdir: Union[str, None] = None) -&gt; None:\n        '''Plots a confusion matrix\n\n        Args:\n            c_mat (np.ndarray): Confusion matrix\n            labels (list): Labels to plot\n        Kwargs:\n            type_data (str): Type of dataset (validation, test, ...)\n            normalized (bool): If the confusion matrix should be normalized\n            subdir (str): Sub-directory for writing the plot\n        '''\n\n        # Get title\n        if normalized:\n            title = f\"Normalized confusion matrix{' - ' + type_data if len(type_data) &gt; 0 else ''}\"\n        else:\n            title = f\"Confusion matrix, without normalization{' - ' + type_data if len(type_data) &gt; 0 else ''}\"\n\n        # Init. plot\n        width = round(10 + 0.5 * len(c_mat))\n        height = round(4 / 5 * width)\n        fig, ax = plt.subplots(figsize=(width, height))\n\n        # Plot\n        if normalized:\n            c_mat = c_mat.astype('float') / c_mat.sum(axis=1)[:, np.newaxis]\n            sns.heatmap(c_mat, annot=True, fmt=\".2f\", cmap=plt.cm.Blues, ax=ax) # type: ignore\n        else:\n            sns.heatmap(c_mat, annot=True, fmt=\"d\", cmap=plt.cm.Blues, ax=ax) # type: ignore\n\n        # labels, title and ticks\n        ax.set_xlabel('Predicted classes', fontsize=height * 2)\n        ax.set_ylabel('Real classes', fontsize=height * 2)\n        ax.set_title(title, fontsize=width * 2)\n        ax.xaxis.set_ticklabels(labels)\n        ax.yaxis.set_ticklabels(labels)\n        plt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\n        plt.setp(ax.get_yticklabels(), rotation=30, horizontalalignment='right')\n        plt.tight_layout()\n\n        # Save\n        plots_path = os.path.join(self.model_dir, 'plots')\n        if subdir is not None:  # Add subdir\n            plots_path = os.path.join(plots_path, subdir)\n        file_name = f\"{type_data + '_' if len(type_data) &gt; 0 else ''}confusion_matrix{'_normalized' if normalized else ''}.png\"\n        if not os.path.exists(plots_path):\n            os.makedirs(plots_path)\n        plt.savefig(os.path.join(plots_path, file_name))\n\n        # Close figures\n        plt.close('all')\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save model\n        if json_data is None:\n            json_data = {}\n\n        json_data['list_classes'] = self.list_classes\n        json_data['dict_classes'] = self.dict_classes\n        json_data['multi_label'] = self.multi_label\n\n        # Save\n        super().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_classifier/#template_num.models_training.classifiers.model_classifier.ModelClassifierMixin.__init__","title":"<code>__init__(level_save='HIGH', multi_label=False, **kwargs)</code>","text":"<p>Initialization of the class</p> Kwargs <p>level_save (str): Level of saving     LOW: stats + configurations + logger keras - /! The model can't be reused /! -     MEDIUM: LOWlevel_save + hdf5 + pkl + plots     HIGH: MEDIUM + predictions multi_label (bool): If the classification must be multi-labels</p> <p>Raises:     ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])</p> Source code in <code>template_num/models_training/classifiers/model_classifier.py</code> <pre><code>def __init__(self, level_save: str = 'HIGH', multi_label: bool = False, **kwargs) -&gt; None:\n    '''Initialization of the class\n\n    Kwargs:\n        level_save (str): Level of saving\n            LOW: stats + configurations + logger keras - /!\\\\ The model can't be reused /!\\\\ -\n            MEDIUM: LOWlevel_save + hdf5 + pkl + plots\n            HIGH: MEDIUM + predictions\n        multi_label (bool): If the classification must be multi-labels\n    Raises:\n        ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\n    '''\n    super().__init__(level_save=level_save, **kwargs)  # forwards level_save &amp; all unused arguments\n\n    if level_save not in ['LOW', 'MEDIUM', 'HIGH']:\n        raise ValueError(f\"The object level_save ({level_save}) is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\")\n\n    # Get logger\n    self.logger = logging.getLogger(__name__)\n\n    # Model type\n    self.model_type = 'classifier'\n\n    # Multi-labels ?\n    self.multi_label = multi_label\n\n    # Classes list to use (set on fit)\n    self.list_classes = None\n    self.dict_classes = None\n\n    # Other options\n    self.level_save = level_save\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_classifier/#template_num.models_training.classifiers.model_classifier.ModelClassifierMixin.get_and_save_metrics","title":"<code>get_and_save_metrics(y_true, y_pred, df_x=None, series_to_add=None, type_data='')</code>","text":"<p>Gets and saves the metrics of a model</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>?</code> <p>Array-like, shape = [n_samples, n_targets]</p> required <code>y_pred</code> <code>?</code> <p>Array-like, shape = [n_samples, n_targets]</p> required <p>Kwargs:     df_x (pd.DataFrame or None): Input dataFrame used for the prediction     series_to_add (list): List of pd.Series to add to the dataframe     type_data (str): Type of dataset (validation, test, ...) Returns:     pd.DataFrame: The dataframe containing the statistics Source code in <code>template_num/models_training/classifiers/model_classifier.py</code> <pre><code>def get_and_save_metrics(self, y_true, y_pred, df_x: Union[pd.DataFrame, None] = None,\n                         series_to_add: Union[List[pd.Series], None] = None,\n                         type_data: str = '') -&gt; pd.DataFrame:\n    '''Gets and saves the metrics of a model\n\n    Args:\n        y_true (?): Array-like, shape = [n_samples, n_targets]\n        y_pred (?): Array-like, shape = [n_samples, n_targets]\n    Kwargs:\n        df_x (pd.DataFrame or None): Input dataFrame used for the prediction\n        series_to_add (list&lt;pd.Series&gt;): List of pd.Series to add to the dataframe\n        type_data (str): Type of dataset (validation, test, ...)\n    Returns:\n        pd.DataFrame: The dataframe containing the statistics\n    '''\n    # Cast to np.array\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Check shapes\n    if not self.multi_label:\n        if len(y_true.shape) == 2 and y_true.shape[1] == 1:\n            y_true = np.ravel(y_true)\n        if len(y_pred.shape) == 2 and y_pred.shape[1] == 1:\n            y_pred = np.ravel(y_pred)\n\n    # Save a predictionn file if wanted\n    if self.level_save == 'HIGH':\n        # Inverse transform\n        y_true_df = list(self.inverse_transform(y_true))\n        y_pred_df = list(self.inverse_transform(y_pred))\n\n        # Concat in a dataframe\n        if df_x is not None:\n            df = df_x.copy()\n            df['y_true'] = y_true_df\n            df['y_pred'] = y_pred_df\n        else:\n            df = pd.DataFrame({'y_true': y_true_df, 'y_pred': y_pred_df})\n        # Add a matched column\n        df['matched'] = (df['y_true'] == df['y_pred']).astype(int)\n        # Add some more columns\n        if series_to_add is not None:\n            for ser in series_to_add:\n                df[ser.name] = ser.reset_index(drop=True).reindex(index=df.index)  # Reindex\n\n        # Save predictions\n        file_path = os.path.join(self.model_dir, f\"predictions{'_' + type_data if len(type_data) &gt; 0 else ''}.csv\")\n        df.sort_values('matched', ascending=True).to_csv(file_path, sep=';', index=None, encoding='utf-8')\n\n    # Gets global f1 score / acc_tot / trues / falses / precision / recall / support\n    if self.multi_label:\n        f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n        trues = np.sum(np.all(np.equal(y_true, y_pred), axis=1))\n        falses = len(y_true) - trues\n        acc_tot = trues / len(y_true)\n        precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n        recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n        support = list(pd.DataFrame(y_true).sum().values)\n        support = [_ / sum(support) for _ in support] + [1.0]\n    else:\n        # We use 'weighted' even in the mono-label case since there can be several classes !\n        f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n        trues = np.sum(y_true == y_pred)\n        falses = np.sum(y_true != y_pred)\n        acc_tot = accuracy_score(y_true, y_pred)\n        precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n        recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n        labels_tmp, counts_tmp = np.unique(y_true, return_counts=True)\n        support = [0.] * len(self.list_classes) + [1.0]\n        for i, cl in enumerate(self.list_classes):\n            if cl in labels_tmp:\n                idx_tmp = list(labels_tmp).index(cl)\n                support[i] = counts_tmp[idx_tmp] / y_pred.shape[0]\n\n    # Global Statistics\n    self.logger.info('-- * * * * * * * * * * * * * * --')\n    self.logger.info(f\"Statistics f1-score{' ' + type_data if len(type_data) &gt; 0 else ''}\")\n    self.logger.info('--------------------------------')\n    self.logger.info(f\"Total accuracy : {round(acc_tot * 100, 2)}% \\t Trues: {trues} \\t Falses: {falses}\")\n    self.logger.info(f\"F1-score (weighted) : {round(f1_weighted, 5)}\")\n    self.logger.info(f\"Precision (weighted) : {round(precision_weighted, 5)}\")\n    self.logger.info(f\"Recall (weighted) : {round(recall_weighted, 5)}\")\n    self.logger.info('--------------------------------')\n\n    # Metrics file\n    dict_df_stats = {}\n\n    # Add metrics depending on mono/multi labels &amp; manage confusion matrices\n    labels = self.list_classes\n    log_stats = len(labels) &lt; 50\n    if self.multi_label:\n        # Details per category\n        mcm = multilabel_confusion_matrix(y_true, y_pred)\n        for i, label in enumerate(labels):\n            c_mat = mcm[i]\n            dict_df_stats[i] = self._update_info_from_c_mat(c_mat, label, log_info=log_stats)\n            # Plot individual confusion matrix if level_save &gt; LOW\n            if self.level_save in ['MEDIUM', 'HIGH']:\n                none_class = 'not_' + label\n                tmp_label = re.sub(r',|:|\\s', '_', label)\n                self._plot_confusion_matrix(c_mat, [none_class, label], type_data=f\"{tmp_label}_{type_data}\",\n                                            normalized=False, subdir=type_data)\n                self._plot_confusion_matrix(c_mat, [none_class, label], type_data=f\"{tmp_label}_{type_data}\",\n                                            normalized=True, subdir=type_data)\n    else:\n        # Plot confusion matrices if level_save &gt; LOW\n        if self.level_save in ['MEDIUM', 'HIGH']:\n            if len(labels) &gt; 50:\n                self.logger.warning(\n                    f\"Warning, there are {len(labels)} categories to plot in the confusion matrix.\\n\"\n                    \"Heavy chances of slowness/display bugs/crashes...\\n\"\n                    \"SKIP the plots\"\n                )\n            else:\n                # Global statistics\n                c_mat = confusion_matrix(y_true, y_pred, labels=labels)\n                self._plot_confusion_matrix(c_mat, labels, type_data=type_data, normalized=False)\n                self._plot_confusion_matrix(c_mat, labels, type_data=type_data, normalized=True)\n\n        # Get statistics per class\n        for i, label in enumerate(labels):\n            label_str = str(label)  # Fix : If label is an int, can cause some problems (e.g. only zeroes in the confusion matrix)\n            none_class = 'None' if label_str != 'None' else 'others'  # Check that the class is not already 'None'\n            y_true_tmp = [label_str if _ == label else none_class for _ in y_true]\n            y_pred_tmp = [label_str if _ == label else none_class for _ in y_pred]\n            c_mat_tmp = confusion_matrix(y_true_tmp, y_pred_tmp, labels=[none_class, label_str])\n            dict_df_stats[i] = self._update_info_from_c_mat(c_mat_tmp, label, log_info=False)\n\n    # Add global statistics\n    dict_df_stats[i+1] = {\n        'Label': 'All',\n        'F1-Score': f1_weighted,\n        'Accuracy': acc_tot,\n        'Precision': precision_weighted,\n        'Recall': recall_weighted,\n        'Trues': trues,\n        'Falses': falses,\n        'True positive': None,\n        'True negative': None,\n        'False positive': None,\n        'False negative': None,\n        'Condition positive': None,\n        'Condition negative': None,\n        'Predicted positive': None,\n        'Predicted negative': None,\n    }\n    df_stats = pd.DataFrame.from_dict(dict_df_stats, orient='index')\n\n    # Add support\n    df_stats['Support'] = support\n\n    # Save .csv\n    file_path = os.path.join(self.model_dir, f\"f1{'_' + type_data if len(type_data) &gt; 0 else ''}@{f1_weighted}.csv\")\n    df_stats.to_csv(file_path, sep=';', index=False, encoding='utf-8')\n\n    # Save accuracy\n    acc_path = os.path.join(self.model_dir, f\"acc{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(acc_tot, 5)}\")\n    with open(acc_path, 'w'):\n        pass\n\n    return df_stats\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_classifier/#template_num.models_training.classifiers.model_classifier.ModelClassifierMixin.get_classes_from_proba","title":"<code>get_classes_from_proba(predicted_proba)</code>","text":"<p>Gets the classes from probabilities</p> <p>Parameters:</p> Name Type Description Default <code>predicted_proba</code> <code>ndarray</code> <p>The probabilities predicted by the model, shape = [n_samples, n_classes]</p> required <p>Returns:     predicted_class (np.ndarray): Shape = [n_samples, n_classes] if multi-labels, shape = [n_samples] otherwise</p> Source code in <code>template_num/models_training/classifiers/model_classifier.py</code> <pre><code>def get_classes_from_proba(self, predicted_proba: np.ndarray) -&gt; np.ndarray:\n    '''Gets the classes from probabilities\n\n    Args:\n        predicted_proba (np.ndarray): The probabilities predicted by the model, shape = [n_samples, n_classes]\n    Returns:\n        predicted_class (np.ndarray): Shape = [n_samples, n_classes] if multi-labels, shape = [n_samples] otherwise\n    '''\n    if not self.multi_label:\n        predicted_class = np.vectorize(lambda x: self.dict_classes[x])(predicted_proba.argmax(axis=-1))\n    else:\n        # If multi-labels, returns a list of 0 and 1\n        predicted_class = np.vectorize(lambda x: 1 if x &gt;= 0.5 else 0)(predicted_proba)\n    return predicted_class\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_classifier/#template_num.models_training.classifiers.model_classifier.ModelClassifierMixin.get_metrics_simple_monolabel","title":"<code>get_metrics_simple_monolabel(y_true, y_pred)</code>","text":"<p>Gets metrics on mono-label predictions Same as the method get_and_save_metrics but without all the fluff (save, etc.)</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>?</code> <p>Array-like, shape = [n_samples,]</p> required <code>y_pred</code> <code>?</code> <p>Array-like, shape = [n_samples,]</p> required <p>Raises:     ValueError: If not in mono-label mode Returns:     pd.DataFrame: The dataframe containing statistics</p> Source code in <code>template_num/models_training/classifiers/model_classifier.py</code> <pre><code>def get_metrics_simple_monolabel(self, y_true, y_pred) -&gt; pd.DataFrame:\n    '''Gets metrics on mono-label predictions\n    Same as the method get_and_save_metrics but without all the fluff (save, etc.)\n\n    Args:\n        y_true (?): Array-like, shape = [n_samples,]\n        y_pred (?): Array-like, shape = [n_samples,]\n    Raises:\n        ValueError: If not in mono-label mode\n    Returns:\n        pd.DataFrame: The dataframe containing statistics\n    '''\n    if self.multi_label:\n        raise ValueError(\"The method get_metrics_simple_monolabel only works for the mono-label case\")\n\n    # Cast to np.array\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Check shapes\n    if len(y_true.shape) == 2 and y_true.shape[1] == 1:\n        y_true = np.ravel(y_true)\n    if len(y_pred.shape) == 2 and y_pred.shape[1] == 1:\n        y_pred = np.ravel(y_pred)\n\n    # Gets global f1 score / acc_tot / trues / falses / precision / recall / support\n    f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n    trues = np.sum(y_true == y_pred)\n    falses = np.sum(y_true != y_pred)\n    acc_tot = accuracy_score(y_true, y_pred)\n    precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n    recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n    labels_tmp, counts_tmp = np.unique(y_true, return_counts=True)\n    support = [0.] * len(self.list_classes) + [1.0]\n    for i, cl in enumerate(self.list_classes):\n        if cl in labels_tmp:\n            idx_tmp = list(labels_tmp).index(cl)\n            support[i] = counts_tmp[idx_tmp] / y_pred.shape[0]\n\n    # DataFrame metrics\n    dict_df_stats = {}\n\n    # Get statistics per class\n    labels = self.list_classes\n    for i, label in enumerate(labels):\n        label_str = str(label)  # Fix : If label is an int, can cause some problems (e.g. only zeroes in the confusion matrix)\n        none_class = 'None' if label_str != 'None' else 'others'  # Check that the class is not already 'None'\n        y_true_tmp = [label_str if _ == label else none_class for _ in y_true]\n        y_pred_tmp = [label_str if _ == label else none_class for _ in y_pred]\n        c_mat_tmp = confusion_matrix(y_true_tmp, y_pred_tmp, labels=[none_class, label_str])\n        dict_df_stats[i] = self._update_info_from_c_mat(c_mat_tmp, label, log_info=False)\n\n    # Add global statistics\n    dict_df_stats[i+1] = {\n        'Label': 'All',\n        'F1-Score': f1_weighted,\n        'Accuracy': acc_tot,\n        'Precision': precision_weighted,\n        'Recall': recall_weighted,\n        'Trues': trues,\n        'Falses': falses,\n        'True positive': None,\n        'True negative': None,\n        'False positive': None,\n        'False negative': None,\n        'Condition positive': None,\n        'Condition negative': None,\n        'Predicted positive': None,\n        'Predicted negative': None,\n    }\n    df_stats = pd.DataFrame.from_dict(dict_df_stats, orient='index')\n\n    # Add support\n    df_stats['Support'] = support\n\n    # Return dataframe\n    return df_stats\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_classifier/#template_num.models_training.classifiers.model_classifier.ModelClassifierMixin.get_metrics_simple_multilabel","title":"<code>get_metrics_simple_multilabel(y_true, y_pred)</code>","text":"<p>Gets metrics on multi-label predictions Same as the method get_and_save_metrics but without all the fluff (save, etc.)</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>?</code> <p>Array-like, shape = [n_samples, n_targets]</p> required <code>y_pred</code> <code>?</code> <p>Array-like, shape = [n_samples, n_targets]</p> required <p>Raises:     ValueError: If not with multi-labels tasks Returns:     pd.DataFrame: The dataframe containing statistics</p> Source code in <code>template_num/models_training/classifiers/model_classifier.py</code> <pre><code>def get_metrics_simple_multilabel(self, y_true, y_pred) -&gt; pd.DataFrame:\n    '''Gets metrics on multi-label predictions\n    Same as the method get_and_save_metrics but without all the fluff (save, etc.)\n\n    Args:\n        y_true (?): Array-like, shape = [n_samples, n_targets]\n        y_pred (?): Array-like, shape = [n_samples, n_targets]\n    Raises:\n        ValueError: If not with multi-labels tasks\n    Returns:\n        pd.DataFrame: The dataframe containing statistics\n    '''\n    if not self.multi_label:\n        raise ValueError(\"The method get_metrics_simple_multilabel only works for multi-labels cases\")\n\n    # Cast to np.array\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Gets global f1 score / acc_tot / trues / falses / precision / recall / support\n    f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n    trues = np.sum(np.all(np.equal(y_true, y_pred), axis=1))\n    falses = len(y_true) - trues\n    acc_tot = trues / len(y_true)\n    precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n    recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n    support = list(pd.DataFrame(y_true).sum().values)\n    support = [_ / sum(support) for _ in support] + [1.0]\n\n    # DataFrame metrics\n    dict_df_stats = {}\n\n    # Add metrics\n    labels = self.list_classes\n    # Details per category\n    mcm = multilabel_confusion_matrix(y_true, y_pred)\n    for i, label in enumerate(labels):\n        c_mat = mcm[i]\n        dict_df_stats[i] = self._update_info_from_c_mat(c_mat, label, log_info=False)\n\n    # Add global statistics\n    dict_df_stats[i+1] = {\n        'Label': 'All',\n        'F1-Score': f1_weighted,\n        'Accuracy': acc_tot,\n        'Precision': precision_weighted,\n        'Recall': recall_weighted,\n        'Trues': trues,\n        'Falses': falses,\n        'True positive': None,\n        'True negative': None,\n        'False positive': None,\n        'False negative': None,\n        'Condition positive': None,\n        'Condition negative': None,\n        'Predicted positive': None,\n        'Predicted negative': None,\n    }\n    df_stats = pd.DataFrame.from_dict(dict_df_stats, orient='index')\n\n    # Add support\n    df_stats['Support'] = support\n\n    # Return dataframe\n    return df_stats\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_classifier/#template_num.models_training.classifiers.model_classifier.ModelClassifierMixin.get_predict_position","title":"<code>get_predict_position(x_test, y_true)</code>","text":"<p>Gets the order of predictions of y_true. Positions start at 1 (not 0)</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>DataFrame</code> <p>DataFrame with the test data to be predicted</p> required <code>y_true</code> <code>?</code> <p>Array-like, shape = [n_samples, n_targets]</p> required <p>Raises:     ValueError: Can't use this method with multi-labels tasks Returns:     predict_positions (np.ndarray): The order of prediction of y_true shape = [n_samples, ]</p> Source code in <code>template_num/models_training/classifiers/model_classifier.py</code> <pre><code>@utils.trained_needed\ndef get_predict_position(self, x_test: pd.DataFrame, y_true) -&gt; np.ndarray:\n    '''Gets the order of predictions of y_true.\n    Positions start at 1 (not 0)\n\n    Args:\n        x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        y_true (?): Array-like, shape = [n_samples, n_targets]\n    Raises:\n        ValueError: Can't use this method with multi-labels tasks\n    Returns:\n        predict_positions (np.ndarray): The order of prediction of y_true shape = [n_samples, ]\n    '''\n    if self.multi_label:\n        raise ValueError(\"The method 'get_predict_position' is unavailable with multi-labels tasks\")\n    # Process\n    # Cast as pd.Series\n    y_true = pd.Series(y_true)\n    # Get predicted probabilities\n    predicted_proba = self.predict(x_test, return_proba=True)\n    # Get position\n    order = predicted_proba.argsort()\n    ranks = len(self.list_classes) - order.argsort()\n    df_probas = pd.DataFrame(ranks, columns=self.list_classes)\n    predict_positions = np.array([df_probas.loc[i, cl] if cl in df_probas.columns else -1 for i, cl in enumerate(y_true)])\n    return predict_positions\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_classifier/#template_num.models_training.classifiers.model_classifier.ModelClassifierMixin.get_top_n_from_proba","title":"<code>get_top_n_from_proba(predicted_proba, n=5)</code>","text":"<p>Gets the Top n predictions from probabilities</p> <p>Parameters:</p> Name Type Description Default <code>predicted_proba</code> <code>ndarray</code> <p>Predicted probabilities = [n_samples, n_classes]</p> required <p>kwargs:     n (int): Number of classes to return Raises:     ValueError: If the number of classes to return is greater than the number of classes of the model Returns:     top_n (list): Top n predicted classes     top_n_proba (list): Top n probabilities (corresponding to the top_n list of classes)</p> Source code in <code>template_num/models_training/classifiers/model_classifier.py</code> <pre><code>def get_top_n_from_proba(self, predicted_proba: np.ndarray, n: int = 5) -&gt; Tuple[list, list]:\n    '''Gets the Top n predictions from probabilities\n\n    Args:\n        predicted_proba (np.ndarray): Predicted probabilities = [n_samples, n_classes]\n    kwargs:\n        n (int): Number of classes to return\n    Raises:\n        ValueError: If the number of classes to return is greater than the number of classes of the model\n    Returns:\n        top_n (list): Top n predicted classes\n        top_n_proba (list): Top n probabilities (corresponding to the top_n list of classes)\n    '''\n    # TODO: Make this method available with multi-labels tasks\n    if self.multi_label:\n        raise ValueError(\"The method 'get_top_n_from_proba' is unavailable with multi-labels tasks\")\n    if self.list_classes is not None and n &gt; len(self.list_classes):\n        raise ValueError(\"The number of classes to return is greater than the number of classes of the model\")\n    # Process\n    idx = predicted_proba.argsort()[:, -n:][:, ::-1]\n    top_n_proba = list(np.take_along_axis(predicted_proba, idx, axis=1))\n    top_n = list(np.vectorize(lambda x: self.dict_classes[x])(idx))\n    return top_n, top_n_proba\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_classifier/#template_num.models_training.classifiers.model_classifier.ModelClassifierMixin.inverse_transform","title":"<code>inverse_transform(y)</code>","text":"<p>Gets a list of classes from the predictions (mainly useful for multi-labels)</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>list | ndarray</code> <p>Array-like, shape = [n_samples, n_classes], arrays of 0s and 1s    OR 1D array shape = [n_classes] (only one prediction)</p> required <p>Raises:     ValueError: If the size of y does not correspond to the number of classes of the model Returns:     List of tuple if multi-labels and several predictions     Tuple if multi-labels and one prediction     List of classes if mono-label</p> Source code in <code>template_num/models_training/classifiers/model_classifier.py</code> <pre><code>def inverse_transform(self, y: Union[list, np.ndarray]) -&gt; Union[list, tuple]:\n    '''Gets a list of classes from the predictions (mainly useful for multi-labels)\n\n    Args:\n        y (list | np.ndarray): Array-like, shape = [n_samples, n_classes], arrays of 0s and 1s\n               OR 1D array shape = [n_classes] (only one prediction)\n    Raises:\n        ValueError: If the size of y does not correspond to the number of classes of the model\n    Returns:\n        List of tuple if multi-labels and several predictions\n        Tuple if multi-labels and one prediction\n        List of classes if mono-label\n    '''\n    # If multi-labels, get classes in tuple\n    if self.multi_label:\n        if y.shape[-1] != len(self.list_classes):  # We consider \"-1\" in order to take care of the case where y is 1D\n            raise ValueError(f\"The size of y ({y.shape[-1]}) does not correspond\"\n                             f\" to the number of classes of the model : ({len(self.list_classes)})\")\n        # Manage 1D array (only one prediction)\n        if len(y.shape) == 1:\n            return tuple(np.array(self.list_classes).compress(y))\n        # Several predictions\n        else:\n            return [tuple(np.array(self.list_classes).compress(indicators)) for indicators in y]\n    # If mono-label, just cast in list if y is np array\n    else:\n        return list(y) if isinstance(y, np.ndarray) else y\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_classifier/#template_num.models_training.classifiers.model_classifier.ModelClassifierMixin.predict_with_proba","title":"<code>predict_with_proba(x_test, **kwargs)</code>","text":"<p>Predictions on test with probabilities</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>DataFrame</code> <p>DataFrame with the test data to be predicted</p> required <p>Returns:     predicted_class (np.ndarray): The predicted classes, shape = [n_samples, n_classes] if multi-labels, shape = [n_samples, 1] otherwise     predicted_proba (np.ndarray): The predicted probabilities for each class, shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/classifiers/model_classifier.py</code> <pre><code>@utils.trained_needed\ndef predict_with_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; Tuple[np.ndarray, np.ndarray]:\n    '''Predictions on test with probabilities\n\n    Args:\n        x_test (pd.DataFrame): DataFrame with the test data to be predicted\n    Returns:\n        predicted_class (np.ndarray): The predicted classes, shape = [n_samples, n_classes] if multi-labels, shape = [n_samples, 1] otherwise\n        predicted_proba (np.ndarray): The predicted probabilities for each class, shape = [n_samples, n_classes]\n    '''\n    # Process\n    predicted_proba = self.predict(x_test, return_proba=True, **kwargs)\n    predicted_class = self.get_classes_from_proba(predicted_proba)\n    return predicted_class, predicted_proba\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_classifier/#template_num.models_training.classifiers.model_classifier.ModelClassifierMixin.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/classifiers/model_classifier.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save model\n    if json_data is None:\n        json_data = {}\n\n    json_data['list_classes'] = self.list_classes\n    json_data['dict_classes'] = self.dict_classes\n    json_data['multi_label'] = self.multi_label\n\n    # Save\n    super().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_xgboost_classifier/","title":"Model xgboost classifier","text":""},{"location":"reference/template_num/models_training/classifiers/model_xgboost_classifier/#template_num.models_training.classifiers.model_xgboost_classifier.ModelXgboostClassifier","title":"<code>ModelXgboostClassifier</code>","text":"<p>             Bases: <code>ModelClassifierMixin</code>, <code>ModelClass</code></p> <p>Xgboost model for classification</p> Source code in <code>template_num/models_training/classifiers/model_xgboost_classifier.py</code> <pre><code>class ModelXgboostClassifier(ModelClassifierMixin, ModelClass):\n    '''Xgboost model for classification'''\n\n    _default_name = 'model_xgboost_classifier'\n\n    def __init__(self, xgboost_params: Union[dict, None] = None, early_stopping_rounds: int = 5, validation_split: float = 0.2, **kwargs) -&gt; None:\n        '''Initialization of the class  (see ModelClass &amp; ModelClassifierMixin for more arguments)\n\n        Kwargs:\n            xgboost_params (dict): Parameters for the Xgboost\n                -&gt; https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier\n            early_stopping_rounds (int): Number of rounds for early stopping\n            validation_split (float): Validation split fraction.\n                Only used if not validation dataset in the fit input\n        '''\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Set parameters\n        if xgboost_params is None:\n            xgboost_params = {}\n        xgboost_params[\"random_state\"] = self.random_seed\n        self.xgboost_params = xgboost_params\n        self.early_stopping_rounds = early_stopping_rounds\n        self.validation_split = validation_split\n\n        # Set objective (if not in params) &amp; init. model\n        if 'objective' not in self.xgboost_params.keys():\n            self.xgboost_params['objective'] = 'binary:logistic'\n            #  List of objectives https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n        # WARNING, if multi-classes, AUTOMATIC backup on multi:softprob (by xgboost)\n        # https://stackoverflow.com/questions/57986259/multiclass-classification-with-xgboost-classifier\n        self.model = XGBClassifier(**self.xgboost_params)\n\n        # If multi-labels, we use MultiOutputClassifier\n        if self.multi_label:\n            self.model = MyMultiOutputClassifier(self.model)\n\n    def fit(self, x_train, y_train, x_valid=None, y_valid=None, with_shuffle: bool = True, **kwargs) -&gt; None:\n        '''Trains the model\n           **kwargs permits compatibility with Keras model\n\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n            y_train (?): Array-like, shape = [n_samples, n_targets]\n        Kwargs:\n            x_valid (?): Array-like, shape = [n_samples, n_features]\n            y_valid (?): Array-like, shape = [n_samples, n_targets]\n            with_shuffle (bool): If x, y must be shuffled before fitting\n        Raises:\n            RuntimeError: If the model is already fitted\n        '''\n        # TODO: Check if the training can be continued\n        if self.trained:\n            self.logger.error(\"We can't train again a xgboost model\")\n            self.logger.error(\"Please train a new model\")\n            raise RuntimeError(\"We can't train again a xgboost model\")\n\n        # We check input format\n        x_train, y_train = self._check_input_format(x_train, y_train, fit_function=True)\n        # If there is a validation set, we also check the format (but fit_function to False)\n        if y_valid is not None and x_valid is not None:\n            x_valid, y_valid = self._check_input_format(x_valid, y_valid, fit_function=False)\n        # Otherwise, we do a random split\n        else:\n            self.logger.warning(f\"Warning, no validation dataset. We split the training set (fraction valid = {self.validation_split})\")\n            x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=self.validation_split, random_state = self.random_seed)\n\n        # Gets the input columns\n        original_list_classes: Optional[List[Any]] = None  # None if no 'columns' attribute or mono-label\n        if self.multi_label and hasattr(y_train, 'columns'):\n            # TODO : tmp mypy fix https://github.com/python/mypy/pull/13544\n            original_list_classes = list(y_train.columns)  # type: ignore\n\n        # Shuffle x, y if wanted\n        if with_shuffle:\n            rng = np.random.RandomState(self.random_seed)\n            p = rng.permutation(len(x_train))\n            x_train = np.array(x_train)[p]\n            y_train = np.array(y_train)[p]\n        # Else still transform to numpy array\n        else:\n            x_train = np.array(x_train)\n            y_train = np.array(y_train)\n\n        # Also get x_valid &amp; y_valid as numpy\n        x_valid = np.array(x_valid)\n        y_valid = np.array(y_valid)\n\n        # NEW from XGBOOST 1.3.2 : class columns should start from 0\n        # https://stackoverflow.com/questions/71996617/invalid-classes-inferred-from-unique-values-of-y-expected-0-1-2-3-4-5-got\n        # We set list_classes and dict_classes now + replace target for mono-label cases (multi-label should already be OHE with 0s and 1s)\n        if not self.multi_label:\n            self.list_classes = list(np.unique(y_train))\n            # Set dict_classes based on list classes\n            self.dict_classes = {i: col for i, col in enumerate(self.list_classes)}\n            inv_dict_classes = {v: k for k, v in self.dict_classes.items()}\n            # Update y_train and y_valid\n            map_func = lambda x: inv_dict_classes[x]\n            y_train = np.vectorize(map_func)(y_train)\n            y_valid = np.vectorize(map_func)(y_valid)\n        else:\n            if original_list_classes is not None:\n                self.list_classes = original_list_classes\n            else:\n                self.logger.warning(\"Can't read the name of the columns of y_train -&gt; inverse transformation won't be possible\")\n                # We still create a list of classes in order to be compatible with other functions\n                self.list_classes = [str(_) for _ in range(pd.DataFrame(y_train).shape[1])]\n            # Set dict_classes based on list classes\n            self.dict_classes = {i: col for i, col in enumerate(self.list_classes)}\n            # Nothing to update as targets are already OHE\n\n        # Set eval set and train\n        eval_set = [(x_train, y_train), (x_valid, y_valid)]  # If there\u2019s more than one item in eval_set, the last entry will be used for early stopping.\n        prior_objective = self.model.objective if not self.multi_label else self.model.estimator.objective\n        self.model.fit(x_train, y_train, eval_set=eval_set, early_stopping_rounds=self.early_stopping_rounds, verbose=True)\n        post_objective = self.model.objective if not self.multi_label else self.model.estimator.objective\n        if prior_objective != post_objective:\n            self.logger.warning(\"Warning: the objective function was automatically changed by XGBOOST\")\n            self.logger.warning(f\"Before: {prior_objective}\")\n            self.logger.warning(f\"After: {post_objective}\")\n\n        # Set trained\n        self.trained = True\n        self.nb_fit += 1\n\n    @utils.trained_needed\n    def predict(self, x_test: pd.DataFrame, return_proba: bool = False, **kwargs) -&gt; np.ndarray:\n        '''Predictions on test set\n\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Kwargs:\n            return_proba (bool): If the function should return the probabilities instead of the classes (Keras compatibility)\n        Returns:\n            (np.ndarray): Array\n                # If not return_proba, shape = [n_samples,] or [n_samples, n_classes]\n                # Else, shape = [n_samples, n_classes]\n        '''\n        # If we want probabilities, we use predict_proba\n        if return_proba:\n            return self.predict_proba(x_test, **kwargs)\n        # Otherwise, returns the prediction :\n        else:\n            # We check input format\n            x_test, _ = self._check_input_format(x_test)\n            # Warning, \"The method returns the model from the last iteration\"\n            # But : \"Predict with X. If the model is trained with early stopping, then best_iteration is used automatically.\"\n            y_proba = self.predict_proba(x_test)\n            y_pred = self.get_classes_from_proba(y_proba)\n            return y_pred\n\n    @utils.trained_needed\n    def predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n        '''Predicts the probabilities on the test set\n\n        Args:\n            x_test (pd.DataFrame): DataFrame to be predicted -&gt; retrieve the probabilities\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n        # We check input format\n        x_test, _ = self._check_input_format(x_test)\n\n        #\n        probas = np.array(self.model.predict_proba(x_test))\n        # If use of MultiOutputClassifier -&gt;  returns probabilities of 0 and 1 for all elements and all classes\n        # Correction in cas where we detect a shape of length &gt; 2 (ie. equals to 3)\n        # Reminder : we do not manage multi-labels multi-classes\n        if len(probas.shape) &gt; 2:\n            probas = np.swapaxes(probas[:, :, 1], 0, 1)\n        return probas\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save model\n        if json_data is None:\n            json_data = {}\n\n        json_data['librairie'] = 'xgboost'\n        json_data['xgboost_params'] = self.xgboost_params\n        json_data['early_stopping_rounds'] = self.early_stopping_rounds\n        json_data['validation_split'] = self.validation_split\n\n        # Save xgboost standalone\n        if self.level_save in ['MEDIUM', 'HIGH']:\n            if not self.multi_label:\n                if self.trained:\n                    save_path = os.path.join(self.model_dir, 'xbgoost_standalone.model')\n                    self.model.save_model(save_path)\n                else:\n                    self.logger.warning(\"Can't save the XGboost in standalone because it hasn't been already fitted\")\n            else:\n                # If multi-labels, we use a multi-output and fits several xgboost (cf. strategy sklearn)\n                # We can't save only one xgboost, so we use pickle to save\n                # Problem : the pickle won't be compatible with updates :'(\n                save_path = os.path.join(self.model_dir, f\"{self.model_name}.pkl\")\n                with open(save_path, 'wb') as f:\n                    pickle.dump(self.model, f)\n\n        # Save\n        super().save(json_data=json_data)\n\n    def reload_from_standalone(self, **kwargs) -&gt; None:\n        '''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            xgboost_path (str): Path to standalone xgboost\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If xgboost_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object xgboost_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n        # Retrieve args\n        configuration_path = kwargs.get('configuration_path', None)\n        xgboost_path = kwargs.get('xgboost_path', None)\n        preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n        # Checks\n        if configuration_path is None:\n            raise ValueError(\"The argument configuration_path can't be None\")\n        if xgboost_path is None:\n            raise ValueError(\"The argument xgboost_path can't be None\")\n        if preprocess_pipeline_path is None:\n            raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n        if not os.path.exists(configuration_path):\n            raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n        if not os.path.exists(xgboost_path):\n            raise FileNotFoundError(f\"The file {xgboost_path} does not exist\")\n        if not os.path.exists(preprocess_pipeline_path):\n            raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n        # Load confs\n        with open(configuration_path, 'r', encoding='utf-8') as f:\n            configs = json.load(f)\n        # Can't set int as keys in json, so need to cast it after reloading\n        # dict_classes keys are always ints\n        if 'dict_classes' in configs.keys():\n            configs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n        elif 'list_classes' in configs.keys():\n            configs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n\n        # Set class vars\n        # self.model_name = # Keep the created name\n        # self.model_dir = # Keep the created folder\n        self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n        self.trained = configs.get('trained', True)  # Consider trained by default\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                          'list_classes', 'dict_classes', 'multi_label', 'random_seed', 'level_save',\n                          'xgboost_params', 'early_stopping_rounds', 'validation_split']:\n            setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n        # Reload xgboost model\n        if not self.multi_label:\n            self.model.load_model(xgboost_path)\n        else:\n            with open(xgboost_path, 'rb') as f:  # type: ignore\n                self.model = pickle.load(f)\n\n        # Reload pipeline preprocessing\n        with open(preprocess_pipeline_path, 'rb') as f:\n            self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_xgboost_classifier/#template_num.models_training.classifiers.model_xgboost_classifier.ModelXgboostClassifier.__init__","title":"<code>__init__(xgboost_params=None, early_stopping_rounds=5, validation_split=0.2, **kwargs)</code>","text":"<p>Initialization of the class  (see ModelClass &amp; ModelClassifierMixin for more arguments)</p> Kwargs <p>xgboost_params (dict): Parameters for the Xgboost     -&gt; https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier early_stopping_rounds (int): Number of rounds for early stopping validation_split (float): Validation split fraction.     Only used if not validation dataset in the fit input</p> Source code in <code>template_num/models_training/classifiers/model_xgboost_classifier.py</code> <pre><code>def __init__(self, xgboost_params: Union[dict, None] = None, early_stopping_rounds: int = 5, validation_split: float = 0.2, **kwargs) -&gt; None:\n    '''Initialization of the class  (see ModelClass &amp; ModelClassifierMixin for more arguments)\n\n    Kwargs:\n        xgboost_params (dict): Parameters for the Xgboost\n            -&gt; https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier\n        early_stopping_rounds (int): Number of rounds for early stopping\n        validation_split (float): Validation split fraction.\n            Only used if not validation dataset in the fit input\n    '''\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Set parameters\n    if xgboost_params is None:\n        xgboost_params = {}\n    xgboost_params[\"random_state\"] = self.random_seed\n    self.xgboost_params = xgboost_params\n    self.early_stopping_rounds = early_stopping_rounds\n    self.validation_split = validation_split\n\n    # Set objective (if not in params) &amp; init. model\n    if 'objective' not in self.xgboost_params.keys():\n        self.xgboost_params['objective'] = 'binary:logistic'\n        #  List of objectives https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n    # WARNING, if multi-classes, AUTOMATIC backup on multi:softprob (by xgboost)\n    # https://stackoverflow.com/questions/57986259/multiclass-classification-with-xgboost-classifier\n    self.model = XGBClassifier(**self.xgboost_params)\n\n    # If multi-labels, we use MultiOutputClassifier\n    if self.multi_label:\n        self.model = MyMultiOutputClassifier(self.model)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_xgboost_classifier/#template_num.models_training.classifiers.model_xgboost_classifier.ModelXgboostClassifier.fit","title":"<code>fit(x_train, y_train, x_valid=None, y_valid=None, with_shuffle=True, **kwargs)</code>","text":"<p>Trains the model    **kwargs permits compatibility with Keras model</p> <p>Parameters:</p> Name Type Description Default <code>x_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <code>y_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_targets]</p> required <p>Kwargs:     x_valid (?): Array-like, shape = [n_samples, n_features]     y_valid (?): Array-like, shape = [n_samples, n_targets]     with_shuffle (bool): If x, y must be shuffled before fitting Raises:     RuntimeError: If the model is already fitted</p> Source code in <code>template_num/models_training/classifiers/model_xgboost_classifier.py</code> <pre><code>def fit(self, x_train, y_train, x_valid=None, y_valid=None, with_shuffle: bool = True, **kwargs) -&gt; None:\n    '''Trains the model\n       **kwargs permits compatibility with Keras model\n\n    Args:\n        x_train (?): Array-like, shape = [n_samples, n_features]\n        y_train (?): Array-like, shape = [n_samples, n_targets]\n    Kwargs:\n        x_valid (?): Array-like, shape = [n_samples, n_features]\n        y_valid (?): Array-like, shape = [n_samples, n_targets]\n        with_shuffle (bool): If x, y must be shuffled before fitting\n    Raises:\n        RuntimeError: If the model is already fitted\n    '''\n    # TODO: Check if the training can be continued\n    if self.trained:\n        self.logger.error(\"We can't train again a xgboost model\")\n        self.logger.error(\"Please train a new model\")\n        raise RuntimeError(\"We can't train again a xgboost model\")\n\n    # We check input format\n    x_train, y_train = self._check_input_format(x_train, y_train, fit_function=True)\n    # If there is a validation set, we also check the format (but fit_function to False)\n    if y_valid is not None and x_valid is not None:\n        x_valid, y_valid = self._check_input_format(x_valid, y_valid, fit_function=False)\n    # Otherwise, we do a random split\n    else:\n        self.logger.warning(f\"Warning, no validation dataset. We split the training set (fraction valid = {self.validation_split})\")\n        x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=self.validation_split, random_state = self.random_seed)\n\n    # Gets the input columns\n    original_list_classes: Optional[List[Any]] = None  # None if no 'columns' attribute or mono-label\n    if self.multi_label and hasattr(y_train, 'columns'):\n        # TODO : tmp mypy fix https://github.com/python/mypy/pull/13544\n        original_list_classes = list(y_train.columns)  # type: ignore\n\n    # Shuffle x, y if wanted\n    if with_shuffle:\n        rng = np.random.RandomState(self.random_seed)\n        p = rng.permutation(len(x_train))\n        x_train = np.array(x_train)[p]\n        y_train = np.array(y_train)[p]\n    # Else still transform to numpy array\n    else:\n        x_train = np.array(x_train)\n        y_train = np.array(y_train)\n\n    # Also get x_valid &amp; y_valid as numpy\n    x_valid = np.array(x_valid)\n    y_valid = np.array(y_valid)\n\n    # NEW from XGBOOST 1.3.2 : class columns should start from 0\n    # https://stackoverflow.com/questions/71996617/invalid-classes-inferred-from-unique-values-of-y-expected-0-1-2-3-4-5-got\n    # We set list_classes and dict_classes now + replace target for mono-label cases (multi-label should already be OHE with 0s and 1s)\n    if not self.multi_label:\n        self.list_classes = list(np.unique(y_train))\n        # Set dict_classes based on list classes\n        self.dict_classes = {i: col for i, col in enumerate(self.list_classes)}\n        inv_dict_classes = {v: k for k, v in self.dict_classes.items()}\n        # Update y_train and y_valid\n        map_func = lambda x: inv_dict_classes[x]\n        y_train = np.vectorize(map_func)(y_train)\n        y_valid = np.vectorize(map_func)(y_valid)\n    else:\n        if original_list_classes is not None:\n            self.list_classes = original_list_classes\n        else:\n            self.logger.warning(\"Can't read the name of the columns of y_train -&gt; inverse transformation won't be possible\")\n            # We still create a list of classes in order to be compatible with other functions\n            self.list_classes = [str(_) for _ in range(pd.DataFrame(y_train).shape[1])]\n        # Set dict_classes based on list classes\n        self.dict_classes = {i: col for i, col in enumerate(self.list_classes)}\n        # Nothing to update as targets are already OHE\n\n    # Set eval set and train\n    eval_set = [(x_train, y_train), (x_valid, y_valid)]  # If there\u2019s more than one item in eval_set, the last entry will be used for early stopping.\n    prior_objective = self.model.objective if not self.multi_label else self.model.estimator.objective\n    self.model.fit(x_train, y_train, eval_set=eval_set, early_stopping_rounds=self.early_stopping_rounds, verbose=True)\n    post_objective = self.model.objective if not self.multi_label else self.model.estimator.objective\n    if prior_objective != post_objective:\n        self.logger.warning(\"Warning: the objective function was automatically changed by XGBOOST\")\n        self.logger.warning(f\"Before: {prior_objective}\")\n        self.logger.warning(f\"After: {post_objective}\")\n\n    # Set trained\n    self.trained = True\n    self.nb_fit += 1\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_xgboost_classifier/#template_num.models_training.classifiers.model_xgboost_classifier.ModelXgboostClassifier.predict","title":"<code>predict(x_test, return_proba=False, **kwargs)</code>","text":"<p>Predictions on test set</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>DataFrame</code> <p>DataFrame with the test data to be predicted</p> required <p>Kwargs:     return_proba (bool): If the function should return the probabilities instead of the classes (Keras compatibility) Returns:     (np.ndarray): Array         # If not return_proba, shape = [n_samples,] or [n_samples, n_classes]         # Else, shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/classifiers/model_xgboost_classifier.py</code> <pre><code>@utils.trained_needed\ndef predict(self, x_test: pd.DataFrame, return_proba: bool = False, **kwargs) -&gt; np.ndarray:\n    '''Predictions on test set\n\n    Args:\n        x_test (pd.DataFrame): DataFrame with the test data to be predicted\n    Kwargs:\n        return_proba (bool): If the function should return the probabilities instead of the classes (Keras compatibility)\n    Returns:\n        (np.ndarray): Array\n            # If not return_proba, shape = [n_samples,] or [n_samples, n_classes]\n            # Else, shape = [n_samples, n_classes]\n    '''\n    # If we want probabilities, we use predict_proba\n    if return_proba:\n        return self.predict_proba(x_test, **kwargs)\n    # Otherwise, returns the prediction :\n    else:\n        # We check input format\n        x_test, _ = self._check_input_format(x_test)\n        # Warning, \"The method returns the model from the last iteration\"\n        # But : \"Predict with X. If the model is trained with early stopping, then best_iteration is used automatically.\"\n        y_proba = self.predict_proba(x_test)\n        y_pred = self.get_classes_from_proba(y_proba)\n        return y_pred\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_xgboost_classifier/#template_num.models_training.classifiers.model_xgboost_classifier.ModelXgboostClassifier.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>DataFrame</code> <p>DataFrame to be predicted -&gt; retrieve the probabilities</p> required <p>Returns:     (np.ndarray): Array, shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/classifiers/model_xgboost_classifier.py</code> <pre><code>@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n    '''Predicts the probabilities on the test set\n\n    Args:\n        x_test (pd.DataFrame): DataFrame to be predicted -&gt; retrieve the probabilities\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n    # We check input format\n    x_test, _ = self._check_input_format(x_test)\n\n    #\n    probas = np.array(self.model.predict_proba(x_test))\n    # If use of MultiOutputClassifier -&gt;  returns probabilities of 0 and 1 for all elements and all classes\n    # Correction in cas where we detect a shape of length &gt; 2 (ie. equals to 3)\n    # Reminder : we do not manage multi-labels multi-classes\n    if len(probas.shape) &gt; 2:\n        probas = np.swapaxes(probas[:, :, 1], 0, 1)\n    return probas\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_xgboost_classifier/#template_num.models_training.classifiers.model_xgboost_classifier.ModelXgboostClassifier.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file xgboost_path (str): Path to standalone xgboost preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:     ValueError: If configuration_path is None     ValueError: If xgboost_path is None     ValueError: If preprocess_pipeline_path is None     FileNotFoundError: If the object configuration_path is not an existing file     FileNotFoundError: If the object xgboost_path is not an existing file     FileNotFoundError: If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/classifiers/model_xgboost_classifier.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n    '''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        xgboost_path (str): Path to standalone xgboost\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If xgboost_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object xgboost_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n    # Retrieve args\n    configuration_path = kwargs.get('configuration_path', None)\n    xgboost_path = kwargs.get('xgboost_path', None)\n    preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n    # Checks\n    if configuration_path is None:\n        raise ValueError(\"The argument configuration_path can't be None\")\n    if xgboost_path is None:\n        raise ValueError(\"The argument xgboost_path can't be None\")\n    if preprocess_pipeline_path is None:\n        raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n    if not os.path.exists(configuration_path):\n        raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n    if not os.path.exists(xgboost_path):\n        raise FileNotFoundError(f\"The file {xgboost_path} does not exist\")\n    if not os.path.exists(preprocess_pipeline_path):\n        raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n    # Load confs\n    with open(configuration_path, 'r', encoding='utf-8') as f:\n        configs = json.load(f)\n    # Can't set int as keys in json, so need to cast it after reloading\n    # dict_classes keys are always ints\n    if 'dict_classes' in configs.keys():\n        configs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n    elif 'list_classes' in configs.keys():\n        configs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n\n    # Set class vars\n    # self.model_name = # Keep the created name\n    # self.model_dir = # Keep the created folder\n    self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n    self.trained = configs.get('trained', True)  # Consider trained by default\n    # Try to read the following attributes from configs and, if absent, keep the current one\n    for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                      'list_classes', 'dict_classes', 'multi_label', 'random_seed', 'level_save',\n                      'xgboost_params', 'early_stopping_rounds', 'validation_split']:\n        setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n    # Reload xgboost model\n    if not self.multi_label:\n        self.model.load_model(xgboost_path)\n    else:\n        with open(xgboost_path, 'rb') as f:  # type: ignore\n            self.model = pickle.load(f)\n\n    # Reload pipeline preprocessing\n    with open(preprocess_pipeline_path, 'rb') as f:\n        self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_xgboost_classifier/#template_num.models_training.classifiers.model_xgboost_classifier.ModelXgboostClassifier.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/classifiers/model_xgboost_classifier.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save model\n    if json_data is None:\n        json_data = {}\n\n    json_data['librairie'] = 'xgboost'\n    json_data['xgboost_params'] = self.xgboost_params\n    json_data['early_stopping_rounds'] = self.early_stopping_rounds\n    json_data['validation_split'] = self.validation_split\n\n    # Save xgboost standalone\n    if self.level_save in ['MEDIUM', 'HIGH']:\n        if not self.multi_label:\n            if self.trained:\n                save_path = os.path.join(self.model_dir, 'xbgoost_standalone.model')\n                self.model.save_model(save_path)\n            else:\n                self.logger.warning(\"Can't save the XGboost in standalone because it hasn't been already fitted\")\n        else:\n            # If multi-labels, we use a multi-output and fits several xgboost (cf. strategy sklearn)\n            # We can't save only one xgboost, so we use pickle to save\n            # Problem : the pickle won't be compatible with updates :'(\n            save_path = os.path.join(self.model_dir, f\"{self.model_name}.pkl\")\n            with open(save_path, 'wb') as f:\n                pickle.dump(self.model, f)\n\n    # Save\n    super().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_xgboost_classifier/#template_num.models_training.classifiers.model_xgboost_classifier.MyMultiOutputClassifier","title":"<code>MyMultiOutputClassifier</code>","text":"<p>             Bases: <code>MultiOutputClassifier</code></p> Source code in <code>template_num/models_training/classifiers/model_xgboost_classifier.py</code> <pre><code>class MyMultiOutputClassifier(MultiOutputClassifier):\n\n    @_deprecate_positional_args\n    def __init__(self, estimator, *, n_jobs=None) -&gt; None:\n        super().__init__(estimator, n_jobs=n_jobs)\n\n    def fit(self, X, y, sample_weight=None, **fit_params) -&gt; Any:\n        ''' Fit the model to data.\n        Fit a separate model for each output variable.\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Data.\n        y : {array-like, sparse matrix} of shape (n_samples, n_outputs)\n            Multi-output targets. An indicator matrix turns on multilabel\n            estimation.\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted.\n            Only supported if the underlying regressor supports sample\n            weights.\n        **fit_params : dict of string -&gt; object\n            Parameters passed to the ``estimator.fit`` method of each step.\n            .. versionadded:: 0.23\n        Returns\n        -------\n        self : object\n        '''\n        if not hasattr(self.estimator, \"fit\"):\n            raise ValueError(\"The base estimator should implement\"\n                             \" a fit method\")\n\n        X, y = self._validate_data(X, y,\n                                   force_all_finite=False,\n                                   multi_output=True, accept_sparse=True)\n\n        if is_classifier(self):\n            check_classification_targets(y)\n\n        if y.ndim == 1:\n            raise ValueError(\"y must have at least two dimensions for \"\n                             \"multi-output regression but has only one.\")\n\n        if (sample_weight is not None and not has_fit_parameter(self.estimator, 'sample_weight')):\n            raise ValueError(\"Underlying estimator does not support\"\n                             \" sample weights.\")\n\n        fit_params_validated = _check_fit_params(X, fit_params)\n\n        # New : extract eval_set\n        if 'eval_set' in fit_params_validated.keys():\n            eval_set = fit_params_validated.pop('eval_set')\n            self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n                delayed(_fit_estimator)(\n                    self.estimator, X, y[:, i], sample_weight,\n                    **fit_params_validated,\n                    eval_set=[(X_test, Y_test[:, i]) for X_test, Y_test in eval_set])\n                for i in range(y.shape[1]))\n        # Pas d'eval_set\n        else:\n            self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n                delayed(_fit_estimator)(\n                    self.estimator, X, y[:, i], sample_weight,\n                    **fit_params_validated)\n                for i in range(y.shape[1]))\n        return self\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_xgboost_classifier/#template_num.models_training.classifiers.model_xgboost_classifier.MyMultiOutputClassifier.fit","title":"<code>fit(X, y, sample_weight=None, **fit_params)</code>","text":"<p>Fit the model to data. Fit a separate model for each output variable. Parameters</p> <p>X : {array-like, sparse matrix} of shape (n_samples, n_features)     Data. y : {array-like, sparse matrix} of shape (n_samples, n_outputs)     Multi-output targets. An indicator matrix turns on multilabel     estimation. sample_weight : array-like of shape (n_samples,), default=None     Sample weights. If None, then samples are equally weighted.     Only supported if the underlying regressor supports sample     weights. **fit_params : dict of string -&gt; object     Parameters passed to the <code>estimator.fit</code> method of each step.     .. versionadded:: 0.23 Returns</p> <p>self : object</p> Source code in <code>template_num/models_training/classifiers/model_xgboost_classifier.py</code> <pre><code>def fit(self, X, y, sample_weight=None, **fit_params) -&gt; Any:\n    ''' Fit the model to data.\n    Fit a separate model for each output variable.\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        Data.\n    y : {array-like, sparse matrix} of shape (n_samples, n_outputs)\n        Multi-output targets. An indicator matrix turns on multilabel\n        estimation.\n    sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights. If None, then samples are equally weighted.\n        Only supported if the underlying regressor supports sample\n        weights.\n    **fit_params : dict of string -&gt; object\n        Parameters passed to the ``estimator.fit`` method of each step.\n        .. versionadded:: 0.23\n    Returns\n    -------\n    self : object\n    '''\n    if not hasattr(self.estimator, \"fit\"):\n        raise ValueError(\"The base estimator should implement\"\n                         \" a fit method\")\n\n    X, y = self._validate_data(X, y,\n                               force_all_finite=False,\n                               multi_output=True, accept_sparse=True)\n\n    if is_classifier(self):\n        check_classification_targets(y)\n\n    if y.ndim == 1:\n        raise ValueError(\"y must have at least two dimensions for \"\n                         \"multi-output regression but has only one.\")\n\n    if (sample_weight is not None and not has_fit_parameter(self.estimator, 'sample_weight')):\n        raise ValueError(\"Underlying estimator does not support\"\n                         \" sample weights.\")\n\n    fit_params_validated = _check_fit_params(X, fit_params)\n\n    # New : extract eval_set\n    if 'eval_set' in fit_params_validated.keys():\n        eval_set = fit_params_validated.pop('eval_set')\n        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n            delayed(_fit_estimator)(\n                self.estimator, X, y[:, i], sample_weight,\n                **fit_params_validated,\n                eval_set=[(X_test, Y_test[:, i]) for X_test, Y_test in eval_set])\n            for i in range(y.shape[1]))\n    # Pas d'eval_set\n    else:\n        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n            delayed(_fit_estimator)(\n                self.estimator, X, y[:, i], sample_weight,\n                **fit_params_validated)\n            for i in range(y.shape[1]))\n    return self\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/","title":"Models sklearn","text":""},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_gbt_classifier/","title":"Model gbt classifier","text":""},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_gbt_classifier/#template_num.models_training.classifiers.models_sklearn.model_gbt_classifier.ModelGBTClassifier","title":"<code>ModelGBTClassifier</code>","text":"<p>             Bases: <code>ModelClassifierMixin</code>, <code>ModelPipeline</code></p> <p>Gradient Boosted Tree model for classification</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_gbt_classifier.py</code> <pre><code>class ModelGBTClassifier(ModelClassifierMixin, ModelPipeline):\n    '''Gradient Boosted Tree model for classification'''\n\n    _default_name = 'model_gbt_classifier'\n\n    def __init__(self, gbt_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)\n\n        Kwargs:\n            gbt_params (dict) : Parameters for the Gradient Boosted Tree\n            multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n                Warning, 'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.\n        Raises:\n            ValueError: If multiclass_strategy is not 'ovo', 'ovr' or None\n        '''\n        if multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\n            raise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Manage model\n        if gbt_params is None:\n            gbt_params = {}\n        gbt_params[\"random_state\"] = self.random_seed\n        self.gbt = GradientBoostingClassifier(**gbt_params)\n        self.multiclass_strategy = multiclass_strategy\n\n        # Can't do multi-labels / multi-classes\n        if not self.multi_label:\n            # If not multi-classes : no impact\n            if multiclass_strategy == 'ovr':\n                self.pipeline = Pipeline([('gbt', OneVsRestClassifier(self.gbt))])\n            elif multiclass_strategy == 'ovo':\n                self.pipeline = Pipeline([('gbt', OneVsOneClassifier(self.gbt))])\n            else:\n                self.pipeline = Pipeline([('gbt', self.gbt)])\n\n        # GradientBoostingClassifier does not natively support multi-labels\n        if self.multi_label:\n            self.pipeline = Pipeline([('gbt', MultiOutputClassifier(self.gbt))])\n\n    @utils.trained_needed\n    def predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n        '''Predicts the probabilities on the test set\n        'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.\n\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n        # Uses super() of the ModelPipeline class if != 'ovo' or multi-labels\n        if self.multi_label or self.multiclass_strategy != 'ovo':\n            return super().predict_proba(x_test=x_test, **kwargs)\n        else:\n            # We check input format\n            x_test, _ = self._check_input_format(x_test)\n            # Get preds\n            preds = self.pipeline.predict(x_test)\n            # Format ['a', 'b', 'c', 'a', ..., 'b']\n            # Transform to \"proba\"\n            transform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\n            probas = np.array([transform_dict[x] for x in preds])\n        return probas\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save model\n        if json_data is None:\n            json_data = {}\n\n        json_data['multiclass_strategy'] = self.multiclass_strategy\n\n        # Save\n        super().save(json_data=json_data)\n\n    def reload_from_standalone(self, **kwargs) -&gt; None:\n        '''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n        # Retrieve args\n        configuration_path = kwargs.get('configuration_path', None)\n        sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n        preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n        # Checks\n        if configuration_path is None:\n            raise ValueError(\"The argument configuration_path can't be None\")\n        if sklearn_pipeline_path is None:\n            raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n        if preprocess_pipeline_path is None:\n            raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n        if not os.path.exists(configuration_path):\n            raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n        if not os.path.exists(sklearn_pipeline_path):\n            raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n        if not os.path.exists(preprocess_pipeline_path):\n            raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n        # Load confs\n        with open(configuration_path, 'r', encoding='utf-8') as f:\n            configs = json.load(f)\n        # Can't set int as keys in json, so need to cast it after reloading\n        # dict_classes keys are always ints\n        if 'dict_classes' in configs.keys():\n            configs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n        elif 'list_classes' in configs.keys():\n            configs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n\n        # Set class vars\n        # self.model_name = # Keep the created name\n        # self.model_dir = # Keep the created folder\n        self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n        self.trained = configs.get('trained', True)  # Consider trained by default\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                          'list_classes', 'dict_classes', 'multi_label', 'random_seed', 'level_save',\n                          'multiclass_strategy']:\n            setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n        # Reload pipeline model\n        with open(sklearn_pipeline_path, 'rb') as f:\n            self.pipeline = pickle.load(f)\n\n        # Manage multi-labels or multi-classes\n        if not self.multi_label and self.multiclass_strategy is None:\n            self.gbt = self.pipeline['gbt']\n        else:\n            self.gbt = self.pipeline['gbt'].estimator\n\n        # Reload pipeline preprocessing\n        with open(preprocess_pipeline_path, 'rb') as f:\n            self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_gbt_classifier/#template_num.models_training.classifiers.models_sklearn.model_gbt_classifier.ModelGBTClassifier.__init__","title":"<code>__init__(gbt_params=None, multiclass_strategy=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)</p> Kwargs <p>gbt_params (dict) : Parameters for the Gradient Boosted Tree multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.     Warning, 'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.</p> <p>Raises:     ValueError: If multiclass_strategy is not 'ovo', 'ovr' or None</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_gbt_classifier.py</code> <pre><code>def __init__(self, gbt_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)\n\n    Kwargs:\n        gbt_params (dict) : Parameters for the Gradient Boosted Tree\n        multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n            Warning, 'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.\n    Raises:\n        ValueError: If multiclass_strategy is not 'ovo', 'ovr' or None\n    '''\n    if multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\n        raise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Manage model\n    if gbt_params is None:\n        gbt_params = {}\n    gbt_params[\"random_state\"] = self.random_seed\n    self.gbt = GradientBoostingClassifier(**gbt_params)\n    self.multiclass_strategy = multiclass_strategy\n\n    # Can't do multi-labels / multi-classes\n    if not self.multi_label:\n        # If not multi-classes : no impact\n        if multiclass_strategy == 'ovr':\n            self.pipeline = Pipeline([('gbt', OneVsRestClassifier(self.gbt))])\n        elif multiclass_strategy == 'ovo':\n            self.pipeline = Pipeline([('gbt', OneVsOneClassifier(self.gbt))])\n        else:\n            self.pipeline = Pipeline([('gbt', self.gbt)])\n\n    # GradientBoostingClassifier does not natively support multi-labels\n    if self.multi_label:\n        self.pipeline = Pipeline([('gbt', MultiOutputClassifier(self.gbt))])\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_gbt_classifier/#template_num.models_training.classifiers.models_sklearn.model_gbt_classifier.ModelGBTClassifier.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set 'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>DataFrame</code> <p>DataFrame with the test data to be predicted</p> required <p>Returns:     (np.ndarray): Array, shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_gbt_classifier.py</code> <pre><code>@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n    '''Predicts the probabilities on the test set\n    'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.\n\n    Args:\n        x_test (pd.DataFrame): DataFrame with the test data to be predicted\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n    # Uses super() of the ModelPipeline class if != 'ovo' or multi-labels\n    if self.multi_label or self.multiclass_strategy != 'ovo':\n        return super().predict_proba(x_test=x_test, **kwargs)\n    else:\n        # We check input format\n        x_test, _ = self._check_input_format(x_test)\n        # Get preds\n        preds = self.pipeline.predict(x_test)\n        # Format ['a', 'b', 'c', 'a', ..., 'b']\n        # Transform to \"proba\"\n        transform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\n        probas = np.array([transform_dict[x] for x in preds])\n    return probas\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_gbt_classifier/#template_num.models_training.classifiers.models_sklearn.model_gbt_classifier.ModelGBTClassifier.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:     ValueError: If configuration_path is None     ValueError: If sklearn_pipeline_path is None     ValueError: If preprocess_pipeline_path is None     FileNotFoundError: If the object configuration_path is not an existing file     FileNotFoundError: If the object sklearn_pipeline_path is not an existing file     FileNotFoundError: If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_gbt_classifier.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n    '''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n    # Retrieve args\n    configuration_path = kwargs.get('configuration_path', None)\n    sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n    preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n    # Checks\n    if configuration_path is None:\n        raise ValueError(\"The argument configuration_path can't be None\")\n    if sklearn_pipeline_path is None:\n        raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n    if preprocess_pipeline_path is None:\n        raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n    if not os.path.exists(configuration_path):\n        raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n    if not os.path.exists(sklearn_pipeline_path):\n        raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n    if not os.path.exists(preprocess_pipeline_path):\n        raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n    # Load confs\n    with open(configuration_path, 'r', encoding='utf-8') as f:\n        configs = json.load(f)\n    # Can't set int as keys in json, so need to cast it after reloading\n    # dict_classes keys are always ints\n    if 'dict_classes' in configs.keys():\n        configs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n    elif 'list_classes' in configs.keys():\n        configs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n\n    # Set class vars\n    # self.model_name = # Keep the created name\n    # self.model_dir = # Keep the created folder\n    self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n    self.trained = configs.get('trained', True)  # Consider trained by default\n    # Try to read the following attributes from configs and, if absent, keep the current one\n    for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                      'list_classes', 'dict_classes', 'multi_label', 'random_seed', 'level_save',\n                      'multiclass_strategy']:\n        setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n    # Reload pipeline model\n    with open(sklearn_pipeline_path, 'rb') as f:\n        self.pipeline = pickle.load(f)\n\n    # Manage multi-labels or multi-classes\n    if not self.multi_label and self.multiclass_strategy is None:\n        self.gbt = self.pipeline['gbt']\n    else:\n        self.gbt = self.pipeline['gbt'].estimator\n\n    # Reload pipeline preprocessing\n    with open(preprocess_pipeline_path, 'rb') as f:\n        self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_gbt_classifier/#template_num.models_training.classifiers.models_sklearn.model_gbt_classifier.ModelGBTClassifier.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_gbt_classifier.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save model\n    if json_data is None:\n        json_data = {}\n\n    json_data['multiclass_strategy'] = self.multiclass_strategy\n\n    # Save\n    super().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_knn_classifier/","title":"Model knn classifier","text":""},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_knn_classifier/#template_num.models_training.classifiers.models_sklearn.model_knn_classifier.ModelKNNClassifier","title":"<code>ModelKNNClassifier</code>","text":"<p>             Bases: <code>ModelClassifierMixin</code>, <code>ModelPipeline</code></p> <p>K-nearest Neighbors model for classification</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_knn_classifier.py</code> <pre><code>class ModelKNNClassifier(ModelClassifierMixin, ModelPipeline):\n    '''K-nearest Neighbors model for classification'''\n\n    _default_name = 'model_knn_classifier'\n\n    def __init__(self, knn_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)\n\n        Kwargs:\n            knn_params (dict) : Parameters for the K-nearest Neighbors\n            multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n        Raises:\n            ValueError: If multiclass_strategy is not 'ovo', 'ovr' or None\n        '''\n        if multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\n            raise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Manage model\n        if knn_params is None:\n            knn_params = {}\n        self.knn = KNeighborsClassifier(**knn_params)\n        self.multiclass_strategy = multiclass_strategy\n\n        # Can't do multi-labels / multi-classes\n        if not self.multi_label:\n            # If not multi-classes : no impact\n            if multiclass_strategy == 'ovr':\n                self.pipeline = Pipeline([('knn', OneVsRestClassifier(self.knn))])\n            elif multiclass_strategy == 'ovo':\n                self.pipeline = Pipeline([('knn', OneVsOneClassifier(self.knn))])\n            else:\n                self.pipeline = Pipeline([('knn', self.knn)])\n\n        # LKNeighborsClassifier natively supports multi_labels\n        if self.multi_label:\n            self.pipeline = Pipeline([('knn', self.knn)])\n\n    @utils.trained_needed\n    def predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n        '''Predicts the probabilities on the test set\n            'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.\n\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n        # Uses super() of the ModelPipeline class if != 'ovo' or multi-labels\n        if self.multi_label or self.multiclass_strategy != 'ovo':\n            return super().predict_proba(x_test=x_test, **kwargs)\n        else:\n            # We check input format\n            x_test, _ = self._check_input_format(x_test)\n            # Get preds\n            preds = self.pipeline.predict(x_test)\n            # Format ['a', 'b', 'c', 'a', ..., 'b']\n            # Transform to \"proba\"\n            transform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\n            probas = np.array([transform_dict[x] for x in preds])\n        return probas\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save model\n        if json_data is None:\n            json_data = {}\n\n        json_data['multiclass_strategy'] = self.multiclass_strategy\n\n        # Save\n        super().save(json_data=json_data)\n\n    def reload_from_standalone(self, **kwargs) -&gt; None:\n        '''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n        # Retrieve args\n        configuration_path = kwargs.get('configuration_path', None)\n        sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n        preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n        # Checks\n        if configuration_path is None:\n            raise ValueError(\"The argument configuration_path can't be None\")\n        if sklearn_pipeline_path is None:\n            raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n        if preprocess_pipeline_path is None:\n            raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n        if not os.path.exists(configuration_path):\n            raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n        if not os.path.exists(sklearn_pipeline_path):\n            raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n        if not os.path.exists(preprocess_pipeline_path):\n            raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n        # Load confs\n        with open(configuration_path, 'r', encoding='utf-8') as f:\n            configs = json.load(f)\n        # Can't set int as keys in json, so need to cast it after reloading\n        # dict_classes keys are always ints\n        if 'dict_classes' in configs.keys():\n            configs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n        elif 'list_classes' in configs.keys():\n            configs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n\n        # Set class vars\n        # self.model_name = # Keep the created name\n        # self.model_dir = # Keep the created folder\n        self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n        self.trained = configs.get('trained', True)  # Consider trained by default\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                          'list_classes', 'dict_classes', 'multi_label', 'level_save',\n                          'multiclass_strategy']:\n            setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n        # Reload pipeline model\n        with open(sklearn_pipeline_path, 'rb') as f:\n            self.pipeline = pickle.load(f)\n\n        # Manage multi-labels or multi-classes\n        if not self.multi_label and self.multiclass_strategy is not None:\n            self.knn = self.pipeline['knn'].estimator\n        else:\n            self.knn = self.pipeline['knn']\n\n        # Reload pipeline preprocessing\n        with open(preprocess_pipeline_path, 'rb') as f:\n            self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_knn_classifier/#template_num.models_training.classifiers.models_sklearn.model_knn_classifier.ModelKNNClassifier.__init__","title":"<code>__init__(knn_params=None, multiclass_strategy=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)</p> Kwargs <p>knn_params (dict) : Parameters for the K-nearest Neighbors multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.</p> <p>Raises:     ValueError: If multiclass_strategy is not 'ovo', 'ovr' or None</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_knn_classifier.py</code> <pre><code>def __init__(self, knn_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)\n\n    Kwargs:\n        knn_params (dict) : Parameters for the K-nearest Neighbors\n        multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n    Raises:\n        ValueError: If multiclass_strategy is not 'ovo', 'ovr' or None\n    '''\n    if multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\n        raise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Manage model\n    if knn_params is None:\n        knn_params = {}\n    self.knn = KNeighborsClassifier(**knn_params)\n    self.multiclass_strategy = multiclass_strategy\n\n    # Can't do multi-labels / multi-classes\n    if not self.multi_label:\n        # If not multi-classes : no impact\n        if multiclass_strategy == 'ovr':\n            self.pipeline = Pipeline([('knn', OneVsRestClassifier(self.knn))])\n        elif multiclass_strategy == 'ovo':\n            self.pipeline = Pipeline([('knn', OneVsOneClassifier(self.knn))])\n        else:\n            self.pipeline = Pipeline([('knn', self.knn)])\n\n    # LKNeighborsClassifier natively supports multi_labels\n    if self.multi_label:\n        self.pipeline = Pipeline([('knn', self.knn)])\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_knn_classifier/#template_num.models_training.classifiers.models_sklearn.model_knn_classifier.ModelKNNClassifier.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set     'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>DataFrame</code> <p>DataFrame with the test data to be predicted</p> required <p>Returns:     (np.ndarray): Array, shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_knn_classifier.py</code> <pre><code>@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n    '''Predicts the probabilities on the test set\n        'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.\n\n    Args:\n        x_test (pd.DataFrame): DataFrame with the test data to be predicted\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n    # Uses super() of the ModelPipeline class if != 'ovo' or multi-labels\n    if self.multi_label or self.multiclass_strategy != 'ovo':\n        return super().predict_proba(x_test=x_test, **kwargs)\n    else:\n        # We check input format\n        x_test, _ = self._check_input_format(x_test)\n        # Get preds\n        preds = self.pipeline.predict(x_test)\n        # Format ['a', 'b', 'c', 'a', ..., 'b']\n        # Transform to \"proba\"\n        transform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\n        probas = np.array([transform_dict[x] for x in preds])\n    return probas\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_knn_classifier/#template_num.models_training.classifiers.models_sklearn.model_knn_classifier.ModelKNNClassifier.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:     ValueError: If configuration_path is None     ValueError: If sklearn_pipeline_path is None     ValueError: If preprocess_pipeline_path is None     FileNotFoundError: If the object configuration_path is not an existing file     FileNotFoundError: If the object sklearn_pipeline_path is not an existing file     FileNotFoundError: If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_knn_classifier.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n    '''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n    # Retrieve args\n    configuration_path = kwargs.get('configuration_path', None)\n    sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n    preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n    # Checks\n    if configuration_path is None:\n        raise ValueError(\"The argument configuration_path can't be None\")\n    if sklearn_pipeline_path is None:\n        raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n    if preprocess_pipeline_path is None:\n        raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n    if not os.path.exists(configuration_path):\n        raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n    if not os.path.exists(sklearn_pipeline_path):\n        raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n    if not os.path.exists(preprocess_pipeline_path):\n        raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n    # Load confs\n    with open(configuration_path, 'r', encoding='utf-8') as f:\n        configs = json.load(f)\n    # Can't set int as keys in json, so need to cast it after reloading\n    # dict_classes keys are always ints\n    if 'dict_classes' in configs.keys():\n        configs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n    elif 'list_classes' in configs.keys():\n        configs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n\n    # Set class vars\n    # self.model_name = # Keep the created name\n    # self.model_dir = # Keep the created folder\n    self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n    self.trained = configs.get('trained', True)  # Consider trained by default\n    # Try to read the following attributes from configs and, if absent, keep the current one\n    for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                      'list_classes', 'dict_classes', 'multi_label', 'level_save',\n                      'multiclass_strategy']:\n        setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n    # Reload pipeline model\n    with open(sklearn_pipeline_path, 'rb') as f:\n        self.pipeline = pickle.load(f)\n\n    # Manage multi-labels or multi-classes\n    if not self.multi_label and self.multiclass_strategy is not None:\n        self.knn = self.pipeline['knn'].estimator\n    else:\n        self.knn = self.pipeline['knn']\n\n    # Reload pipeline preprocessing\n    with open(preprocess_pipeline_path, 'rb') as f:\n        self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_knn_classifier/#template_num.models_training.classifiers.models_sklearn.model_knn_classifier.ModelKNNClassifier.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_knn_classifier.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save model\n    if json_data is None:\n        json_data = {}\n\n    json_data['multiclass_strategy'] = self.multiclass_strategy\n\n    # Save\n    super().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_lgbm_classifier/","title":"Model lgbm classifier","text":""},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_lgbm_classifier/#template_num.models_training.classifiers.models_sklearn.model_lgbm_classifier.ModelLGBMClassifier","title":"<code>ModelLGBMClassifier</code>","text":"<p>             Bases: <code>ModelClassifierMixin</code>, <code>ModelPipeline</code></p> <p>Light GBM model for Classification</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_lgbm_classifier.py</code> <pre><code>class ModelLGBMClassifier(ModelClassifierMixin, ModelPipeline):\n    '''Light GBM model for Classification'''\n\n    _default_name = 'model_lgbm_classifier'\n\n    def __init__(self, lgbm_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)\n\n        Kwargs:\n            lgbm_params (dict) : Parameters for the Light GBM\n            multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n        Raises:\n            If multiclass_strategy is not 'ovo', 'ovr' or None\n        '''\n        if multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\n            raise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Manage model\n        if lgbm_params is None:\n            lgbm_params = {}\n        lgbm_params[\"random_state\"] = self.random_seed\n        self.lgbm = LGBMClassifier(**lgbm_params)\n        self.multiclass_strategy = multiclass_strategy\n\n        # Can't do multi-labels / multi-classes\n        if not self.multi_label:\n            # If not multi-classes : no impact\n            if multiclass_strategy == 'ovr':\n                self.pipeline = Pipeline([('lgbm', OneVsRestClassifier(self.lgbm))])\n            elif multiclass_strategy == 'ovo':\n                self.pipeline = Pipeline([('lgbm', OneVsOneClassifier(self.lgbm))])\n            else:\n                self.pipeline = Pipeline([('lgbm', self.lgbm)])\n\n        # LGBMClassifier does not natively support multi-labels\n        if self.multi_label:\n            self.pipeline = Pipeline([('lgbm', MultiOutputClassifier(self.lgbm))])\n\n    @utils.trained_needed\n    def predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n        '''Predicts the probabilities on the test set\n            'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.\n\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n        # Uses super() of the ModelPipeline class if != 'ovo' or multi-labels\n        if self.multi_label or self.multiclass_strategy != 'ovo':\n            return super().predict_proba(x_test=x_test, **kwargs)\n        else:\n            # We check input format\n            x_test, _ = self._check_input_format(x_test)\n            # Get preds\n            preds = self.pipeline.predict(x_test)\n            # Format ['a', 'b', 'c', 'a', ..., 'b']\n            # Transform to \"proba\"\n            transform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\n            probas = np.array([transform_dict[x] for x in preds])\n        return probas\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save model\n        if json_data is None:\n            json_data = {}\n\n        json_data['multiclass_strategy'] = self.multiclass_strategy\n\n        # Save\n        super().save(json_data=json_data)\n\n    def reload_from_standalone(self, **kwargs) -&gt; None:\n        '''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n        # Retrieve args\n        configuration_path = kwargs.get('configuration_path', None)\n        sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n        preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n        # Checks\n        if configuration_path is None:\n            raise ValueError(\"The argument configuration_path can't be None\")\n        if sklearn_pipeline_path is None:\n            raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n        if preprocess_pipeline_path is None:\n            raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n        if not os.path.exists(configuration_path):\n            raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n        if not os.path.exists(sklearn_pipeline_path):\n            raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n        if not os.path.exists(preprocess_pipeline_path):\n            raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n        # Load confs\n        with open(configuration_path, 'r', encoding='utf-8') as f:\n            configs = json.load(f)\n        # Can't set int as keys in json, so need to cast it after reloading\n        # dict_classes keys are always ints\n        if 'dict_classes' in configs.keys():\n            configs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n        elif 'list_classes' in configs.keys():\n            configs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n\n        # Set class vars\n        # self.model_name = # Keep the created name\n        # self.model_dir = # Keep the created folder\n        self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n        self.trained = configs.get('trained', True)  # Consider trained by default\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                          'list_classes', 'dict_classes', 'multi_label', 'random_seed', 'level_save',\n                          'multiclass_strategy']:\n            setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n        # Reload pipeline model\n        with open(sklearn_pipeline_path, 'rb') as f:\n            self.pipeline = pickle.load(f)\n\n        # Manage multi-labels or multi-classes\n        if not self.multi_label and self.multiclass_strategy is None:\n            self.lgbm = self.pipeline['lgbm']\n        else:\n            self.lgbm = self.pipeline['lgbm'].estimator\n\n        # Reload pipeline preprocessing\n        with open(preprocess_pipeline_path, 'rb') as f:\n            self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_lgbm_classifier/#template_num.models_training.classifiers.models_sklearn.model_lgbm_classifier.ModelLGBMClassifier.__init__","title":"<code>__init__(lgbm_params=None, multiclass_strategy=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)</p> Kwargs <p>lgbm_params (dict) : Parameters for the Light GBM multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.</p> <p>Raises:     If multiclass_strategy is not 'ovo', 'ovr' or None</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_lgbm_classifier.py</code> <pre><code>def __init__(self, lgbm_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)\n\n    Kwargs:\n        lgbm_params (dict) : Parameters for the Light GBM\n        multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n    Raises:\n        If multiclass_strategy is not 'ovo', 'ovr' or None\n    '''\n    if multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\n        raise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Manage model\n    if lgbm_params is None:\n        lgbm_params = {}\n    lgbm_params[\"random_state\"] = self.random_seed\n    self.lgbm = LGBMClassifier(**lgbm_params)\n    self.multiclass_strategy = multiclass_strategy\n\n    # Can't do multi-labels / multi-classes\n    if not self.multi_label:\n        # If not multi-classes : no impact\n        if multiclass_strategy == 'ovr':\n            self.pipeline = Pipeline([('lgbm', OneVsRestClassifier(self.lgbm))])\n        elif multiclass_strategy == 'ovo':\n            self.pipeline = Pipeline([('lgbm', OneVsOneClassifier(self.lgbm))])\n        else:\n            self.pipeline = Pipeline([('lgbm', self.lgbm)])\n\n    # LGBMClassifier does not natively support multi-labels\n    if self.multi_label:\n        self.pipeline = Pipeline([('lgbm', MultiOutputClassifier(self.lgbm))])\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_lgbm_classifier/#template_num.models_training.classifiers.models_sklearn.model_lgbm_classifier.ModelLGBMClassifier.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set     'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>DataFrame</code> <p>DataFrame with the test data to be predicted</p> required <p>Returns:     (np.ndarray): Array, shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_lgbm_classifier.py</code> <pre><code>@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n    '''Predicts the probabilities on the test set\n        'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.\n\n    Args:\n        x_test (pd.DataFrame): DataFrame with the test data to be predicted\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n    # Uses super() of the ModelPipeline class if != 'ovo' or multi-labels\n    if self.multi_label or self.multiclass_strategy != 'ovo':\n        return super().predict_proba(x_test=x_test, **kwargs)\n    else:\n        # We check input format\n        x_test, _ = self._check_input_format(x_test)\n        # Get preds\n        preds = self.pipeline.predict(x_test)\n        # Format ['a', 'b', 'c', 'a', ..., 'b']\n        # Transform to \"proba\"\n        transform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\n        probas = np.array([transform_dict[x] for x in preds])\n    return probas\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_lgbm_classifier/#template_num.models_training.classifiers.models_sklearn.model_lgbm_classifier.ModelLGBMClassifier.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:     ValueError: If configuration_path is None     ValueError: If sklearn_pipeline_path is None     ValueError: If preprocess_pipeline_path is None     FileNotFoundError: If the object configuration_path is not an existing file     FileNotFoundError: If the object sklearn_pipeline_path is not an existing file     FileNotFoundError: If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_lgbm_classifier.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n    '''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n    # Retrieve args\n    configuration_path = kwargs.get('configuration_path', None)\n    sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n    preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n    # Checks\n    if configuration_path is None:\n        raise ValueError(\"The argument configuration_path can't be None\")\n    if sklearn_pipeline_path is None:\n        raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n    if preprocess_pipeline_path is None:\n        raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n    if not os.path.exists(configuration_path):\n        raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n    if not os.path.exists(sklearn_pipeline_path):\n        raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n    if not os.path.exists(preprocess_pipeline_path):\n        raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n    # Load confs\n    with open(configuration_path, 'r', encoding='utf-8') as f:\n        configs = json.load(f)\n    # Can't set int as keys in json, so need to cast it after reloading\n    # dict_classes keys are always ints\n    if 'dict_classes' in configs.keys():\n        configs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n    elif 'list_classes' in configs.keys():\n        configs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n\n    # Set class vars\n    # self.model_name = # Keep the created name\n    # self.model_dir = # Keep the created folder\n    self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n    self.trained = configs.get('trained', True)  # Consider trained by default\n    # Try to read the following attributes from configs and, if absent, keep the current one\n    for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                      'list_classes', 'dict_classes', 'multi_label', 'random_seed', 'level_save',\n                      'multiclass_strategy']:\n        setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n    # Reload pipeline model\n    with open(sklearn_pipeline_path, 'rb') as f:\n        self.pipeline = pickle.load(f)\n\n    # Manage multi-labels or multi-classes\n    if not self.multi_label and self.multiclass_strategy is None:\n        self.lgbm = self.pipeline['lgbm']\n    else:\n        self.lgbm = self.pipeline['lgbm'].estimator\n\n    # Reload pipeline preprocessing\n    with open(preprocess_pipeline_path, 'rb') as f:\n        self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_lgbm_classifier/#template_num.models_training.classifiers.models_sklearn.model_lgbm_classifier.ModelLGBMClassifier.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_lgbm_classifier.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save model\n    if json_data is None:\n        json_data = {}\n\n    json_data['multiclass_strategy'] = self.multiclass_strategy\n\n    # Save\n    super().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_logistic_regression_classifier/","title":"Model logistic regression classifier","text":""},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_logistic_regression_classifier/#template_num.models_training.classifiers.models_sklearn.model_logistic_regression_classifier.ModelLogisticRegressionClassifier","title":"<code>ModelLogisticRegressionClassifier</code>","text":"<p>             Bases: <code>ModelClassifierMixin</code>, <code>ModelPipeline</code></p> <p>Logistic Regression mode for classification</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_logistic_regression_classifier.py</code> <pre><code>class ModelLogisticRegressionClassifier(ModelClassifierMixin, ModelPipeline):\n    '''Logistic Regression mode for classification'''\n\n    _default_name = 'model_lr_classifier'\n\n    def __init__(self, lr_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)\n\n        Kwargs:\n            lr_params (dict) : Parameters for the Logistic Regression\n            multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n        Raises:\n            multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n        '''\n        if multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\n            raise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Manage model\n        if lr_params is None:\n            lr_params = {}\n        self.lr = LogisticRegression(**lr_params)\n        self.multiclass_strategy = multiclass_strategy\n\n        # Can't do multi-labels / multi-classes\n        if not self.multi_label:\n            # If not multi-classes : no impact\n            if multiclass_strategy == 'ovr':\n                self.pipeline = Pipeline([('lr', OneVsRestClassifier(self.lr))])\n            elif multiclass_strategy == 'ovo':\n                self.pipeline = Pipeline([('lr', OneVsOneClassifier(self.lr))])\n            else:\n                self.pipeline = Pipeline([('lr', self.lr)])\n\n        # LogisticRegression does not natively support multi-labels\n        if self.multi_label:\n            self.pipeline = Pipeline([('lr', MultiOutputClassifier(self.lr))])\n\n    @utils.trained_needed\n    def predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n        '''Predicts the probabilities on the test set\n            'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.\n\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n        # Uses super() of the ModelPipeline class if != 'ovo' or multi-labels\n        if self.multi_label or self.multiclass_strategy != 'ovo':\n            return super().predict_proba(x_test=x_test, **kwargs)\n        else:\n            # We check input format\n            x_test, _ = self._check_input_format(x_test)\n            # Get preds\n            preds = self.pipeline.predict(x_test)\n            # Format ['a', 'b', 'c', 'a', ..., 'b']\n            # Transform to \"proba\"\n            transform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\n            probas = np.array([transform_dict[x] for x in preds])\n        return probas\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save model\n        if json_data is None:\n            json_data = {}\n\n        json_data['multiclass_strategy'] = self.multiclass_strategy\n\n        # Save\n        super().save(json_data=json_data)\n\n    def reload_from_standalone(self, **kwargs) -&gt; None:\n        '''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n        # Retrieve args\n        configuration_path = kwargs.get('configuration_path', None)\n        sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n        preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n        # Checks\n        if configuration_path is None:\n            raise ValueError(\"The argument configuration_path can't be None\")\n        if sklearn_pipeline_path is None:\n            raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n        if preprocess_pipeline_path is None:\n            raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n        if not os.path.exists(configuration_path):\n            raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n        if not os.path.exists(sklearn_pipeline_path):\n            raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n        if not os.path.exists(preprocess_pipeline_path):\n            raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n        # Load confs\n        with open(configuration_path, 'r', encoding='utf-8') as f:\n            configs = json.load(f)\n        # Can't set int as keys in json, so need to cast it after reloading\n        # dict_classes keys are always ints\n        if 'dict_classes' in configs.keys():\n            configs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n        elif 'list_classes' in configs.keys():\n            configs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n\n        # Set class vars\n        # self.model_name = # Keep the created name\n        # self.model_dir = # Keep the created folder\n        self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n        self.trained = configs.get('trained', True)  # Consider trained by default\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                          'list_classes', 'dict_classes', 'multi_label', 'level_save',\n                          'multiclass_strategy']:\n            setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n        # Reload pipeline model\n        with open(sklearn_pipeline_path, 'rb') as f:\n            self.pipeline = pickle.load(f)\n\n        # Manage multi-labels or multi-classes\n        if not self.multi_label and self.multiclass_strategy is None:\n            self.lr = self.pipeline['lr']\n        else:\n            self.lr = self.pipeline['lr'].estimator\n\n        # Reload pipeline preprocessing\n        with open(preprocess_pipeline_path, 'rb') as f:\n            self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_logistic_regression_classifier/#template_num.models_training.classifiers.models_sklearn.model_logistic_regression_classifier.ModelLogisticRegressionClassifier.__init__","title":"<code>__init__(lr_params=None, multiclass_strategy=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)</p> Kwargs <p>lr_params (dict) : Parameters for the Logistic Regression multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.</p> <p>Raises:     multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_logistic_regression_classifier.py</code> <pre><code>def __init__(self, lr_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)\n\n    Kwargs:\n        lr_params (dict) : Parameters for the Logistic Regression\n        multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n    Raises:\n        multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n    '''\n    if multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\n        raise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Manage model\n    if lr_params is None:\n        lr_params = {}\n    self.lr = LogisticRegression(**lr_params)\n    self.multiclass_strategy = multiclass_strategy\n\n    # Can't do multi-labels / multi-classes\n    if not self.multi_label:\n        # If not multi-classes : no impact\n        if multiclass_strategy == 'ovr':\n            self.pipeline = Pipeline([('lr', OneVsRestClassifier(self.lr))])\n        elif multiclass_strategy == 'ovo':\n            self.pipeline = Pipeline([('lr', OneVsOneClassifier(self.lr))])\n        else:\n            self.pipeline = Pipeline([('lr', self.lr)])\n\n    # LogisticRegression does not natively support multi-labels\n    if self.multi_label:\n        self.pipeline = Pipeline([('lr', MultiOutputClassifier(self.lr))])\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_logistic_regression_classifier/#template_num.models_training.classifiers.models_sklearn.model_logistic_regression_classifier.ModelLogisticRegressionClassifier.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set     'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>DataFrame</code> <p>DataFrame with the test data to be predicted</p> required <p>Returns:     (np.ndarray): Array, shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_logistic_regression_classifier.py</code> <pre><code>@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n    '''Predicts the probabilities on the test set\n        'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.\n\n    Args:\n        x_test (pd.DataFrame): DataFrame with the test data to be predicted\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n    # Uses super() of the ModelPipeline class if != 'ovo' or multi-labels\n    if self.multi_label or self.multiclass_strategy != 'ovo':\n        return super().predict_proba(x_test=x_test, **kwargs)\n    else:\n        # We check input format\n        x_test, _ = self._check_input_format(x_test)\n        # Get preds\n        preds = self.pipeline.predict(x_test)\n        # Format ['a', 'b', 'c', 'a', ..., 'b']\n        # Transform to \"proba\"\n        transform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\n        probas = np.array([transform_dict[x] for x in preds])\n    return probas\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_logistic_regression_classifier/#template_num.models_training.classifiers.models_sklearn.model_logistic_regression_classifier.ModelLogisticRegressionClassifier.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:     ValueError: If configuration_path is None     ValueError: If sklearn_pipeline_path is None     ValueError: If preprocess_pipeline_path is None     FileNotFoundError: If the object configuration_path is not an existing file     FileNotFoundError: If the object sklearn_pipeline_path is not an existing file     FileNotFoundError: If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_logistic_regression_classifier.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n    '''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n    # Retrieve args\n    configuration_path = kwargs.get('configuration_path', None)\n    sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n    preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n    # Checks\n    if configuration_path is None:\n        raise ValueError(\"The argument configuration_path can't be None\")\n    if sklearn_pipeline_path is None:\n        raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n    if preprocess_pipeline_path is None:\n        raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n    if not os.path.exists(configuration_path):\n        raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n    if not os.path.exists(sklearn_pipeline_path):\n        raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n    if not os.path.exists(preprocess_pipeline_path):\n        raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n    # Load confs\n    with open(configuration_path, 'r', encoding='utf-8') as f:\n        configs = json.load(f)\n    # Can't set int as keys in json, so need to cast it after reloading\n    # dict_classes keys are always ints\n    if 'dict_classes' in configs.keys():\n        configs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n    elif 'list_classes' in configs.keys():\n        configs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n\n    # Set class vars\n    # self.model_name = # Keep the created name\n    # self.model_dir = # Keep the created folder\n    self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n    self.trained = configs.get('trained', True)  # Consider trained by default\n    # Try to read the following attributes from configs and, if absent, keep the current one\n    for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                      'list_classes', 'dict_classes', 'multi_label', 'level_save',\n                      'multiclass_strategy']:\n        setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n    # Reload pipeline model\n    with open(sklearn_pipeline_path, 'rb') as f:\n        self.pipeline = pickle.load(f)\n\n    # Manage multi-labels or multi-classes\n    if not self.multi_label and self.multiclass_strategy is None:\n        self.lr = self.pipeline['lr']\n    else:\n        self.lr = self.pipeline['lr'].estimator\n\n    # Reload pipeline preprocessing\n    with open(preprocess_pipeline_path, 'rb') as f:\n        self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_logistic_regression_classifier/#template_num.models_training.classifiers.models_sklearn.model_logistic_regression_classifier.ModelLogisticRegressionClassifier.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_logistic_regression_classifier.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save model\n    if json_data is None:\n        json_data = {}\n\n    json_data['multiclass_strategy'] = self.multiclass_strategy\n\n    # Save\n    super().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_rf_classifier/","title":"Model rf classifier","text":""},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_rf_classifier/#template_num.models_training.classifiers.models_sklearn.model_rf_classifier.ModelRFClassifier","title":"<code>ModelRFClassifier</code>","text":"<p>             Bases: <code>ModelClassifierMixin</code>, <code>ModelPipeline</code></p> <p>Random Forest model for classification</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_rf_classifier.py</code> <pre><code>class ModelRFClassifier(ModelClassifierMixin, ModelPipeline):\n    '''Random Forest model for classification'''\n\n    _default_name = 'model_rf_classifier'\n\n    def __init__(self, rf_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)\n\n        Kwargs:\n            rf_params (dict) : Parameters for the Random Forest\n            multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n        Raises:\n            If multiclass_strategy is not 'ovo', 'ovr' or None\n        '''\n        if multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\n            raise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Manage model\n        if rf_params is None:\n            rf_params = {}\n        rf_params[\"random_state\"] = self.random_seed\n        self.rf = RandomForestClassifier(**rf_params)\n        self.multiclass_strategy = multiclass_strategy\n\n        # Can't do multi-labels / multi-classes\n        if not self.multi_label:\n            # If not multi-classes : no impact\n            if multiclass_strategy == 'ovr':\n                self.pipeline = Pipeline([('rf', OneVsRestClassifier(self.rf))])\n            elif multiclass_strategy == 'ovo':\n                self.pipeline = Pipeline([('rf', OneVsOneClassifier(self.rf))])\n            else:\n                self.pipeline = Pipeline([('rf', self.rf)])\n\n        # RandomForest natively supports multi_labels\n        if self.multi_label:\n            self.pipeline = Pipeline([('rf', self.rf)])\n\n    @utils.trained_needed\n    def predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n        '''Predicts the probabilities on the test set\n            'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.\n\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n        # Uses super() of the ModelPipeline class if != 'ovo' or multi-labels\n        if self.multi_label or self.multiclass_strategy != 'ovo':\n            return super().predict_proba(x_test=x_test, **kwargs)\n        else:\n            # We check input format\n            x_test, _ = self._check_input_format(x_test)\n            # Get preds\n            preds = self.pipeline.predict(x_test)\n            # Format ['a', 'b', 'c', 'a', ..., 'b']\n            # Transform to \"proba\"\n            transform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\n            probas = np.array([transform_dict[x] for x in preds])\n        return probas\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save model\n        if json_data is None:\n            json_data = {}\n\n        json_data['multiclass_strategy'] = self.multiclass_strategy\n\n        # Save\n        super().save(json_data=json_data)\n\n    def reload_from_standalone(self, **kwargs) -&gt; None:\n        '''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n        # Retrieve args\n        configuration_path = kwargs.get('configuration_path', None)\n        sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n        preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n        # Checks\n        if configuration_path is None:\n            raise ValueError(\"The argument configuration_path can't be None\")\n        if sklearn_pipeline_path is None:\n            raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n        if preprocess_pipeline_path is None:\n            raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n        if not os.path.exists(configuration_path):\n            raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n        if not os.path.exists(sklearn_pipeline_path):\n            raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n        if not os.path.exists(preprocess_pipeline_path):\n            raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n        # Load confs\n        with open(configuration_path, 'r', encoding='utf-8') as f:\n            configs = json.load(f)\n        # Can't set int as keys in json, so need to cast it after reloading\n        # dict_classes keys are always ints\n        if 'dict_classes' in configs.keys():\n            configs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n        elif 'list_classes' in configs.keys():\n            configs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n\n        # Set class vars\n        # self.model_name = # Keep the created name\n        # self.model_dir = # Keep the created folder\n        self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n        self.trained = configs.get('trained', True)  # Consider trained by default\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                          'list_classes', 'dict_classes', 'multi_label', 'random_seed', 'level_save',\n                          'multiclass_strategy']:\n            setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n        # Reload pipeline model\n        with open(sklearn_pipeline_path, 'rb') as f:\n            self.pipeline = pickle.load(f)\n\n        # Manage multi-labels or multi-classes\n        if not self.multi_label and self.multiclass_strategy is not None:\n            self.rf = self.pipeline['rf'].estimator\n        else:\n            self.rf = self.pipeline['rf']\n\n        # Reload pipeline preprocessing\n        with open(preprocess_pipeline_path, 'rb') as f:\n            self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_rf_classifier/#template_num.models_training.classifiers.models_sklearn.model_rf_classifier.ModelRFClassifier.__init__","title":"<code>__init__(rf_params=None, multiclass_strategy=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)</p> Kwargs <p>rf_params (dict) : Parameters for the Random Forest multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.</p> <p>Raises:     If multiclass_strategy is not 'ovo', 'ovr' or None</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_rf_classifier.py</code> <pre><code>def __init__(self, rf_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)\n\n    Kwargs:\n        rf_params (dict) : Parameters for the Random Forest\n        multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n    Raises:\n        If multiclass_strategy is not 'ovo', 'ovr' or None\n    '''\n    if multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\n        raise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Manage model\n    if rf_params is None:\n        rf_params = {}\n    rf_params[\"random_state\"] = self.random_seed\n    self.rf = RandomForestClassifier(**rf_params)\n    self.multiclass_strategy = multiclass_strategy\n\n    # Can't do multi-labels / multi-classes\n    if not self.multi_label:\n        # If not multi-classes : no impact\n        if multiclass_strategy == 'ovr':\n            self.pipeline = Pipeline([('rf', OneVsRestClassifier(self.rf))])\n        elif multiclass_strategy == 'ovo':\n            self.pipeline = Pipeline([('rf', OneVsOneClassifier(self.rf))])\n        else:\n            self.pipeline = Pipeline([('rf', self.rf)])\n\n    # RandomForest natively supports multi_labels\n    if self.multi_label:\n        self.pipeline = Pipeline([('rf', self.rf)])\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_rf_classifier/#template_num.models_training.classifiers.models_sklearn.model_rf_classifier.ModelRFClassifier.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set     'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>DataFrame</code> <p>DataFrame with the test data to be predicted</p> required <p>Returns:     (np.ndarray): Array, shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_rf_classifier.py</code> <pre><code>@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n    '''Predicts the probabilities on the test set\n        'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.\n\n    Args:\n        x_test (pd.DataFrame): DataFrame with the test data to be predicted\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n    # Uses super() of the ModelPipeline class if != 'ovo' or multi-labels\n    if self.multi_label or self.multiclass_strategy != 'ovo':\n        return super().predict_proba(x_test=x_test, **kwargs)\n    else:\n        # We check input format\n        x_test, _ = self._check_input_format(x_test)\n        # Get preds\n        preds = self.pipeline.predict(x_test)\n        # Format ['a', 'b', 'c', 'a', ..., 'b']\n        # Transform to \"proba\"\n        transform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\n        probas = np.array([transform_dict[x] for x in preds])\n    return probas\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_rf_classifier/#template_num.models_training.classifiers.models_sklearn.model_rf_classifier.ModelRFClassifier.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:     ValueError: If configuration_path is None     ValueError: If sklearn_pipeline_path is None     ValueError: If preprocess_pipeline_path is None     FileNotFoundError: If the object configuration_path is not an existing file     FileNotFoundError: If the object sklearn_pipeline_path is not an existing file     FileNotFoundError: If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_rf_classifier.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n    '''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n    # Retrieve args\n    configuration_path = kwargs.get('configuration_path', None)\n    sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n    preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n    # Checks\n    if configuration_path is None:\n        raise ValueError(\"The argument configuration_path can't be None\")\n    if sklearn_pipeline_path is None:\n        raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n    if preprocess_pipeline_path is None:\n        raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n    if not os.path.exists(configuration_path):\n        raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n    if not os.path.exists(sklearn_pipeline_path):\n        raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n    if not os.path.exists(preprocess_pipeline_path):\n        raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n    # Load confs\n    with open(configuration_path, 'r', encoding='utf-8') as f:\n        configs = json.load(f)\n    # Can't set int as keys in json, so need to cast it after reloading\n    # dict_classes keys are always ints\n    if 'dict_classes' in configs.keys():\n        configs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n    elif 'list_classes' in configs.keys():\n        configs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n\n    # Set class vars\n    # self.model_name = # Keep the created name\n    # self.model_dir = # Keep the created folder\n    self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n    self.trained = configs.get('trained', True)  # Consider trained by default\n    # Try to read the following attributes from configs and, if absent, keep the current one\n    for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                      'list_classes', 'dict_classes', 'multi_label', 'random_seed', 'level_save',\n                      'multiclass_strategy']:\n        setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n    # Reload pipeline model\n    with open(sklearn_pipeline_path, 'rb') as f:\n        self.pipeline = pickle.load(f)\n\n    # Manage multi-labels or multi-classes\n    if not self.multi_label and self.multiclass_strategy is not None:\n        self.rf = self.pipeline['rf'].estimator\n    else:\n        self.rf = self.pipeline['rf']\n\n    # Reload pipeline preprocessing\n    with open(preprocess_pipeline_path, 'rb') as f:\n        self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_rf_classifier/#template_num.models_training.classifiers.models_sklearn.model_rf_classifier.ModelRFClassifier.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_rf_classifier.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save model\n    if json_data is None:\n        json_data = {}\n\n    json_data['multiclass_strategy'] = self.multiclass_strategy\n\n    # Save\n    super().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_ridge_classifier/","title":"Model ridge classifier","text":""},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_ridge_classifier/#template_num.models_training.classifiers.models_sklearn.model_ridge_classifier.ModelRidgeClassifier","title":"<code>ModelRidgeClassifier</code>","text":"<p>             Bases: <code>ModelClassifierMixin</code>, <code>ModelPipeline</code></p> <p>Ridge Classifier model for classification</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_ridge_classifier.py</code> <pre><code>class ModelRidgeClassifier(ModelClassifierMixin, ModelPipeline):\n    '''Ridge Classifier model for classification'''\n\n    _default_name = 'model_ridge_classifier'\n\n    def __init__(self, ridge_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)\n\n        Kwargs:\n            ridge_params (dict) : Parameters for the Ridge Classifier\n            multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n        Raises:\n            If multiclass_strategy is not 'ovo', 'ovr' or None\n        '''\n        if multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\n            raise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Manage model\n        if ridge_params is None:\n            ridge_params = {}\n        ridge_params[\"random_state\"] = self.random_seed\n        self.ridge = RidgeClassifier(**ridge_params)\n        self.multiclass_strategy = multiclass_strategy\n\n        # Can't do multi-labels / multi-classes\n        if not self.multi_label:\n            # If not multi-classes : no impact\n            if multiclass_strategy == 'ovr':\n                self.pipeline = Pipeline([('ridge', OneVsRestClassifier(self.ridge))])\n            elif multiclass_strategy == 'ovo':\n                self.pipeline = Pipeline([('ridge', OneVsOneClassifier(self.ridge))])\n            else:\n                self.pipeline = Pipeline([('ridge', self.ridge)])\n\n        # RidgeClassifier does not natively support multi-labels\n        if self.multi_label:\n            self.pipeline = Pipeline([('ridge', MultiOutputClassifier(self.ridge))])\n\n    @utils.trained_needed\n    def predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n        '''Predicts the probabilities on the test set\n        - /!\\\\ RIDGE CLASSIFIER DOESN'T RETURN PROBABILITIES, THE MODEL WILL GIVES 0 AND 1 AS PROBABILITIES /!\\\\ -\n\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n        # We check input format\n        x_test, _ = self._check_input_format(x_test)\n        # Get preds\n        if not self.multi_label:\n            preds = self.pipeline.predict(x_test)\n            # Format ['a', 'b', 'c', 'a', ..., 'b']\n            # Transform to \"proba\"\n            transform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\n            probas = np.array([transform_dict[x] for x in preds])\n        else:\n            preds = self.pipeline.predict(x_test)\n            # Already right format, but in int !\n            probas = np.array([[float(_) for _ in x] for x in preds])\n        return probas\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save model\n        if json_data is None:\n            json_data = {}\n\n        json_data['multiclass_strategy'] = self.multiclass_strategy\n\n        # Save\n        super().save(json_data=json_data)\n\n    def reload_from_standalone(self, **kwargs) -&gt; None:\n        '''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n        # Retrieve args\n        configuration_path = kwargs.get('configuration_path', None)\n        sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n        preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n        # Checks\n        if configuration_path is None:\n            raise ValueError(\"The argument configuration_path can't be None\")\n        if sklearn_pipeline_path is None:\n            raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n        if preprocess_pipeline_path is None:\n            raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n        if not os.path.exists(configuration_path):\n            raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n        if not os.path.exists(sklearn_pipeline_path):\n            raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n        if not os.path.exists(preprocess_pipeline_path):\n            raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n        # Load confs\n        with open(configuration_path, 'r', encoding='utf-8') as f:\n            configs = json.load(f)\n        # Can't set int as keys in json, so need to cast it after reloading\n        # dict_classes keys are always ints\n        if 'dict_classes' in configs.keys():\n            configs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n        elif 'list_classes' in configs.keys():\n            configs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n\n        # Set class vars\n        # self.model_name = # Keep the created name\n        # self.model_dir = # Keep the created folder\n        self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n        self.trained = configs.get('trained', True)  # Consider trained by default\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                          'list_classes', 'dict_classes', 'multi_label', 'random_seed', 'level_save',\n                          'multiclass_strategy']:\n            setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n        # Reload pipeline model\n        with open(sklearn_pipeline_path, 'rb') as f:\n            self.pipeline = pickle.load(f)\n\n        # Manage multi-labels or multi-classes\n        if not self.multi_label and self.multiclass_strategy is None:\n            self.ridge = self.pipeline['ridge']\n        else:\n            self.ridge = self.pipeline['ridge'].estimator\n\n        # Reload pipeline preprocessing\n        with open(preprocess_pipeline_path, 'rb') as f:\n            self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_ridge_classifier/#template_num.models_training.classifiers.models_sklearn.model_ridge_classifier.ModelRidgeClassifier.__init__","title":"<code>__init__(ridge_params=None, multiclass_strategy=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)</p> Kwargs <p>ridge_params (dict) : Parameters for the Ridge Classifier multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.</p> <p>Raises:     If multiclass_strategy is not 'ovo', 'ovr' or None</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_ridge_classifier.py</code> <pre><code>def __init__(self, ridge_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)\n\n    Kwargs:\n        ridge_params (dict) : Parameters for the Ridge Classifier\n        multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n    Raises:\n        If multiclass_strategy is not 'ovo', 'ovr' or None\n    '''\n    if multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\n        raise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Manage model\n    if ridge_params is None:\n        ridge_params = {}\n    ridge_params[\"random_state\"] = self.random_seed\n    self.ridge = RidgeClassifier(**ridge_params)\n    self.multiclass_strategy = multiclass_strategy\n\n    # Can't do multi-labels / multi-classes\n    if not self.multi_label:\n        # If not multi-classes : no impact\n        if multiclass_strategy == 'ovr':\n            self.pipeline = Pipeline([('ridge', OneVsRestClassifier(self.ridge))])\n        elif multiclass_strategy == 'ovo':\n            self.pipeline = Pipeline([('ridge', OneVsOneClassifier(self.ridge))])\n        else:\n            self.pipeline = Pipeline([('ridge', self.ridge)])\n\n    # RidgeClassifier does not natively support multi-labels\n    if self.multi_label:\n        self.pipeline = Pipeline([('ridge', MultiOutputClassifier(self.ridge))])\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_ridge_classifier/#template_num.models_training.classifiers.models_sklearn.model_ridge_classifier.ModelRidgeClassifier.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set - /! RIDGE CLASSIFIER DOESN'T RETURN PROBABILITIES, THE MODEL WILL GIVES 0 AND 1 AS PROBABILITIES /! -</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>DataFrame</code> <p>DataFrame with the test data to be predicted</p> required <p>Returns:     (np.ndarray): Array, shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_ridge_classifier.py</code> <pre><code>@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n    '''Predicts the probabilities on the test set\n    - /!\\\\ RIDGE CLASSIFIER DOESN'T RETURN PROBABILITIES, THE MODEL WILL GIVES 0 AND 1 AS PROBABILITIES /!\\\\ -\n\n    Args:\n        x_test (pd.DataFrame): DataFrame with the test data to be predicted\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n    # We check input format\n    x_test, _ = self._check_input_format(x_test)\n    # Get preds\n    if not self.multi_label:\n        preds = self.pipeline.predict(x_test)\n        # Format ['a', 'b', 'c', 'a', ..., 'b']\n        # Transform to \"proba\"\n        transform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\n        probas = np.array([transform_dict[x] for x in preds])\n    else:\n        preds = self.pipeline.predict(x_test)\n        # Already right format, but in int !\n        probas = np.array([[float(_) for _ in x] for x in preds])\n    return probas\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_ridge_classifier/#template_num.models_training.classifiers.models_sklearn.model_ridge_classifier.ModelRidgeClassifier.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:     ValueError: If configuration_path is None     ValueError: If sklearn_pipeline_path is None     ValueError: If preprocess_pipeline_path is None     FileNotFoundError: If the object configuration_path is not an existing file     FileNotFoundError: If the object sklearn_pipeline_path is not an existing file     FileNotFoundError: If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_ridge_classifier.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n    '''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n    # Retrieve args\n    configuration_path = kwargs.get('configuration_path', None)\n    sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n    preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n    # Checks\n    if configuration_path is None:\n        raise ValueError(\"The argument configuration_path can't be None\")\n    if sklearn_pipeline_path is None:\n        raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n    if preprocess_pipeline_path is None:\n        raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n    if not os.path.exists(configuration_path):\n        raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n    if not os.path.exists(sklearn_pipeline_path):\n        raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n    if not os.path.exists(preprocess_pipeline_path):\n        raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n    # Load confs\n    with open(configuration_path, 'r', encoding='utf-8') as f:\n        configs = json.load(f)\n    # Can't set int as keys in json, so need to cast it after reloading\n    # dict_classes keys are always ints\n    if 'dict_classes' in configs.keys():\n        configs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n    elif 'list_classes' in configs.keys():\n        configs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n\n    # Set class vars\n    # self.model_name = # Keep the created name\n    # self.model_dir = # Keep the created folder\n    self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n    self.trained = configs.get('trained', True)  # Consider trained by default\n    # Try to read the following attributes from configs and, if absent, keep the current one\n    for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                      'list_classes', 'dict_classes', 'multi_label', 'random_seed', 'level_save',\n                      'multiclass_strategy']:\n        setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n    # Reload pipeline model\n    with open(sklearn_pipeline_path, 'rb') as f:\n        self.pipeline = pickle.load(f)\n\n    # Manage multi-labels or multi-classes\n    if not self.multi_label and self.multiclass_strategy is None:\n        self.ridge = self.pipeline['ridge']\n    else:\n        self.ridge = self.pipeline['ridge'].estimator\n\n    # Reload pipeline preprocessing\n    with open(preprocess_pipeline_path, 'rb') as f:\n        self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_ridge_classifier/#template_num.models_training.classifiers.models_sklearn.model_ridge_classifier.ModelRidgeClassifier.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_ridge_classifier.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save model\n    if json_data is None:\n        json_data = {}\n\n    json_data['multiclass_strategy'] = self.multiclass_strategy\n\n    # Save\n    super().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_sgd_classifier/","title":"Model sgd classifier","text":""},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_sgd_classifier/#template_num.models_training.classifiers.models_sklearn.model_sgd_classifier.ModelSGDClassifier","title":"<code>ModelSGDClassifier</code>","text":"<p>             Bases: <code>ModelClassifierMixin</code>, <code>ModelPipeline</code></p> <p>Stochastic Gradient Descent model for classification</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_sgd_classifier.py</code> <pre><code>class ModelSGDClassifier(ModelClassifierMixin, ModelPipeline):\n    '''Stochastic Gradient Descent model for classification'''\n\n    _default_name = 'model_sgd_classifier'\n\n    def __init__(self, sgd_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)\n\n        Kwargs:\n            sgd_params (dict) : Parameters for the Stochastic Gradient Descent\n            multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n        Raises:\n            If multiclass_strategy is not 'ovo', 'ovr' or None\n        '''\n        if multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\n            raise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Manage model\n        if sgd_params is None:\n            sgd_params = {}\n        sgd_params[\"random_state\"] = self.random_seed\n        self.sgd = SGDClassifier(**sgd_params)\n        self.multiclass_strategy = multiclass_strategy\n\n        # Can't do multi-labels / multi-classes\n        if not self.multi_label:\n            # If not multi-classes : no impact\n            if multiclass_strategy == 'ovr':\n                self.pipeline = Pipeline([('sgd', OneVsRestClassifier(self.sgd))])\n            elif multiclass_strategy == 'ovo':\n                self.pipeline = Pipeline([('sgd', OneVsOneClassifier(self.sgd))])\n            else:\n                self.pipeline = Pipeline([('sgd', self.sgd)])\n\n        # SGDClassifier does not natively support multi-labels\n        if self.multi_label:\n            self.pipeline = Pipeline([('sgd', MultiOutputClassifier(self.sgd))])\n\n    @utils.trained_needed\n    def predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n        '''Predicts the probabilities on the test set\n            'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.\n\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n        # Can't use probabilities if loss not in ['log', 'modified_huber'] or 'ovo' and not multi-labels\n        if self.sgd.loss not in ['log', 'modified_huber'] or (self.multiclass_strategy == 'ovo' and not self.multi_label):\n            # We check input format\n            x_test, _ = self._check_input_format(x_test)\n            # Get preds\n            if not self.multi_label:\n                preds = self.pipeline.predict(x_test)\n                # Format ['a', 'b', 'c', 'a', ..., 'b']\n                # Transform to \"proba\"\n                transform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\n                probas = np.array([transform_dict[x] for x in preds])\n            else:\n                preds = self.pipeline.predict(x_test)\n                # Already right format, but in int !\n                probas = np.array([[float(_) for _ in x] for x in preds])\n        # Otherwise, use super() of the pipeline class if != 'ovo' or multi-labels\n        else:\n            return super().predict_proba(x_test=x_test, **kwargs)\n        return probas\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save model\n        if json_data is None:\n            json_data = {}\n\n        json_data['multiclass_strategy'] = self.multiclass_strategy\n\n        # Save\n        super().save(json_data=json_data)\n\n    def reload_from_standalone(self, **kwargs) -&gt; None:\n        '''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n        # Retrieve args\n        configuration_path = kwargs.get('configuration_path', None)\n        sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n        preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n        # Checks\n        if configuration_path is None:\n            raise ValueError(\"The argument configuration_path can't be None\")\n        if sklearn_pipeline_path is None:\n            raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n        if preprocess_pipeline_path is None:\n            raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n        if not os.path.exists(configuration_path):\n            raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n        if not os.path.exists(sklearn_pipeline_path):\n            raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n        if not os.path.exists(preprocess_pipeline_path):\n            raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n        # Load confs\n        with open(configuration_path, 'r', encoding='utf-8') as f:\n            configs = json.load(f)\n        # Can't set int as keys in json, so need to cast it after reloading\n        # dict_classes keys are always ints\n        if 'dict_classes' in configs.keys():\n            configs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n        elif 'list_classes' in configs.keys():\n            configs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n\n        # Set class vars\n        # self.model_name = # Keep the created name\n        # self.model_dir = # Keep the created folder\n        self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n        self.trained = configs.get('trained', True)  # Consider trained by default\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                          'list_classes', 'dict_classes', 'multi_label', 'random_seed', 'level_save',\n                          'multiclass_strategy']:\n            setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n        # Reload pipeline model\n        with open(sklearn_pipeline_path, 'rb') as f:\n            self.pipeline = pickle.load(f)\n\n        # Manage multi-labels or multi-classes\n        if not self.multi_label and self.multiclass_strategy is None:\n            self.sgd = self.pipeline['sgd']\n        else:\n            self.sgd = self.pipeline['sgd'].estimator\n\n        # Reload pipeline preprocessing\n        with open(preprocess_pipeline_path, 'rb') as f:\n            self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_sgd_classifier/#template_num.models_training.classifiers.models_sklearn.model_sgd_classifier.ModelSGDClassifier.__init__","title":"<code>__init__(sgd_params=None, multiclass_strategy=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)</p> Kwargs <p>sgd_params (dict) : Parameters for the Stochastic Gradient Descent multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.</p> <p>Raises:     If multiclass_strategy is not 'ovo', 'ovr' or None</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_sgd_classifier.py</code> <pre><code>def __init__(self, sgd_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)\n\n    Kwargs:\n        sgd_params (dict) : Parameters for the Stochastic Gradient Descent\n        multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n    Raises:\n        If multiclass_strategy is not 'ovo', 'ovr' or None\n    '''\n    if multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\n        raise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Manage model\n    if sgd_params is None:\n        sgd_params = {}\n    sgd_params[\"random_state\"] = self.random_seed\n    self.sgd = SGDClassifier(**sgd_params)\n    self.multiclass_strategy = multiclass_strategy\n\n    # Can't do multi-labels / multi-classes\n    if not self.multi_label:\n        # If not multi-classes : no impact\n        if multiclass_strategy == 'ovr':\n            self.pipeline = Pipeline([('sgd', OneVsRestClassifier(self.sgd))])\n        elif multiclass_strategy == 'ovo':\n            self.pipeline = Pipeline([('sgd', OneVsOneClassifier(self.sgd))])\n        else:\n            self.pipeline = Pipeline([('sgd', self.sgd)])\n\n    # SGDClassifier does not natively support multi-labels\n    if self.multi_label:\n        self.pipeline = Pipeline([('sgd', MultiOutputClassifier(self.sgd))])\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_sgd_classifier/#template_num.models_training.classifiers.models_sklearn.model_sgd_classifier.ModelSGDClassifier.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set     'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>DataFrame</code> <p>DataFrame with the test data to be predicted</p> required <p>Returns:     (np.ndarray): Array, shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_sgd_classifier.py</code> <pre><code>@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n    '''Predicts the probabilities on the test set\n        'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.\n\n    Args:\n        x_test (pd.DataFrame): DataFrame with the test data to be predicted\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n    # Can't use probabilities if loss not in ['log', 'modified_huber'] or 'ovo' and not multi-labels\n    if self.sgd.loss not in ['log', 'modified_huber'] or (self.multiclass_strategy == 'ovo' and not self.multi_label):\n        # We check input format\n        x_test, _ = self._check_input_format(x_test)\n        # Get preds\n        if not self.multi_label:\n            preds = self.pipeline.predict(x_test)\n            # Format ['a', 'b', 'c', 'a', ..., 'b']\n            # Transform to \"proba\"\n            transform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\n            probas = np.array([transform_dict[x] for x in preds])\n        else:\n            preds = self.pipeline.predict(x_test)\n            # Already right format, but in int !\n            probas = np.array([[float(_) for _ in x] for x in preds])\n    # Otherwise, use super() of the pipeline class if != 'ovo' or multi-labels\n    else:\n        return super().predict_proba(x_test=x_test, **kwargs)\n    return probas\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_sgd_classifier/#template_num.models_training.classifiers.models_sklearn.model_sgd_classifier.ModelSGDClassifier.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:     ValueError: If configuration_path is None     ValueError: If sklearn_pipeline_path is None     ValueError: If preprocess_pipeline_path is None     FileNotFoundError: If the object configuration_path is not an existing file     FileNotFoundError: If the object sklearn_pipeline_path is not an existing file     FileNotFoundError: If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_sgd_classifier.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n    '''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n    # Retrieve args\n    configuration_path = kwargs.get('configuration_path', None)\n    sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n    preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n    # Checks\n    if configuration_path is None:\n        raise ValueError(\"The argument configuration_path can't be None\")\n    if sklearn_pipeline_path is None:\n        raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n    if preprocess_pipeline_path is None:\n        raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n    if not os.path.exists(configuration_path):\n        raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n    if not os.path.exists(sklearn_pipeline_path):\n        raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n    if not os.path.exists(preprocess_pipeline_path):\n        raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n    # Load confs\n    with open(configuration_path, 'r', encoding='utf-8') as f:\n        configs = json.load(f)\n    # Can't set int as keys in json, so need to cast it after reloading\n    # dict_classes keys are always ints\n    if 'dict_classes' in configs.keys():\n        configs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n    elif 'list_classes' in configs.keys():\n        configs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n\n    # Set class vars\n    # self.model_name = # Keep the created name\n    # self.model_dir = # Keep the created folder\n    self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n    self.trained = configs.get('trained', True)  # Consider trained by default\n    # Try to read the following attributes from configs and, if absent, keep the current one\n    for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                      'list_classes', 'dict_classes', 'multi_label', 'random_seed', 'level_save',\n                      'multiclass_strategy']:\n        setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n    # Reload pipeline model\n    with open(sklearn_pipeline_path, 'rb') as f:\n        self.pipeline = pickle.load(f)\n\n    # Manage multi-labels or multi-classes\n    if not self.multi_label and self.multiclass_strategy is None:\n        self.sgd = self.pipeline['sgd']\n    else:\n        self.sgd = self.pipeline['sgd'].estimator\n\n    # Reload pipeline preprocessing\n    with open(preprocess_pipeline_path, 'rb') as f:\n        self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_sgd_classifier/#template_num.models_training.classifiers.models_sklearn.model_sgd_classifier.ModelSGDClassifier.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_sgd_classifier.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save model\n    if json_data is None:\n        json_data = {}\n\n    json_data['multiclass_strategy'] = self.multiclass_strategy\n\n    # Save\n    super().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_svm_classifier/","title":"Model svm classifier","text":""},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_svm_classifier/#template_num.models_training.classifiers.models_sklearn.model_svm_classifier.ModelSVMClassifier","title":"<code>ModelSVMClassifier</code>","text":"<p>             Bases: <code>ModelClassifierMixin</code>, <code>ModelPipeline</code></p> <p>Support Vector Machine model for classification</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_svm_classifier.py</code> <pre><code>class ModelSVMClassifier(ModelClassifierMixin, ModelPipeline):\n    '''Support Vector Machine model for classification'''\n\n    _default_name = 'model_svm_classifier'\n\n    def __init__(self, svm_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)\n\n        Kwargs:\n            svm_params (dict) : Parameters for the Support Vector Machine\n            multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n        Raises:\n            If multiclass_strategy is not 'ovo', 'ovr' or None\n        '''\n        if multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\n            raise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Manage model\n        if svm_params is None:\n            svm_params = {}\n        self.svm = SVC(**svm_params)\n        self.multiclass_strategy = multiclass_strategy\n\n        # Can't do multi-labels / multi-classes\n        if not self.multi_label:\n            # If not multi-classes : no impact\n            if multiclass_strategy == 'ovr':\n                self.pipeline = Pipeline([('svm', OneVsRestClassifier(self.svm))])\n            elif multiclass_strategy == 'ovo':\n                self.pipeline = Pipeline([('svm', OneVsOneClassifier(self.svm))])\n            else:\n                self.pipeline = Pipeline([('svm', self.svm)])\n\n        # SVC does not natively support multi-labels\n        if self.multi_label:\n            self.pipeline = Pipeline([('svm', MultiOutputClassifier(self.svm))])\n\n    @utils.trained_needed\n    def predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n        '''Predicts the probabilities on the test set\n        - /!\\\\ SVC CLASSIFIER DOESN'T RETURN PROBABILITIES, THE MODEL WILL GIVES 0 AND 1 AS PROBABILITIES /!\\\\ -\n            (in truth, you could use probability = True in the definition of the SVC but the probabilities are 'inconsistent' with the predictions)\n\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n        # We check input format\n        x_test, _ = self._check_input_format(x_test)\n        if not self.multi_label:\n            preds = self.pipeline.predict(x_test)\n            # Format ['a', 'b', 'c', 'a', ..., 'b']\n            # Transform to \"proba\"\n            transform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\n            probas = np.array([transform_dict[x] for x in preds])\n        else:\n            preds = self.pipeline.predict(x_test)\n            # Already right format, but in int !\n            probas = np.array([[float(_) for _ in x] for x in preds])\n        return probas\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save model\n        if json_data is None:\n            json_data = {}\n\n        json_data['multiclass_strategy'] = self.multiclass_strategy\n\n        # Save\n        super().save(json_data=json_data)\n\n    def reload_from_standalone(self, **kwargs) -&gt; None:\n        '''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n        # Retrieve args\n        configuration_path = kwargs.get('configuration_path', None)\n        sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n        preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n        # Checks\n        if configuration_path is None:\n            raise ValueError(\"The argument configuration_path can't be None\")\n        if sklearn_pipeline_path is None:\n            raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n        if preprocess_pipeline_path is None:\n            raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n        if not os.path.exists(configuration_path):\n            raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n        if not os.path.exists(sklearn_pipeline_path):\n            raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n        if not os.path.exists(preprocess_pipeline_path):\n            raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n        # Load confs\n        with open(configuration_path, 'r', encoding='utf-8') as f:\n            configs = json.load(f)\n        # Can't set int as keys in json, so need to cast it after reloading\n        # dict_classes keys are always ints\n        if 'dict_classes' in configs.keys():\n            configs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n        elif 'list_classes' in configs.keys():\n            configs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n\n        # Set class vars\n        # self.model_name = # Keep the created name\n        # self.model_dir = # Keep the created folder\n        self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n        self.trained = configs.get('trained', True)  # Consider trained by default\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                          'list_classes', 'dict_classes', 'multi_label', 'level_save',\n                          'multiclass_strategy']:\n            setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n        # Reload pipeline model\n        with open(sklearn_pipeline_path, 'rb') as f:\n            self.pipeline = pickle.load(f)\n\n        # Manage multi-labels or multi-classes\n        if not self.multi_label and self.multiclass_strategy is None:\n            self.svm = self.pipeline['svm']\n        else:\n            self.svm = self.pipeline['svm'].estimator\n\n        # Reload pipeline preprocessing\n        with open(preprocess_pipeline_path, 'rb') as f:\n            self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_svm_classifier/#template_num.models_training.classifiers.models_sklearn.model_svm_classifier.ModelSVMClassifier.__init__","title":"<code>__init__(svm_params=None, multiclass_strategy=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)</p> Kwargs <p>svm_params (dict) : Parameters for the Support Vector Machine multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.</p> <p>Raises:     If multiclass_strategy is not 'ovo', 'ovr' or None</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_svm_classifier.py</code> <pre><code>def __init__(self, svm_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)\n\n    Kwargs:\n        svm_params (dict) : Parameters for the Support Vector Machine\n        multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n    Raises:\n        If multiclass_strategy is not 'ovo', 'ovr' or None\n    '''\n    if multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\n        raise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Manage model\n    if svm_params is None:\n        svm_params = {}\n    self.svm = SVC(**svm_params)\n    self.multiclass_strategy = multiclass_strategy\n\n    # Can't do multi-labels / multi-classes\n    if not self.multi_label:\n        # If not multi-classes : no impact\n        if multiclass_strategy == 'ovr':\n            self.pipeline = Pipeline([('svm', OneVsRestClassifier(self.svm))])\n        elif multiclass_strategy == 'ovo':\n            self.pipeline = Pipeline([('svm', OneVsOneClassifier(self.svm))])\n        else:\n            self.pipeline = Pipeline([('svm', self.svm)])\n\n    # SVC does not natively support multi-labels\n    if self.multi_label:\n        self.pipeline = Pipeline([('svm', MultiOutputClassifier(self.svm))])\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_svm_classifier/#template_num.models_training.classifiers.models_sklearn.model_svm_classifier.ModelSVMClassifier.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set - /! SVC CLASSIFIER DOESN'T RETURN PROBABILITIES, THE MODEL WILL GIVES 0 AND 1 AS PROBABILITIES /! -     (in truth, you could use probability = True in the definition of the SVC but the probabilities are 'inconsistent' with the predictions)</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>DataFrame</code> <p>DataFrame with the test data to be predicted</p> required <p>Returns:     (np.ndarray): Array, shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_svm_classifier.py</code> <pre><code>@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n    '''Predicts the probabilities on the test set\n    - /!\\\\ SVC CLASSIFIER DOESN'T RETURN PROBABILITIES, THE MODEL WILL GIVES 0 AND 1 AS PROBABILITIES /!\\\\ -\n        (in truth, you could use probability = True in the definition of the SVC but the probabilities are 'inconsistent' with the predictions)\n\n    Args:\n        x_test (pd.DataFrame): DataFrame with the test data to be predicted\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n    # We check input format\n    x_test, _ = self._check_input_format(x_test)\n    if not self.multi_label:\n        preds = self.pipeline.predict(x_test)\n        # Format ['a', 'b', 'c', 'a', ..., 'b']\n        # Transform to \"proba\"\n        transform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\n        probas = np.array([transform_dict[x] for x in preds])\n    else:\n        preds = self.pipeline.predict(x_test)\n        # Already right format, but in int !\n        probas = np.array([[float(_) for _ in x] for x in preds])\n    return probas\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_svm_classifier/#template_num.models_training.classifiers.models_sklearn.model_svm_classifier.ModelSVMClassifier.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:     ValueError: If configuration_path is None     ValueError: If sklearn_pipeline_path is None     ValueError: If preprocess_pipeline_path is None     FileNotFoundError: If the object configuration_path is not an existing file     FileNotFoundError: If the object sklearn_pipeline_path is not an existing file     FileNotFoundError: If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_svm_classifier.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n    '''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n    # Retrieve args\n    configuration_path = kwargs.get('configuration_path', None)\n    sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n    preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n    # Checks\n    if configuration_path is None:\n        raise ValueError(\"The argument configuration_path can't be None\")\n    if sklearn_pipeline_path is None:\n        raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n    if preprocess_pipeline_path is None:\n        raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n    if not os.path.exists(configuration_path):\n        raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n    if not os.path.exists(sklearn_pipeline_path):\n        raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n    if not os.path.exists(preprocess_pipeline_path):\n        raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n    # Load confs\n    with open(configuration_path, 'r', encoding='utf-8') as f:\n        configs = json.load(f)\n    # Can't set int as keys in json, so need to cast it after reloading\n    # dict_classes keys are always ints\n    if 'dict_classes' in configs.keys():\n        configs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n    elif 'list_classes' in configs.keys():\n        configs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n\n    # Set class vars\n    # self.model_name = # Keep the created name\n    # self.model_dir = # Keep the created folder\n    self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n    self.trained = configs.get('trained', True)  # Consider trained by default\n    # Try to read the following attributes from configs and, if absent, keep the current one\n    for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                      'list_classes', 'dict_classes', 'multi_label', 'level_save',\n                      'multiclass_strategy']:\n        setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n    # Reload pipeline model\n    with open(sklearn_pipeline_path, 'rb') as f:\n        self.pipeline = pickle.load(f)\n\n    # Manage multi-labels or multi-classes\n    if not self.multi_label and self.multiclass_strategy is None:\n        self.svm = self.pipeline['svm']\n    else:\n        self.svm = self.pipeline['svm'].estimator\n\n    # Reload pipeline preprocessing\n    with open(preprocess_pipeline_path, 'rb') as f:\n        self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_svm_classifier/#template_num.models_training.classifiers.models_sklearn.model_svm_classifier.ModelSVMClassifier.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_svm_classifier.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save model\n    if json_data is None:\n        json_data = {}\n\n    json_data['multiclass_strategy'] = self.multiclass_strategy\n\n    # Save\n    super().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_tensorflow/","title":"Models tensorflow","text":""},{"location":"reference/template_num/models_training/classifiers/models_tensorflow/model_dense_classifier/","title":"Model dense classifier","text":""},{"location":"reference/template_num/models_training/classifiers/models_tensorflow/model_dense_classifier/#template_num.models_training.classifiers.models_tensorflow.model_dense_classifier.ModelDenseClassifier","title":"<code>ModelDenseClassifier</code>","text":"<p>             Bases: <code>ModelClassifierMixin</code>, <code>ModelKeras</code></p> <p>Dense model for classification</p> Source code in <code>template_num/models_training/classifiers/models_tensorflow/model_dense_classifier.py</code> <pre><code>class ModelDenseClassifier(ModelClassifierMixin, ModelKeras):\n    '''Dense model for classification'''\n\n    _default_name = 'model_dense_classifier'\n\n    def __init__(self, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelClass, ModelKeras &amp; ModelClassifierMixin for more arguments)'''\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n    def _get_model(self) -&gt; Model:\n        '''Gets a model structure - returns the instance model instead if already defined\n\n        Returns:\n            (Model): a Keras model\n        '''\n        # Return model if already set\n        if self.model is not None:\n            return self.model\n\n        # Get input/output dimensions\n        input_dim = len(self.x_col)\n        num_classes = len(self.list_classes)\n\n        # Get random_state\n        random_state = np.random.RandomState(self.random_seed)\n        limit = int(1e9)\n\n        # Process\n        input_layer = Input(shape=(input_dim,))\n\n        x = Dense(64, activation=None, kernel_initializer=HeUniform(random_state.randint(limit)))(input_layer)\n        x = BatchNormalization(momentum=0.9)(x)\n        x = ELU(alpha=1.0)(x)\n        x = Dropout(0.2, seed=random_state.randint(limit))(x)\n\n        x = Dense(64, activation=None, kernel_initializer=HeUniform(random_state.randint(limit)))(x)\n        x = BatchNormalization(momentum=0.9)(x)\n        x = ELU(alpha=1.0)(x)\n        x = Dropout(0.2, seed=random_state.randint(limit))(x)\n\n        # Last layer\n        activation = 'sigmoid' if self.multi_label else 'softmax'\n        out = Dense(num_classes, activation=activation, kernel_initializer=GlorotUniform(random_state.randint(limit)))(x)\n\n        # Set model\n        model = Model(inputs=input_layer, outputs=[out])\n\n        # Set optimizer\n        lr = self.keras_params['learning_rate'] if 'learning_rate' in self.keras_params.keys() else 0.001\n        decay = self.keras_params['decay'] if 'decay' in self.keras_params.keys() else 0.0\n        self.logger.info(f\"Learning rate: {lr}\")\n        self.logger.info(f\"Decay: {decay}\")\n        optimizer = Adam(lr=lr, decay=decay)\n\n        # Set loss &amp; metrics\n        loss = utils_deep_keras.f1_loss if self.multi_label else 'categorical_crossentropy'\n        metrics: List[Union[str, Callable]] = ['accuracy'] if not self.multi_label else ['categorical_accuracy', 'categorical_crossentropy', utils_deep_keras.f1, utils_deep_keras.precision, utils_deep_keras.recall, utils_deep_keras.f1_loss]  # type: ignore\n\n        # Compile model\n        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n        if self.logger.getEffectiveLevel() &lt; logging.ERROR:\n            model.summary()\n\n        # Try to save model as png if level_save &gt; 'LOW'\n        if self.level_save in ['MEDIUM', 'HIGH']:\n            self._save_model_png(model)\n\n        # Return\n        return model\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save configuration JSON\n        if json_data is None:\n            json_data = {}\n\n        # Save\n        super().save(json_data=json_data)\n\n    def reload_from_standalone(self, **kwargs) -&gt; None:\n        '''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            hdf5_path (str): Path to hdf5 file\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If hdf5_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object hdf5_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n        # Retrieve args\n        configuration_path = kwargs.get('configuration_path', None)\n        hdf5_path = kwargs.get('hdf5_path', None)\n        preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n        # Checks\n        if configuration_path is None:\n            raise ValueError(\"The argument configuration_path can't be None\")\n        if hdf5_path is None:\n            raise ValueError(\"The argument hdf5_path can't be None\")\n        if preprocess_pipeline_path is None:\n            raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n        if not os.path.exists(configuration_path):\n            raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n        if not os.path.exists(hdf5_path):\n            raise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\n        if not os.path.exists(preprocess_pipeline_path):\n            raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n        # Load confs\n        with open(configuration_path, 'r', encoding='utf-8') as f:\n            configs = json.load(f)\n        # Can't set int as keys in json, so need to cast it after reloading\n        # dict_classes keys are always ints\n        if 'dict_classes' in configs.keys():\n            configs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n        elif 'list_classes' in configs.keys():\n            configs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n\n        # Set class vars\n        # self.model_name = # Keep the created name\n        # self.model_dir = # Keep the created folder\n        self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n        self.trained = configs.get('trained', True)  # Consider trained by default\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                          'list_classes', 'dict_classes', 'multi_label', 'random_seed', 'level_save',\n                          'batch_size', 'epochs', 'validation_split', 'patience', 'keras_params']:\n            setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n        # Reload model\n        self.model = load_model_keras(hdf5_path, custom_objects=self.custom_objects)\n\n        # Save best hdf5 in new folder\n        new_hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\n        shutil.copyfile(hdf5_path, new_hdf5_path)\n\n        # Reload pipeline preprocessing\n        with open(preprocess_pipeline_path, 'rb') as f:\n            self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_tensorflow/model_dense_classifier/#template_num.models_training.classifiers.models_tensorflow.model_dense_classifier.ModelDenseClassifier.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialization of the class (see ModelClass, ModelKeras &amp; ModelClassifierMixin for more arguments)</p> Source code in <code>template_num/models_training/classifiers/models_tensorflow/model_dense_classifier.py</code> <pre><code>def __init__(self, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelClass, ModelKeras &amp; ModelClassifierMixin for more arguments)'''\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_tensorflow/model_dense_classifier/#template_num.models_training.classifiers.models_tensorflow.model_dense_classifier.ModelDenseClassifier.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file hdf5_path (str): Path to hdf5 file preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:     ValueError: If configuration_path is None     ValueError: If hdf5_path is None     ValueError: If preprocess_pipeline_path is None     FileNotFoundError: If the object configuration_path is not an existing file     FileNotFoundError: If the object hdf5_path is not an existing file     FileNotFoundError: If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/classifiers/models_tensorflow/model_dense_classifier.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n    '''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        hdf5_path (str): Path to hdf5 file\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If hdf5_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object hdf5_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n    # Retrieve args\n    configuration_path = kwargs.get('configuration_path', None)\n    hdf5_path = kwargs.get('hdf5_path', None)\n    preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n    # Checks\n    if configuration_path is None:\n        raise ValueError(\"The argument configuration_path can't be None\")\n    if hdf5_path is None:\n        raise ValueError(\"The argument hdf5_path can't be None\")\n    if preprocess_pipeline_path is None:\n        raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n    if not os.path.exists(configuration_path):\n        raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n    if not os.path.exists(hdf5_path):\n        raise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\n    if not os.path.exists(preprocess_pipeline_path):\n        raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n    # Load confs\n    with open(configuration_path, 'r', encoding='utf-8') as f:\n        configs = json.load(f)\n    # Can't set int as keys in json, so need to cast it after reloading\n    # dict_classes keys are always ints\n    if 'dict_classes' in configs.keys():\n        configs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n    elif 'list_classes' in configs.keys():\n        configs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n\n    # Set class vars\n    # self.model_name = # Keep the created name\n    # self.model_dir = # Keep the created folder\n    self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n    self.trained = configs.get('trained', True)  # Consider trained by default\n    # Try to read the following attributes from configs and, if absent, keep the current one\n    for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                      'list_classes', 'dict_classes', 'multi_label', 'random_seed', 'level_save',\n                      'batch_size', 'epochs', 'validation_split', 'patience', 'keras_params']:\n        setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n    # Reload model\n    self.model = load_model_keras(hdf5_path, custom_objects=self.custom_objects)\n\n    # Save best hdf5 in new folder\n    new_hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\n    shutil.copyfile(hdf5_path, new_hdf5_path)\n\n    # Reload pipeline preprocessing\n    with open(preprocess_pipeline_path, 'rb') as f:\n        self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_tensorflow/model_dense_classifier/#template_num.models_training.classifiers.models_tensorflow.model_dense_classifier.ModelDenseClassifier.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/classifiers/models_tensorflow/model_dense_classifier.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save configuration JSON\n    if json_data is None:\n        json_data = {}\n\n    # Save\n    super().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/","title":"Regressors","text":""},{"location":"reference/template_num/models_training/regressors/model_aggregation_regressor/","title":"Model aggregation regressor","text":""},{"location":"reference/template_num/models_training/regressors/model_aggregation_regressor/#template_num.models_training.regressors.model_aggregation_regressor.ModelAggregationRegressor","title":"<code>ModelAggregationRegressor</code>","text":"<p>             Bases: <code>ModelRegressorMixin</code>, <code>ModelClass</code></p> <p>Model for aggregating several regressor models</p> Source code in <code>template_num/models_training/regressors/model_aggregation_regressor.py</code> <pre><code>class ModelAggregationRegressor(ModelRegressorMixin, ModelClass):\n    '''Model for aggregating several regressor models'''\n    _default_name = 'model_aggregation_regressor'\n\n    _dict_aggregation_function = {'median_predict': median_predict,\n                                  'mean_predict': mean_predict}\n\n    def __init__(self, list_models: Union[list, None] = None, aggregation_function: Union[Callable, str] = 'median_predict', **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelClass for more arguments)\n\n        Kwargs:\n            list_models (list) : The list of model to be aggregated\n            aggregation_function (Callable or str) : The aggregation function used\n        Raises:\n            ValueError : If the object list_model has other model than model regressor (model_aggregation_regressor is only compatible with model regressor)\n            ValueError : If the object aggregation_function is a str but not found in the dictionary dict_aggregation_function\n        '''\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Get the aggregation function\n\n        if isinstance(aggregation_function, str):\n            if aggregation_function not in self._dict_aggregation_function.keys():\n                raise ValueError(f\"The aggregation_function ({aggregation_function}) is not a valid option ({self._dict_aggregation_function.keys()})\")\n            aggregation_function = self._dict_aggregation_function[aggregation_function] # type: ignore\n\n        # Manage aggregated models\n        self.aggregation_function = aggregation_function\n\n        self.sub_models = list_models # Transform the list into a list of dictionnaries [{'name': xxx, 'model': xxx}, ...]\n\n        # Error: The classifier and regressor models cannot be combined in list_models\n        if False in [isinstance(sub_model['model'], ModelRegressorMixin) for sub_model in self.sub_models]:\n            raise ValueError(f\"model_aggregation_regressor only accepts regressor models\")\n\n        self.trained = self._check_trained()\n\n        # Set nb_fit to 1 if already trained\n        if self.trained:\n            self.nb_fit = 1\n\n    @property\n    def aggregation_function(self):\n        '''Getter for aggregation_function'''\n        return self._aggregation_function\n\n    @aggregation_function.setter\n    def aggregation_function(self, agg_function: Union[Callable, str]):\n        '''Setter for aggregation_function\n        If a string, try to match a predefined function\n\n        Raises:\n            ValueError: If the object aggregation_function is a str but not found in the dictionary of predefined aggregation functions\n        '''\n        # Retrieve aggregation function from dict if a string\n        if isinstance(agg_function, str):\n            # Get infos\n            if agg_function not in self._dict_aggregation_function.keys():\n                raise ValueError(f\"The aggregation_function ({agg_function}) is not a valid option (must be chosen in {self._dict_aggregation_function.keys()})\")\n            agg_function = self._dict_aggregation_function[agg_function]\n            # Apply checks\n        self._aggregation_function = agg_function\n\n    @aggregation_function.deleter\n    def aggregation_function(self):\n        '''Deleter for aggregation_function'''\n        self._aggregation_function = None\n\n    @property\n    def sub_models(self):\n        '''Getter for sub_models'''\n        return self._sub_models\n\n    @sub_models.setter\n    def sub_models(self, list_models: Union[list, None] = None):\n        '''Setter for sub_models\n\n        Kwargs:\n            list_models (list) : The list of models to be aggregated\n        '''\n        list_models = [] if list_models is None else list_models\n        sub_models = []  # Init list of models\n        for model in list_models:\n            # If a string (a model name), reload it\n            if isinstance(model, str):\n                real_model, _ = utils_models.load_model(model)\n                dict_model = {'name': model, 'model': real_model}\n            else:\n                dict_model = {'name': os.path.split(model.model_dir)[-1], 'model': model}\n            sub_models.append(dict_model.copy())\n        self._sub_models = sub_models.copy()\n\n    @sub_models.deleter\n    def sub_models(self):\n        '''Deleter for sub_models'''\n        self._sub_models = None\n\n    def _check_trained(self) -&gt; bool:\n        '''Checks various attributes related to the fitting of underlying models\n\n        Returns:\n            bool: is the aggregation model is considered fitted\n        '''\n        # Check fitted\n        models_trained = {sub_model['model'].trained for sub_model in self.sub_models}\n        if len(models_trained) &gt; 0 and all(models_trained):\n            # All models trained\n            trained = True\n        # No model or not fitted\n        else:\n            trained = False\n        return trained\n\n    def fit(self, x_train, y_train, **kwargs) -&gt; None:\n        '''Trains the model\n           **kwargs enables Keras model compatibility.\n\n        Args:\n            x_train (?): Array-like, shape = [n_samples]\n            y_train (?): Array-like, shape = [n_samples]\n        '''\n        # We check input format\n        x_train, y_train = self._check_input_format(x_train, y_train, fit_function=True)\n\n        # Fit each model\n        for sub_model in self.sub_models:\n            if not sub_model['model'].trained:\n                sub_model['model'].fit(x_train, y_train, **kwargs)\n\n        # Set nb_fit to 1 if not already trained\n        if not self.trained:\n            self.nb_fit = 1\n\n        self.trained = self._check_trained()\n\n    @utils.trained_needed\n    def predict(self, x_test, return_proba: bool = False, alternative_version: bool = False, **kwargs) -&gt; np.ndarray:\n        '''Prediction\n\n        Args:\n            x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n            return_proba (bool): If the function should return the probabilities instead of the classes (Keras compatibility)\n        Kwargs:\n            alternative_version (bool): If an alternative version (`tf.function` + `model.__call__`) must be used for Keras models. Should be faster with low nb of inputs.\n        Returns:\n            (np.ndarray): Array of shape = [n_samples]\n        Raises:\n            ValueError: If return_proba=True\n        '''\n        if return_proba:\n            raise ValueError(f\"Models of the type {self.model_type} can't handle probabilities\")\n        preds = self._predict_sub_models(x_test, alternative_version=alternative_version, **kwargs)\n        return np.array([self.aggregation_function(array) for array in preds]) # type: ignore\n\n    @utils.trained_needed\n    def predict_proba(self, x_test, **kwargs) -&gt; None:\n        '''Predicts the probabilities on the test set - raise ValueError\n\n        Args:\n            x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n        Raises:\n            ValueError: Models of type regressor do not implement the method predict_proba\n        '''\n        raise ValueError(f\"Models of type regressor do not implement the method predict_proba\")\n\n    @utils.trained_needed\n    def _predict_sub_models(self, x_test, alternative_version: bool = False, **kwargs) -&gt; np.ndarray:\n        '''Recover the predictions of each model being aggregated\n\n        Args:\n            x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n        Kwargs:\n            alternative_version (bool): If an alternative version (`tf.function` + `model.__call__`) must be used for Keras models. Should be faster with low nb of inputs.\n        Returns:\n            (np.ndarray): array of shape = [n_samples, nb_model]\n        '''\n        array_predict = np.array([sub_model['model'].predict(x_test, alternative_version=alternative_version) for sub_model in self.sub_models])\n        array_predict = np.transpose(array_predict, (1, 0))\n        return array_predict\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        if json_data is None:\n            json_data = {}\n\n        # Specific aggregation - save some wanted entries\n        train_keys = ['filename', 'filename_valid', 'preprocess_str']\n        default_json_data = {key: json_data.get(key, None) for key in train_keys}\n        default_json_data['aggregator_dir'] = self.model_dir\n        # Save each trained and unsaved model\n        for sub_model in self.sub_models:\n            path_config = os.path.join(sub_model['model'].model_dir, 'configurations.json')\n            if os.path.exists(path_config):\n                with open(path_config, 'r', encoding='utf-8') as f:\n                    configs = json.load(f)\n                    trained = configs.get('trained', False)\n                    if not trained:\n                        sub_model['model'].save(default_json_data)\n            else:\n                sub_model['model'].save(default_json_data)\n\n        json_data['list_models_name'] = [sub_model['name'] for sub_model in self.sub_models]\n\n        aggregation_function = self.aggregation_function\n\n        # Save aggregation_function if not None &amp; level_save &gt; LOW\n        if (self.aggregation_function is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n            # Manage paths\n            aggregation_function_path = os.path.join(self.model_dir, \"aggregation_function.pkl\")\n            # Save as pickle\n            with open(aggregation_function_path, 'wb') as f:\n                pickle.dump(self.aggregation_function, f)\n\n        # Save\n        models_list = [sub_model['name'] for sub_model in self.sub_models]\n        delattr(self, \"sub_models\")\n        delattr(self, \"aggregation_function\")\n        super().save(json_data=json_data)\n        setattr(self, \"aggregation_function\", aggregation_function)\n        setattr(self, \"sub_models\", models_list)\n\n        # Add message in model_upload_instructions.md\n        md_path = os.path.join(self.model_dir, f\"model_upload_instructions.md\")\n        line = \"/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\   The aggregation model is a special model, please ensure that all sub-models and the aggregation model are manually saved together in order to be able to load it  /!\\\\/!\\\\/!\\\\/!\\\\/!\\\\ \\n\"\n        self.prepend_line(md_path, line)\n\n    def prepend_line(self, file_name: str, line: str) -&gt; None:\n        ''' Insert given string as a new line at the beginning of a file\n\n        Kwargs:\n            file_name (str): Path to file\n            line (str): line to insert\n        '''\n        with open(file_name, 'r+') as f:\n            lines = f.readlines()\n            lines.insert(0, line)\n            f.seek(0)\n            f.writelines(lines)\n\n    def reload_from_standalone(self, **kwargs) -&gt; None:\n        '''Reloads a model aggregation from its configuration and \"standalones\" files\n            Reloads list model from \"list_models\" files\n\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n            aggregation_function_path (str): Path to aggregation_function_path\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If preprocess_pipeline_path is None\n            ValueError: If aggregation_function_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n            FileNotFoundError: If the object aggregation_function_path is not an existing file\n        '''\n        # Retrieve args\n        configuration_path = kwargs.get('configuration_path', None)\n        preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n        aggregation_function_path = kwargs.get('aggregation_function_path', None)\n\n        # Checks\n        if configuration_path is None:\n            raise ValueError(\"The argument configuration_path can't be None\")\n        if preprocess_pipeline_path is None:\n            raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n        if aggregation_function_path is None:\n            raise ValueError(\"The argument aggregation_function_path can't be None\")\n        if not os.path.exists(configuration_path):\n            raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n        if not os.path.exists(preprocess_pipeline_path):\n            raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n        if not os.path.exists(aggregation_function_path):\n            raise FileNotFoundError(f\"The file {aggregation_function_path} does not exist\")\n\n        # Load confs\n        with open(configuration_path, 'r', encoding='utf-8') as f:\n            configs = json.load(f)\n\n        # Reload pipeline preprocessing\n        with open(preprocess_pipeline_path, 'rb') as f:\n            self.preprocess_pipeline = pickle.load(f)\n\n        # Reload aggregation_function_path\n        with open(aggregation_function_path, 'rb') as f:\n            self.aggregation_function = pickle.load(f)\n\n        # Set class vars\n        # self.model_name = # Keep the created name\n        # self.model_dir = # Keep the created folder\n        self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n        self.trained = configs.get('trained', True)  # Consider trained by default\n        self.sub_models = configs.get('list_models_name', [])  # Transform the list into a list of dictionnaries [{'name': xxx, 'model': xxx}, ...]\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['x_col', 'y_col', 'level_save']:\n            setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_aggregation_regressor/#template_num.models_training.regressors.model_aggregation_regressor.ModelAggregationRegressor.aggregation_function","title":"<code>aggregation_function</code>  <code>deletable</code> <code>property</code> <code>writable</code>","text":"<p>Getter for aggregation_function</p>"},{"location":"reference/template_num/models_training/regressors/model_aggregation_regressor/#template_num.models_training.regressors.model_aggregation_regressor.ModelAggregationRegressor.sub_models","title":"<code>sub_models</code>  <code>deletable</code> <code>property</code> <code>writable</code>","text":"<p>Getter for sub_models</p>"},{"location":"reference/template_num/models_training/regressors/model_aggregation_regressor/#template_num.models_training.regressors.model_aggregation_regressor.ModelAggregationRegressor.__init__","title":"<code>__init__(list_models=None, aggregation_function='median_predict', **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass for more arguments)</p> Kwargs <p>list_models (list) : The list of model to be aggregated aggregation_function (Callable or str) : The aggregation function used</p> <p>Raises:     ValueError : If the object list_model has other model than model regressor (model_aggregation_regressor is only compatible with model regressor)     ValueError : If the object aggregation_function is a str but not found in the dictionary dict_aggregation_function</p> Source code in <code>template_num/models_training/regressors/model_aggregation_regressor.py</code> <pre><code>def __init__(self, list_models: Union[list, None] = None, aggregation_function: Union[Callable, str] = 'median_predict', **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelClass for more arguments)\n\n    Kwargs:\n        list_models (list) : The list of model to be aggregated\n        aggregation_function (Callable or str) : The aggregation function used\n    Raises:\n        ValueError : If the object list_model has other model than model regressor (model_aggregation_regressor is only compatible with model regressor)\n        ValueError : If the object aggregation_function is a str but not found in the dictionary dict_aggregation_function\n    '''\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Get the aggregation function\n\n    if isinstance(aggregation_function, str):\n        if aggregation_function not in self._dict_aggregation_function.keys():\n            raise ValueError(f\"The aggregation_function ({aggregation_function}) is not a valid option ({self._dict_aggregation_function.keys()})\")\n        aggregation_function = self._dict_aggregation_function[aggregation_function] # type: ignore\n\n    # Manage aggregated models\n    self.aggregation_function = aggregation_function\n\n    self.sub_models = list_models # Transform the list into a list of dictionnaries [{'name': xxx, 'model': xxx}, ...]\n\n    # Error: The classifier and regressor models cannot be combined in list_models\n    if False in [isinstance(sub_model['model'], ModelRegressorMixin) for sub_model in self.sub_models]:\n        raise ValueError(f\"model_aggregation_regressor only accepts regressor models\")\n\n    self.trained = self._check_trained()\n\n    # Set nb_fit to 1 if already trained\n    if self.trained:\n        self.nb_fit = 1\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_aggregation_regressor/#template_num.models_training.regressors.model_aggregation_regressor.ModelAggregationRegressor.fit","title":"<code>fit(x_train, y_train, **kwargs)</code>","text":"<p>Trains the model    **kwargs enables Keras model compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>x_train</code> <code>?</code> <p>Array-like, shape = [n_samples]</p> required <code>y_train</code> <code>?</code> <p>Array-like, shape = [n_samples]</p> required Source code in <code>template_num/models_training/regressors/model_aggregation_regressor.py</code> <pre><code>def fit(self, x_train, y_train, **kwargs) -&gt; None:\n    '''Trains the model\n       **kwargs enables Keras model compatibility.\n\n    Args:\n        x_train (?): Array-like, shape = [n_samples]\n        y_train (?): Array-like, shape = [n_samples]\n    '''\n    # We check input format\n    x_train, y_train = self._check_input_format(x_train, y_train, fit_function=True)\n\n    # Fit each model\n    for sub_model in self.sub_models:\n        if not sub_model['model'].trained:\n            sub_model['model'].fit(x_train, y_train, **kwargs)\n\n    # Set nb_fit to 1 if not already trained\n    if not self.trained:\n        self.nb_fit = 1\n\n    self.trained = self._check_trained()\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_aggregation_regressor/#template_num.models_training.regressors.model_aggregation_regressor.ModelAggregationRegressor.predict","title":"<code>predict(x_test, return_proba=False, alternative_version=False, **kwargs)</code>","text":"<p>Prediction</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>array-like or sparse matrix of shape = [n_samples, n_features]</p> required <code>return_proba</code> <code>bool</code> <p>If the function should return the probabilities instead of the classes (Keras compatibility)</p> <code>False</code> <p>Kwargs:     alternative_version (bool): If an alternative version (<code>tf.function</code> + <code>model.__call__</code>) must be used for Keras models. Should be faster with low nb of inputs. Returns:     (np.ndarray): Array of shape = [n_samples] Raises:     ValueError: If return_proba=True</p> Source code in <code>template_num/models_training/regressors/model_aggregation_regressor.py</code> <pre><code>@utils.trained_needed\ndef predict(self, x_test, return_proba: bool = False, alternative_version: bool = False, **kwargs) -&gt; np.ndarray:\n    '''Prediction\n\n    Args:\n        x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n        return_proba (bool): If the function should return the probabilities instead of the classes (Keras compatibility)\n    Kwargs:\n        alternative_version (bool): If an alternative version (`tf.function` + `model.__call__`) must be used for Keras models. Should be faster with low nb of inputs.\n    Returns:\n        (np.ndarray): Array of shape = [n_samples]\n    Raises:\n        ValueError: If return_proba=True\n    '''\n    if return_proba:\n        raise ValueError(f\"Models of the type {self.model_type} can't handle probabilities\")\n    preds = self._predict_sub_models(x_test, alternative_version=alternative_version, **kwargs)\n    return np.array([self.aggregation_function(array) for array in preds]) # type: ignore\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_aggregation_regressor/#template_num.models_training.regressors.model_aggregation_regressor.ModelAggregationRegressor.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set - raise ValueError</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>array-like or sparse matrix of shape = [n_samples, n_features]</p> required <p>Raises:     ValueError: Models of type regressor do not implement the method predict_proba</p> Source code in <code>template_num/models_training/regressors/model_aggregation_regressor.py</code> <pre><code>@utils.trained_needed\ndef predict_proba(self, x_test, **kwargs) -&gt; None:\n    '''Predicts the probabilities on the test set - raise ValueError\n\n    Args:\n        x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n    Raises:\n        ValueError: Models of type regressor do not implement the method predict_proba\n    '''\n    raise ValueError(f\"Models of type regressor do not implement the method predict_proba\")\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_aggregation_regressor/#template_num.models_training.regressors.model_aggregation_regressor.ModelAggregationRegressor.prepend_line","title":"<code>prepend_line(file_name, line)</code>","text":"<p>Insert given string as a new line at the beginning of a file</p> Kwargs <p>file_name (str): Path to file line (str): line to insert</p> Source code in <code>template_num/models_training/regressors/model_aggregation_regressor.py</code> <pre><code>def prepend_line(self, file_name: str, line: str) -&gt; None:\n    ''' Insert given string as a new line at the beginning of a file\n\n    Kwargs:\n        file_name (str): Path to file\n        line (str): line to insert\n    '''\n    with open(file_name, 'r+') as f:\n        lines = f.readlines()\n        lines.insert(0, line)\n        f.seek(0)\n        f.writelines(lines)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_aggregation_regressor/#template_num.models_training.regressors.model_aggregation_regressor.ModelAggregationRegressor.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model aggregation from its configuration and \"standalones\" files     Reloads list model from \"list_models\" files</p> Kwargs <p>configuration_path (str): Path to configuration file preprocess_pipeline_path (str): Path to preprocess pipeline aggregation_function_path (str): Path to aggregation_function_path</p> <p>Raises:     ValueError: If configuration_path is None     ValueError: If preprocess_pipeline_path is None     ValueError: If aggregation_function_path is None     FileNotFoundError: If the object configuration_path is not an existing file     FileNotFoundError: If the object preprocess_pipeline_path is not an existing file     FileNotFoundError: If the object aggregation_function_path is not an existing file</p> Source code in <code>template_num/models_training/regressors/model_aggregation_regressor.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n    '''Reloads a model aggregation from its configuration and \"standalones\" files\n        Reloads list model from \"list_models\" files\n\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n        aggregation_function_path (str): Path to aggregation_function_path\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If preprocess_pipeline_path is None\n        ValueError: If aggregation_function_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        FileNotFoundError: If the object aggregation_function_path is not an existing file\n    '''\n    # Retrieve args\n    configuration_path = kwargs.get('configuration_path', None)\n    preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n    aggregation_function_path = kwargs.get('aggregation_function_path', None)\n\n    # Checks\n    if configuration_path is None:\n        raise ValueError(\"The argument configuration_path can't be None\")\n    if preprocess_pipeline_path is None:\n        raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n    if aggregation_function_path is None:\n        raise ValueError(\"The argument aggregation_function_path can't be None\")\n    if not os.path.exists(configuration_path):\n        raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n    if not os.path.exists(preprocess_pipeline_path):\n        raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n    if not os.path.exists(aggregation_function_path):\n        raise FileNotFoundError(f\"The file {aggregation_function_path} does not exist\")\n\n    # Load confs\n    with open(configuration_path, 'r', encoding='utf-8') as f:\n        configs = json.load(f)\n\n    # Reload pipeline preprocessing\n    with open(preprocess_pipeline_path, 'rb') as f:\n        self.preprocess_pipeline = pickle.load(f)\n\n    # Reload aggregation_function_path\n    with open(aggregation_function_path, 'rb') as f:\n        self.aggregation_function = pickle.load(f)\n\n    # Set class vars\n    # self.model_name = # Keep the created name\n    # self.model_dir = # Keep the created folder\n    self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n    self.trained = configs.get('trained', True)  # Consider trained by default\n    self.sub_models = configs.get('list_models_name', [])  # Transform the list into a list of dictionnaries [{'name': xxx, 'model': xxx}, ...]\n    # Try to read the following attributes from configs and, if absent, keep the current one\n    for attribute in ['x_col', 'y_col', 'level_save']:\n        setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_aggregation_regressor/#template_num.models_training.regressors.model_aggregation_regressor.ModelAggregationRegressor.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/regressors/model_aggregation_regressor.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    if json_data is None:\n        json_data = {}\n\n    # Specific aggregation - save some wanted entries\n    train_keys = ['filename', 'filename_valid', 'preprocess_str']\n    default_json_data = {key: json_data.get(key, None) for key in train_keys}\n    default_json_data['aggregator_dir'] = self.model_dir\n    # Save each trained and unsaved model\n    for sub_model in self.sub_models:\n        path_config = os.path.join(sub_model['model'].model_dir, 'configurations.json')\n        if os.path.exists(path_config):\n            with open(path_config, 'r', encoding='utf-8') as f:\n                configs = json.load(f)\n                trained = configs.get('trained', False)\n                if not trained:\n                    sub_model['model'].save(default_json_data)\n        else:\n            sub_model['model'].save(default_json_data)\n\n    json_data['list_models_name'] = [sub_model['name'] for sub_model in self.sub_models]\n\n    aggregation_function = self.aggregation_function\n\n    # Save aggregation_function if not None &amp; level_save &gt; LOW\n    if (self.aggregation_function is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n        # Manage paths\n        aggregation_function_path = os.path.join(self.model_dir, \"aggregation_function.pkl\")\n        # Save as pickle\n        with open(aggregation_function_path, 'wb') as f:\n            pickle.dump(self.aggregation_function, f)\n\n    # Save\n    models_list = [sub_model['name'] for sub_model in self.sub_models]\n    delattr(self, \"sub_models\")\n    delattr(self, \"aggregation_function\")\n    super().save(json_data=json_data)\n    setattr(self, \"aggregation_function\", aggregation_function)\n    setattr(self, \"sub_models\", models_list)\n\n    # Add message in model_upload_instructions.md\n    md_path = os.path.join(self.model_dir, f\"model_upload_instructions.md\")\n    line = \"/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\   The aggregation model is a special model, please ensure that all sub-models and the aggregation model are manually saved together in order to be able to load it  /!\\\\/!\\\\/!\\\\/!\\\\/!\\\\ \\n\"\n    self.prepend_line(md_path, line)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_aggregation_regressor/#template_num.models_training.regressors.model_aggregation_regressor.mean_predict","title":"<code>mean_predict(predictions)</code>","text":"<p>Returns the mean of predictions of each model</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>np.ndarray) </code> <p>The array containing the predictions of each models (shape (n_models))</p> required <p>Return:     (np.float64) : The mean of the predictions</p> Source code in <code>template_num/models_training/regressors/model_aggregation_regressor.py</code> <pre><code>def mean_predict(predictions: np.ndarray) -&gt; np.float64:\n    '''Returns the mean of predictions of each model\n\n    Args:\n        predictions (np.ndarray) : The array containing the predictions of each models (shape (n_models))\n    Return:\n        (np.float64) : The mean of the predictions\n    '''\n    return np.mean(predictions)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_aggregation_regressor/#template_num.models_training.regressors.model_aggregation_regressor.median_predict","title":"<code>median_predict(predictions)</code>","text":"<p>Returns the median of the predictions of each model</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>np.ndarray) </code> <p>The array containing the predictions of each models (shape (n_models))</p> required <p>Return:     (np.float64) : The median of the predictions</p> Source code in <code>template_num/models_training/regressors/model_aggregation_regressor.py</code> <pre><code>def median_predict(predictions: np.ndarray) -&gt; np.float64:\n    '''Returns the median of the predictions of each model\n\n    Args:\n        predictions (np.ndarray) : The array containing the predictions of each models (shape (n_models))\n    Return:\n        (np.float64) : The median of the predictions\n    '''\n    return np.median(predictions)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_regressor/","title":"Model regressor","text":""},{"location":"reference/template_num/models_training/regressors/model_regressor/#template_num.models_training.regressors.model_regressor.ModelRegressorMixin","title":"<code>ModelRegressorMixin</code>","text":"<p>Parent class (Mixin) for regressor models</p> Source code in <code>template_num/models_training/regressors/model_regressor.py</code> <pre><code>class ModelRegressorMixin:\n    '''Parent class (Mixin) for regressor models'''\n\n    def __init__(self, level_save: str = 'HIGH', **kwargs) -&gt; None:\n        '''Initialization of the class\n\n        Kwargs:\n            level_save (str): Level of saving\n                LOW: stats + configurations + logger keras - /!\\\\ The model can't be reused /!\\\\ -\n                MEDIUM: LOW + hdf5 + pkl + plots\n                HIGH: MEDIUM + predictions\n        Raises:\n             ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\n        '''\n        super().__init__(level_save=level_save, **kwargs)  # forwards level_save &amp; all unused arguments\n\n        if level_save not in ['LOW', 'MEDIUM', 'HIGH']:\n            raise ValueError(f\"The object level_save ({level_save}) is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\")\n\n        # Get logger\n        self.logger = logging.getLogger(__name__)\n\n        # Model type\n        self.model_type = 'regressor'\n\n        # TODO: add multi-outputs !\n\n        # Other options\n        self.level_save = level_save\n\n    def inverse_transform(self, y: Union[list, np.ndarray]) -&gt; Union[list, tuple]:\n        '''Identity function - Manages compatibility with classifiers\n\n        Args:\n            y (list | np.ndarray): Array-like, shape = [n_samples, 1]\n        Returns:\n            (np.ndarray): List, shape = [n_samples, 1]\n        '''\n        return list(y) if isinstance(y, np.ndarray) else y\n\n    def get_and_save_metrics(self, y_true, y_pred, df_x: Union[pd.DataFrame, None] = None,\n                             series_to_add: Union[List[pd.Series], None] = None,\n                             type_data: str = '') -&gt; pd.DataFrame:\n        '''Gets and saves the metrics of a model\n\n        Args:\n            y_true (?): Array-like, shape = [n_samples,]\n            y_pred (?): Array-like, shape = [n_samples,]\n        Kwargs:\n            df_x (pd.DataFrame or None): Input dataFrame used for the prediction\n            series_to_add (list&lt;pd.Series&gt;): List of pd.Series to add to the dataframe\n            type_data (str): Type of dataset (validation, test, ...)\n        Returns:\n            pd.DataFrame: The dataframe containing the statistics\n        '''\n\n        # Cast to np.array\n        y_true = np.array(y_true)\n        y_pred = np.array(y_pred)\n\n        # Save a predictionn file if wanted\n        if self.level_save == 'HIGH':\n            # Inverse transform\n            y_true_df = list(self.inverse_transform(y_true))\n            y_pred_df = list(self.inverse_transform(y_pred))\n\n            # Concat in a dataframe\n            if df_x is not None:\n                df = df_x.copy()\n                df['y_true'] = y_true_df\n                df['y_pred'] = y_pred_df\n            else:\n                df = pd.DataFrame({'y_true': y_true_df, 'y_pred': y_pred_df})\n            # Add column abs_err\n            df.loc[:, 'abs_err'] = df[['y_true', 'y_pred']].apply(lambda x: x.y_true - x.y_pred, axis=1)\n            # Add column rel_err\n            df.loc[:, 'rel_err'] = df[['y_true', 'y_pred']].apply(lambda x: (x.y_true - x.y_pred) / abs(x.y_true), axis=1)\n            # Add some more columns\n            if series_to_add is not None:\n                for ser in series_to_add:\n                    df[ser.name] = ser.reset_index(drop=True).reindex(index=df.index)  # Reindex correctly\n\n            # Save predictions\n            file_path = os.path.join(self.model_dir, f\"predictions{'_' + type_data if len(type_data) &gt; 0 else ''}.csv\")\n            df.sort_values('abs_err', ascending=True).to_csv(file_path, sep=';', index=None, encoding='utf-8')\n\n        # Get global metrics\n        metric_mae = mean_absolute_error(y_true, y_pred)\n        metric_mse = mean_squared_error(y_true, y_pred)\n        metric_rmse = mean_squared_error(y_true, y_pred, squared=False)\n        metric_explained_variance_score = explained_variance_score(y_true, y_pred)\n        metric_r2 = r2_score(y_true, y_pred)\n\n        # Global statistics\n        self.logger.info('-- * * * * * * * * * * * * * * --')\n        self.logger.info(f\"Statistics{' ' + type_data if len(type_data) &gt; 0 else ''}\")\n        self.logger.info('--------------------------------')\n        self.logger.info(f\"MAE : {round(metric_mae, 5)}\")\n        self.logger.info(f\"MSE : {round(metric_mse, 5)}\")\n        self.logger.info(f\"RMSE : {round(metric_rmse, 5)}\")\n        self.logger.info(f\"Explained variance : {round(metric_explained_variance_score, 5)}\")\n        self.logger.info(f\"R\u00b2 (coefficient of determination) : {round(metric_r2, 5)}\")\n        self.logger.info('--------------------------------')\n\n        # Metrics file\n        # TODO : add multi-outputs and stats for each output\n\n        # Add global statistics\n        dict_df_stats = {0: {\n            'Label': 'All',\n            'MAE': metric_mae,\n            'MSE': metric_mse,\n            'RMSE': metric_rmse,\n            'Explained variance': metric_explained_variance_score,\n            'Coefficient of determination': metric_r2,\n        }}\n        df_stats = pd.DataFrame.from_dict(dict_df_stats, orient='index')\n\n        # Save .csv\n        file_path = os.path.join(self.model_dir, f\"mae{'_' + type_data if len(type_data) &gt; 0 else ''}@{metric_mae}.csv\")\n        df_stats.to_csv(file_path, sep=';', index=False, encoding='utf-8')\n\n        # Save some metrics\n        mae_path = os.path.join(self.model_dir, f\"mae{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(metric_mae, 5)}\")\n        with open(mae_path, 'w'):\n            pass\n        mse_path = os.path.join(self.model_dir, f\"mse{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(metric_mse, 5)}\")\n        with open(mse_path, 'w'):\n            pass\n        rmse_path = os.path.join(self.model_dir, f\"rmse{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(metric_rmse, 5)}\")\n        with open(rmse_path, 'w'):\n            pass\n        explained_variance_path = os.path.join(self.model_dir, f\"explained_variance{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(metric_explained_variance_score, 5)}\")\n        with open(explained_variance_path, 'w'):\n            pass\n        r2_path = os.path.join(self.model_dir, f\"r2{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(metric_r2, 5)}\")\n        with open(r2_path, 'w'):\n            pass\n\n        # Plots\n        if self.level_save in ['MEDIUM', 'HIGH']:\n            # TODO: put a condition on the maximum number of points ?\n            is_train = True if type_data == 'train' else False\n            if is_train:\n                self.plot_prediction_errors(y_true_train=y_true, y_pred_train=y_pred,\n                                            y_true_test=None, y_pred_test=None,\n                                            type_data=type_data)\n                self.plot_residuals(y_true_train=y_true, y_pred_train=y_pred,\n                                    y_true_test=None, y_pred_test=None,\n                                    type_data=type_data)\n            else:\n                self.plot_prediction_errors(y_true_train=None, y_pred_train=None,\n                                            y_true_test=y_true, y_pred_test=y_pred,\n                                            type_data=type_data)\n                self.plot_residuals(y_true_train=None, y_pred_train=None,\n                                    y_true_test=y_true, y_pred_test=y_pred,\n                                    type_data=type_data)\n\n        # Return metrics\n        return df_stats\n\n    def get_metrics_simple(self, y_true, y_pred) -&gt; pd.DataFrame:\n        '''Gets metrics on predictions (single-output for now)\n        Same as the method get_and_save_metrics but without all the fluff (save, etc.)\n\n        Args:\n            y_true (?): Array-like, shape = [n_samples]\n            y_pred (?): Array-like, shape = [n_samples]\n        Returns:\n            pd.DataFrame: The dataframe containing statistics\n        '''\n        # Cast to np.array\n        y_true = np.array(y_true)\n        y_pred = np.array(y_pred)\n\n        # Get global metrics:\n        metric_mae = mean_absolute_error(y_true, y_pred)\n        metric_mse = mean_squared_error(y_true, y_pred)\n        metric_rmse = mean_squared_error(y_true, y_pred, squared=False)\n        metric_explained_variance_score = explained_variance_score(y_true, y_pred)\n        metric_r2 = r2_score(y_true, y_pred)\n\n        # Metrics file\n        # TODO : add multi-outputs and stats for each output\n\n        # Add global statistics\n        dict_df_stats = {0: {\n            'Label': 'All',\n            'MAE': metric_mae,\n            'MSE': metric_mse,\n            'RMSE': metric_rmse,\n            'Explained variance': metric_explained_variance_score,\n            'Coefficient of determination': metric_r2,\n        }}\n        df_stats = pd.DataFrame.from_dict(dict_df_stats, orient='index')\n\n        # Return dataframe\n        return df_stats\n\n    def plot_prediction_errors(self, y_true_train: Union[np.ndarray, None] = None, y_pred_train: Union[np.ndarray, None] = None,\n                               y_true_test: Union[np.ndarray, None] = None, y_pred_test: Union[np.ndarray, None] = None,\n                               type_data: str = '') -&gt; None:\n        '''Plots prediction errors\n\n        We use yellowbrick for the plots + a trick to be model agnostic\n\n        Kwargs:\n            y_true_train (np.ndarray): Array-like, shape = [n_samples]\n            y_pred_train (np.ndarray): Array-like, shape = [n_samples]\n            y_true_test (np.ndarray): Array-like, shape = [n_samples]\n            y_pred_test (np.ndarray): Array-like, shape = [n_samples]\n            type_data (str): Type of the dataset (validation, test, ...)\n        Raises:\n            ValueError: If a \"true\" is given, but not the corresponding \"pred\" (or vice-versa)\n        '''\n        if (y_true_train is not None and y_pred_train is None) or (y_true_train is None and y_pred_train is not None):\n            raise ValueError('\"true\" and \"pred\" must both be given, or not at all - train')\n        if (y_true_test is not None and y_pred_test is None) or (y_true_test is None and y_pred_test is not None):\n            raise ValueError('\"true\" and \"pred\" must both be given, or not at all - test')\n\n        # Get figure &amp; ax\n        fig, ax = plt.subplots(figsize=(12, 10))\n\n        # Set visualizer\n        visualizer = PredictionError(LinearRegression(), ax=ax, bestfit=False, is_fitted=True)  # Trick model not used\n        visualizer.name = self.model_name\n\n        # PredictionError does not support train and test at the same time :'(\n\n        # Train\n        if y_true_train is not None:\n            visualizer.score_ = r2_score(y_true_train, y_pred_train)\n            visualizer.draw(y_true_train, y_pred_train)\n\n        # Test\n        if y_true_test is not None:\n            visualizer.score_ = r2_score(y_true_test, y_pred_test)\n            visualizer.draw(y_true_test, y_pred_test)\n\n        # Save\n        plots_path = os.path.join(self.model_dir, 'plots')\n        if not os.path.exists(plots_path):\n            os.makedirs(plots_path)\n        file_name = f\"{type_data + '_' if len(type_data) &gt; 0 else ''}errors.png\"\n        visualizer.show(outpath=os.path.join(plots_path, file_name))\n\n        # Close figures\n        plt.close('all')\n\n    def plot_residuals(self, y_true_train: Union[np.ndarray, None] = None, y_pred_train: Union[np.ndarray, None] = None,\n                       y_true_test: Union[np.ndarray, None] = None, y_pred_test: Union[np.ndarray, None] = None,\n                       type_data: str = '') -&gt; None:\n        '''Plots the \"residuals\" from the predictions\n\n        Uses yellowbrick for the plots plus a trick in order to be model agnostic\n\n        Kwargs:\n            y_true_train (np.ndarray): Array-like, shape = [n_samples]\n            y_pred_train (np.ndarray): Array-like, shape = [n_samples]\n            y_true_test (np.ndarray): Array-like, shape = [n_samples]\n            y_pred_test (np.ndarray): Array-like, shape = [n_samples]\n            type_data (str): Type of the dataset (validation, test, ...)\n        Raises:\n            ValueError: If a \"true\" is given, but not the corresponding \"pred\" (or vice-versa)\n        '''\n        if (y_true_train is not None and y_pred_train is None) or (y_true_train is None and y_pred_train is not None):\n            raise ValueError('\"true\" and \"pred\" must both be given, or not at all - train')\n        if (y_true_test is not None and y_pred_test is None) or (y_true_test is None and y_pred_test is not None):\n            raise ValueError('\"true\" and \"pred\" must both be given, or not at all - test')\n\n        # Get figure &amp; ax\n        fig, ax = plt.subplots(figsize=(12, 10))\n\n        # Set visualizer\n        visualizer = ResidualsPlot(LinearRegression(), ax=ax, is_fitted=True)  # Trick model not used\n        visualizer.name = self.model_name\n\n        # Train\n        if y_true_train is not None:\n            visualizer.train_score_ = r2_score(y_true_train, y_pred_train)\n            residuals = y_pred_train - y_true_train\n            visualizer.draw(y_pred_train, residuals, train=True)\n\n        # Test\n        if y_true_test is not None:\n            visualizer.test_score_ = r2_score(y_true_test, y_pred_test)\n            residuals = y_pred_test - y_true_test\n            visualizer.draw(y_pred_test, residuals, train=False)\n\n        # Save\n        plots_path = os.path.join(self.model_dir, 'plots')\n        if not os.path.exists(plots_path):\n            os.makedirs(plots_path)\n        file_name = f\"{type_data + '_' if len(type_data) &gt; 0 else ''}residuals.png\"\n        visualizer.show(outpath=os.path.join(plots_path, file_name))\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_regressor/#template_num.models_training.regressors.model_regressor.ModelRegressorMixin.__init__","title":"<code>__init__(level_save='HIGH', **kwargs)</code>","text":"<p>Initialization of the class</p> Kwargs <p>level_save (str): Level of saving     LOW: stats + configurations + logger keras - /! The model can't be reused /! -     MEDIUM: LOW + hdf5 + pkl + plots     HIGH: MEDIUM + predictions</p> <p>Raises:      ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])</p> Source code in <code>template_num/models_training/regressors/model_regressor.py</code> <pre><code>def __init__(self, level_save: str = 'HIGH', **kwargs) -&gt; None:\n    '''Initialization of the class\n\n    Kwargs:\n        level_save (str): Level of saving\n            LOW: stats + configurations + logger keras - /!\\\\ The model can't be reused /!\\\\ -\n            MEDIUM: LOW + hdf5 + pkl + plots\n            HIGH: MEDIUM + predictions\n    Raises:\n         ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\n    '''\n    super().__init__(level_save=level_save, **kwargs)  # forwards level_save &amp; all unused arguments\n\n    if level_save not in ['LOW', 'MEDIUM', 'HIGH']:\n        raise ValueError(f\"The object level_save ({level_save}) is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\")\n\n    # Get logger\n    self.logger = logging.getLogger(__name__)\n\n    # Model type\n    self.model_type = 'regressor'\n\n    # TODO: add multi-outputs !\n\n    # Other options\n    self.level_save = level_save\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_regressor/#template_num.models_training.regressors.model_regressor.ModelRegressorMixin.get_and_save_metrics","title":"<code>get_and_save_metrics(y_true, y_pred, df_x=None, series_to_add=None, type_data='')</code>","text":"<p>Gets and saves the metrics of a model</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>?</code> <p>Array-like, shape = [n_samples,]</p> required <code>y_pred</code> <code>?</code> <p>Array-like, shape = [n_samples,]</p> required <p>Kwargs:     df_x (pd.DataFrame or None): Input dataFrame used for the prediction     series_to_add (list): List of pd.Series to add to the dataframe     type_data (str): Type of dataset (validation, test, ...) Returns:     pd.DataFrame: The dataframe containing the statistics Source code in <code>template_num/models_training/regressors/model_regressor.py</code> <pre><code>def get_and_save_metrics(self, y_true, y_pred, df_x: Union[pd.DataFrame, None] = None,\n                         series_to_add: Union[List[pd.Series], None] = None,\n                         type_data: str = '') -&gt; pd.DataFrame:\n    '''Gets and saves the metrics of a model\n\n    Args:\n        y_true (?): Array-like, shape = [n_samples,]\n        y_pred (?): Array-like, shape = [n_samples,]\n    Kwargs:\n        df_x (pd.DataFrame or None): Input dataFrame used for the prediction\n        series_to_add (list&lt;pd.Series&gt;): List of pd.Series to add to the dataframe\n        type_data (str): Type of dataset (validation, test, ...)\n    Returns:\n        pd.DataFrame: The dataframe containing the statistics\n    '''\n\n    # Cast to np.array\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Save a predictionn file if wanted\n    if self.level_save == 'HIGH':\n        # Inverse transform\n        y_true_df = list(self.inverse_transform(y_true))\n        y_pred_df = list(self.inverse_transform(y_pred))\n\n        # Concat in a dataframe\n        if df_x is not None:\n            df = df_x.copy()\n            df['y_true'] = y_true_df\n            df['y_pred'] = y_pred_df\n        else:\n            df = pd.DataFrame({'y_true': y_true_df, 'y_pred': y_pred_df})\n        # Add column abs_err\n        df.loc[:, 'abs_err'] = df[['y_true', 'y_pred']].apply(lambda x: x.y_true - x.y_pred, axis=1)\n        # Add column rel_err\n        df.loc[:, 'rel_err'] = df[['y_true', 'y_pred']].apply(lambda x: (x.y_true - x.y_pred) / abs(x.y_true), axis=1)\n        # Add some more columns\n        if series_to_add is not None:\n            for ser in series_to_add:\n                df[ser.name] = ser.reset_index(drop=True).reindex(index=df.index)  # Reindex correctly\n\n        # Save predictions\n        file_path = os.path.join(self.model_dir, f\"predictions{'_' + type_data if len(type_data) &gt; 0 else ''}.csv\")\n        df.sort_values('abs_err', ascending=True).to_csv(file_path, sep=';', index=None, encoding='utf-8')\n\n    # Get global metrics\n    metric_mae = mean_absolute_error(y_true, y_pred)\n    metric_mse = mean_squared_error(y_true, y_pred)\n    metric_rmse = mean_squared_error(y_true, y_pred, squared=False)\n    metric_explained_variance_score = explained_variance_score(y_true, y_pred)\n    metric_r2 = r2_score(y_true, y_pred)\n\n    # Global statistics\n    self.logger.info('-- * * * * * * * * * * * * * * --')\n    self.logger.info(f\"Statistics{' ' + type_data if len(type_data) &gt; 0 else ''}\")\n    self.logger.info('--------------------------------')\n    self.logger.info(f\"MAE : {round(metric_mae, 5)}\")\n    self.logger.info(f\"MSE : {round(metric_mse, 5)}\")\n    self.logger.info(f\"RMSE : {round(metric_rmse, 5)}\")\n    self.logger.info(f\"Explained variance : {round(metric_explained_variance_score, 5)}\")\n    self.logger.info(f\"R\u00b2 (coefficient of determination) : {round(metric_r2, 5)}\")\n    self.logger.info('--------------------------------')\n\n    # Metrics file\n    # TODO : add multi-outputs and stats for each output\n\n    # Add global statistics\n    dict_df_stats = {0: {\n        'Label': 'All',\n        'MAE': metric_mae,\n        'MSE': metric_mse,\n        'RMSE': metric_rmse,\n        'Explained variance': metric_explained_variance_score,\n        'Coefficient of determination': metric_r2,\n    }}\n    df_stats = pd.DataFrame.from_dict(dict_df_stats, orient='index')\n\n    # Save .csv\n    file_path = os.path.join(self.model_dir, f\"mae{'_' + type_data if len(type_data) &gt; 0 else ''}@{metric_mae}.csv\")\n    df_stats.to_csv(file_path, sep=';', index=False, encoding='utf-8')\n\n    # Save some metrics\n    mae_path = os.path.join(self.model_dir, f\"mae{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(metric_mae, 5)}\")\n    with open(mae_path, 'w'):\n        pass\n    mse_path = os.path.join(self.model_dir, f\"mse{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(metric_mse, 5)}\")\n    with open(mse_path, 'w'):\n        pass\n    rmse_path = os.path.join(self.model_dir, f\"rmse{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(metric_rmse, 5)}\")\n    with open(rmse_path, 'w'):\n        pass\n    explained_variance_path = os.path.join(self.model_dir, f\"explained_variance{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(metric_explained_variance_score, 5)}\")\n    with open(explained_variance_path, 'w'):\n        pass\n    r2_path = os.path.join(self.model_dir, f\"r2{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(metric_r2, 5)}\")\n    with open(r2_path, 'w'):\n        pass\n\n    # Plots\n    if self.level_save in ['MEDIUM', 'HIGH']:\n        # TODO: put a condition on the maximum number of points ?\n        is_train = True if type_data == 'train' else False\n        if is_train:\n            self.plot_prediction_errors(y_true_train=y_true, y_pred_train=y_pred,\n                                        y_true_test=None, y_pred_test=None,\n                                        type_data=type_data)\n            self.plot_residuals(y_true_train=y_true, y_pred_train=y_pred,\n                                y_true_test=None, y_pred_test=None,\n                                type_data=type_data)\n        else:\n            self.plot_prediction_errors(y_true_train=None, y_pred_train=None,\n                                        y_true_test=y_true, y_pred_test=y_pred,\n                                        type_data=type_data)\n            self.plot_residuals(y_true_train=None, y_pred_train=None,\n                                y_true_test=y_true, y_pred_test=y_pred,\n                                type_data=type_data)\n\n    # Return metrics\n    return df_stats\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_regressor/#template_num.models_training.regressors.model_regressor.ModelRegressorMixin.get_metrics_simple","title":"<code>get_metrics_simple(y_true, y_pred)</code>","text":"<p>Gets metrics on predictions (single-output for now) Same as the method get_and_save_metrics but without all the fluff (save, etc.)</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>?</code> <p>Array-like, shape = [n_samples]</p> required <code>y_pred</code> <code>?</code> <p>Array-like, shape = [n_samples]</p> required <p>Returns:     pd.DataFrame: The dataframe containing statistics</p> Source code in <code>template_num/models_training/regressors/model_regressor.py</code> <pre><code>def get_metrics_simple(self, y_true, y_pred) -&gt; pd.DataFrame:\n    '''Gets metrics on predictions (single-output for now)\n    Same as the method get_and_save_metrics but without all the fluff (save, etc.)\n\n    Args:\n        y_true (?): Array-like, shape = [n_samples]\n        y_pred (?): Array-like, shape = [n_samples]\n    Returns:\n        pd.DataFrame: The dataframe containing statistics\n    '''\n    # Cast to np.array\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Get global metrics:\n    metric_mae = mean_absolute_error(y_true, y_pred)\n    metric_mse = mean_squared_error(y_true, y_pred)\n    metric_rmse = mean_squared_error(y_true, y_pred, squared=False)\n    metric_explained_variance_score = explained_variance_score(y_true, y_pred)\n    metric_r2 = r2_score(y_true, y_pred)\n\n    # Metrics file\n    # TODO : add multi-outputs and stats for each output\n\n    # Add global statistics\n    dict_df_stats = {0: {\n        'Label': 'All',\n        'MAE': metric_mae,\n        'MSE': metric_mse,\n        'RMSE': metric_rmse,\n        'Explained variance': metric_explained_variance_score,\n        'Coefficient of determination': metric_r2,\n    }}\n    df_stats = pd.DataFrame.from_dict(dict_df_stats, orient='index')\n\n    # Return dataframe\n    return df_stats\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_regressor/#template_num.models_training.regressors.model_regressor.ModelRegressorMixin.inverse_transform","title":"<code>inverse_transform(y)</code>","text":"<p>Identity function - Manages compatibility with classifiers</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>list | ndarray</code> <p>Array-like, shape = [n_samples, 1]</p> required <p>Returns:     (np.ndarray): List, shape = [n_samples, 1]</p> Source code in <code>template_num/models_training/regressors/model_regressor.py</code> <pre><code>def inverse_transform(self, y: Union[list, np.ndarray]) -&gt; Union[list, tuple]:\n    '''Identity function - Manages compatibility with classifiers\n\n    Args:\n        y (list | np.ndarray): Array-like, shape = [n_samples, 1]\n    Returns:\n        (np.ndarray): List, shape = [n_samples, 1]\n    '''\n    return list(y) if isinstance(y, np.ndarray) else y\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_regressor/#template_num.models_training.regressors.model_regressor.ModelRegressorMixin.plot_prediction_errors","title":"<code>plot_prediction_errors(y_true_train=None, y_pred_train=None, y_true_test=None, y_pred_test=None, type_data='')</code>","text":"<p>Plots prediction errors</p> <p>We use yellowbrick for the plots + a trick to be model agnostic</p> Kwargs <p>y_true_train (np.ndarray): Array-like, shape = [n_samples] y_pred_train (np.ndarray): Array-like, shape = [n_samples] y_true_test (np.ndarray): Array-like, shape = [n_samples] y_pred_test (np.ndarray): Array-like, shape = [n_samples] type_data (str): Type of the dataset (validation, test, ...)</p> <p>Raises:     ValueError: If a \"true\" is given, but not the corresponding \"pred\" (or vice-versa)</p> Source code in <code>template_num/models_training/regressors/model_regressor.py</code> <pre><code>def plot_prediction_errors(self, y_true_train: Union[np.ndarray, None] = None, y_pred_train: Union[np.ndarray, None] = None,\n                           y_true_test: Union[np.ndarray, None] = None, y_pred_test: Union[np.ndarray, None] = None,\n                           type_data: str = '') -&gt; None:\n    '''Plots prediction errors\n\n    We use yellowbrick for the plots + a trick to be model agnostic\n\n    Kwargs:\n        y_true_train (np.ndarray): Array-like, shape = [n_samples]\n        y_pred_train (np.ndarray): Array-like, shape = [n_samples]\n        y_true_test (np.ndarray): Array-like, shape = [n_samples]\n        y_pred_test (np.ndarray): Array-like, shape = [n_samples]\n        type_data (str): Type of the dataset (validation, test, ...)\n    Raises:\n        ValueError: If a \"true\" is given, but not the corresponding \"pred\" (or vice-versa)\n    '''\n    if (y_true_train is not None and y_pred_train is None) or (y_true_train is None and y_pred_train is not None):\n        raise ValueError('\"true\" and \"pred\" must both be given, or not at all - train')\n    if (y_true_test is not None and y_pred_test is None) or (y_true_test is None and y_pred_test is not None):\n        raise ValueError('\"true\" and \"pred\" must both be given, or not at all - test')\n\n    # Get figure &amp; ax\n    fig, ax = plt.subplots(figsize=(12, 10))\n\n    # Set visualizer\n    visualizer = PredictionError(LinearRegression(), ax=ax, bestfit=False, is_fitted=True)  # Trick model not used\n    visualizer.name = self.model_name\n\n    # PredictionError does not support train and test at the same time :'(\n\n    # Train\n    if y_true_train is not None:\n        visualizer.score_ = r2_score(y_true_train, y_pred_train)\n        visualizer.draw(y_true_train, y_pred_train)\n\n    # Test\n    if y_true_test is not None:\n        visualizer.score_ = r2_score(y_true_test, y_pred_test)\n        visualizer.draw(y_true_test, y_pred_test)\n\n    # Save\n    plots_path = os.path.join(self.model_dir, 'plots')\n    if not os.path.exists(plots_path):\n        os.makedirs(plots_path)\n    file_name = f\"{type_data + '_' if len(type_data) &gt; 0 else ''}errors.png\"\n    visualizer.show(outpath=os.path.join(plots_path, file_name))\n\n    # Close figures\n    plt.close('all')\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_regressor/#template_num.models_training.regressors.model_regressor.ModelRegressorMixin.plot_residuals","title":"<code>plot_residuals(y_true_train=None, y_pred_train=None, y_true_test=None, y_pred_test=None, type_data='')</code>","text":"<p>Plots the \"residuals\" from the predictions</p> <p>Uses yellowbrick for the plots plus a trick in order to be model agnostic</p> Kwargs <p>y_true_train (np.ndarray): Array-like, shape = [n_samples] y_pred_train (np.ndarray): Array-like, shape = [n_samples] y_true_test (np.ndarray): Array-like, shape = [n_samples] y_pred_test (np.ndarray): Array-like, shape = [n_samples] type_data (str): Type of the dataset (validation, test, ...)</p> <p>Raises:     ValueError: If a \"true\" is given, but not the corresponding \"pred\" (or vice-versa)</p> Source code in <code>template_num/models_training/regressors/model_regressor.py</code> <pre><code>def plot_residuals(self, y_true_train: Union[np.ndarray, None] = None, y_pred_train: Union[np.ndarray, None] = None,\n                   y_true_test: Union[np.ndarray, None] = None, y_pred_test: Union[np.ndarray, None] = None,\n                   type_data: str = '') -&gt; None:\n    '''Plots the \"residuals\" from the predictions\n\n    Uses yellowbrick for the plots plus a trick in order to be model agnostic\n\n    Kwargs:\n        y_true_train (np.ndarray): Array-like, shape = [n_samples]\n        y_pred_train (np.ndarray): Array-like, shape = [n_samples]\n        y_true_test (np.ndarray): Array-like, shape = [n_samples]\n        y_pred_test (np.ndarray): Array-like, shape = [n_samples]\n        type_data (str): Type of the dataset (validation, test, ...)\n    Raises:\n        ValueError: If a \"true\" is given, but not the corresponding \"pred\" (or vice-versa)\n    '''\n    if (y_true_train is not None and y_pred_train is None) or (y_true_train is None and y_pred_train is not None):\n        raise ValueError('\"true\" and \"pred\" must both be given, or not at all - train')\n    if (y_true_test is not None and y_pred_test is None) or (y_true_test is None and y_pred_test is not None):\n        raise ValueError('\"true\" and \"pred\" must both be given, or not at all - test')\n\n    # Get figure &amp; ax\n    fig, ax = plt.subplots(figsize=(12, 10))\n\n    # Set visualizer\n    visualizer = ResidualsPlot(LinearRegression(), ax=ax, is_fitted=True)  # Trick model not used\n    visualizer.name = self.model_name\n\n    # Train\n    if y_true_train is not None:\n        visualizer.train_score_ = r2_score(y_true_train, y_pred_train)\n        residuals = y_pred_train - y_true_train\n        visualizer.draw(y_pred_train, residuals, train=True)\n\n    # Test\n    if y_true_test is not None:\n        visualizer.test_score_ = r2_score(y_true_test, y_pred_test)\n        residuals = y_pred_test - y_true_test\n        visualizer.draw(y_pred_test, residuals, train=False)\n\n    # Save\n    plots_path = os.path.join(self.model_dir, 'plots')\n    if not os.path.exists(plots_path):\n        os.makedirs(plots_path)\n    file_name = f\"{type_data + '_' if len(type_data) &gt; 0 else ''}residuals.png\"\n    visualizer.show(outpath=os.path.join(plots_path, file_name))\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_xgboost_regressor/","title":"Model xgboost regressor","text":""},{"location":"reference/template_num/models_training/regressors/model_xgboost_regressor/#template_num.models_training.regressors.model_xgboost_regressor.ModelXgboostRegressor","title":"<code>ModelXgboostRegressor</code>","text":"<p>             Bases: <code>ModelRegressorMixin</code>, <code>ModelClass</code></p> <p>Xgboost model for regression</p> Source code in <code>template_num/models_training/regressors/model_xgboost_regressor.py</code> <pre><code>class ModelXgboostRegressor(ModelRegressorMixin, ModelClass):\n    '''Xgboost model for regression'''\n\n    _default_name = 'model_xgboost_regressor'\n\n    def __init__(self, xgboost_params: Union[dict, None] = None, early_stopping_rounds: int = 5, validation_split: float = 0.2, **kwargs) -&gt; None:\n        '''Initialization of the class  (see ModelClass &amp; ModelRegressorMixin for more arguments)\n\n        Kwargs:\n            xgboost_params (dict): Parameters for the Xgboost\n                -&gt; https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor\n            early_stopping_rounds (int): Number of rounds for early stopping\n            validation_split (float): Validation split fraction.\n                Only used if not validation dataset in the fit input\n        '''\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Set parameters\n        if xgboost_params is None:\n            xgboost_params = {}\n        xgboost_params[\"random_state\"] = self.random_seed\n        self.xgboost_params = xgboost_params\n        self.early_stopping_rounds = early_stopping_rounds\n        self.validation_split = validation_split\n\n        # Set objective (if not in params) &amp; init. model\n        if 'objective' not in self.xgboost_params.keys():\n            self.xgboost_params['objective'] = 'reg:squarederror'\n            #  List of objectives https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n        self.model = XGBRegressor(**self.xgboost_params)\n\n    def fit(self, x_train, y_train, x_valid=None, y_valid=None, with_shuffle: bool = True, **kwargs) -&gt; None:\n        '''Trains the model\n           **kwargs permits compatibility with Keras model\n\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n            y_train (?): Array-like, shape = [n_samples,]\n        Kwargs:\n            x_valid (?): Array-like, shape = [n_samples, n_features]\n            y_valid (?): Array-like, shape = [n_samples,]\n            with_shuffle (bool): If x, y must be shuffled before fitting\n        Raises:\n            RuntimeError: If the model is already fitted\n        '''\n        # TODO: Check if we can continue the training of a xgboost\n        if self.trained:\n            self.logger.error(\"We can't train again a xgboost model\")\n            self.logger.error(\"Please train a new model\")\n            raise RuntimeError(\"We can't train again a xgboost model\")\n\n        # We check input format\n        x_train, y_train = self._check_input_format(x_train, y_train, fit_function=True)\n        # If there is a validation set, we also check the format (but fit_function to False)\n        if y_valid is not None and x_valid is not None:\n            x_valid, y_valid = self._check_input_format(x_valid, y_valid, fit_function=False)\n        # Otherwise, we do a random split\n        else:\n            self.logger.warning(f\"Warning, no validation dataset. We split the training set (fraction valid = {self.validation_split})\")\n            x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=self.validation_split, random_state=self.random_seed)\n\n        # Shuffle x, y if wanted\n        if with_shuffle:\n            rng = np.random.RandomState(self.random_seed)\n            p = rng.permutation(len(x_train))\n            x_train = np.array(x_train)[p]\n            y_train = np.array(y_train)[p]\n        # Else still transform to numpy array\n        else:\n            x_train = np.array(x_train)\n            y_train = np.array(y_train)\n\n        # Also get x_valid &amp; y_valid as numpy\n        x_valid = np.array(x_valid)\n        y_valid = np.array(y_valid)\n\n        # Set eval set and train\n        eval_set = [(x_train, y_train), (x_valid, y_valid)]  # If there\u2019s more than one item in eval_set, the last entry will be used for early stopping.\n        prior_objective = self.model.objective\n        self.model.fit(x_train, y_train, eval_set=eval_set, early_stopping_rounds=self.early_stopping_rounds, verbose=True)\n        post_objective = self.model.objective\n        if prior_objective != post_objective:\n            self.logger.warning(\"Warning: the objective function was automatically changed by XGBOOST\")\n            self.logger.warning(f\"Before: {prior_objective}\")\n            self.logger.warning(f\"After: {post_objective}\")\n\n        # Set trained\n        self.trained = True\n        self.nb_fit += 1\n\n    @utils.trained_needed\n    def predict(self, x_test: pd.DataFrame, return_proba: bool = False, **kwargs) -&gt; np.ndarray:\n        '''Predictions on test set\n\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Kwargs:\n            return_proba (bool): Present for compatibility with other models. Raises an error if True\n        Raises:\n            ValueError: If return_proba is True\n        Returns:\n            (np.ndarray): Array, shape = [n_samples,]\n        '''\n        # Manage errors\n        if return_proba:\n            raise ValueError(\"Models of type model_xgboost_regressor can't handle probabilities\")\n        # We check input format\n        x_test, _ = self._check_input_format(x_test)\n        # Warning, \"The method returns the model from the last iteration\"\n        # But : \"Predict with X. If the model is trained with early stopping, then best_iteration is used automatically.\"\n        y_pred = self.model.predict(x_test)\n        return y_pred\n\n    @utils.trained_needed\n    def predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n        '''Predicts the probabilities on the test set. Here for compatibility\n\n        Args:\n            x_test (pd.DataFrame): Array-like, shape = [n_samples]\n        Raises:\n            ValueError: Model_xgboost_regressor does not implement predict_proba\n        Returns:\n            (np.ndarray): Array, shape = [n_samples,]\n        '''\n        # For compatibility\n        raise ValueError(\"Models of type model_xgboost_regressor do not implement the method predict_proba\")\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save model\n        if json_data is None:\n            json_data = {}\n\n        json_data['librairie'] = 'xgboost'\n        json_data['xgboost_params'] = self.xgboost_params\n        json_data['early_stopping_rounds'] = self.early_stopping_rounds\n        json_data['validation_split'] = self.validation_split\n\n        # Save xgboost standalone\n        if self.level_save in ['MEDIUM', 'HIGH']:\n            if self.trained:\n                save_path = os.path.join(self.model_dir, 'xbgoost_standalone.model')\n                self.model.save_model(save_path)\n            else:\n                self.logger.warning(\"Can't save the XGboost in standalone because it hasn't been already fitted\")\n\n        # Save\n        super().save(json_data=json_data)\n\n    def reload_from_standalone(self, **kwargs) -&gt; None:\n        '''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            xgboost_path (str): Path to standalone xgboost\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If xgboost_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object xgboost_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n        # Retrieve args\n        configuration_path = kwargs.get('configuration_path', None)\n        xgboost_path = kwargs.get('xgboost_path', None)\n        preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n        # Checks\n        if configuration_path is None:\n            raise ValueError(\"The argument configuration_path can't be None\")\n        if xgboost_path is None:\n            raise ValueError(\"The argument xgboost_path can't be None\")\n        if preprocess_pipeline_path is None:\n            raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n        if not os.path.exists(configuration_path):\n            raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n        if not os.path.exists(xgboost_path):\n            raise FileNotFoundError(f\"The file {xgboost_path} does not exist\")\n        if not os.path.exists(preprocess_pipeline_path):\n            raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n        # Load confs\n        with open(configuration_path, 'r', encoding='utf-8') as f:\n            configs = json.load(f)\n        # Set class vars\n        # self.model_name = # Keep the created name\n        # self.model_dir = # Keep the created folder\n        self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n        self.trained = configs.get('trained', True)  # Consider trained by default\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                          'random_seed', 'level_save', 'xgboost_params', 'early_stopping_rounds', 'validation_split']:\n            setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n        # Reload xgboost model\n        self.model.load_model(xgboost_path)  # load data\n\n        # Reload pipeline preprocessing\n        with open(preprocess_pipeline_path, 'rb') as f:\n            self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_xgboost_regressor/#template_num.models_training.regressors.model_xgboost_regressor.ModelXgboostRegressor.__init__","title":"<code>__init__(xgboost_params=None, early_stopping_rounds=5, validation_split=0.2, **kwargs)</code>","text":"<p>Initialization of the class  (see ModelClass &amp; ModelRegressorMixin for more arguments)</p> Kwargs <p>xgboost_params (dict): Parameters for the Xgboost     -&gt; https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor early_stopping_rounds (int): Number of rounds for early stopping validation_split (float): Validation split fraction.     Only used if not validation dataset in the fit input</p> Source code in <code>template_num/models_training/regressors/model_xgboost_regressor.py</code> <pre><code>def __init__(self, xgboost_params: Union[dict, None] = None, early_stopping_rounds: int = 5, validation_split: float = 0.2, **kwargs) -&gt; None:\n    '''Initialization of the class  (see ModelClass &amp; ModelRegressorMixin for more arguments)\n\n    Kwargs:\n        xgboost_params (dict): Parameters for the Xgboost\n            -&gt; https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor\n        early_stopping_rounds (int): Number of rounds for early stopping\n        validation_split (float): Validation split fraction.\n            Only used if not validation dataset in the fit input\n    '''\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Set parameters\n    if xgboost_params is None:\n        xgboost_params = {}\n    xgboost_params[\"random_state\"] = self.random_seed\n    self.xgboost_params = xgboost_params\n    self.early_stopping_rounds = early_stopping_rounds\n    self.validation_split = validation_split\n\n    # Set objective (if not in params) &amp; init. model\n    if 'objective' not in self.xgboost_params.keys():\n        self.xgboost_params['objective'] = 'reg:squarederror'\n        #  List of objectives https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n    self.model = XGBRegressor(**self.xgboost_params)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_xgboost_regressor/#template_num.models_training.regressors.model_xgboost_regressor.ModelXgboostRegressor.fit","title":"<code>fit(x_train, y_train, x_valid=None, y_valid=None, with_shuffle=True, **kwargs)</code>","text":"<p>Trains the model    **kwargs permits compatibility with Keras model</p> <p>Parameters:</p> Name Type Description Default <code>x_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <code>y_train</code> <code>?</code> <p>Array-like, shape = [n_samples,]</p> required <p>Kwargs:     x_valid (?): Array-like, shape = [n_samples, n_features]     y_valid (?): Array-like, shape = [n_samples,]     with_shuffle (bool): If x, y must be shuffled before fitting Raises:     RuntimeError: If the model is already fitted</p> Source code in <code>template_num/models_training/regressors/model_xgboost_regressor.py</code> <pre><code>def fit(self, x_train, y_train, x_valid=None, y_valid=None, with_shuffle: bool = True, **kwargs) -&gt; None:\n    '''Trains the model\n       **kwargs permits compatibility with Keras model\n\n    Args:\n        x_train (?): Array-like, shape = [n_samples, n_features]\n        y_train (?): Array-like, shape = [n_samples,]\n    Kwargs:\n        x_valid (?): Array-like, shape = [n_samples, n_features]\n        y_valid (?): Array-like, shape = [n_samples,]\n        with_shuffle (bool): If x, y must be shuffled before fitting\n    Raises:\n        RuntimeError: If the model is already fitted\n    '''\n    # TODO: Check if we can continue the training of a xgboost\n    if self.trained:\n        self.logger.error(\"We can't train again a xgboost model\")\n        self.logger.error(\"Please train a new model\")\n        raise RuntimeError(\"We can't train again a xgboost model\")\n\n    # We check input format\n    x_train, y_train = self._check_input_format(x_train, y_train, fit_function=True)\n    # If there is a validation set, we also check the format (but fit_function to False)\n    if y_valid is not None and x_valid is not None:\n        x_valid, y_valid = self._check_input_format(x_valid, y_valid, fit_function=False)\n    # Otherwise, we do a random split\n    else:\n        self.logger.warning(f\"Warning, no validation dataset. We split the training set (fraction valid = {self.validation_split})\")\n        x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=self.validation_split, random_state=self.random_seed)\n\n    # Shuffle x, y if wanted\n    if with_shuffle:\n        rng = np.random.RandomState(self.random_seed)\n        p = rng.permutation(len(x_train))\n        x_train = np.array(x_train)[p]\n        y_train = np.array(y_train)[p]\n    # Else still transform to numpy array\n    else:\n        x_train = np.array(x_train)\n        y_train = np.array(y_train)\n\n    # Also get x_valid &amp; y_valid as numpy\n    x_valid = np.array(x_valid)\n    y_valid = np.array(y_valid)\n\n    # Set eval set and train\n    eval_set = [(x_train, y_train), (x_valid, y_valid)]  # If there\u2019s more than one item in eval_set, the last entry will be used for early stopping.\n    prior_objective = self.model.objective\n    self.model.fit(x_train, y_train, eval_set=eval_set, early_stopping_rounds=self.early_stopping_rounds, verbose=True)\n    post_objective = self.model.objective\n    if prior_objective != post_objective:\n        self.logger.warning(\"Warning: the objective function was automatically changed by XGBOOST\")\n        self.logger.warning(f\"Before: {prior_objective}\")\n        self.logger.warning(f\"After: {post_objective}\")\n\n    # Set trained\n    self.trained = True\n    self.nb_fit += 1\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_xgboost_regressor/#template_num.models_training.regressors.model_xgboost_regressor.ModelXgboostRegressor.predict","title":"<code>predict(x_test, return_proba=False, **kwargs)</code>","text":"<p>Predictions on test set</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>DataFrame</code> <p>DataFrame with the test data to be predicted</p> required <p>Kwargs:     return_proba (bool): Present for compatibility with other models. Raises an error if True Raises:     ValueError: If return_proba is True Returns:     (np.ndarray): Array, shape = [n_samples,]</p> Source code in <code>template_num/models_training/regressors/model_xgboost_regressor.py</code> <pre><code>@utils.trained_needed\ndef predict(self, x_test: pd.DataFrame, return_proba: bool = False, **kwargs) -&gt; np.ndarray:\n    '''Predictions on test set\n\n    Args:\n        x_test (pd.DataFrame): DataFrame with the test data to be predicted\n    Kwargs:\n        return_proba (bool): Present for compatibility with other models. Raises an error if True\n    Raises:\n        ValueError: If return_proba is True\n    Returns:\n        (np.ndarray): Array, shape = [n_samples,]\n    '''\n    # Manage errors\n    if return_proba:\n        raise ValueError(\"Models of type model_xgboost_regressor can't handle probabilities\")\n    # We check input format\n    x_test, _ = self._check_input_format(x_test)\n    # Warning, \"The method returns the model from the last iteration\"\n    # But : \"Predict with X. If the model is trained with early stopping, then best_iteration is used automatically.\"\n    y_pred = self.model.predict(x_test)\n    return y_pred\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_xgboost_regressor/#template_num.models_training.regressors.model_xgboost_regressor.ModelXgboostRegressor.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set. Here for compatibility</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>DataFrame</code> <p>Array-like, shape = [n_samples]</p> required <p>Raises:     ValueError: Model_xgboost_regressor does not implement predict_proba Returns:     (np.ndarray): Array, shape = [n_samples,]</p> Source code in <code>template_num/models_training/regressors/model_xgboost_regressor.py</code> <pre><code>@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n    '''Predicts the probabilities on the test set. Here for compatibility\n\n    Args:\n        x_test (pd.DataFrame): Array-like, shape = [n_samples]\n    Raises:\n        ValueError: Model_xgboost_regressor does not implement predict_proba\n    Returns:\n        (np.ndarray): Array, shape = [n_samples,]\n    '''\n    # For compatibility\n    raise ValueError(\"Models of type model_xgboost_regressor do not implement the method predict_proba\")\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_xgboost_regressor/#template_num.models_training.regressors.model_xgboost_regressor.ModelXgboostRegressor.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file xgboost_path (str): Path to standalone xgboost preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:     ValueError: If configuration_path is None     ValueError: If xgboost_path is None     ValueError: If preprocess_pipeline_path is None     FileNotFoundError: If the object configuration_path is not an existing file     FileNotFoundError: If the object xgboost_path is not an existing file     FileNotFoundError: If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/regressors/model_xgboost_regressor.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n    '''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        xgboost_path (str): Path to standalone xgboost\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If xgboost_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object xgboost_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n    # Retrieve args\n    configuration_path = kwargs.get('configuration_path', None)\n    xgboost_path = kwargs.get('xgboost_path', None)\n    preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n    # Checks\n    if configuration_path is None:\n        raise ValueError(\"The argument configuration_path can't be None\")\n    if xgboost_path is None:\n        raise ValueError(\"The argument xgboost_path can't be None\")\n    if preprocess_pipeline_path is None:\n        raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n    if not os.path.exists(configuration_path):\n        raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n    if not os.path.exists(xgboost_path):\n        raise FileNotFoundError(f\"The file {xgboost_path} does not exist\")\n    if not os.path.exists(preprocess_pipeline_path):\n        raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n    # Load confs\n    with open(configuration_path, 'r', encoding='utf-8') as f:\n        configs = json.load(f)\n    # Set class vars\n    # self.model_name = # Keep the created name\n    # self.model_dir = # Keep the created folder\n    self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n    self.trained = configs.get('trained', True)  # Consider trained by default\n    # Try to read the following attributes from configs and, if absent, keep the current one\n    for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                      'random_seed', 'level_save', 'xgboost_params', 'early_stopping_rounds', 'validation_split']:\n        setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n    # Reload xgboost model\n    self.model.load_model(xgboost_path)  # load data\n\n    # Reload pipeline preprocessing\n    with open(preprocess_pipeline_path, 'rb') as f:\n        self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_xgboost_regressor/#template_num.models_training.regressors.model_xgboost_regressor.ModelXgboostRegressor.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/regressors/model_xgboost_regressor.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save model\n    if json_data is None:\n        json_data = {}\n\n    json_data['librairie'] = 'xgboost'\n    json_data['xgboost_params'] = self.xgboost_params\n    json_data['early_stopping_rounds'] = self.early_stopping_rounds\n    json_data['validation_split'] = self.validation_split\n\n    # Save xgboost standalone\n    if self.level_save in ['MEDIUM', 'HIGH']:\n        if self.trained:\n            save_path = os.path.join(self.model_dir, 'xbgoost_standalone.model')\n            self.model.save_model(save_path)\n        else:\n            self.logger.warning(\"Can't save the XGboost in standalone because it hasn't been already fitted\")\n\n    # Save\n    super().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/","title":"Models sklearn","text":""},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_bayesian_ridge_regressor/","title":"Model bayesian ridge regressor","text":""},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_bayesian_ridge_regressor/#template_num.models_training.regressors.models_sklearn.model_bayesian_ridge_regressor.ModelBayesianRidgeRegressor","title":"<code>ModelBayesianRidgeRegressor</code>","text":"<p>             Bases: <code>ModelRegressorMixin</code>, <code>ModelPipeline</code></p> <p>Bayesian ridge model for regression</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_bayesian_ridge_regressor.py</code> <pre><code>class ModelBayesianRidgeRegressor(ModelRegressorMixin, ModelPipeline):\n    '''Bayesian ridge model for regression'''\n\n    _default_name = 'model_bayesian_ridge_regressor'\n\n    def __init__(self, bayesian_ridge_params: Union[dict, None] = None, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n\n        Kwargs:\n            bayesian_ridge_params (dict) : Parameters for Bayesian Ridge\n        '''\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Manage model\n        if bayesian_ridge_params is None:\n            bayesian_ridge_params = {}\n        self.bayesian_ridge = BayesianRidge(**bayesian_ridge_params)\n        # We define a pipeline in order to be compatible with other models\n        self.pipeline = Pipeline([('bayesian_ridge', self.bayesian_ridge)])\n\n    def reload_from_standalone(self, **kwargs) -&gt; None:\n        '''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n        # Retrieve args\n        configuration_path = kwargs.get('configuration_path', None)\n        sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n        preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n        # Checks\n        if configuration_path is None:\n            raise ValueError(\"The argument configuration_path can't be None\")\n        if sklearn_pipeline_path is None:\n            raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n        if preprocess_pipeline_path is None:\n            raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n        if not os.path.exists(configuration_path):\n            raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n        if not os.path.exists(sklearn_pipeline_path):\n            raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n        if not os.path.exists(preprocess_pipeline_path):\n            raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n        # Load confs\n        with open(configuration_path, 'r', encoding='utf-8') as f:\n            configs = json.load(f)\n\n        # Set class vars\n        # self.model_name = # Keep the created name\n        # self.model_dir = # Keep the created folder\n        self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n        self.trained = configs.get('trained', True)  # Consider trained by default\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                          'level_save']:\n            setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n        # Reload pipeline model\n        with open(sklearn_pipeline_path, 'rb') as f:\n            self.pipeline = pickle.load(f)\n\n        # Reload pipeline elements\n        self.bayesian_ridge = self.pipeline['bayesian_ridge']\n\n        # Reload pipeline preprocessing\n        with open(preprocess_pipeline_path, 'rb') as f:\n            self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_bayesian_ridge_regressor/#template_num.models_training.regressors.models_sklearn.model_bayesian_ridge_regressor.ModelBayesianRidgeRegressor.__init__","title":"<code>__init__(bayesian_ridge_params=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)</p> Kwargs <p>bayesian_ridge_params (dict) : Parameters for Bayesian Ridge</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_bayesian_ridge_regressor.py</code> <pre><code>def __init__(self, bayesian_ridge_params: Union[dict, None] = None, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n\n    Kwargs:\n        bayesian_ridge_params (dict) : Parameters for Bayesian Ridge\n    '''\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Manage model\n    if bayesian_ridge_params is None:\n        bayesian_ridge_params = {}\n    self.bayesian_ridge = BayesianRidge(**bayesian_ridge_params)\n    # We define a pipeline in order to be compatible with other models\n    self.pipeline = Pipeline([('bayesian_ridge', self.bayesian_ridge)])\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_bayesian_ridge_regressor/#template_num.models_training.regressors.models_sklearn.model_bayesian_ridge_regressor.ModelBayesianRidgeRegressor.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:     ValueError: If configuration_path is None     ValueError: If sklearn_pipeline_path is None     ValueError: If preprocess_pipeline_path is None     FileNotFoundError: If the object configuration_path is not an existing file     FileNotFoundError: If the object sklearn_pipeline_path is not an existing file     FileNotFoundError: If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_bayesian_ridge_regressor.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n    '''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n    # Retrieve args\n    configuration_path = kwargs.get('configuration_path', None)\n    sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n    preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n    # Checks\n    if configuration_path is None:\n        raise ValueError(\"The argument configuration_path can't be None\")\n    if sklearn_pipeline_path is None:\n        raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n    if preprocess_pipeline_path is None:\n        raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n    if not os.path.exists(configuration_path):\n        raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n    if not os.path.exists(sklearn_pipeline_path):\n        raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n    if not os.path.exists(preprocess_pipeline_path):\n        raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n    # Load confs\n    with open(configuration_path, 'r', encoding='utf-8') as f:\n        configs = json.load(f)\n\n    # Set class vars\n    # self.model_name = # Keep the created name\n    # self.model_dir = # Keep the created folder\n    self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n    self.trained = configs.get('trained', True)  # Consider trained by default\n    # Try to read the following attributes from configs and, if absent, keep the current one\n    for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                      'level_save']:\n        setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n    # Reload pipeline model\n    with open(sklearn_pipeline_path, 'rb') as f:\n        self.pipeline = pickle.load(f)\n\n    # Reload pipeline elements\n    self.bayesian_ridge = self.pipeline['bayesian_ridge']\n\n    # Reload pipeline preprocessing\n    with open(preprocess_pipeline_path, 'rb') as f:\n        self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_elasticnet_regressor/","title":"Model elasticnet regressor","text":""},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_elasticnet_regressor/#template_num.models_training.regressors.models_sklearn.model_elasticnet_regressor.ModelElasticNetRegressor","title":"<code>ModelElasticNetRegressor</code>","text":"<p>             Bases: <code>ModelRegressorMixin</code>, <code>ModelPipeline</code></p> <p>Elastic Net model for regression</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_elasticnet_regressor.py</code> <pre><code>class ModelElasticNetRegressor(ModelRegressorMixin, ModelPipeline):\n    '''Elastic Net model for regression'''\n\n    _default_name = 'model_elasticnet_regressor'\n\n    def __init__(self, elasticnet_params: Union[dict, None] = None, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n\n        Kwargs:\n            elasticnet_params (dict) : Parameters for the Elastic Net\n        '''\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Manage model\n        if elasticnet_params is None:\n            elasticnet_params = {}\n        self.elasticnet = ElasticNet(**elasticnet_params)\n        # We define a pipeline in order to be compatible with other models\n        self.pipeline = Pipeline([('elasticnet', self.elasticnet)])\n\n    def reload_from_standalone(self, **kwargs) -&gt; None:\n        '''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n        # Retrieve args\n        configuration_path = kwargs.get('configuration_path', None)\n        sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n        preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n        # Checks\n        if configuration_path is None:\n            raise ValueError(\"The argument configuration_path can't be None\")\n        if sklearn_pipeline_path is None:\n            raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n        if preprocess_pipeline_path is None:\n            raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n        if not os.path.exists(configuration_path):\n            raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n        if not os.path.exists(sklearn_pipeline_path):\n            raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n        if not os.path.exists(preprocess_pipeline_path):\n            raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n        # Load confs\n        with open(configuration_path, 'r', encoding='utf-8') as f:\n            configs = json.load(f)\n\n        # Set class vars\n        # self.model_name = # Keep the created name\n        # self.model_dir = # Keep the created folder\n        self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n        self.trained = configs.get('trained', True)  # Consider trained by default\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                          'level_save']:\n            setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n        # Reload pipeline model\n        with open(sklearn_pipeline_path, 'rb') as f:\n            self.pipeline = pickle.load(f)\n\n        # Reload pipeline elements\n        self.elasticnet = self.pipeline['elasticnet']\n\n        # Reload pipeline preprocessing\n        with open(preprocess_pipeline_path, 'rb') as f:\n            self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_elasticnet_regressor/#template_num.models_training.regressors.models_sklearn.model_elasticnet_regressor.ModelElasticNetRegressor.__init__","title":"<code>__init__(elasticnet_params=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)</p> Kwargs <p>elasticnet_params (dict) : Parameters for the Elastic Net</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_elasticnet_regressor.py</code> <pre><code>def __init__(self, elasticnet_params: Union[dict, None] = None, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n\n    Kwargs:\n        elasticnet_params (dict) : Parameters for the Elastic Net\n    '''\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Manage model\n    if elasticnet_params is None:\n        elasticnet_params = {}\n    self.elasticnet = ElasticNet(**elasticnet_params)\n    # We define a pipeline in order to be compatible with other models\n    self.pipeline = Pipeline([('elasticnet', self.elasticnet)])\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_elasticnet_regressor/#template_num.models_training.regressors.models_sklearn.model_elasticnet_regressor.ModelElasticNetRegressor.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:     ValueError: If configuration_path is None     ValueError: If sklearn_pipeline_path is None     ValueError: If preprocess_pipeline_path is None     FileNotFoundError: If the object configuration_path is not an existing file     FileNotFoundError: If the object sklearn_pipeline_path is not an existing file     FileNotFoundError: If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_elasticnet_regressor.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n    '''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n    # Retrieve args\n    configuration_path = kwargs.get('configuration_path', None)\n    sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n    preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n    # Checks\n    if configuration_path is None:\n        raise ValueError(\"The argument configuration_path can't be None\")\n    if sklearn_pipeline_path is None:\n        raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n    if preprocess_pipeline_path is None:\n        raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n    if not os.path.exists(configuration_path):\n        raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n    if not os.path.exists(sklearn_pipeline_path):\n        raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n    if not os.path.exists(preprocess_pipeline_path):\n        raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n    # Load confs\n    with open(configuration_path, 'r', encoding='utf-8') as f:\n        configs = json.load(f)\n\n    # Set class vars\n    # self.model_name = # Keep the created name\n    # self.model_dir = # Keep the created folder\n    self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n    self.trained = configs.get('trained', True)  # Consider trained by default\n    # Try to read the following attributes from configs and, if absent, keep the current one\n    for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                      'level_save']:\n        setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n    # Reload pipeline model\n    with open(sklearn_pipeline_path, 'rb') as f:\n        self.pipeline = pickle.load(f)\n\n    # Reload pipeline elements\n    self.elasticnet = self.pipeline['elasticnet']\n\n    # Reload pipeline preprocessing\n    with open(preprocess_pipeline_path, 'rb') as f:\n        self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_gbt_regressor/","title":"Model gbt regressor","text":""},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_gbt_regressor/#template_num.models_training.regressors.models_sklearn.model_gbt_regressor.ModelGBTRegressor","title":"<code>ModelGBTRegressor</code>","text":"<p>             Bases: <code>ModelRegressorMixin</code>, <code>ModelPipeline</code></p> <p>Gradient Boosting Tree model for regression</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_gbt_regressor.py</code> <pre><code>class ModelGBTRegressor(ModelRegressorMixin, ModelPipeline):\n    '''Gradient Boosting Tree model for regression'''\n\n    _default_name = 'model_gbt_regressor'\n\n    def __init__(self, gbt_params: Union[dict, None] = None, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n\n        Kwargs:\n            gbt_params (dict) : Parameters for the Gradient Boosting Tree\n        '''\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Manage model\n        if gbt_params is None:\n            gbt_params = {}\n        gbt_params[\"random_state\"] = self.random_seed\n        self.gbt = GradientBoostingRegressor(**gbt_params)\n        # We define a pipeline in order to be compatible with other models\n        self.pipeline = Pipeline([('gbt', self.gbt)])\n\n    def reload_from_standalone(self, **kwargs) -&gt; None:\n        '''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n        # Retrieve args\n        configuration_path = kwargs.get('configuration_path', None)\n        sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n        preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n        # Checks\n        if configuration_path is None:\n            raise ValueError(\"The argument configuration_path can't be None\")\n        if sklearn_pipeline_path is None:\n            raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n        if preprocess_pipeline_path is None:\n            raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n        if not os.path.exists(configuration_path):\n            raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n        if not os.path.exists(sklearn_pipeline_path):\n            raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n        if not os.path.exists(preprocess_pipeline_path):\n            raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n        # Load confs\n        with open(configuration_path, 'r', encoding='utf-8') as f:\n            configs = json.load(f)\n\n        # Set class vars\n        # self.model_name = # Keep the created name\n        # self.model_dir = # Keep the created folder\n        self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n        self.trained = configs.get('trained', True)  # Consider trained by default\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                          'random_seed', 'level_save']:\n            setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n        # Reload pipeline model\n        with open(sklearn_pipeline_path, 'rb') as f:\n            self.pipeline = pickle.load(f)\n\n        # Reload pipeline elements\n        self.gbt = self.pipeline['gbt']\n\n        # Reload pipeline preprocessing\n        with open(preprocess_pipeline_path, 'rb') as f:\n            self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_gbt_regressor/#template_num.models_training.regressors.models_sklearn.model_gbt_regressor.ModelGBTRegressor.__init__","title":"<code>__init__(gbt_params=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)</p> Kwargs <p>gbt_params (dict) : Parameters for the Gradient Boosting Tree</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_gbt_regressor.py</code> <pre><code>def __init__(self, gbt_params: Union[dict, None] = None, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n\n    Kwargs:\n        gbt_params (dict) : Parameters for the Gradient Boosting Tree\n    '''\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Manage model\n    if gbt_params is None:\n        gbt_params = {}\n    gbt_params[\"random_state\"] = self.random_seed\n    self.gbt = GradientBoostingRegressor(**gbt_params)\n    # We define a pipeline in order to be compatible with other models\n    self.pipeline = Pipeline([('gbt', self.gbt)])\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_gbt_regressor/#template_num.models_training.regressors.models_sklearn.model_gbt_regressor.ModelGBTRegressor.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:     ValueError: If configuration_path is None     ValueError: If sklearn_pipeline_path is None     ValueError: If preprocess_pipeline_path is None     FileNotFoundError: If the object configuration_path is not an existing file     FileNotFoundError: If the object sklearn_pipeline_path is not an existing file     FileNotFoundError: If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_gbt_regressor.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n    '''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n    # Retrieve args\n    configuration_path = kwargs.get('configuration_path', None)\n    sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n    preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n    # Checks\n    if configuration_path is None:\n        raise ValueError(\"The argument configuration_path can't be None\")\n    if sklearn_pipeline_path is None:\n        raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n    if preprocess_pipeline_path is None:\n        raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n    if not os.path.exists(configuration_path):\n        raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n    if not os.path.exists(sklearn_pipeline_path):\n        raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n    if not os.path.exists(preprocess_pipeline_path):\n        raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n    # Load confs\n    with open(configuration_path, 'r', encoding='utf-8') as f:\n        configs = json.load(f)\n\n    # Set class vars\n    # self.model_name = # Keep the created name\n    # self.model_dir = # Keep the created folder\n    self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n    self.trained = configs.get('trained', True)  # Consider trained by default\n    # Try to read the following attributes from configs and, if absent, keep the current one\n    for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                      'random_seed', 'level_save']:\n        setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n    # Reload pipeline model\n    with open(sklearn_pipeline_path, 'rb') as f:\n        self.pipeline = pickle.load(f)\n\n    # Reload pipeline elements\n    self.gbt = self.pipeline['gbt']\n\n    # Reload pipeline preprocessing\n    with open(preprocess_pipeline_path, 'rb') as f:\n        self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_kernel_ridge_regressor/","title":"Model kernel ridge regressor","text":""},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_kernel_ridge_regressor/#template_num.models_training.regressors.models_sklearn.model_kernel_ridge_regressor.ModelKernelRidgeRegressor","title":"<code>ModelKernelRidgeRegressor</code>","text":"<p>             Bases: <code>ModelRegressorMixin</code>, <code>ModelPipeline</code></p> <p>Kernel Ridge model for regression</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_kernel_ridge_regressor.py</code> <pre><code>class ModelKernelRidgeRegressor(ModelRegressorMixin, ModelPipeline):\n    '''Kernel Ridge model for regression'''\n\n    _default_name = 'model_kernel_ridge_regressor'\n\n    def __init__(self, kernel_ridge_params: Union[dict, None] = None, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n\n        Kwargs:\n            kernel_ridge_params (dict) : Parameters for the Kernel Ridge\n        '''\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Manage model\n        if kernel_ridge_params is None:\n            kernel_ridge_params = {}\n        self.kernel_ridge = KernelRidge(**kernel_ridge_params)\n        # We define a pipeline in order to be compatible with other models\n        self.pipeline = Pipeline([('kernel_ridge', self.kernel_ridge)])\n\n    def reload_from_standalone(self, **kwargs) -&gt; None:\n        '''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n        # Retrieve args\n        configuration_path = kwargs.get('configuration_path', None)\n        sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n        preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n        # Checks\n        if configuration_path is None:\n            raise ValueError(\"The argument configuration_path can't be None\")\n        if sklearn_pipeline_path is None:\n            raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n        if preprocess_pipeline_path is None:\n            raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n        if not os.path.exists(configuration_path):\n            raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n        if not os.path.exists(sklearn_pipeline_path):\n            raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n        if not os.path.exists(preprocess_pipeline_path):\n            raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n        # Load confs\n        with open(configuration_path, 'r', encoding='utf-8') as f:\n            configs = json.load(f)\n\n        # Set class vars\n        # self.model_name = # Keep the created name\n        # self.model_dir = # Keep the created folder\n        self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n        self.trained = configs.get('trained', True)  # Consider trained by default\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                          'level_save']:\n            setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n        # Reload pipeline model\n        with open(sklearn_pipeline_path, 'rb') as f:\n            self.pipeline = pickle.load(f)\n\n        # Reload pipeline elements\n        self.kernel_ridge = self.pipeline['kernel_ridge']\n\n        # Reload pipeline preprocessing\n        with open(preprocess_pipeline_path, 'rb') as f:\n            self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_kernel_ridge_regressor/#template_num.models_training.regressors.models_sklearn.model_kernel_ridge_regressor.ModelKernelRidgeRegressor.__init__","title":"<code>__init__(kernel_ridge_params=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)</p> Kwargs <p>kernel_ridge_params (dict) : Parameters for the Kernel Ridge</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_kernel_ridge_regressor.py</code> <pre><code>def __init__(self, kernel_ridge_params: Union[dict, None] = None, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n\n    Kwargs:\n        kernel_ridge_params (dict) : Parameters for the Kernel Ridge\n    '''\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Manage model\n    if kernel_ridge_params is None:\n        kernel_ridge_params = {}\n    self.kernel_ridge = KernelRidge(**kernel_ridge_params)\n    # We define a pipeline in order to be compatible with other models\n    self.pipeline = Pipeline([('kernel_ridge', self.kernel_ridge)])\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_kernel_ridge_regressor/#template_num.models_training.regressors.models_sklearn.model_kernel_ridge_regressor.ModelKernelRidgeRegressor.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:     ValueError: If configuration_path is None     ValueError: If sklearn_pipeline_path is None     ValueError: If preprocess_pipeline_path is None     FileNotFoundError: If the object configuration_path is not an existing file     FileNotFoundError: If the object sklearn_pipeline_path is not an existing file     FileNotFoundError: If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_kernel_ridge_regressor.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n    '''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n    # Retrieve args\n    configuration_path = kwargs.get('configuration_path', None)\n    sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n    preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n    # Checks\n    if configuration_path is None:\n        raise ValueError(\"The argument configuration_path can't be None\")\n    if sklearn_pipeline_path is None:\n        raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n    if preprocess_pipeline_path is None:\n        raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n    if not os.path.exists(configuration_path):\n        raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n    if not os.path.exists(sklearn_pipeline_path):\n        raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n    if not os.path.exists(preprocess_pipeline_path):\n        raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n    # Load confs\n    with open(configuration_path, 'r', encoding='utf-8') as f:\n        configs = json.load(f)\n\n    # Set class vars\n    # self.model_name = # Keep the created name\n    # self.model_dir = # Keep the created folder\n    self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n    self.trained = configs.get('trained', True)  # Consider trained by default\n    # Try to read the following attributes from configs and, if absent, keep the current one\n    for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                      'level_save']:\n        setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n    # Reload pipeline model\n    with open(sklearn_pipeline_path, 'rb') as f:\n        self.pipeline = pickle.load(f)\n\n    # Reload pipeline elements\n    self.kernel_ridge = self.pipeline['kernel_ridge']\n\n    # Reload pipeline preprocessing\n    with open(preprocess_pipeline_path, 'rb') as f:\n        self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_knn_regressor/","title":"Model knn regressor","text":""},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_knn_regressor/#template_num.models_training.regressors.models_sklearn.model_knn_regressor.ModelKNNRegressor","title":"<code>ModelKNNRegressor</code>","text":"<p>             Bases: <code>ModelRegressorMixin</code>, <code>ModelPipeline</code></p> <p>K-nearest Neighbors model for regression</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_knn_regressor.py</code> <pre><code>class ModelKNNRegressor(ModelRegressorMixin, ModelPipeline):\n    '''K-nearest Neighbors model for regression'''\n\n    _default_name = 'model_knn_regressor'\n\n    def __init__(self, knn_params: Union[dict, None] = None, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n\n        Kwargs:\n            knn_params (dict) : Parameters for the K-nearest Neighbors\n        '''\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Manage model\n        if knn_params is None:\n            knn_params = {}\n        self.knn = KNeighborsRegressor(**knn_params)\n        # We define a pipeline in order to be compatible with other models\n        self.pipeline = Pipeline([('knn', self.knn)])\n\n    def reload_from_standalone(self, **kwargs) -&gt; None:\n        '''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n        # Retrieve args\n        configuration_path = kwargs.get('configuration_path', None)\n        sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n        preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n        # Checks\n        if configuration_path is None:\n            raise ValueError(\"The argument configuration_path can't be None\")\n        if sklearn_pipeline_path is None:\n            raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n        if preprocess_pipeline_path is None:\n            raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n        if not os.path.exists(configuration_path):\n            raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n        if not os.path.exists(sklearn_pipeline_path):\n            raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n        if not os.path.exists(preprocess_pipeline_path):\n            raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n        # Load confs\n        with open(configuration_path, 'r', encoding='utf-8') as f:\n            configs = json.load(f)\n\n        # Set class vars\n        # self.model_name = # Keep the created name\n        # self.model_dir = # Keep the created folder\n        self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n        self.trained = configs.get('trained', True)  # Consider trained by default\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                          'level_save']:\n            setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n        # Reload pipeline model\n        with open(sklearn_pipeline_path, 'rb') as f:\n            self.pipeline = pickle.load(f)\n\n        # Reload pipeline elements\n        self.knn = self.pipeline['knn']\n\n        # Reload pipeline preprocessing\n        with open(preprocess_pipeline_path, 'rb') as f:\n            self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_knn_regressor/#template_num.models_training.regressors.models_sklearn.model_knn_regressor.ModelKNNRegressor.__init__","title":"<code>__init__(knn_params=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)</p> Kwargs <p>knn_params (dict) : Parameters for the K-nearest Neighbors</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_knn_regressor.py</code> <pre><code>def __init__(self, knn_params: Union[dict, None] = None, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n\n    Kwargs:\n        knn_params (dict) : Parameters for the K-nearest Neighbors\n    '''\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Manage model\n    if knn_params is None:\n        knn_params = {}\n    self.knn = KNeighborsRegressor(**knn_params)\n    # We define a pipeline in order to be compatible with other models\n    self.pipeline = Pipeline([('knn', self.knn)])\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_knn_regressor/#template_num.models_training.regressors.models_sklearn.model_knn_regressor.ModelKNNRegressor.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:     ValueError: If configuration_path is None     ValueError: If sklearn_pipeline_path is None     ValueError: If preprocess_pipeline_path is None     FileNotFoundError: If the object configuration_path is not an existing file     FileNotFoundError: If the object sklearn_pipeline_path is not an existing file     FileNotFoundError: If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_knn_regressor.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n    '''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n    # Retrieve args\n    configuration_path = kwargs.get('configuration_path', None)\n    sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n    preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n    # Checks\n    if configuration_path is None:\n        raise ValueError(\"The argument configuration_path can't be None\")\n    if sklearn_pipeline_path is None:\n        raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n    if preprocess_pipeline_path is None:\n        raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n    if not os.path.exists(configuration_path):\n        raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n    if not os.path.exists(sklearn_pipeline_path):\n        raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n    if not os.path.exists(preprocess_pipeline_path):\n        raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n    # Load confs\n    with open(configuration_path, 'r', encoding='utf-8') as f:\n        configs = json.load(f)\n\n    # Set class vars\n    # self.model_name = # Keep the created name\n    # self.model_dir = # Keep the created folder\n    self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n    self.trained = configs.get('trained', True)  # Consider trained by default\n    # Try to read the following attributes from configs and, if absent, keep the current one\n    for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                      'level_save']:\n        setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n    # Reload pipeline model\n    with open(sklearn_pipeline_path, 'rb') as f:\n        self.pipeline = pickle.load(f)\n\n    # Reload pipeline elements\n    self.knn = self.pipeline['knn']\n\n    # Reload pipeline preprocessing\n    with open(preprocess_pipeline_path, 'rb') as f:\n        self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_lgbm_regressor/","title":"Model lgbm regressor","text":""},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_lgbm_regressor/#template_num.models_training.regressors.models_sklearn.model_lgbm_regressor.ModelLGBMRegressor","title":"<code>ModelLGBMRegressor</code>","text":"<p>             Bases: <code>ModelRegressorMixin</code>, <code>ModelPipeline</code></p> <p>Light GBM model for regression</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_lgbm_regressor.py</code> <pre><code>class ModelLGBMRegressor(ModelRegressorMixin, ModelPipeline):\n    '''Light GBM model for regression'''\n\n    _default_name = 'model_lgbm_regressor'\n\n    def __init__(self, lgbm_params: Union[dict, None] = None, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n\n        Kwargs:\n            lgbm_params (dict) : Parameters for the Light GBM\n        '''\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Manage model\n        if lgbm_params is None:\n            lgbm_params = {}\n        lgbm_params[\"random_state\"] = self.random_seed\n        self.lgbm = LGBMRegressor(**lgbm_params)\n        # We define a pipeline in order to be compatible with other models\n        self.pipeline = Pipeline([('lgbm', self.lgbm)])\n\n    def reload_from_standalone(self, **kwargs) -&gt; None:\n        '''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n        # Retrieve args\n        configuration_path = kwargs.get('configuration_path', None)\n        sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n        preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n        # Checks\n        if configuration_path is None:\n            raise ValueError(\"The argument configuration_path can't be None\")\n        if sklearn_pipeline_path is None:\n            raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n        if preprocess_pipeline_path is None:\n            raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n        if not os.path.exists(configuration_path):\n            raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n        if not os.path.exists(sklearn_pipeline_path):\n            raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n        if not os.path.exists(preprocess_pipeline_path):\n            raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n        # Load confs\n        with open(configuration_path, 'r', encoding='utf-8') as f:\n            configs = json.load(f)\n\n        # Set class vars\n        # self.model_name = # Keep the created name\n        # self.model_dir = # Keep the created folder\n        self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n        self.trained = configs.get('trained', True)  # Consider trained by default\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                          'random_seed', 'level_save']:\n            setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n        # Reload pipeline model\n        with open(sklearn_pipeline_path, 'rb') as f:\n            self.pipeline = pickle.load(f)\n\n        # Reload pipeline elements\n        self.lgbm = self.pipeline['lgbm']\n\n        # Reload pipeline preprocessing\n        with open(preprocess_pipeline_path, 'rb') as f:\n            self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_lgbm_regressor/#template_num.models_training.regressors.models_sklearn.model_lgbm_regressor.ModelLGBMRegressor.__init__","title":"<code>__init__(lgbm_params=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)</p> Kwargs <p>lgbm_params (dict) : Parameters for the Light GBM</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_lgbm_regressor.py</code> <pre><code>def __init__(self, lgbm_params: Union[dict, None] = None, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n\n    Kwargs:\n        lgbm_params (dict) : Parameters for the Light GBM\n    '''\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Manage model\n    if lgbm_params is None:\n        lgbm_params = {}\n    lgbm_params[\"random_state\"] = self.random_seed\n    self.lgbm = LGBMRegressor(**lgbm_params)\n    # We define a pipeline in order to be compatible with other models\n    self.pipeline = Pipeline([('lgbm', self.lgbm)])\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_lgbm_regressor/#template_num.models_training.regressors.models_sklearn.model_lgbm_regressor.ModelLGBMRegressor.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:     ValueError: If configuration_path is None     ValueError: If sklearn_pipeline_path is None     ValueError: If preprocess_pipeline_path is None     FileNotFoundError: If the object configuration_path is not an existing file     FileNotFoundError: If the object sklearn_pipeline_path is not an existing file     FileNotFoundError: If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_lgbm_regressor.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n    '''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n    # Retrieve args\n    configuration_path = kwargs.get('configuration_path', None)\n    sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n    preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n    # Checks\n    if configuration_path is None:\n        raise ValueError(\"The argument configuration_path can't be None\")\n    if sklearn_pipeline_path is None:\n        raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n    if preprocess_pipeline_path is None:\n        raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n    if not os.path.exists(configuration_path):\n        raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n    if not os.path.exists(sklearn_pipeline_path):\n        raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n    if not os.path.exists(preprocess_pipeline_path):\n        raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n    # Load confs\n    with open(configuration_path, 'r', encoding='utf-8') as f:\n        configs = json.load(f)\n\n    # Set class vars\n    # self.model_name = # Keep the created name\n    # self.model_dir = # Keep the created folder\n    self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n    self.trained = configs.get('trained', True)  # Consider trained by default\n    # Try to read the following attributes from configs and, if absent, keep the current one\n    for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                      'random_seed', 'level_save']:\n        setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n    # Reload pipeline model\n    with open(sklearn_pipeline_path, 'rb') as f:\n        self.pipeline = pickle.load(f)\n\n    # Reload pipeline elements\n    self.lgbm = self.pipeline['lgbm']\n\n    # Reload pipeline preprocessing\n    with open(preprocess_pipeline_path, 'rb') as f:\n        self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_pls_regressor/","title":"Model pls regressor","text":""},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_pls_regressor/#template_num.models_training.regressors.models_sklearn.model_pls_regressor.ModelPLSRegressor","title":"<code>ModelPLSRegressor</code>","text":"<p>             Bases: <code>ModelRegressorMixin</code>, <code>ModelPipeline</code></p> <p>Partial Least Squares model for regression</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_pls_regressor.py</code> <pre><code>class ModelPLSRegressor(ModelRegressorMixin, ModelPipeline):\n    '''Partial Least Squares model for regression'''\n\n    _default_name = 'model_pls_regressor'\n\n    def __init__(self, pls_params: Union[dict, None] = None, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n\n        Kwargs:\n            pls_params (dict) : Parameters for the Partial Least Squares\n        '''\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Manage model\n        if pls_params is None:\n            pls_params = {}\n        self.pls = PLSRegression(**pls_params)\n        # We define a pipeline in order to be compatible with other models\n        self.pipeline = Pipeline([('pls', self.pls)])\n\n    def reload_from_standalone(self, **kwargs) -&gt; None:\n        '''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n        # Retrieve args\n        configuration_path = kwargs.get('configuration_path', None)\n        sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n        preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n        # Checks\n        if configuration_path is None:\n            raise ValueError(\"The argument configuration_path can't be None\")\n        if sklearn_pipeline_path is None:\n            raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n        if preprocess_pipeline_path is None:\n            raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n        if not os.path.exists(configuration_path):\n            raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n        if not os.path.exists(sklearn_pipeline_path):\n            raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n        if not os.path.exists(preprocess_pipeline_path):\n            raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n        # Load confs\n        with open(configuration_path, 'r', encoding='utf-8') as f:\n            configs = json.load(f)\n\n        # Set class vars\n        # self.model_name = # Keep the created name\n        # self.model_dir = # Keep the created folder\n        self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n        self.trained = configs.get('trained', True)  # Consider trained by default\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                          'level_save']:\n            setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n        # Reload pipeline model\n        with open(sklearn_pipeline_path, 'rb') as f:\n            self.pipeline = pickle.load(f)\n\n        # Reload pipeline elements\n        self.pls = self.pipeline['pls']\n\n        # Reload pipeline preprocessing\n        with open(preprocess_pipeline_path, 'rb') as f:\n            self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_pls_regressor/#template_num.models_training.regressors.models_sklearn.model_pls_regressor.ModelPLSRegressor.__init__","title":"<code>__init__(pls_params=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)</p> Kwargs <p>pls_params (dict) : Parameters for the Partial Least Squares</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_pls_regressor.py</code> <pre><code>def __init__(self, pls_params: Union[dict, None] = None, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n\n    Kwargs:\n        pls_params (dict) : Parameters for the Partial Least Squares\n    '''\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Manage model\n    if pls_params is None:\n        pls_params = {}\n    self.pls = PLSRegression(**pls_params)\n    # We define a pipeline in order to be compatible with other models\n    self.pipeline = Pipeline([('pls', self.pls)])\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_pls_regressor/#template_num.models_training.regressors.models_sklearn.model_pls_regressor.ModelPLSRegressor.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:     ValueError: If configuration_path is None     ValueError: If sklearn_pipeline_path is None     ValueError: If preprocess_pipeline_path is None     FileNotFoundError: If the object configuration_path is not an existing file     FileNotFoundError: If the object sklearn_pipeline_path is not an existing file     FileNotFoundError: If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_pls_regressor.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n    '''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n    # Retrieve args\n    configuration_path = kwargs.get('configuration_path', None)\n    sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n    preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n    # Checks\n    if configuration_path is None:\n        raise ValueError(\"The argument configuration_path can't be None\")\n    if sklearn_pipeline_path is None:\n        raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n    if preprocess_pipeline_path is None:\n        raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n    if not os.path.exists(configuration_path):\n        raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n    if not os.path.exists(sklearn_pipeline_path):\n        raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n    if not os.path.exists(preprocess_pipeline_path):\n        raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n    # Load confs\n    with open(configuration_path, 'r', encoding='utf-8') as f:\n        configs = json.load(f)\n\n    # Set class vars\n    # self.model_name = # Keep the created name\n    # self.model_dir = # Keep the created folder\n    self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n    self.trained = configs.get('trained', True)  # Consider trained by default\n    # Try to read the following attributes from configs and, if absent, keep the current one\n    for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                      'level_save']:\n        setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n    # Reload pipeline model\n    with open(sklearn_pipeline_path, 'rb') as f:\n        self.pipeline = pickle.load(f)\n\n    # Reload pipeline elements\n    self.pls = self.pipeline['pls']\n\n    # Reload pipeline preprocessing\n    with open(preprocess_pipeline_path, 'rb') as f:\n        self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_rf_regressor/","title":"Model rf regressor","text":""},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_rf_regressor/#template_num.models_training.regressors.models_sklearn.model_rf_regressor.ModelRFRegressor","title":"<code>ModelRFRegressor</code>","text":"<p>             Bases: <code>ModelRegressorMixin</code>, <code>ModelPipeline</code></p> <p>Random Forest model for regression</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_rf_regressor.py</code> <pre><code>class ModelRFRegressor(ModelRegressorMixin, ModelPipeline):\n    '''Random Forest model for regression'''\n\n    _default_name = 'model_rf_regressor'\n\n    def __init__(self, rf_params: Union[dict, None] = None, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n\n        Kwargs:\n            rf_params (dict) : Parameters for the Random Forest\n        '''\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Manage model\n        if rf_params is None:\n            rf_params = {}\n        rf_params[\"random_state\"] = self.random_seed\n        self.rf = RandomForestRegressor(**rf_params)\n        # We define a pipeline in order to be compatible with other models\n        self.pipeline = Pipeline([('rf', self.rf)])\n\n    def reload_from_standalone(self, **kwargs) -&gt; None:\n        '''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n        # Retrieve args\n        configuration_path = kwargs.get('configuration_path', None)\n        sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n        preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n        # Checks\n        if configuration_path is None:\n            raise ValueError(\"The argument configuration_path can't be None\")\n        if sklearn_pipeline_path is None:\n            raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n        if preprocess_pipeline_path is None:\n            raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n        if not os.path.exists(configuration_path):\n            raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n        if not os.path.exists(sklearn_pipeline_path):\n            raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n        if not os.path.exists(preprocess_pipeline_path):\n            raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n        # Load confs\n        with open(configuration_path, 'r', encoding='utf-8') as f:\n            configs = json.load(f)\n\n        # Set class vars\n        # self.model_name = # Keep the created name\n        # self.model_dir = # Keep the created folder\n        self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n        self.trained = configs.get('trained', True)  # Consider trained by default\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                          'random_seed', 'level_save']:\n            setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n        # Reload pipeline model\n        with open(sklearn_pipeline_path, 'rb') as f:\n            self.pipeline = pickle.load(f)\n\n        # Reload pipeline elements\n        self.rf = self.pipeline['rf']\n\n        # Reload pipeline preprocessing\n        with open(preprocess_pipeline_path, 'rb') as f:\n            self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_rf_regressor/#template_num.models_training.regressors.models_sklearn.model_rf_regressor.ModelRFRegressor.__init__","title":"<code>__init__(rf_params=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)</p> Kwargs <p>rf_params (dict) : Parameters for the Random Forest</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_rf_regressor.py</code> <pre><code>def __init__(self, rf_params: Union[dict, None] = None, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n\n    Kwargs:\n        rf_params (dict) : Parameters for the Random Forest\n    '''\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Manage model\n    if rf_params is None:\n        rf_params = {}\n    rf_params[\"random_state\"] = self.random_seed\n    self.rf = RandomForestRegressor(**rf_params)\n    # We define a pipeline in order to be compatible with other models\n    self.pipeline = Pipeline([('rf', self.rf)])\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_rf_regressor/#template_num.models_training.regressors.models_sklearn.model_rf_regressor.ModelRFRegressor.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:     ValueError: If configuration_path is None     ValueError: If sklearn_pipeline_path is None     ValueError: If preprocess_pipeline_path is None     FileNotFoundError: If the object configuration_path is not an existing file     FileNotFoundError: If the object sklearn_pipeline_path is not an existing file     FileNotFoundError: If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_rf_regressor.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n    '''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n    # Retrieve args\n    configuration_path = kwargs.get('configuration_path', None)\n    sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n    preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n    # Checks\n    if configuration_path is None:\n        raise ValueError(\"The argument configuration_path can't be None\")\n    if sklearn_pipeline_path is None:\n        raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n    if preprocess_pipeline_path is None:\n        raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n    if not os.path.exists(configuration_path):\n        raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n    if not os.path.exists(sklearn_pipeline_path):\n        raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n    if not os.path.exists(preprocess_pipeline_path):\n        raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n    # Load confs\n    with open(configuration_path, 'r', encoding='utf-8') as f:\n        configs = json.load(f)\n\n    # Set class vars\n    # self.model_name = # Keep the created name\n    # self.model_dir = # Keep the created folder\n    self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n    self.trained = configs.get('trained', True)  # Consider trained by default\n    # Try to read the following attributes from configs and, if absent, keep the current one\n    for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                      'random_seed', 'level_save']:\n        setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n    # Reload pipeline model\n    with open(sklearn_pipeline_path, 'rb') as f:\n        self.pipeline = pickle.load(f)\n\n    # Reload pipeline elements\n    self.rf = self.pipeline['rf']\n\n    # Reload pipeline preprocessing\n    with open(preprocess_pipeline_path, 'rb') as f:\n        self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_sgd_regressor/","title":"Model sgd regressor","text":""},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_sgd_regressor/#template_num.models_training.regressors.models_sklearn.model_sgd_regressor.ModelSGDRegressor","title":"<code>ModelSGDRegressor</code>","text":"<p>             Bases: <code>ModelRegressorMixin</code>, <code>ModelPipeline</code></p> <p>Stochastic Gradient Descent model for regression</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_sgd_regressor.py</code> <pre><code>class ModelSGDRegressor(ModelRegressorMixin, ModelPipeline):\n    '''Stochastic Gradient Descent model for regression'''\n\n    _default_name = 'model_sgd_regressor'\n\n    def __init__(self, sgd_params: Union[dict, None] = None, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n\n        Kwargs:\n            sgd_params (dict) : Parameters for the Stochastic Gradient Descent\n        '''\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Manage model\n        if sgd_params is None:\n            sgd_params = {}\n        sgd_params[\"random_state\"] = self.random_seed\n        self.sgd = SGDRegressor(**sgd_params)\n        # We define a pipeline in order to be compatible with other models\n        self.pipeline = Pipeline([('sgd', self.sgd)])\n\n    def reload_from_standalone(self, **kwargs) -&gt; None:\n        '''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n        # Retrieve args\n        configuration_path = kwargs.get('configuration_path', None)\n        sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n        preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n        # Checks\n        if configuration_path is None:\n            raise ValueError(\"The argument configuration_path can't be None\")\n        if sklearn_pipeline_path is None:\n            raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n        if preprocess_pipeline_path is None:\n            raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n        if not os.path.exists(configuration_path):\n            raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n        if not os.path.exists(sklearn_pipeline_path):\n            raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n        if not os.path.exists(preprocess_pipeline_path):\n            raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n        # Load confs\n        with open(configuration_path, 'r', encoding='utf-8') as f:\n            configs = json.load(f)\n\n        # Set class vars\n        # self.model_name = # Keep the created name\n        # self.model_dir = # Keep the created folder\n        self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n        self.trained = configs.get('trained', True)  # Consider trained by default\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                          'random_seed', 'level_save']:\n            setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n        # Reload pipeline model\n        with open(sklearn_pipeline_path, 'rb') as f:\n            self.pipeline = pickle.load(f)\n\n        # Reload pipeline elements\n        self.sgd = self.pipeline['sgd']\n\n        # Reload pipeline preprocessing\n        with open(preprocess_pipeline_path, 'rb') as f:\n            self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_sgd_regressor/#template_num.models_training.regressors.models_sklearn.model_sgd_regressor.ModelSGDRegressor.__init__","title":"<code>__init__(sgd_params=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)</p> Kwargs <p>sgd_params (dict) : Parameters for the Stochastic Gradient Descent</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_sgd_regressor.py</code> <pre><code>def __init__(self, sgd_params: Union[dict, None] = None, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n\n    Kwargs:\n        sgd_params (dict) : Parameters for the Stochastic Gradient Descent\n    '''\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Manage model\n    if sgd_params is None:\n        sgd_params = {}\n    sgd_params[\"random_state\"] = self.random_seed\n    self.sgd = SGDRegressor(**sgd_params)\n    # We define a pipeline in order to be compatible with other models\n    self.pipeline = Pipeline([('sgd', self.sgd)])\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_sgd_regressor/#template_num.models_training.regressors.models_sklearn.model_sgd_regressor.ModelSGDRegressor.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:     ValueError: If configuration_path is None     ValueError: If sklearn_pipeline_path is None     ValueError: If preprocess_pipeline_path is None     FileNotFoundError: If the object configuration_path is not an existing file     FileNotFoundError: If the object sklearn_pipeline_path is not an existing file     FileNotFoundError: If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_sgd_regressor.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n    '''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n    # Retrieve args\n    configuration_path = kwargs.get('configuration_path', None)\n    sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n    preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n    # Checks\n    if configuration_path is None:\n        raise ValueError(\"The argument configuration_path can't be None\")\n    if sklearn_pipeline_path is None:\n        raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n    if preprocess_pipeline_path is None:\n        raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n    if not os.path.exists(configuration_path):\n        raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n    if not os.path.exists(sklearn_pipeline_path):\n        raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n    if not os.path.exists(preprocess_pipeline_path):\n        raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n    # Load confs\n    with open(configuration_path, 'r', encoding='utf-8') as f:\n        configs = json.load(f)\n\n    # Set class vars\n    # self.model_name = # Keep the created name\n    # self.model_dir = # Keep the created folder\n    self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n    self.trained = configs.get('trained', True)  # Consider trained by default\n    # Try to read the following attributes from configs and, if absent, keep the current one\n    for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                      'random_seed', 'level_save']:\n        setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n    # Reload pipeline model\n    with open(sklearn_pipeline_path, 'rb') as f:\n        self.pipeline = pickle.load(f)\n\n    # Reload pipeline elements\n    self.sgd = self.pipeline['sgd']\n\n    # Reload pipeline preprocessing\n    with open(preprocess_pipeline_path, 'rb') as f:\n        self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_svr_regressor/","title":"Model svr regressor","text":""},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_svr_regressor/#template_num.models_training.regressors.models_sklearn.model_svr_regressor.ModelSVRRegressor","title":"<code>ModelSVRRegressor</code>","text":"<p>             Bases: <code>ModelRegressorMixin</code>, <code>ModelPipeline</code></p> <p>Support Vector Regression model for regression</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_svr_regressor.py</code> <pre><code>class ModelSVRRegressor(ModelRegressorMixin, ModelPipeline):\n    '''Support Vector Regression model for regression'''\n\n    _default_name = 'model_svr_regressor'\n\n    def __init__(self, svr_params: Union[dict, None] = None, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n\n        Kwargs:\n            svr_params (dict) : Parameters for the Support Vector Regression\n        '''\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Manage model\n        if svr_params is None:\n            svr_params = {}\n        self.svr = SVR(**svr_params)\n        # We define a pipeline in order to be compatible with other models\n        self.pipeline = Pipeline([('svr', self.svr)])\n\n    def reload_from_standalone(self, **kwargs) -&gt; None:\n        '''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n        # Retrieve args\n        configuration_path = kwargs.get('configuration_path', None)\n        sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n        preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n        # Checks\n        if configuration_path is None:\n            raise ValueError(\"The argument configuration_path can't be None\")\n        if sklearn_pipeline_path is None:\n            raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n        if preprocess_pipeline_path is None:\n            raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n        if not os.path.exists(configuration_path):\n            raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n        if not os.path.exists(sklearn_pipeline_path):\n            raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n        if not os.path.exists(preprocess_pipeline_path):\n            raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n        # Load confs\n        with open(configuration_path, 'r', encoding='utf-8') as f:\n            configs = json.load(f)\n\n        # Set class vars\n        # self.model_name = # Keep the created name\n        # self.model_dir = # Keep the created folder\n        self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n        self.trained = configs.get('trained', True)  # Consider trained by default\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                          'level_save']:\n            setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n        # Reload pipeline model\n        with open(sklearn_pipeline_path, 'rb') as f:\n            self.pipeline = pickle.load(f)\n\n        # Reload pipeline elements\n        self.svr = self.pipeline['svr']\n\n        # Reload pipeline preprocessing\n        with open(preprocess_pipeline_path, 'rb') as f:\n            self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_svr_regressor/#template_num.models_training.regressors.models_sklearn.model_svr_regressor.ModelSVRRegressor.__init__","title":"<code>__init__(svr_params=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)</p> Kwargs <p>svr_params (dict) : Parameters for the Support Vector Regression</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_svr_regressor.py</code> <pre><code>def __init__(self, svr_params: Union[dict, None] = None, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n\n    Kwargs:\n        svr_params (dict) : Parameters for the Support Vector Regression\n    '''\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Manage model\n    if svr_params is None:\n        svr_params = {}\n    self.svr = SVR(**svr_params)\n    # We define a pipeline in order to be compatible with other models\n    self.pipeline = Pipeline([('svr', self.svr)])\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_svr_regressor/#template_num.models_training.regressors.models_sklearn.model_svr_regressor.ModelSVRRegressor.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:     ValueError: If configuration_path is None     ValueError: If sklearn_pipeline_path is None     ValueError: If preprocess_pipeline_path is None     FileNotFoundError: If the object configuration_path is not an existing file     FileNotFoundError: If the object sklearn_pipeline_path is not an existing file     FileNotFoundError: If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_svr_regressor.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n    '''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n    # Retrieve args\n    configuration_path = kwargs.get('configuration_path', None)\n    sklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n    preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n    # Checks\n    if configuration_path is None:\n        raise ValueError(\"The argument configuration_path can't be None\")\n    if sklearn_pipeline_path is None:\n        raise ValueError(\"The argument sklearn_pipeline_path can't be None\")\n    if preprocess_pipeline_path is None:\n        raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n    if not os.path.exists(configuration_path):\n        raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n    if not os.path.exists(sklearn_pipeline_path):\n        raise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n    if not os.path.exists(preprocess_pipeline_path):\n        raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n    # Load confs\n    with open(configuration_path, 'r', encoding='utf-8') as f:\n        configs = json.load(f)\n\n    # Set class vars\n    # self.model_name = # Keep the created name\n    # self.model_dir = # Keep the created folder\n    self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n    self.trained = configs.get('trained', True)  # Consider trained by default\n    # Try to read the following attributes from configs and, if absent, keep the current one\n    for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                      'level_save']:\n        setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n    # Reload pipeline model\n    with open(sklearn_pipeline_path, 'rb') as f:\n        self.pipeline = pickle.load(f)\n\n    # Reload pipeline elements\n    self.svr = self.pipeline['svr']\n\n    # Reload pipeline preprocessing\n    with open(preprocess_pipeline_path, 'rb') as f:\n        self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_tensorflow/","title":"Models tensorflow","text":""},{"location":"reference/template_num/models_training/regressors/models_tensorflow/model_dense_regressor/","title":"Model dense regressor","text":""},{"location":"reference/template_num/models_training/regressors/models_tensorflow/model_dense_regressor/#template_num.models_training.regressors.models_tensorflow.model_dense_regressor.ModelDenseRegressor","title":"<code>ModelDenseRegressor</code>","text":"<p>             Bases: <code>ModelRegressorMixin</code>, <code>ModelKeras</code></p> <p>Dense model for regression</p> Source code in <code>template_num/models_training/regressors/models_tensorflow/model_dense_regressor.py</code> <pre><code>class ModelDenseRegressor(ModelRegressorMixin, ModelKeras):\n    '''Dense model for regression'''\n\n    _default_name = 'model_dense_regressor'\n\n    def __init__(self, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelClass, ModelKeras &amp; ModelRegressor for more arguments)'''\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n    def _get_model(self) -&gt; Model:\n        '''Gets a model structure - returns the instance model instead if already defined\n\n        Returns:\n            (Model): A model\n        '''\n        # Return model if already set\n        if self.model is not None:\n            return self.model\n\n        # Get input/output dimensions\n        input_dim = len(self.x_col)\n\n        # Get random_state\n        random_state = np.random.RandomState(self.random_seed)\n        limit = int(1e9)\n\n        # Process\n        input_layer = Input(shape=(input_dim,))\n\n        x = Dense(64, activation=None, kernel_initializer=HeUniform(random_state.randint(limit)))(input_layer)\n        x = BatchNormalization(momentum=0.9)(x)\n        x = ELU(alpha=1.0)(x)\n        x = Dropout(0.2, seed=random_state.randint(limit))(x)\n\n        x = Dense(64, activation=None, kernel_initializer=HeUniform(random_state.randint(limit)))(x)\n        x = BatchNormalization(momentum=0.9)(x)\n        x = ELU(alpha=1.0)(x)\n        x = Dropout(0.2, seed=random_state.randint(limit))(x)\n\n        # Last layer\n        activation = None  # 'relu' if result should be &gt; 0\n        out = Dense(1, activation=activation, kernel_initializer=GlorotUniform(random_state.randint(limit)))(x)\n\n        # Set model\n        model = Model(inputs=input_layer, outputs=[out])\n\n        # Set optimizer\n        lr = self.keras_params['learning_rate'] if 'learning_rate' in self.keras_params.keys() else 0.001\n        decay = self.keras_params['decay'] if 'decay' in self.keras_params.keys() else 0.0\n        self.logger.info(f\"Learning rate: {lr}\")\n        self.logger.info(f\"Decay: {decay}\")\n        optimizer = Adam(lr=lr, decay=decay)\n\n        # Set loss &amp; metrics\n        loss = 'mean_squared_error'  # could be 'mean_absolute_error'\n        metrics: List[Union[str, Callable]] = ['mean_squared_error', 'mean_absolute_error', utils_deep_keras.root_mean_squared_error]\n\n        # Compile model\n        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n        if self.logger.getEffectiveLevel() &lt; logging.ERROR:\n            model.summary()\n\n        # Try to save model as png if level_save &gt; 'LOW'\n        if self.level_save in ['MEDIUM', 'HIGH']:\n            self._save_model_png(model)\n\n        # Return\n        return model\n\n    def reload_from_standalone(self, **kwargs) -&gt; None:\n        '''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            hdf5_path (str): Path to hdf5 file\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If hdf5_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object hdf5_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n        # Retrieve args\n        configuration_path = kwargs.get('configuration_path', None)\n        hdf5_path = kwargs.get('hdf5_path', None)\n        preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n        # Checks\n        if configuration_path is None:\n            raise ValueError(\"The argument configuration_path can't be None\")\n        if hdf5_path is None:\n            raise ValueError(\"The argument hdf5_path can't be None\")\n        if preprocess_pipeline_path is None:\n            raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n        if not os.path.exists(configuration_path):\n            raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n        if not os.path.exists(hdf5_path):\n            raise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\n        if not os.path.exists(preprocess_pipeline_path):\n            raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n        # Load confs\n        with open(configuration_path, 'r', encoding='utf-8') as f:\n            configs = json.load(f)\n\n        # Set class vars\n        # self.model_name = # Keep the created name\n        # self.model_dir = # Keep the created folder\n        self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n        self.trained = configs.get('trained', True)  # Consider trained by default\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                          'random_seed', 'level_save', 'batch_size', 'epochs', 'validation_split',\n                          'patience', 'keras_params']:\n            setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n        # Reload model\n        self.model = load_model_keras(hdf5_path, custom_objects=self.custom_objects)\n\n        # Save best hdf5 in new folder\n        new_hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\n        shutil.copyfile(hdf5_path, new_hdf5_path)\n\n        # Reload pipeline preprocessing\n        with open(preprocess_pipeline_path, 'rb') as f:\n            self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_tensorflow/model_dense_regressor/#template_num.models_training.regressors.models_tensorflow.model_dense_regressor.ModelDenseRegressor.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialization of the class (see ModelClass, ModelKeras &amp; ModelRegressor for more arguments)</p> Source code in <code>template_num/models_training/regressors/models_tensorflow/model_dense_regressor.py</code> <pre><code>def __init__(self, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelClass, ModelKeras &amp; ModelRegressor for more arguments)'''\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_tensorflow/model_dense_regressor/#template_num.models_training.regressors.models_tensorflow.model_dense_regressor.ModelDenseRegressor.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file hdf5_path (str): Path to hdf5 file preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:     ValueError: If configuration_path is None     ValueError: If hdf5_path is None     ValueError: If preprocess_pipeline_path is None     FileNotFoundError: If the object configuration_path is not an existing file     FileNotFoundError: If the object hdf5_path is not an existing file     FileNotFoundError: If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/regressors/models_tensorflow/model_dense_regressor.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n    '''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        hdf5_path (str): Path to hdf5 file\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If hdf5_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object hdf5_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n    # Retrieve args\n    configuration_path = kwargs.get('configuration_path', None)\n    hdf5_path = kwargs.get('hdf5_path', None)\n    preprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n\n    # Checks\n    if configuration_path is None:\n        raise ValueError(\"The argument configuration_path can't be None\")\n    if hdf5_path is None:\n        raise ValueError(\"The argument hdf5_path can't be None\")\n    if preprocess_pipeline_path is None:\n        raise ValueError(\"The argument preprocess_pipeline_path can't be None\")\n    if not os.path.exists(configuration_path):\n        raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n    if not os.path.exists(hdf5_path):\n        raise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\n    if not os.path.exists(preprocess_pipeline_path):\n        raise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n\n    # Load confs\n    with open(configuration_path, 'r', encoding='utf-8') as f:\n        configs = json.load(f)\n\n    # Set class vars\n    # self.model_name = # Keep the created name\n    # self.model_dir = # Keep the created folder\n    self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n    self.trained = configs.get('trained', True)  # Consider trained by default\n    # Try to read the following attributes from configs and, if absent, keep the current one\n    for attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n                      'random_seed', 'level_save', 'batch_size', 'epochs', 'validation_split',\n                      'patience', 'keras_params']:\n        setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n    # Reload model\n    self.model = load_model_keras(hdf5_path, custom_objects=self.custom_objects)\n\n    # Save best hdf5 in new folder\n    new_hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\n    shutil.copyfile(hdf5_path, new_hdf5_path)\n\n    # Reload pipeline preprocessing\n    with open(preprocess_pipeline_path, 'rb') as f:\n        self.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/monitoring/","title":"Monitoring","text":""},{"location":"reference/template_num/monitoring/mlflow_logger/","title":"Mlflow logger","text":""},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger","title":"<code>MLflowLogger</code>","text":"<p>Abstracts how MlFlow works</p> Source code in <code>template_num/monitoring/mlflow_logger.py</code> <pre><code>class MLflowLogger:\n    '''Abstracts how MlFlow works'''\n\n    def __init__(self, experiment_name: str, tracking_uri: str = '', artifact_uri: str = '') -&gt; None:\n        '''Class initialization\n        Args:\n            experiment_name (str):  Name of the experiment to activate\n        Kwargs:\n            tracking_uri (str): URI of the tracking server\n            artifact_uri (str): URI where to store artifacts\n        '''\n        # Get logger\n        self.logger = logging.getLogger(__name__)\n\n        # Backup to local save if no uri (i.e. empty string)\n        if not tracking_uri:\n            tracking_uri = pathlib.Path(os.path.join(utils.get_data_path(), 'experiments', 'mlruns')).as_uri()\n        # Add \"file\" scheme if no scheme in the tracking_uri\n        elif not urlparse(tracking_uri).scheme:\n            tracking_uri = pathlib.Path(tracking_uri).resolve().as_uri()\n\n        # If no artifact_uri and tracking_uri scheme is \"file\", we set a default artifact_uri in experiments folder\n        # Otherwise we suppose artifact_uri is configured by the system\n        if not artifact_uri and urlparse(tracking_uri).scheme == \"file\":\n            artifact_uri = pathlib.Path(os.path.join(utils.get_data_path(), 'experiments', 'mlruns_artifacts')).as_uri()\n\n        # Set tracking URI &amp; experiment name\n        self.tracking_uri = tracking_uri\n\n        # Get the experiment if it exists and check if there is a connection error by doing it\n        try:\n            experiment = mlflow.get_experiment_by_name(experiment_name)\n        except Exception as e:\n            self.logger.error(repr(e))\n            raise ConnectionError(f\"Can't reach MLflow at {self.tracking_uri}. Please check the URI.\")\n\n        # If the experiment exists, we recover experiment id and artifact_uri (which is link to the experiment)\n        if experiment:\n            experiment_id = experiment.experiment_id\n            artifact_uri = experiment.artifact_location\n        # Otherwise we create a new experiment with the provided artifact_uri\n        else:\n            experiment_id = mlflow.create_experiment(experiment_name, artifact_location=artifact_uri)\n            experiment = mlflow.get_experiment_by_name(experiment_name)\n            artifact_uri = experiment.artifact_location\n\n        mlflow.set_experiment(experiment_id=experiment_id)\n\n        self.__experiment_id = experiment_id\n        self.__experiment_name = experiment_name\n        self.__artifact_uri = artifact_uri\n\n        self.logger.info(f'MLflow running. Metrics available @ {self.tracking_uri}. Experiment artifacts availaible @ {self.artifact_uri}')\n\n    @property\n    def tracking_uri(self) -&gt; str:\n        '''Current tracking uri'''\n        return mlflow.get_tracking_uri()\n\n    @tracking_uri.setter\n    def tracking_uri(self, uri:str) -&gt; None:\n        '''Set tracking uri'''\n        mlflow.set_tracking_uri(uri)\n\n    @property\n    def experiment_id(self) -&gt; str:\n        '''Experiment id. It can not be changed.'''\n        return self.__experiment_id\n\n    @property\n    def experiment_name(self) -&gt; str:\n        '''Experiment name. It can not be changed.'''\n        return self.__experiment_name\n\n    @property\n    def artifact_uri(self) -&gt; str:\n        '''Experiment artifact URI. It can not be changed.'''\n        return self.__artifact_uri\n\n    def end_run(self) -&gt; None:\n        '''Stops an MLflow run'''\n        try:\n            mlflow.end_run()\n        except Exception:\n            self.logger.error(\"Can't stop mlflow run\")\n\n    def log_metric(self, key: str, value, step: Union[int, None] = None) -&gt; None:\n        '''Logs a metric on mlflow\n\n        Args:\n            key (str): Name of the metric\n            value (float, ?): Value of the metric\n        Kwargs:\n            step (int): Step of the metric\n        '''\n        # Check for None\n        if value is None:\n            value = math.nan\n        # Log metric\n        mlflow.log_metric(key, value, step)\n\n    def log_metrics(self, metrics: dict, step: Union[int, None] = None) -&gt; None:\n        '''Logs a set of metrics in mlflow\n\n        Args:\n            metrics (dict): Metrics to log\n        Kwargs:\n            step (int): Step of the metric\n        '''\n        # Check for Nones\n        for k, v in metrics.items():\n            if v is None:\n                metrics[k] = math.nan\n        # Log metrics\n        mlflow.log_metrics(metrics, step)\n\n    def log_param(self, key: str, value) -&gt; None:\n        '''Logs a parameter in mlflow\n\n        Args:\n            key (str): Name of the parameter\n            value (str, ?): Value of the parameter (which will be cast to str if not already of type str)\n        '''\n        if value is None:\n            value = 'None'\n        # Log parameter\n        mlflow.log_param(key, value)\n\n    def log_params(self, params: dict) -&gt; None:\n        '''Logs a set of parameters in mlflow\n\n        Args:\n            params (dict): Name and value of each parameter\n        '''\n        # Check for Nones\n        for k, v in params.items():\n            if v is None:\n                params[k] = 'None'\n        # Log parameters\n        mlflow.log_params(params)\n\n    def set_tag(self, key: str, value) -&gt; None:\n        '''Logs a tag in mlflow\n\n        Args:\n            key (str): Name of the tag\n            value (str, ?): Value of the tag (which will be cast to str if not already of type str)\n        Raises:\n            ValueError: If the object value is None\n        '''\n        if value is None:\n            raise ValueError('value must not be None')\n        # Log tag\n        mlflow.set_tag(key, value)\n\n    def set_tags(self, tags: dict) -&gt; None:\n        '''Logs a set of tags in mlflow\n\n        Args:\n            tags (dict): Name and value of each tag\n        '''\n        # Log tags\n        mlflow.set_tags(tags)\n\n    def valid_name(self, key: str) -&gt; bool:\n        '''Validates key names\n\n        Args:\n            key (str): Key to check\n        Returns:\n            bool: If key is a valid mlflow key\n        '''\n        if mlflow.utils.validation._VALID_PARAM_AND_METRIC_NAMES.match(key):\n            return True\n        else:\n            return False\n\n    def log_df_stats(self, df_stats: pd.DataFrame, label_col: str = 'Label') -&gt; None:\n        '''Log a dataframe containing metrics from a training\n\n        Args:\n            df_stats (pd.Dataframe): Dataframe containing metrics from a training\n        Kwargs:\n            label_col (str): default labelc column name\n        '''\n        if label_col not in df_stats.columns:\n            raise ValueError(f\"The provided label column name ({label_col}) not found in df_stats' columns.\")\n\n        # Get metrics columns\n        metrics_columns = [col for col in df_stats.columns if col != label_col]\n\n        # Log labels\n        labels = df_stats[label_col].values\n        for i, label in enumerate(labels):  # type: ignore\n            self.log_param(f'Label {i}', label)\n\n        # Log metrics\n        ml_flow_metrics = {}\n        for i, row in df_stats.iterrows():\n            for j, col in enumerate(metrics_columns):\n                metric_key = f\"{row[label_col]} --- {col}\"\n                # Check that mlflow accepts the key, otherwise, replace it\n                # TODO: could be improved ...\n                if not self.valid_name(metric_key):\n                    metric_key = f\"Label {i} --- {col}\"\n                if not self.valid_name(metric_key):\n                    metric_key = f\"{row[label_col]} --- Col {j}\"\n                if not self.valid_name(metric_key):\n                    metric_key = f\"Label {i} --- Col {j}\"\n                ml_flow_metrics[metric_key] = row[col]\n\n        # Log metrics\n        self.log_metrics(ml_flow_metrics)\n\n    def log_dict(self, dictionary: dict, artifact_file: str) -&gt; None:\n        '''Logs a dictionary as an artifact in MLflow\n\n        Args:\n            dictionary (dict): A dictionary\n            artifact_file (str): The run-relative artifact file path in posixpath format to which the dictionary is saved\n        '''\n        mlflow.log_dict(dictionary=dictionary, artifact_file=artifact_file)\n\n    def log_text(self, text: str, artifact_file: str) -&gt; None:\n        '''Logs a text as an artifact in MLflow\n\n        Args:\n            text (str): A text\n            artifact_file (str): The run-relative artifact file path in posixpath format to which the dictionary is saved\n        '''\n        mlflow.log_text(text=text, artifact_file=artifact_file)\n\n    def log_figure(self, figure: Figure, artifact_file: str) -&gt; None:\n        '''Logs a text as an artifact in MLflow\n\n        Args:\n            figure (matplotlib.figure.Figure): A matplotlib figure\n            artifact_file (str): The run-relative artifact file path in posixpath format to which the figure is saved\n        '''\n        mlflow.log_figure(figure=figure, artifact_file=artifact_file)\n</code></pre>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.artifact_uri","title":"<code>artifact_uri: str</code>  <code>property</code>","text":"<p>Experiment artifact URI. It can not be changed.</p>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.experiment_id","title":"<code>experiment_id: str</code>  <code>property</code>","text":"<p>Experiment id. It can not be changed.</p>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.experiment_name","title":"<code>experiment_name: str</code>  <code>property</code>","text":"<p>Experiment name. It can not be changed.</p>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.tracking_uri","title":"<code>tracking_uri: str</code>  <code>property</code> <code>writable</code>","text":"<p>Current tracking uri</p>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.__init__","title":"<code>__init__(experiment_name, tracking_uri='', artifact_uri='')</code>","text":"<p>Class initialization Args:     experiment_name (str):  Name of the experiment to activate Kwargs:     tracking_uri (str): URI of the tracking server     artifact_uri (str): URI where to store artifacts</p> Source code in <code>template_num/monitoring/mlflow_logger.py</code> <pre><code>def __init__(self, experiment_name: str, tracking_uri: str = '', artifact_uri: str = '') -&gt; None:\n    '''Class initialization\n    Args:\n        experiment_name (str):  Name of the experiment to activate\n    Kwargs:\n        tracking_uri (str): URI of the tracking server\n        artifact_uri (str): URI where to store artifacts\n    '''\n    # Get logger\n    self.logger = logging.getLogger(__name__)\n\n    # Backup to local save if no uri (i.e. empty string)\n    if not tracking_uri:\n        tracking_uri = pathlib.Path(os.path.join(utils.get_data_path(), 'experiments', 'mlruns')).as_uri()\n    # Add \"file\" scheme if no scheme in the tracking_uri\n    elif not urlparse(tracking_uri).scheme:\n        tracking_uri = pathlib.Path(tracking_uri).resolve().as_uri()\n\n    # If no artifact_uri and tracking_uri scheme is \"file\", we set a default artifact_uri in experiments folder\n    # Otherwise we suppose artifact_uri is configured by the system\n    if not artifact_uri and urlparse(tracking_uri).scheme == \"file\":\n        artifact_uri = pathlib.Path(os.path.join(utils.get_data_path(), 'experiments', 'mlruns_artifacts')).as_uri()\n\n    # Set tracking URI &amp; experiment name\n    self.tracking_uri = tracking_uri\n\n    # Get the experiment if it exists and check if there is a connection error by doing it\n    try:\n        experiment = mlflow.get_experiment_by_name(experiment_name)\n    except Exception as e:\n        self.logger.error(repr(e))\n        raise ConnectionError(f\"Can't reach MLflow at {self.tracking_uri}. Please check the URI.\")\n\n    # If the experiment exists, we recover experiment id and artifact_uri (which is link to the experiment)\n    if experiment:\n        experiment_id = experiment.experiment_id\n        artifact_uri = experiment.artifact_location\n    # Otherwise we create a new experiment with the provided artifact_uri\n    else:\n        experiment_id = mlflow.create_experiment(experiment_name, artifact_location=artifact_uri)\n        experiment = mlflow.get_experiment_by_name(experiment_name)\n        artifact_uri = experiment.artifact_location\n\n    mlflow.set_experiment(experiment_id=experiment_id)\n\n    self.__experiment_id = experiment_id\n    self.__experiment_name = experiment_name\n    self.__artifact_uri = artifact_uri\n\n    self.logger.info(f'MLflow running. Metrics available @ {self.tracking_uri}. Experiment artifacts availaible @ {self.artifact_uri}')\n</code></pre>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.end_run","title":"<code>end_run()</code>","text":"<p>Stops an MLflow run</p> Source code in <code>template_num/monitoring/mlflow_logger.py</code> <pre><code>def end_run(self) -&gt; None:\n    '''Stops an MLflow run'''\n    try:\n        mlflow.end_run()\n    except Exception:\n        self.logger.error(\"Can't stop mlflow run\")\n</code></pre>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.log_df_stats","title":"<code>log_df_stats(df_stats, label_col='Label')</code>","text":"<p>Log a dataframe containing metrics from a training</p> <p>Parameters:</p> Name Type Description Default <code>df_stats</code> <code>Dataframe</code> <p>Dataframe containing metrics from a training</p> required <p>Kwargs:     label_col (str): default labelc column name</p> Source code in <code>template_num/monitoring/mlflow_logger.py</code> <pre><code>def log_df_stats(self, df_stats: pd.DataFrame, label_col: str = 'Label') -&gt; None:\n    '''Log a dataframe containing metrics from a training\n\n    Args:\n        df_stats (pd.Dataframe): Dataframe containing metrics from a training\n    Kwargs:\n        label_col (str): default labelc column name\n    '''\n    if label_col not in df_stats.columns:\n        raise ValueError(f\"The provided label column name ({label_col}) not found in df_stats' columns.\")\n\n    # Get metrics columns\n    metrics_columns = [col for col in df_stats.columns if col != label_col]\n\n    # Log labels\n    labels = df_stats[label_col].values\n    for i, label in enumerate(labels):  # type: ignore\n        self.log_param(f'Label {i}', label)\n\n    # Log metrics\n    ml_flow_metrics = {}\n    for i, row in df_stats.iterrows():\n        for j, col in enumerate(metrics_columns):\n            metric_key = f\"{row[label_col]} --- {col}\"\n            # Check that mlflow accepts the key, otherwise, replace it\n            # TODO: could be improved ...\n            if not self.valid_name(metric_key):\n                metric_key = f\"Label {i} --- {col}\"\n            if not self.valid_name(metric_key):\n                metric_key = f\"{row[label_col]} --- Col {j}\"\n            if not self.valid_name(metric_key):\n                metric_key = f\"Label {i} --- Col {j}\"\n            ml_flow_metrics[metric_key] = row[col]\n\n    # Log metrics\n    self.log_metrics(ml_flow_metrics)\n</code></pre>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.log_dict","title":"<code>log_dict(dictionary, artifact_file)</code>","text":"<p>Logs a dictionary as an artifact in MLflow</p> <p>Parameters:</p> Name Type Description Default <code>dictionary</code> <code>dict</code> <p>A dictionary</p> required <code>artifact_file</code> <code>str</code> <p>The run-relative artifact file path in posixpath format to which the dictionary is saved</p> required Source code in <code>template_num/monitoring/mlflow_logger.py</code> <pre><code>def log_dict(self, dictionary: dict, artifact_file: str) -&gt; None:\n    '''Logs a dictionary as an artifact in MLflow\n\n    Args:\n        dictionary (dict): A dictionary\n        artifact_file (str): The run-relative artifact file path in posixpath format to which the dictionary is saved\n    '''\n    mlflow.log_dict(dictionary=dictionary, artifact_file=artifact_file)\n</code></pre>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.log_figure","title":"<code>log_figure(figure, artifact_file)</code>","text":"<p>Logs a text as an artifact in MLflow</p> <p>Parameters:</p> Name Type Description Default <code>figure</code> <code>Figure</code> <p>A matplotlib figure</p> required <code>artifact_file</code> <code>str</code> <p>The run-relative artifact file path in posixpath format to which the figure is saved</p> required Source code in <code>template_num/monitoring/mlflow_logger.py</code> <pre><code>def log_figure(self, figure: Figure, artifact_file: str) -&gt; None:\n    '''Logs a text as an artifact in MLflow\n\n    Args:\n        figure (matplotlib.figure.Figure): A matplotlib figure\n        artifact_file (str): The run-relative artifact file path in posixpath format to which the figure is saved\n    '''\n    mlflow.log_figure(figure=figure, artifact_file=artifact_file)\n</code></pre>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.log_metric","title":"<code>log_metric(key, value, step=None)</code>","text":"<p>Logs a metric on mlflow</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of the metric</p> required <code>value</code> <code>float, ?</code> <p>Value of the metric</p> required <p>Kwargs:     step (int): Step of the metric</p> Source code in <code>template_num/monitoring/mlflow_logger.py</code> <pre><code>def log_metric(self, key: str, value, step: Union[int, None] = None) -&gt; None:\n    '''Logs a metric on mlflow\n\n    Args:\n        key (str): Name of the metric\n        value (float, ?): Value of the metric\n    Kwargs:\n        step (int): Step of the metric\n    '''\n    # Check for None\n    if value is None:\n        value = math.nan\n    # Log metric\n    mlflow.log_metric(key, value, step)\n</code></pre>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.log_metrics","title":"<code>log_metrics(metrics, step=None)</code>","text":"<p>Logs a set of metrics in mlflow</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>dict</code> <p>Metrics to log</p> required <p>Kwargs:     step (int): Step of the metric</p> Source code in <code>template_num/monitoring/mlflow_logger.py</code> <pre><code>def log_metrics(self, metrics: dict, step: Union[int, None] = None) -&gt; None:\n    '''Logs a set of metrics in mlflow\n\n    Args:\n        metrics (dict): Metrics to log\n    Kwargs:\n        step (int): Step of the metric\n    '''\n    # Check for Nones\n    for k, v in metrics.items():\n        if v is None:\n            metrics[k] = math.nan\n    # Log metrics\n    mlflow.log_metrics(metrics, step)\n</code></pre>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.log_param","title":"<code>log_param(key, value)</code>","text":"<p>Logs a parameter in mlflow</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of the parameter</p> required <code>value</code> <code>str, ?</code> <p>Value of the parameter (which will be cast to str if not already of type str)</p> required Source code in <code>template_num/monitoring/mlflow_logger.py</code> <pre><code>def log_param(self, key: str, value) -&gt; None:\n    '''Logs a parameter in mlflow\n\n    Args:\n        key (str): Name of the parameter\n        value (str, ?): Value of the parameter (which will be cast to str if not already of type str)\n    '''\n    if value is None:\n        value = 'None'\n    # Log parameter\n    mlflow.log_param(key, value)\n</code></pre>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.log_params","title":"<code>log_params(params)</code>","text":"<p>Logs a set of parameters in mlflow</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Name and value of each parameter</p> required Source code in <code>template_num/monitoring/mlflow_logger.py</code> <pre><code>def log_params(self, params: dict) -&gt; None:\n    '''Logs a set of parameters in mlflow\n\n    Args:\n        params (dict): Name and value of each parameter\n    '''\n    # Check for Nones\n    for k, v in params.items():\n        if v is None:\n            params[k] = 'None'\n    # Log parameters\n    mlflow.log_params(params)\n</code></pre>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.log_text","title":"<code>log_text(text, artifact_file)</code>","text":"<p>Logs a text as an artifact in MLflow</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>A text</p> required <code>artifact_file</code> <code>str</code> <p>The run-relative artifact file path in posixpath format to which the dictionary is saved</p> required Source code in <code>template_num/monitoring/mlflow_logger.py</code> <pre><code>def log_text(self, text: str, artifact_file: str) -&gt; None:\n    '''Logs a text as an artifact in MLflow\n\n    Args:\n        text (str): A text\n        artifact_file (str): The run-relative artifact file path in posixpath format to which the dictionary is saved\n    '''\n    mlflow.log_text(text=text, artifact_file=artifact_file)\n</code></pre>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.set_tag","title":"<code>set_tag(key, value)</code>","text":"<p>Logs a tag in mlflow</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of the tag</p> required <code>value</code> <code>str, ?</code> <p>Value of the tag (which will be cast to str if not already of type str)</p> required <p>Raises:     ValueError: If the object value is None</p> Source code in <code>template_num/monitoring/mlflow_logger.py</code> <pre><code>def set_tag(self, key: str, value) -&gt; None:\n    '''Logs a tag in mlflow\n\n    Args:\n        key (str): Name of the tag\n        value (str, ?): Value of the tag (which will be cast to str if not already of type str)\n    Raises:\n        ValueError: If the object value is None\n    '''\n    if value is None:\n        raise ValueError('value must not be None')\n    # Log tag\n    mlflow.set_tag(key, value)\n</code></pre>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.set_tags","title":"<code>set_tags(tags)</code>","text":"<p>Logs a set of tags in mlflow</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>dict</code> <p>Name and value of each tag</p> required Source code in <code>template_num/monitoring/mlflow_logger.py</code> <pre><code>def set_tags(self, tags: dict) -&gt; None:\n    '''Logs a set of tags in mlflow\n\n    Args:\n        tags (dict): Name and value of each tag\n    '''\n    # Log tags\n    mlflow.set_tags(tags)\n</code></pre>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.valid_name","title":"<code>valid_name(key)</code>","text":"<p>Validates key names</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Key to check</p> required <p>Returns:     bool: If key is a valid mlflow key</p> Source code in <code>template_num/monitoring/mlflow_logger.py</code> <pre><code>def valid_name(self, key: str) -&gt; bool:\n    '''Validates key names\n\n    Args:\n        key (str): Key to check\n    Returns:\n        bool: If key is a valid mlflow key\n    '''\n    if mlflow.utils.validation._VALID_PARAM_AND_METRIC_NAMES.match(key):\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"reference/template_num/monitoring/model_explainer/","title":"Model explainer","text":""},{"location":"reference/template_num/monitoring/model_explainer/#template_num.monitoring.model_explainer.Explainer","title":"<code>Explainer</code>","text":"<p>Parent class for the explainers</p> Source code in <code>template_num/monitoring/model_explainer.py</code> <pre><code>class Explainer:\n    '''Parent class for the explainers'''\n\n    def __init__(self, *args, **kwargs) -&gt; None:\n        '''Initialization of the parent class'''\n        self.logger = logging.getLogger(__name__)\n\n    def explain_instance(self, content: pd.DataFrame, **kwargs) -&gt; Any:\n        '''Explains a prediction\n\n        Args:\n            content (pd.DataFrame): Single entry to be explained\n        Returns:\n            (?): An explanation object\n        '''\n        raise NotImplementedError(\"'explain_instance' needs to be overridden\")\n\n    def explain_instance_as_html(self, content: pd.DataFrame, **kwargs) -&gt; str:\n        '''Explains a prediction - returns an HTML object\n\n        Args:\n            content (pd.DataFrame): Single entry to be explained\n        Returns:\n            str: An HTML code with the explanation\n        '''\n        raise NotImplementedError(\"'explain_instance_as_html' needs to be overridden\")\n\n    def explain_instance_as_json(self, content: pd.DataFrame, **kwargs) -&gt; Union[dict, list]:\n        '''Explains a prediction - returns an JSON serializable object\n\n        Args:\n            content (str): Text to be explained\n        Returns:\n            Union[dict, list]: A JSON serializable object containing the explanation\n        '''\n        raise NotImplementedError(\"'explain_instance_as_json' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_num/monitoring/model_explainer/#template_num.monitoring.model_explainer.Explainer.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialization of the parent class</p> Source code in <code>template_num/monitoring/model_explainer.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n    '''Initialization of the parent class'''\n    self.logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/template_num/monitoring/model_explainer/#template_num.monitoring.model_explainer.Explainer.explain_instance","title":"<code>explain_instance(content, **kwargs)</code>","text":"<p>Explains a prediction</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>DataFrame</code> <p>Single entry to be explained</p> required <p>Returns:     (?): An explanation object</p> Source code in <code>template_num/monitoring/model_explainer.py</code> <pre><code>def explain_instance(self, content: pd.DataFrame, **kwargs) -&gt; Any:\n    '''Explains a prediction\n\n    Args:\n        content (pd.DataFrame): Single entry to be explained\n    Returns:\n        (?): An explanation object\n    '''\n    raise NotImplementedError(\"'explain_instance' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_num/monitoring/model_explainer/#template_num.monitoring.model_explainer.Explainer.explain_instance_as_html","title":"<code>explain_instance_as_html(content, **kwargs)</code>","text":"<p>Explains a prediction - returns an HTML object</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>DataFrame</code> <p>Single entry to be explained</p> required <p>Returns:     str: An HTML code with the explanation</p> Source code in <code>template_num/monitoring/model_explainer.py</code> <pre><code>def explain_instance_as_html(self, content: pd.DataFrame, **kwargs) -&gt; str:\n    '''Explains a prediction - returns an HTML object\n\n    Args:\n        content (pd.DataFrame): Single entry to be explained\n    Returns:\n        str: An HTML code with the explanation\n    '''\n    raise NotImplementedError(\"'explain_instance_as_html' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_num/monitoring/model_explainer/#template_num.monitoring.model_explainer.Explainer.explain_instance_as_json","title":"<code>explain_instance_as_json(content, **kwargs)</code>","text":"<p>Explains a prediction - returns an JSON serializable object</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Text to be explained</p> required <p>Returns:     Union[dict, list]: A JSON serializable object containing the explanation</p> Source code in <code>template_num/monitoring/model_explainer.py</code> <pre><code>def explain_instance_as_json(self, content: pd.DataFrame, **kwargs) -&gt; Union[dict, list]:\n    '''Explains a prediction - returns an JSON serializable object\n\n    Args:\n        content (str): Text to be explained\n    Returns:\n        Union[dict, list]: A JSON serializable object containing the explanation\n    '''\n    raise NotImplementedError(\"'explain_instance_as_json' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_num/monitoring/model_explainer/#template_num.monitoring.model_explainer.ShapExplainer","title":"<code>ShapExplainer</code>","text":"<p>             Bases: <code>Explainer</code></p> <p>Shap Explainer wrapper class</p> Source code in <code>template_num/monitoring/model_explainer.py</code> <pre><code>class ShapExplainer(Explainer):\n    '''Shap Explainer wrapper class'''\n\n    def __init__(self, model: Type[ModelClass], anchor_data: pd.DataFrame, anchor_preprocessed: bool = False) -&gt; None:\n        ''' Initialization\n\n        Args:\n            model: A model instance with predict (regressors) or predict_proba (classifiers) functions\n            anchor_data (pd.DataFrame): data anchor needed by shap (usually 100 data points)\n        Kwargs:\n            anchor_preprocessed (bool): If the anchor data has already been preprocessed\n        Raises:\n            TypeError: If the provided model is a regressor and does not implement a `predict` function\n            TypeError: If the provided model is a classifier and does not implement a `predict_proba` function\n        '''\n        super().__init__()\n        pred_op = getattr(model, \"predict\", None)\n        pred_proba_op = getattr(model, \"predict_proba\", None)\n\n        if model.model_type == 'regressor':\n            if pred_op is None or not callable(pred_op):\n                raise TypeError(\"The supplied model must implement a predict() function\")\n        # Check classifier\n        if model.model_type == 'classifier':\n            if pred_proba_op is None or not callable(pred_proba_op):\n                raise TypeError(\"The supplied model must implement a predict_proba() function\")\n\n        # Set attributes\n        self.model = model\n        self.model_type = model.model_type\n        # Our explainers will explain a prediction for a given class / label\n        # These atributes are set on the fly and will change the proba function used by the explainer\n        self.current_class_or_label_index = 0\n        fn_output = self.classifier_fn if self.model_type == 'classifier' else self.regressor_fn\n\n        # Preprocess the anchor data\n        if not anchor_preprocessed:\n            if self.model.preprocess_pipeline is not None:\n                anchor_prep = utils_models.apply_pipeline(anchor_data, self.model.preprocess_pipeline)\n            else:\n                anchor_prep = anchor_data.copy()\n        else:\n            # Check columns\n            try:\n                anchor_prep = anchor_data[self.model.x_col]\n            except:\n                raise ValueError(\"Provided anchor data (already preprocessed) do not match model's inputs columns\")\n        # Create the explainer\n        self.explainer = shap.Explainer(fn_output, anchor_prep)\n\n    def classifier_fn(self, content_prep: pd.DataFrame) -&gt; np.ndarray:\n        '''Function to get probabilities from a dataset (already preprocessed) - classifiers\n\n        Args:\n            content_prep (pd.DataFrame): dataset (already preprocessed) to be considered\n        Returns:\n            np.array: probabilities\n        '''\n        # Get probabilities\n        # Mypy raises a false error here, needs to be ignored\n        return self.model.predict_proba(content_prep)[:, self.current_class_or_label_index]  # type: ignore\n\n    def regressor_fn(self, content_prep: pd.DataFrame) -&gt; np.ndarray:\n        '''Function to get predictions from a dataset (already preprocessed) - regressors\n\n        Args:\n            content_prep (pd.DataFrame): dataset (already preprocessed) to be considered\n        Returns:\n            np.array: predictions\n        '''\n        # Get predictions\n        # Mypy raises a false error here, needs to be ignored\n        return self.model.predict(content_prep)  # type: ignore\n\n    def explain_instance(self, content: pd.DataFrame, class_or_label_index: Union[int, None] = None, **kwargs) -&gt; shap.Explanation:\n        '''Explains predictions by returning a shap.Explanation object\n\n        This function calls the Shap module.\n\n        Args:\n            content (pd.DataFrame): Entries to be explained\n        Kwargs:\n            class_or_label_index (int): for classification only. Class or label index to be considered.\n        Returns:\n            shap.Explanation: Shap Explanation object\n        '''\n        # Apply preprocessing\n        if self.model.preprocess_pipeline is not None:\n            df_prep = utils_models.apply_pipeline(content, self.model.preprocess_pipeline)\n        else:\n            df_prep = content.copy()\n            logger.warning(\"No preprocessing pipeline found - we consider no preprocessing, but it should not be so !\")\n        # Set index (if needed)\n        if class_or_label_index is not None:\n            self.current_class_or_label_index = class_or_label_index\n        # Get explanations\n        return self.explainer(df_prep)  # Shap values\n\n    def explain_instance_as_html(self, content: pd.DataFrame, class_or_label_index: Union[int, None] = None, **kwargs) -&gt; str:\n        '''Explains a prediction - returns an HTML object\n\n        Args:\n            content (pd.DataFrame): Single entry to be explained\n        Kwargs:\n            class_or_label_index (int): for classification only. Class or label index to be considered.\n        Returns:\n            str: An HTML code with the explanation\n        '''\n        shap_values = self.explain_instance(content, class_or_label_index=class_or_label_index)\n        # Waterfall figure\n        plt.clf()\n        waterfall_fig = shap.plots.waterfall(shap_values[0], show=False)\n        with tempfile.TemporaryFile('w+b') as plt_file:\n            waterfall_fig.savefig(plt_file, format='png', bbox_inches='tight')\n            plt_file.seek(0)\n            encoded = base64.b64encode(plt_file.read())\n        plt.clf()\n        html_waterfall = f\"&lt;img src=\\'data:image/png;base64, {encoded.decode('utf-8')}\\' class=\\\"shap_fig\\\" &gt;\"  # Class name is used in the demonstrator\n        # Force figure\n        html_force = shap.plots.force(shap_values[0]).html()\n        # Combine &amp; return\n        final_html = f\"&lt;head&gt;{shap.getjs()}&lt;/head&gt;{html_waterfall}&lt;br&gt;&lt;body&gt;{html_force}&lt;/body&gt;\"\n        return final_html\n\n    def explain_instance_as_json(self, content: pd.DataFrame, class_or_label_index: Union[int, None] = None, **kwargs) -&gt; Union[dict, list]:\n        '''Explains predictions by returning a JSON serializable object\n\n        This function calls the Shap module.\n\n        Args:\n            content (pd.DataFrame): entries to be explained\n        Kwargs:\n            class_or_label_index (int): for classification only. Class or label index to be considered.\n        Returns:\n            (Union[dict, list]): Shap values\n        '''\n        return [\n            {\n                \"features\": explanation.feature_names, \n                \"preprocessed_values\": explanation.data, \n                \"shap_values\": explanation.values, \n                \"shap_base_values\": explanation.base_values,\n            }\n            for explanation in self.explain_instance(content, class_or_label_index=class_or_label_index, **kwargs)\n        ]\n</code></pre>"},{"location":"reference/template_num/monitoring/model_explainer/#template_num.monitoring.model_explainer.ShapExplainer.__init__","title":"<code>__init__(model, anchor_data, anchor_preprocessed=False)</code>","text":"<p>Initialization</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Type[ModelClass]</code> <p>A model instance with predict (regressors) or predict_proba (classifiers) functions</p> required <code>anchor_data</code> <code>DataFrame</code> <p>data anchor needed by shap (usually 100 data points)</p> required <p>Kwargs:     anchor_preprocessed (bool): If the anchor data has already been preprocessed Raises:     TypeError: If the provided model is a regressor and does not implement a <code>predict</code> function     TypeError: If the provided model is a classifier and does not implement a <code>predict_proba</code> function</p> Source code in <code>template_num/monitoring/model_explainer.py</code> <pre><code>def __init__(self, model: Type[ModelClass], anchor_data: pd.DataFrame, anchor_preprocessed: bool = False) -&gt; None:\n    ''' Initialization\n\n    Args:\n        model: A model instance with predict (regressors) or predict_proba (classifiers) functions\n        anchor_data (pd.DataFrame): data anchor needed by shap (usually 100 data points)\n    Kwargs:\n        anchor_preprocessed (bool): If the anchor data has already been preprocessed\n    Raises:\n        TypeError: If the provided model is a regressor and does not implement a `predict` function\n        TypeError: If the provided model is a classifier and does not implement a `predict_proba` function\n    '''\n    super().__init__()\n    pred_op = getattr(model, \"predict\", None)\n    pred_proba_op = getattr(model, \"predict_proba\", None)\n\n    if model.model_type == 'regressor':\n        if pred_op is None or not callable(pred_op):\n            raise TypeError(\"The supplied model must implement a predict() function\")\n    # Check classifier\n    if model.model_type == 'classifier':\n        if pred_proba_op is None or not callable(pred_proba_op):\n            raise TypeError(\"The supplied model must implement a predict_proba() function\")\n\n    # Set attributes\n    self.model = model\n    self.model_type = model.model_type\n    # Our explainers will explain a prediction for a given class / label\n    # These atributes are set on the fly and will change the proba function used by the explainer\n    self.current_class_or_label_index = 0\n    fn_output = self.classifier_fn if self.model_type == 'classifier' else self.regressor_fn\n\n    # Preprocess the anchor data\n    if not anchor_preprocessed:\n        if self.model.preprocess_pipeline is not None:\n            anchor_prep = utils_models.apply_pipeline(anchor_data, self.model.preprocess_pipeline)\n        else:\n            anchor_prep = anchor_data.copy()\n    else:\n        # Check columns\n        try:\n            anchor_prep = anchor_data[self.model.x_col]\n        except:\n            raise ValueError(\"Provided anchor data (already preprocessed) do not match model's inputs columns\")\n    # Create the explainer\n    self.explainer = shap.Explainer(fn_output, anchor_prep)\n</code></pre>"},{"location":"reference/template_num/monitoring/model_explainer/#template_num.monitoring.model_explainer.ShapExplainer.classifier_fn","title":"<code>classifier_fn(content_prep)</code>","text":"<p>Function to get probabilities from a dataset (already preprocessed) - classifiers</p> <p>Parameters:</p> Name Type Description Default <code>content_prep</code> <code>DataFrame</code> <p>dataset (already preprocessed) to be considered</p> required <p>Returns:     np.array: probabilities</p> Source code in <code>template_num/monitoring/model_explainer.py</code> <pre><code>def classifier_fn(self, content_prep: pd.DataFrame) -&gt; np.ndarray:\n    '''Function to get probabilities from a dataset (already preprocessed) - classifiers\n\n    Args:\n        content_prep (pd.DataFrame): dataset (already preprocessed) to be considered\n    Returns:\n        np.array: probabilities\n    '''\n    # Get probabilities\n    # Mypy raises a false error here, needs to be ignored\n    return self.model.predict_proba(content_prep)[:, self.current_class_or_label_index]  # type: ignore\n</code></pre>"},{"location":"reference/template_num/monitoring/model_explainer/#template_num.monitoring.model_explainer.ShapExplainer.explain_instance","title":"<code>explain_instance(content, class_or_label_index=None, **kwargs)</code>","text":"<p>Explains predictions by returning a shap.Explanation object</p> <p>This function calls the Shap module.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>DataFrame</code> <p>Entries to be explained</p> required <p>Kwargs:     class_or_label_index (int): for classification only. Class or label index to be considered. Returns:     shap.Explanation: Shap Explanation object</p> Source code in <code>template_num/monitoring/model_explainer.py</code> <pre><code>def explain_instance(self, content: pd.DataFrame, class_or_label_index: Union[int, None] = None, **kwargs) -&gt; shap.Explanation:\n    '''Explains predictions by returning a shap.Explanation object\n\n    This function calls the Shap module.\n\n    Args:\n        content (pd.DataFrame): Entries to be explained\n    Kwargs:\n        class_or_label_index (int): for classification only. Class or label index to be considered.\n    Returns:\n        shap.Explanation: Shap Explanation object\n    '''\n    # Apply preprocessing\n    if self.model.preprocess_pipeline is not None:\n        df_prep = utils_models.apply_pipeline(content, self.model.preprocess_pipeline)\n    else:\n        df_prep = content.copy()\n        logger.warning(\"No preprocessing pipeline found - we consider no preprocessing, but it should not be so !\")\n    # Set index (if needed)\n    if class_or_label_index is not None:\n        self.current_class_or_label_index = class_or_label_index\n    # Get explanations\n    return self.explainer(df_prep)  # Shap values\n</code></pre>"},{"location":"reference/template_num/monitoring/model_explainer/#template_num.monitoring.model_explainer.ShapExplainer.explain_instance_as_html","title":"<code>explain_instance_as_html(content, class_or_label_index=None, **kwargs)</code>","text":"<p>Explains a prediction - returns an HTML object</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>DataFrame</code> <p>Single entry to be explained</p> required <p>Kwargs:     class_or_label_index (int): for classification only. Class or label index to be considered. Returns:     str: An HTML code with the explanation</p> Source code in <code>template_num/monitoring/model_explainer.py</code> <pre><code>def explain_instance_as_html(self, content: pd.DataFrame, class_or_label_index: Union[int, None] = None, **kwargs) -&gt; str:\n    '''Explains a prediction - returns an HTML object\n\n    Args:\n        content (pd.DataFrame): Single entry to be explained\n    Kwargs:\n        class_or_label_index (int): for classification only. Class or label index to be considered.\n    Returns:\n        str: An HTML code with the explanation\n    '''\n    shap_values = self.explain_instance(content, class_or_label_index=class_or_label_index)\n    # Waterfall figure\n    plt.clf()\n    waterfall_fig = shap.plots.waterfall(shap_values[0], show=False)\n    with tempfile.TemporaryFile('w+b') as plt_file:\n        waterfall_fig.savefig(plt_file, format='png', bbox_inches='tight')\n        plt_file.seek(0)\n        encoded = base64.b64encode(plt_file.read())\n    plt.clf()\n    html_waterfall = f\"&lt;img src=\\'data:image/png;base64, {encoded.decode('utf-8')}\\' class=\\\"shap_fig\\\" &gt;\"  # Class name is used in the demonstrator\n    # Force figure\n    html_force = shap.plots.force(shap_values[0]).html()\n    # Combine &amp; return\n    final_html = f\"&lt;head&gt;{shap.getjs()}&lt;/head&gt;{html_waterfall}&lt;br&gt;&lt;body&gt;{html_force}&lt;/body&gt;\"\n    return final_html\n</code></pre>"},{"location":"reference/template_num/monitoring/model_explainer/#template_num.monitoring.model_explainer.ShapExplainer.explain_instance_as_json","title":"<code>explain_instance_as_json(content, class_or_label_index=None, **kwargs)</code>","text":"<p>Explains predictions by returning a JSON serializable object</p> <p>This function calls the Shap module.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>DataFrame</code> <p>entries to be explained</p> required <p>Kwargs:     class_or_label_index (int): for classification only. Class or label index to be considered. Returns:     (Union[dict, list]): Shap values</p> Source code in <code>template_num/monitoring/model_explainer.py</code> <pre><code>def explain_instance_as_json(self, content: pd.DataFrame, class_or_label_index: Union[int, None] = None, **kwargs) -&gt; Union[dict, list]:\n    '''Explains predictions by returning a JSON serializable object\n\n    This function calls the Shap module.\n\n    Args:\n        content (pd.DataFrame): entries to be explained\n    Kwargs:\n        class_or_label_index (int): for classification only. Class or label index to be considered.\n    Returns:\n        (Union[dict, list]): Shap values\n    '''\n    return [\n        {\n            \"features\": explanation.feature_names, \n            \"preprocessed_values\": explanation.data, \n            \"shap_values\": explanation.values, \n            \"shap_base_values\": explanation.base_values,\n        }\n        for explanation in self.explain_instance(content, class_or_label_index=class_or_label_index, **kwargs)\n    ]\n</code></pre>"},{"location":"reference/template_num/monitoring/model_explainer/#template_num.monitoring.model_explainer.ShapExplainer.regressor_fn","title":"<code>regressor_fn(content_prep)</code>","text":"<p>Function to get predictions from a dataset (already preprocessed) - regressors</p> <p>Parameters:</p> Name Type Description Default <code>content_prep</code> <code>DataFrame</code> <p>dataset (already preprocessed) to be considered</p> required <p>Returns:     np.array: predictions</p> Source code in <code>template_num/monitoring/model_explainer.py</code> <pre><code>def regressor_fn(self, content_prep: pd.DataFrame) -&gt; np.ndarray:\n    '''Function to get predictions from a dataset (already preprocessed) - regressors\n\n    Args:\n        content_prep (pd.DataFrame): dataset (already preprocessed) to be considered\n    Returns:\n        np.array: predictions\n    '''\n    # Get predictions\n    # Mypy raises a false error here, needs to be ignored\n    return self.model.predict(content_prep)  # type: ignore\n</code></pre>"},{"location":"reference/template_num/preprocessing/","title":"Preprocessing","text":""},{"location":"reference/template_num/preprocessing/column_preprocessors/","title":"Column preprocessors","text":""},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.AutoBinner","title":"<code>AutoBinner</code>","text":"<p>             Bases: <code>Estimator</code></p> <p>Automatically creates a \"other\" category when the categories are heavily unbalanced /! Replaces the values of some categories /!\\</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>class AutoBinner(Estimator):\n    '''Automatically creates a \"other\" category when the categories are heavily unbalanced\n    /!\\\\ Replaces the values of some categories /!\\\\\n    '''\n\n    def __init__(self, strategy: str = \"auto\", min_cat_count: int = 3, threshold: float = 0.05) -&gt; None:\n        '''Initialization of the class\n\n        Kwargs:\n            strategy (str): 'auto' or 'threshold'\n                - 'auto': Aggregates all categories as long as their cumulated frequence is less than threshold\n                - 'threshold': Aggregates all category whose frequence is less than threshold\n            min_cat_count (int): Minimal number of category to keep\n            threshold (float): The threshold to consider\n        Raises:\n            ValueError: The object strategy must be in the list of allowed strategies\n            ValueError: The object min_cat_count must be non negative\n            ValueError: The object threshold must be in ]0,1[\n        '''\n        super().__init__(None)\n\n        allowed_strategies = [\"threshold\", \"auto\"]\n        self.strategy = strategy\n        if self.strategy not in allowed_strategies:\n            raise ValueError(f\"Can only use these strategies: {allowed_strategies}. \"\n                             f\"Got strategy={strategy}\")\n        if min_cat_count &lt; 0:\n            raise ValueError(\"min_cat_count must be non negative\")\n        if not (0 &lt; threshold &lt; 1):\n            raise ValueError(f\"threshold must be in ]0,1[, not {threshold}\")\n\n        # Set attributes\n        self.min_cat_count = min_cat_count\n        self.threshold = threshold\n        self.kept_cat_by_index: Dict[int, list] = {}\n\n    def fit(self, X: Union[np.ndarray, pd.DataFrame], y: Any = None) -&gt; Any:\n        '''Fit the AutoBinner on X.\n\n        Args:\n            X (np.ndarray or pd.DataFrame): Shape (n_samples, n_features)\n        Kwargs:\n            y: Not used here\n        Returns:\n            self: AutoBinner\n        '''\n        X = self._validate_input(X)\n        # If x is a numpy array, casts it in pd.DataFrame\n        if isinstance(X, np.ndarray):\n            X = pd.DataFrame(X)\n\n        self.input_length = X.shape[1]\n        # Fits column one by one\n        for col_index in range(self.input_length):\n            # Get col serie\n            X_tmp_ser = X.iloc[:, col_index]\n            # Get unique vals\n            unique_cat = list(X_tmp_ser.unique())\n            # If less vals than min threshold, set this column allowed values with all uniques values\n            if len(unique_cat) &lt;= self.min_cat_count:\n                self.kept_cat_by_index[col_index] = unique_cat\n                continue\n\n            # If more vals than min threshold, keep values based on strategy\n            table = X_tmp_ser.value_counts() / X_tmp_ser.count()\n            table = table.sort_values()\n            if self.strategy == 'auto':\n                table = np.cumsum(table)\n            # If only one category is less than the threshold, we do not need to transform it\n            if table[1] &gt; self.threshold:\n                self.kept_cat_by_index[col_index] = unique_cat\n                continue\n            # Otherwise, we get rid of the superfluous categories\n            else:\n                to_remove = list(table[table &lt; self.threshold].index)\n                for item in to_remove:\n                    unique_cat.remove(item)\n                self.kept_cat_by_index[col_index] = unique_cat\n\n        self.fitted_ = True\n        return self\n\n    def transform(self, X: Union[np.ndarray, pd.DataFrame]) -&gt; np.ndarray:\n        '''Imputes all missing values in X.\n\n        Args:\n            X (np.ndarray or pd.DataFrame): Shape (n_samples, n_features)\n                The input data to complete.\n        '''\n        check_is_fitted(self, 'fitted_')\n        X = self._validate_input(X)\n        # If x is a numpy array, casts it in pd.DataFrame\n        if isinstance(X, np.ndarray):\n            X = pd.DataFrame(X)\n\n        for col_index in range(self.input_length):\n            X.iloc[:, col_index] = X.iloc[:, col_index].apply(lambda x: x if x in self.kept_cat_by_index[col_index] else 'other_')\n\n        return X.to_numpy()  # Compatibility, returns a numpy array\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.AutoBinner.__init__","title":"<code>__init__(strategy='auto', min_cat_count=3, threshold=0.05)</code>","text":"<p>Initialization of the class</p> Kwargs <p>strategy (str): 'auto' or 'threshold'     - 'auto': Aggregates all categories as long as their cumulated frequence is less than threshold     - 'threshold': Aggregates all category whose frequence is less than threshold min_cat_count (int): Minimal number of category to keep threshold (float): The threshold to consider</p> <p>Raises:     ValueError: The object strategy must be in the list of allowed strategies     ValueError: The object min_cat_count must be non negative     ValueError: The object threshold must be in ]0,1[</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def __init__(self, strategy: str = \"auto\", min_cat_count: int = 3, threshold: float = 0.05) -&gt; None:\n    '''Initialization of the class\n\n    Kwargs:\n        strategy (str): 'auto' or 'threshold'\n            - 'auto': Aggregates all categories as long as their cumulated frequence is less than threshold\n            - 'threshold': Aggregates all category whose frequence is less than threshold\n        min_cat_count (int): Minimal number of category to keep\n        threshold (float): The threshold to consider\n    Raises:\n        ValueError: The object strategy must be in the list of allowed strategies\n        ValueError: The object min_cat_count must be non negative\n        ValueError: The object threshold must be in ]0,1[\n    '''\n    super().__init__(None)\n\n    allowed_strategies = [\"threshold\", \"auto\"]\n    self.strategy = strategy\n    if self.strategy not in allowed_strategies:\n        raise ValueError(f\"Can only use these strategies: {allowed_strategies}. \"\n                         f\"Got strategy={strategy}\")\n    if min_cat_count &lt; 0:\n        raise ValueError(\"min_cat_count must be non negative\")\n    if not (0 &lt; threshold &lt; 1):\n        raise ValueError(f\"threshold must be in ]0,1[, not {threshold}\")\n\n    # Set attributes\n    self.min_cat_count = min_cat_count\n    self.threshold = threshold\n    self.kept_cat_by_index: Dict[int, list] = {}\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.AutoBinner.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the AutoBinner on X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray or DataFrame</code> <p>Shape (n_samples, n_features)</p> required <p>Kwargs:     y: Not used here Returns:     self: AutoBinner</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def fit(self, X: Union[np.ndarray, pd.DataFrame], y: Any = None) -&gt; Any:\n    '''Fit the AutoBinner on X.\n\n    Args:\n        X (np.ndarray or pd.DataFrame): Shape (n_samples, n_features)\n    Kwargs:\n        y: Not used here\n    Returns:\n        self: AutoBinner\n    '''\n    X = self._validate_input(X)\n    # If x is a numpy array, casts it in pd.DataFrame\n    if isinstance(X, np.ndarray):\n        X = pd.DataFrame(X)\n\n    self.input_length = X.shape[1]\n    # Fits column one by one\n    for col_index in range(self.input_length):\n        # Get col serie\n        X_tmp_ser = X.iloc[:, col_index]\n        # Get unique vals\n        unique_cat = list(X_tmp_ser.unique())\n        # If less vals than min threshold, set this column allowed values with all uniques values\n        if len(unique_cat) &lt;= self.min_cat_count:\n            self.kept_cat_by_index[col_index] = unique_cat\n            continue\n\n        # If more vals than min threshold, keep values based on strategy\n        table = X_tmp_ser.value_counts() / X_tmp_ser.count()\n        table = table.sort_values()\n        if self.strategy == 'auto':\n            table = np.cumsum(table)\n        # If only one category is less than the threshold, we do not need to transform it\n        if table[1] &gt; self.threshold:\n            self.kept_cat_by_index[col_index] = unique_cat\n            continue\n        # Otherwise, we get rid of the superfluous categories\n        else:\n            to_remove = list(table[table &lt; self.threshold].index)\n            for item in to_remove:\n                unique_cat.remove(item)\n            self.kept_cat_by_index[col_index] = unique_cat\n\n    self.fitted_ = True\n    return self\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.AutoBinner.transform","title":"<code>transform(X)</code>","text":"<p>Imputes all missing values in X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray or DataFrame</code> <p>Shape (n_samples, n_features) The input data to complete.</p> required Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def transform(self, X: Union[np.ndarray, pd.DataFrame]) -&gt; np.ndarray:\n    '''Imputes all missing values in X.\n\n    Args:\n        X (np.ndarray or pd.DataFrame): Shape (n_samples, n_features)\n            The input data to complete.\n    '''\n    check_is_fitted(self, 'fitted_')\n    X = self._validate_input(X)\n    # If x is a numpy array, casts it in pd.DataFrame\n    if isinstance(X, np.ndarray):\n        X = pd.DataFrame(X)\n\n    for col_index in range(self.input_length):\n        X.iloc[:, col_index] = X.iloc[:, col_index].apply(lambda x: x if x in self.kept_cat_by_index[col_index] else 'other_')\n\n    return X.to_numpy()  # Compatibility, returns a numpy array\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.AutoLogTransform","title":"<code>AutoLogTransform</code>","text":"<p>             Bases: <code>Estimator</code></p> <p>Automatically applies a log transformation on numerical data if the distribution of the variables    is skewed (abs(skew) &gt; min_skewness) and if there is an amplitude superior to min_amplitude between    the 10th and 90th percentiles</p> <p>WARNING: YOUR DATA MUST BE POSITIVE IN ORDER FOR EVERYTHING TO WORK CORRECTLY</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>class AutoLogTransform(Estimator):\n    '''Automatically applies a log transformation on numerical data if the distribution of the variables\n       is skewed (abs(skew) &gt; min_skewness) and if there is an amplitude superior to min_amplitude between\n       the 10th and 90th percentiles\n\n    WARNING: YOUR DATA MUST BE POSITIVE IN ORDER FOR EVERYTHING TO WORK CORRECTLY\n    '''\n\n    def __init__(self, min_skewness: float = 2, min_amplitude: float = 10E3) -&gt; None:\n        '''Initialization of the class\n\n        Kwargs:\n            min_skewness (float): Absolute value of the required skewness to apply a log transformation\n            min_amplitude (float): Minimal value of the amplitude between the 10th and 90th percentiles\n                                   required to apply a log transformation\n        '''\n\n        super().__init__(None)\n        # Set attributes\n        self.min_skewness = min_skewness\n        self.min_amplitude = min_amplitude\n\n        # Columns on which to apply the transformation\n        # Set on fit\n        # Warning: sklearn does not support columns name, so we can only use indexes\n        # Hence, X input must expose same columns order (this won't be checked)\n        self.applicable_columns_index: Optional[List[Any]] = None\n\n    def fit(self, X: Union[np.ndarray, pd.DataFrame], y: Any = None) -&gt; Any:\n        '''Fit transformer\n\n        Args:\n            X (np.ndarray or pd.DataFrame): Array-like, shape = [n_samples, n_features]\n        Kwargs:\n            y (None): Not used here\n        Returns:\n            self\n        '''\n        X = self._validate_input(X)\n        # If x is a numpy array, casts it in pd.DataFrame\n        if isinstance(X, np.ndarray):\n            X = pd.DataFrame(X)\n        # Otherwise, we reset the columns name because sklearn can't manage them\n        else:\n            X = X.rename(columns={col: i for i, col in enumerate(X.columns)})\n        self.input_length = X.shape[1]\n\n        # Get applicable columns\n        skew = X.skew()\n        candidates = list(skew[abs(skew) &gt; self.min_skewness].index)\n        if len(candidates) &gt; 0:\n            q10 = X.iloc[:, candidates].quantile(q=0.1)\n            q90 = X.iloc[:, candidates].quantile(q=0.9)\n            amp = q90 - q10\n            # Update applicable_columns_index\n            self.applicable_columns_index = list(amp[amp &gt; self.min_amplitude].index)\n\n        self.fitted_ = True\n        return self\n\n    def transform(self, X: Union[np.ndarray, pd.DataFrame]) -&gt; np.ndarray:\n        '''Transforms X - apply log on applicable columns\n\n        Args:\n            X (np.ndarray or pd.DataFrame): Array-like, shape = [n_samples, n_features]\n        Returns:\n            X_out: Array-like, shape [n_samples, n_features]\n                        Transformed input.\n        '''\n        # Validate input\n        check_is_fitted(self, 'fitted_')\n        X = self._validate_input(X)\n\n        # If x is a numpy array, casts it in pd.DataFrame\n        if isinstance(X, np.ndarray):\n            X = pd.DataFrame(X)\n\n        # Log transformation on the applicable columns\n        if len(self.applicable_columns_index) &gt; 0:\n            X.iloc[:, self.applicable_columns_index] = np.log(X.iloc[:, self.applicable_columns_index])\n\n        # Compatibility -&gt; returns numpy array\n        return X.to_numpy()\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.AutoLogTransform.__init__","title":"<code>__init__(min_skewness=2, min_amplitude=10000.0)</code>","text":"<p>Initialization of the class</p> Kwargs <p>min_skewness (float): Absolute value of the required skewness to apply a log transformation min_amplitude (float): Minimal value of the amplitude between the 10th and 90th percentiles                        required to apply a log transformation</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def __init__(self, min_skewness: float = 2, min_amplitude: float = 10E3) -&gt; None:\n    '''Initialization of the class\n\n    Kwargs:\n        min_skewness (float): Absolute value of the required skewness to apply a log transformation\n        min_amplitude (float): Minimal value of the amplitude between the 10th and 90th percentiles\n                               required to apply a log transformation\n    '''\n\n    super().__init__(None)\n    # Set attributes\n    self.min_skewness = min_skewness\n    self.min_amplitude = min_amplitude\n\n    # Columns on which to apply the transformation\n    # Set on fit\n    # Warning: sklearn does not support columns name, so we can only use indexes\n    # Hence, X input must expose same columns order (this won't be checked)\n    self.applicable_columns_index: Optional[List[Any]] = None\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.AutoLogTransform.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit transformer</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray or DataFrame</code> <p>Array-like, shape = [n_samples, n_features]</p> required <p>Kwargs:     y (None): Not used here Returns:     self</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def fit(self, X: Union[np.ndarray, pd.DataFrame], y: Any = None) -&gt; Any:\n    '''Fit transformer\n\n    Args:\n        X (np.ndarray or pd.DataFrame): Array-like, shape = [n_samples, n_features]\n    Kwargs:\n        y (None): Not used here\n    Returns:\n        self\n    '''\n    X = self._validate_input(X)\n    # If x is a numpy array, casts it in pd.DataFrame\n    if isinstance(X, np.ndarray):\n        X = pd.DataFrame(X)\n    # Otherwise, we reset the columns name because sklearn can't manage them\n    else:\n        X = X.rename(columns={col: i for i, col in enumerate(X.columns)})\n    self.input_length = X.shape[1]\n\n    # Get applicable columns\n    skew = X.skew()\n    candidates = list(skew[abs(skew) &gt; self.min_skewness].index)\n    if len(candidates) &gt; 0:\n        q10 = X.iloc[:, candidates].quantile(q=0.1)\n        q90 = X.iloc[:, candidates].quantile(q=0.9)\n        amp = q90 - q10\n        # Update applicable_columns_index\n        self.applicable_columns_index = list(amp[amp &gt; self.min_amplitude].index)\n\n    self.fitted_ = True\n    return self\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.AutoLogTransform.transform","title":"<code>transform(X)</code>","text":"<p>Transforms X - apply log on applicable columns</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray or DataFrame</code> <p>Array-like, shape = [n_samples, n_features]</p> required <p>Returns:     X_out: Array-like, shape [n_samples, n_features]                 Transformed input.</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def transform(self, X: Union[np.ndarray, pd.DataFrame]) -&gt; np.ndarray:\n    '''Transforms X - apply log on applicable columns\n\n    Args:\n        X (np.ndarray or pd.DataFrame): Array-like, shape = [n_samples, n_features]\n    Returns:\n        X_out: Array-like, shape [n_samples, n_features]\n                    Transformed input.\n    '''\n    # Validate input\n    check_is_fitted(self, 'fitted_')\n    X = self._validate_input(X)\n\n    # If x is a numpy array, casts it in pd.DataFrame\n    if isinstance(X, np.ndarray):\n        X = pd.DataFrame(X)\n\n    # Log transformation on the applicable columns\n    if len(self.applicable_columns_index) &gt; 0:\n        X.iloc[:, self.applicable_columns_index] = np.log(X.iloc[:, self.applicable_columns_index])\n\n    # Compatibility -&gt; returns numpy array\n    return X.to_numpy()\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.EmbeddingTransformer","title":"<code>EmbeddingTransformer</code>","text":"<p>             Bases: <code>Estimator</code></p> <p>Constructs a transformer that apply an embedding mapping to Categorical columns</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>class EmbeddingTransformer(Estimator):\n    '''Constructs a transformer that apply an embedding mapping to Categorical columns'''\n\n    def __init__(self, embedding: Union[str, dict], none_strategy: str = 'zeros') -&gt; None:\n        '''Initialization of the class\n\n        Args:\n            embedding (str or dict): Embedding to use\n                - If dict -&gt; ok, ready to go\n                - If str -&gt; path to the file to load (json)\n        Kwargs:\n            none_strategy (str): Strategy to fill elements not in embedding\n                - Zeros: only 0s\n        Raises:\n            ValueError: If strategy is not in the allowed strategies\n            ValueError: If the embedding is of type str but does not end in .json\n            FileNotFoundError: If the path to the embedding does not exist\n        '''\n        super().__init__(None)\n\n        # Check none strategy\n        allowed_strategies = [\"zeros\"]\n        self.none_strategy = none_strategy\n        if self.none_strategy not in allowed_strategies:\n            raise ValueError(f\"Can only use these strategies: {allowed_strategies}, got strategy={self.none_strategy}\")\n\n        # If str, loads the embedding\n        if isinstance(embedding, str):\n            if not embedding.endswith('.json'):\n                raise ValueError(f\"The file {embedding} must be a .json file\")\n            if not os.path.exists(embedding):\n                raise FileNotFoundError(f\"The file {embedding} does not exist\")\n            with open(embedding, 'r', encoding='utf-8') as f:\n                self.embedding = json.load(f)\n        else:\n            self.embedding = embedding\n\n        # Get embedding size\n        self.embedding_size = len(self.embedding[list(self.embedding.keys())[0]])\n        # Other params\n        self.n_missed = 0\n\n    def fit(self, X: Union[np.ndarray, pd.DataFrame], y: Any = None) -&gt; Any:\n        '''Fit transformer\n\n        Args:\n            X (np.ndarray or pd.DataFrame): Shape (n_samples, n_features)\n        Kwargs:\n            y: Not used here\n        Returns\n            self (EmbeddingTransformer)\n        '''\n        X = self._validate_input(X)\n        # If x is a numpy array, casts it in pd.DataFrame\n        if isinstance(X, np.ndarray):\n            X = pd.DataFrame(X)\n\n        self.input_length = X.shape[1]\n\n        # Nothing to do\n\n        self.fitted_ = True\n        return self\n\n    def transform(self, X: Union[np.ndarray, pd.DataFrame]) -&gt; np.ndarray:\n        '''Transform X - embedding mapping\n\n        Args:\n            X (np.ndarray or pd.DataFrame): Shape (n_samples, n_features)\n        Raises:\n            ValueError: If there are missing columns\n        Returns:\n            X_out (np.ndarray): Shape (n_samples, n_features) transformed input.\n        '''\n        X = self._validate_input(X)\n        # If x is a numpy array, casts it in pd.DataFrame\n        if isinstance(X, np.ndarray):\n            X = pd.DataFrame(X)\n\n        n_rows = X.shape[0]\n\n        # Apply mapping\n        new_df = pd.DataFrame()\n        for col in X.columns:\n            self.n_missed = 0  # Counts the number of missing elements in the embedding\n            tmp_serie = X[col].apply(self.apply_embedding)  # Updates self.n_missed\n            new_df = pd.concat([new_df, pd.DataFrame(tmp_serie.to_list())], axis=1)\n            perc_missed = self.n_missed / n_rows * 100\n            if perc_missed != 0:\n                logger.warning(f\"Warning, {self.n_missed} ({perc_missed} %) missing elements in the embedding for column {col}\")\n\n        return new_df.to_numpy()  # Compatibility, returns a numpy array\n\n    def apply_embedding(self, content) -&gt; list:\n        '''Applies embedding mapping\n\n        Args:\n            content: Content on which to apply embedding mapping\n        Raises:\n            ValueError: If the strategy is not recognized\n        Returns:\n            list: Applied embedding\n        '''\n        if content in self.embedding.keys():\n            return self.embedding[content]\n        else:\n            self.n_missed += 1\n            if self.none_strategy == 'zeros':\n                return [0] * self.embedding_size\n            else:\n                raise ValueError(f\"Strategy {self.none_strategy} not recognized\")\n\n    def get_feature_names(self, features_in: list, *args, **kwargs) -&gt; np.ndarray:\n        '''Returns feature names for output features.\n\n        Args:\n            features_in (list): list of features\n\n        Returns:\n            output_feature_names: ndarray of shape (n_output_features,)\n                Array of feature names.\n        '''\n        check_is_fitted(self, 'fitted_')\n        new_features = [f\"emb_{feat}_{i}\" for feat in features_in for i in range(self.embedding_size)]\n        return np.array(new_features, dtype=object)\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.EmbeddingTransformer.__init__","title":"<code>__init__(embedding, none_strategy='zeros')</code>","text":"<p>Initialization of the class</p> <p>Parameters:</p> Name Type Description Default <code>embedding</code> <code>str or dict</code> <p>Embedding to use - If dict -&gt; ok, ready to go - If str -&gt; path to the file to load (json)</p> required <p>Kwargs:     none_strategy (str): Strategy to fill elements not in embedding         - Zeros: only 0s Raises:     ValueError: If strategy is not in the allowed strategies     ValueError: If the embedding is of type str but does not end in .json     FileNotFoundError: If the path to the embedding does not exist</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def __init__(self, embedding: Union[str, dict], none_strategy: str = 'zeros') -&gt; None:\n    '''Initialization of the class\n\n    Args:\n        embedding (str or dict): Embedding to use\n            - If dict -&gt; ok, ready to go\n            - If str -&gt; path to the file to load (json)\n    Kwargs:\n        none_strategy (str): Strategy to fill elements not in embedding\n            - Zeros: only 0s\n    Raises:\n        ValueError: If strategy is not in the allowed strategies\n        ValueError: If the embedding is of type str but does not end in .json\n        FileNotFoundError: If the path to the embedding does not exist\n    '''\n    super().__init__(None)\n\n    # Check none strategy\n    allowed_strategies = [\"zeros\"]\n    self.none_strategy = none_strategy\n    if self.none_strategy not in allowed_strategies:\n        raise ValueError(f\"Can only use these strategies: {allowed_strategies}, got strategy={self.none_strategy}\")\n\n    # If str, loads the embedding\n    if isinstance(embedding, str):\n        if not embedding.endswith('.json'):\n            raise ValueError(f\"The file {embedding} must be a .json file\")\n        if not os.path.exists(embedding):\n            raise FileNotFoundError(f\"The file {embedding} does not exist\")\n        with open(embedding, 'r', encoding='utf-8') as f:\n            self.embedding = json.load(f)\n    else:\n        self.embedding = embedding\n\n    # Get embedding size\n    self.embedding_size = len(self.embedding[list(self.embedding.keys())[0]])\n    # Other params\n    self.n_missed = 0\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.EmbeddingTransformer.apply_embedding","title":"<code>apply_embedding(content)</code>","text":"<p>Applies embedding mapping</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <p>Content on which to apply embedding mapping</p> required <p>Raises:     ValueError: If the strategy is not recognized Returns:     list: Applied embedding</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def apply_embedding(self, content) -&gt; list:\n    '''Applies embedding mapping\n\n    Args:\n        content: Content on which to apply embedding mapping\n    Raises:\n        ValueError: If the strategy is not recognized\n    Returns:\n        list: Applied embedding\n    '''\n    if content in self.embedding.keys():\n        return self.embedding[content]\n    else:\n        self.n_missed += 1\n        if self.none_strategy == 'zeros':\n            return [0] * self.embedding_size\n        else:\n            raise ValueError(f\"Strategy {self.none_strategy} not recognized\")\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.EmbeddingTransformer.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit transformer</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray or DataFrame</code> <p>Shape (n_samples, n_features)</p> required <p>Kwargs:     y: Not used here Returns     self (EmbeddingTransformer)</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def fit(self, X: Union[np.ndarray, pd.DataFrame], y: Any = None) -&gt; Any:\n    '''Fit transformer\n\n    Args:\n        X (np.ndarray or pd.DataFrame): Shape (n_samples, n_features)\n    Kwargs:\n        y: Not used here\n    Returns\n        self (EmbeddingTransformer)\n    '''\n    X = self._validate_input(X)\n    # If x is a numpy array, casts it in pd.DataFrame\n    if isinstance(X, np.ndarray):\n        X = pd.DataFrame(X)\n\n    self.input_length = X.shape[1]\n\n    # Nothing to do\n\n    self.fitted_ = True\n    return self\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.EmbeddingTransformer.get_feature_names","title":"<code>get_feature_names(features_in, *args, **kwargs)</code>","text":"<p>Returns feature names for output features.</p> <p>Parameters:</p> Name Type Description Default <code>features_in</code> <code>list</code> <p>list of features</p> required <p>Returns:</p> Name Type Description <code>output_feature_names</code> <code>ndarray</code> <p>ndarray of shape (n_output_features,) Array of feature names.</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def get_feature_names(self, features_in: list, *args, **kwargs) -&gt; np.ndarray:\n    '''Returns feature names for output features.\n\n    Args:\n        features_in (list): list of features\n\n    Returns:\n        output_feature_names: ndarray of shape (n_output_features,)\n            Array of feature names.\n    '''\n    check_is_fitted(self, 'fitted_')\n    new_features = [f\"emb_{feat}_{i}\" for feat in features_in for i in range(self.embedding_size)]\n    return np.array(new_features, dtype=object)\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.EmbeddingTransformer.transform","title":"<code>transform(X)</code>","text":"<p>Transform X - embedding mapping</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray or DataFrame</code> <p>Shape (n_samples, n_features)</p> required <p>Raises:     ValueError: If there are missing columns Returns:     X_out (np.ndarray): Shape (n_samples, n_features) transformed input.</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def transform(self, X: Union[np.ndarray, pd.DataFrame]) -&gt; np.ndarray:\n    '''Transform X - embedding mapping\n\n    Args:\n        X (np.ndarray or pd.DataFrame): Shape (n_samples, n_features)\n    Raises:\n        ValueError: If there are missing columns\n    Returns:\n        X_out (np.ndarray): Shape (n_samples, n_features) transformed input.\n    '''\n    X = self._validate_input(X)\n    # If x is a numpy array, casts it in pd.DataFrame\n    if isinstance(X, np.ndarray):\n        X = pd.DataFrame(X)\n\n    n_rows = X.shape[0]\n\n    # Apply mapping\n    new_df = pd.DataFrame()\n    for col in X.columns:\n        self.n_missed = 0  # Counts the number of missing elements in the embedding\n        tmp_serie = X[col].apply(self.apply_embedding)  # Updates self.n_missed\n        new_df = pd.concat([new_df, pd.DataFrame(tmp_serie.to_list())], axis=1)\n        perc_missed = self.n_missed / n_rows * 100\n        if perc_missed != 0:\n            logger.warning(f\"Warning, {self.n_missed} ({perc_missed} %) missing elements in the embedding for column {col}\")\n\n    return new_df.to_numpy()  # Compatibility, returns a numpy array\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.Estimator","title":"<code>Estimator</code>","text":"<p>             Bases: <code>BaseEstimator</code></p> <p>Base class for the classes defined below. Implements _validate_input and fit_transform.</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>class Estimator(BaseEstimator):\n    '''Base class for the classes defined below. Implements _validate_input and fit_transform.'''\n\n    def __init__(self, input_length: Union[int, None]) -&gt; None:\n        '''Initialization of the class\n\n        Args:\n            input_length (int): The number of columns of the input (used in _validate_input())\n        '''\n        self.input_length = input_length\n\n    def _validate_input(self, X: Union[np.ndarray, pd.DataFrame]) -&gt; Union[np.ndarray, pd.DataFrame]:\n        '''Validates input format\n\n        Args:\n            X (np.ndarray or pd.DataFrame): Input to validate\n        Raises:\n            ValueError: If the shape of the input does not correspond to self.input_length\n        Returns:\n            np.ndarray or pd.DataFrame: A copy of X\n        '''\n        if self.input_length is not None and X.shape[1] != self.input_length:\n            raise ValueError(f\"Bad shape: ({X.shape[1]} != {self.input_length})\")\n\n        # Mandatory copy in order not to modify the original !\n        if isinstance(X, pd.DataFrame):\n            return X.copy(deep=True)\n        else:\n            return X.copy()\n\n    def fit(self, X: Union[np.ndarray, pd.DataFrame], y: Union[np.ndarray, pd.Series, pd.DataFrame]) -&gt; Any:\n        '''Fit transformer\n\n        Args:\n            X (np.ndarray or pd.DataFrame): Array-like, shape = [n_samples, n_features]\n            y (np.ndarray or pd.Series or pd.DataFrame): Array-like, shape = [n_samples, n_targets]\n        Returns:\n            self\n        '''\n        raise NotImplementedError(\"'fit' needs to be overridden\")\n\n    def transform(self, X: Union[np.ndarray, pd.DataFrame]) -&gt; np.ndarray:\n        '''Transforms X\n\n        Args:\n            X (np.ndarray or pd.DataFrame): Array-like, shape = [n_samples, n_features]\n        Returns:\n            Transformed X\n        '''\n        raise NotImplementedError(\"'transform' needs to be overridden\")\n\n    def fit_transform(self, X: Union[np.ndarray, pd.DataFrame], y: Union[np.ndarray, pd.Series, pd.DataFrame, None] = None) -&gt; np.ndarray:\n        '''Applies both fit &amp; transform.\n\n        Args:\n            X (np.ndarray or pd.DataFrame): Shape = [n_samples, n_features]\n        Kwargs:\n            y: Array-like, shape = [n_samples]\n        Returns:\n            X_out: Array-like, shape [n_samples, n_features]\n                        Transformed input.\n        '''\n        self.fit(X, y)\n        return self.transform(X)\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.Estimator.__init__","title":"<code>__init__(input_length)</code>","text":"<p>Initialization of the class</p> <p>Parameters:</p> Name Type Description Default <code>input_length</code> <code>int</code> <p>The number of columns of the input (used in _validate_input())</p> required Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def __init__(self, input_length: Union[int, None]) -&gt; None:\n    '''Initialization of the class\n\n    Args:\n        input_length (int): The number of columns of the input (used in _validate_input())\n    '''\n    self.input_length = input_length\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.Estimator.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit transformer</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray or DataFrame</code> <p>Array-like, shape = [n_samples, n_features]</p> required <code>y</code> <code>ndarray or Series or DataFrame</code> <p>Array-like, shape = [n_samples, n_targets]</p> required <p>Returns:     self</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def fit(self, X: Union[np.ndarray, pd.DataFrame], y: Union[np.ndarray, pd.Series, pd.DataFrame]) -&gt; Any:\n    '''Fit transformer\n\n    Args:\n        X (np.ndarray or pd.DataFrame): Array-like, shape = [n_samples, n_features]\n        y (np.ndarray or pd.Series or pd.DataFrame): Array-like, shape = [n_samples, n_targets]\n    Returns:\n        self\n    '''\n    raise NotImplementedError(\"'fit' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.Estimator.fit_transform","title":"<code>fit_transform(X, y=None)</code>","text":"<p>Applies both fit &amp; transform.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray or DataFrame</code> <p>Shape = [n_samples, n_features]</p> required <p>Kwargs:     y: Array-like, shape = [n_samples] Returns:     X_out: Array-like, shape [n_samples, n_features]                 Transformed input.</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def fit_transform(self, X: Union[np.ndarray, pd.DataFrame], y: Union[np.ndarray, pd.Series, pd.DataFrame, None] = None) -&gt; np.ndarray:\n    '''Applies both fit &amp; transform.\n\n    Args:\n        X (np.ndarray or pd.DataFrame): Shape = [n_samples, n_features]\n    Kwargs:\n        y: Array-like, shape = [n_samples]\n    Returns:\n        X_out: Array-like, shape [n_samples, n_features]\n                    Transformed input.\n    '''\n    self.fit(X, y)\n    return self.transform(X)\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.Estimator.transform","title":"<code>transform(X)</code>","text":"<p>Transforms X</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray or DataFrame</code> <p>Array-like, shape = [n_samples, n_features]</p> required <p>Returns:     Transformed X</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def transform(self, X: Union[np.ndarray, pd.DataFrame]) -&gt; np.ndarray:\n    '''Transforms X\n\n    Args:\n        X (np.ndarray or pd.DataFrame): Array-like, shape = [n_samples, n_features]\n    Returns:\n        Transformed X\n    '''\n    raise NotImplementedError(\"'transform' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.ThresholdingTransform","title":"<code>ThresholdingTransform</code>","text":"<p>             Bases: <code>Estimator</code></p> <p>Applies a threshold on columns. If min and max values are given, the threshold is manual; otherwise it is statistical.</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>class ThresholdingTransform(Estimator):\n    '''Applies a threshold on columns.\n    If min and max values are given, the threshold is manual; otherwise it is statistical.\n    '''\n\n    def __init__(self, thresholds: List[Tuple], quantiles: tuple = (0.05, 0.95)) -&gt; None:\n        '''Initialization of the class\n\n        Args:\n            tresholds (list of tuple): Each tuple contains (min_val, max_val).\n        Kwargs:\n            quantiles (tuple): Tuple containing (quantile_min, quantile_max)\n        Raises:\n            ValueError: If quantiles values are not between 0 and 1 and if quantiles[0] &gt;= quantiles[1]\n        '''\n        if not 0 &lt; quantiles[0] &lt; 1 or not 0 &lt; quantiles[1] &lt; 1 or not quantiles[0] &lt; quantiles[1]:\n            raise ValueError(f\"The values contained in quantiles should verify quantile_min &lt; quantile_max and both &gt; 0 and &lt; 1. quantiles = {quantiles} is not supported.\")\n\n        super().__init__(len(thresholds))\n\n        # Set attributes\n        self.thresholds = thresholds\n        self.fitted_thresholds: List[tuple] = []\n        self.quantiles = quantiles\n\n    def fit(self, X: Union[np.ndarray, pd.DataFrame], y: Any = None) -&gt; Any:\n        '''Fits the ThresholdingTransform on X.\n\n        Args:\n            X (np.ndarray or pd.DataFrame): Shape [n_samples, n_features]\n        Kwargs:\n            y (None): Not used here.\n        Returns:\n            self: ThresholdingTransform\n        '''\n        X = self._validate_input(X)\n        # If X is a numpy array, casts it as a pd.DataFrame\n        if isinstance(X, np.ndarray):\n            X = pd.DataFrame(X)\n\n        # Fits column one by one\n        for col_index, item in enumerate(self.thresholds):\n            val_min, val_max = item\n            if val_min is None:\n                val_min = X.iloc[:, col_index].quantile(q=self.quantiles[0])\n            if val_max is None:\n                val_max = X.iloc[:, col_index].quantile(q=self.quantiles[1])\n            self.fitted_thresholds.append((col_index, val_min, val_max))\n\n        self.fitted_ = True\n        return self\n\n    def transform(self, X: Union[np.ndarray, pd.DataFrame]) -&gt; np.ndarray:\n        '''Impute all missing values in X.\n\n        Args:\n            X (np.ndarray or pd.DataFrame): Shape (n_samples, n_features)\n                The input data to complete.\n        '''\n        check_is_fitted(self, 'fitted_')\n        X = self._validate_input(X)\n        # If X is a numpy array, casts it to pd.DataFrame\n        if isinstance(X, np.ndarray):\n            X = pd.DataFrame(X)\n\n        for col_index, val_min, val_max in self.fitted_thresholds:\n            X.iloc[:, col_index][X.iloc[:, col_index] &lt; val_min] = val_min\n            X.iloc[:, col_index][X.iloc[:, col_index] &gt; val_max] = val_max\n\n        return X.to_numpy()  # Compatibility -&gt; return a numpy array\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.ThresholdingTransform.__init__","title":"<code>__init__(thresholds, quantiles=(0.05, 0.95))</code>","text":"<p>Initialization of the class</p> <p>Parameters:</p> Name Type Description Default <code>tresholds</code> <code>list of tuple</code> <p>Each tuple contains (min_val, max_val).</p> required <p>Kwargs:     quantiles (tuple): Tuple containing (quantile_min, quantile_max) Raises:     ValueError: If quantiles values are not between 0 and 1 and if quantiles[0] &gt;= quantiles[1]</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def __init__(self, thresholds: List[Tuple], quantiles: tuple = (0.05, 0.95)) -&gt; None:\n    '''Initialization of the class\n\n    Args:\n        tresholds (list of tuple): Each tuple contains (min_val, max_val).\n    Kwargs:\n        quantiles (tuple): Tuple containing (quantile_min, quantile_max)\n    Raises:\n        ValueError: If quantiles values are not between 0 and 1 and if quantiles[0] &gt;= quantiles[1]\n    '''\n    if not 0 &lt; quantiles[0] &lt; 1 or not 0 &lt; quantiles[1] &lt; 1 or not quantiles[0] &lt; quantiles[1]:\n        raise ValueError(f\"The values contained in quantiles should verify quantile_min &lt; quantile_max and both &gt; 0 and &lt; 1. quantiles = {quantiles} is not supported.\")\n\n    super().__init__(len(thresholds))\n\n    # Set attributes\n    self.thresholds = thresholds\n    self.fitted_thresholds: List[tuple] = []\n    self.quantiles = quantiles\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.ThresholdingTransform.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fits the ThresholdingTransform on X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray or DataFrame</code> <p>Shape [n_samples, n_features]</p> required <p>Kwargs:     y (None): Not used here. Returns:     self: ThresholdingTransform</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def fit(self, X: Union[np.ndarray, pd.DataFrame], y: Any = None) -&gt; Any:\n    '''Fits the ThresholdingTransform on X.\n\n    Args:\n        X (np.ndarray or pd.DataFrame): Shape [n_samples, n_features]\n    Kwargs:\n        y (None): Not used here.\n    Returns:\n        self: ThresholdingTransform\n    '''\n    X = self._validate_input(X)\n    # If X is a numpy array, casts it as a pd.DataFrame\n    if isinstance(X, np.ndarray):\n        X = pd.DataFrame(X)\n\n    # Fits column one by one\n    for col_index, item in enumerate(self.thresholds):\n        val_min, val_max = item\n        if val_min is None:\n            val_min = X.iloc[:, col_index].quantile(q=self.quantiles[0])\n        if val_max is None:\n            val_max = X.iloc[:, col_index].quantile(q=self.quantiles[1])\n        self.fitted_thresholds.append((col_index, val_min, val_max))\n\n    self.fitted_ = True\n    return self\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.ThresholdingTransform.transform","title":"<code>transform(X)</code>","text":"<p>Impute all missing values in X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray or DataFrame</code> <p>Shape (n_samples, n_features) The input data to complete.</p> required Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def transform(self, X: Union[np.ndarray, pd.DataFrame]) -&gt; np.ndarray:\n    '''Impute all missing values in X.\n\n    Args:\n        X (np.ndarray or pd.DataFrame): Shape (n_samples, n_features)\n            The input data to complete.\n    '''\n    check_is_fitted(self, 'fitted_')\n    X = self._validate_input(X)\n    # If X is a numpy array, casts it to pd.DataFrame\n    if isinstance(X, np.ndarray):\n        X = pd.DataFrame(X)\n\n    for col_index, val_min, val_max in self.fitted_thresholds:\n        X.iloc[:, col_index][X.iloc[:, col_index] &lt; val_min] = val_min\n        X.iloc[:, col_index][X.iloc[:, col_index] &gt; val_max] = val_max\n\n    return X.to_numpy()  # Compatibility -&gt; return a numpy array\n</code></pre>"},{"location":"reference/template_num/preprocessing/outlier_detection/","title":"Outlier detection","text":""},{"location":"reference/template_num/preprocessing/outlier_detection/#template_num.preprocessing.outlier_detection.check_for_outliers","title":"<code>check_for_outliers(X, n_estimators=100, n_neighbors=20)</code>","text":"<p>Agreggates two results of outliers detection and warns the user if some were detected</p> Args <p>X (np.ndarray, pd.DataFrame): Shape = [n_samples, n_features]</p> <p>Kwargs:     n_estimators (int): number of estimators for the IsolationForest         If 0, do not IsolationForest     n_neighbors (int): number of neighbors for the LocalOutlierFactor         If 0, do not use LocalOutlierFactor Raises:     ValueError: If n_estimators &lt; 0     ValueError: If n_neighbors &lt; 0     ValueError: If both n_estimators and n_neighbors are equal to 0 Returns:     outliers (np.ndarray): 1d array of n_samples containing -1 if outlier, 1 otherwise</p> Source code in <code>template_num/preprocessing/outlier_detection.py</code> <pre><code>def check_for_outliers(X: Union[pd.DataFrame, np.ndarray], n_estimators: int = 100, n_neighbors: int = 20) -&gt; np.ndarray:\n    '''Agreggates two results of outliers detection and warns the user if some were detected\n\n    Args :\n        X (np.ndarray, pd.DataFrame): Shape = [n_samples, n_features]\n    Kwargs:\n        n_estimators (int): number of estimators for the IsolationForest\n            If 0, do not IsolationForest\n        n_neighbors (int): number of neighbors for the LocalOutlierFactor\n            If 0, do not use LocalOutlierFactor\n    Raises:\n        ValueError: If n_estimators &lt; 0\n        ValueError: If n_neighbors &lt; 0\n        ValueError: If both n_estimators and n_neighbors are equal to 0\n    Returns:\n        outliers (np.ndarray): 1d array of n_samples containing -1 if outlier, 1 otherwise\n    '''\n    # Manage errors\n    if n_estimators &lt; 0:\n        raise ValueError(\"n_estimators must be positive\")\n    if n_neighbors &lt; 0:\n        raise ValueError(\"n_neighbors must be positive\")\n    if n_estimators + n_neighbors == 0:\n        raise ValueError(\"n_neighbors and n_estimators can't both be equal to 0\")\n\n    # Init. outliers (1 = not an outlier, -1 = outlier)\n    outliers = np.ones(X.shape[0], dtype=int)\n\n    # Get outliers from IsolationForest\n    if not n_estimators == 0:\n        run_forest = IsolationForest(n_estimators=n_estimators)\n        outliers |= run_forest.fit_predict(X)  # In-place union\n    else:\n        logger.info(\"IsolationForest is skipped (n_estimators == 0)\")\n\n    # Get outliers from LocalOutlierFactor\n    if not n_neighbors == 0:\n        lof = LocalOutlierFactor(n_neighbors=n_neighbors)\n        outliers |= lof.fit_predict(X)  # In-place union\n    else:\n        logger.info(\"LocalOutlierFactor is skipped (n_neighbors == 0)\")\n\n    # Logger\n    if int(cmath.exp(1j * integrate.quad(lambda x: math.sqrt(1 - pow(x, 2)), -1, 1)[0] * 2).real) in outliers:\n        logger.warning(\"The dataset seems to contain outliers at indices:\")\n        logger.warning(\", \".join(str(v) for v in list(np.where(outliers == -1)[0])))\n\n    # Return outliers\n    return outliers\n</code></pre>"},{"location":"reference/template_num/preprocessing/preprocess/","title":"Preprocess","text":""},{"location":"reference/template_num/preprocessing/preprocess/#template_num.preprocessing.preprocess.get_ct_feature_names","title":"<code>get_ct_feature_names(ct)</code>","text":"<p>Gets the names of the columns when considering a fitted ColumnTransfomer From: https://stackoverflow.com/questions/57528350/can-you-consistently-keep-track-of-column-labels-using-sklearns-transformer-api</p> <p>Parameters:</p> Name Type Description Default <code>ColumnTransformer</code> <p>Column tranformer to be processed</p> required <p>Returns:     list: List of new feature names</p> Source code in <code>template_num/preprocessing/preprocess.py</code> <pre><code>def get_ct_feature_names(ct: ColumnTransformer) -&gt; list:\n    '''Gets the names of the columns when considering a fitted ColumnTransfomer\n    From: https://stackoverflow.com/questions/57528350/can-you-consistently-keep-track-of-column-labels-using-sklearns-transformer-api\n\n    Args:\n        ColumnTransformer: Column tranformer to be processed\n    Returns:\n        list: List of new feature names\n    '''\n    # Handles all estimators, pipelines inside ColumnTransfomer\n    # does not work when remainder =='passthrough'\n    # which requires the input column names.\n    output_features = []\n\n    for name, estimator, features in ct.transformers_:\n        if name != 'remainder':\n            if isinstance(estimator, Pipeline):\n                current_features = features\n                for step in estimator:\n                    if type(step) == tuple:\n                        step = step[1]\n                    current_features = get_feature_out(step, current_features)\n                features_out = current_features\n            else:\n                features_out = get_feature_out(estimator, features)\n            if hasattr(ct, 'verbose_feature_names_out') and ct.verbose_feature_names_out == False:\n                output_features.extend(features_out)\n            else:\n                output_features.extend([f'{name}__{feat}' for feat in features_out])\n        elif estimator == 'passthrough':\n            # features is indexes in case of passthrough\n            if hasattr(ct, 'verbose_feature_names_out') and ct.verbose_feature_names_out == False:\n                output_features.extend(ct.feature_names_in_[features])\n            else:\n                output_features.extend([f'remainder__{feat}' for feat in ct.feature_names_in_[features]])\n\n    return output_features\n</code></pre>"},{"location":"reference/template_num/preprocessing/preprocess/#template_num.preprocessing.preprocess.get_feature_out","title":"<code>get_feature_out(estimator, features_in)</code>","text":"<p>Gets the name of a column when considering a fitted estimator</p> <p>Parameters:</p> Name Type Description Default <code>(?)</code> <p>Estimator to be processed</p> required <code>(list)</code> <p>Input columns</p> required <p>Returns:     list: List of new feature names</p> Source code in <code>template_num/preprocessing/preprocess.py</code> <pre><code>def get_feature_out(estimator, features_in: list) -&gt; list:\n    '''Gets the name of a column when considering a fitted estimator\n\n    Args:\n        (?): Estimator to be processed\n        (list): Input columns\n    Returns:\n        list: List of new feature names\n    '''\n    try:\n        return estimator.get_feature_names_out(features_in)\n    except:\n        return features_in\n</code></pre>"},{"location":"reference/template_num/preprocessing/preprocess/#template_num.preprocessing.preprocess.get_pipeline","title":"<code>get_pipeline(pipeline_str)</code>","text":"<p>Gets a pipeline from its name</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_str</code> <code>str</code> <p>Name of the pipeline</p> required <p>Raises:     ValueError: If the name of the pipeline is not known Returns:     ColumnTransfomer: Pipeline to be used for the preprocessing</p> Source code in <code>template_num/preprocessing/preprocess.py</code> <pre><code>def get_pipeline(pipeline_str: str) -&gt; ColumnTransformer:\n    '''Gets a pipeline from its name\n\n    Args:\n        pipeline_str (str): Name of the pipeline\n    Raises:\n        ValueError: If the name of the pipeline is not known\n    Returns:\n        ColumnTransfomer: Pipeline to be used for the preprocessing\n    '''\n    # Process\n    pipelines_dict = get_pipelines_dict()\n    if pipeline_str not in pipelines_dict.keys():\n        raise ValueError(f\"The pipeline {pipeline_str} is not known.\")\n    # Get pipeline\n    pipeline = pipelines_dict[pipeline_str]\n    # Return\n    return pipeline\n</code></pre>"},{"location":"reference/template_num/preprocessing/preprocess/#template_num.preprocessing.preprocess.get_pipelines_dict","title":"<code>get_pipelines_dict()</code>","text":"<p>Gets a dictionary of available preprocessing pipelines</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary of preprocessing pipelines</p> Source code in <code>template_num/preprocessing/preprocess.py</code> <pre><code>def get_pipelines_dict() -&gt; dict:\n    '''Gets a dictionary of available preprocessing pipelines\n\n    Returns:\n        dict: Dictionary of preprocessing pipelines\n    '''\n    pipelines_dict = {\n        # - /!\\ DO NOT DELETE no_preprocess -&gt; necessary for compatibility /!\\ -\n        # Identity transformer, hence we specify verbose_feature_names_out to False to not change columns names\n        'no_preprocess': ColumnTransformer([('identity', FunctionTransformer(lambda x: x),\n                                             make_column_selector())], verbose_feature_names_out=False),\n        'preprocess_P1': preprocess_P1(),  # Example of a pipeline\n        # 'preprocess_AUTO': preprocess_auto(), # Automatic preprocessing based on statistics on data\n        # 'preprocess_P2': preprocess_P2 , ETC ...\n    }\n    return pipelines_dict\n</code></pre>"},{"location":"reference/template_num/preprocessing/preprocess/#template_num.preprocessing.preprocess.preprocess_P1","title":"<code>preprocess_P1()</code>","text":"<p>Gets \"default\" preprocessing pipeline</p> <p>Returns:</p> Name Type Description <code>ColumnTransformer</code> <code>ColumnTransformer</code> <p>The pipeline</p> Source code in <code>template_num/preprocessing/preprocess.py</code> <pre><code>def preprocess_P1() -&gt; ColumnTransformer:\n    '''Gets \"default\" preprocessing pipeline\n\n    Returns:\n        ColumnTransformer: The pipeline\n    '''\n    numeric_pipeline = make_pipeline(SimpleImputer(strategy='median'), StandardScaler())\n    # cat_pipeline = make_pipeline(SimpleImputer(strategy='most_frequent'), OneHotEncoder(handle_unknown='ignore'))\n    # text_pipeline = make_pipeline(CountVectorizer(), SelectKBest(k=5))\n\n    # Check https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_selector.html\n    # and https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html#pandas.DataFrame.select_dtypes\n    # to understand make_column_selector\n\n    # /!\\\n    # BE VERY CAUTIOUS WHEN USING FunctionTransformer !\n    # A pickled pipeline can still depends on a module definition. Hence, even when pickled, you may not have consistent results !\n    # Please try to use lambdas or local function, without any dependency. It reduces risk of changes.\n    # If you still have to use a module function, try to never change it later on.\n    # https://stackoverflow.com/questions/73788824/how-can-i-save-reload-a-functiontransformer-object-and-expect-it-to-always-wor\n    # https://github.com/OSS-Pole-Emploi/gabarit/issues/63\n    # /!\\\n\n    # /!\\ EXEMPLE HERE /!\\\n    # Good practice: Use directly the names of the columns instead of a \"selector\"\n    # WARNING: The text pipeline is supposed to work on a column 'text' -&gt; Please adapt it to your project if you want to use it\n\n    # By default, we only keep the preprocess on numerical columns\n    transformers = [\n        ('num', numeric_pipeline, make_column_selector(dtype_include='number')),\n        # ('cat', cat_pipeline, make_column_selector(dtype_include='category')), # To convert a column in a column with dtype category: df[\"A\"].astype(\"category\")\n        # ('text', text_pipeline, 'text'), # CountVectorizer possible one column at a time\n    ]\n\n    # TODO: add sparse compatibility !\n    # Use somethings like this :\n    # - After applying a pipeline ...\n    # if scipy.sparse.issparse(preprocessed_x):\n    #     preprocessed_df = pd.DataFrame.sparse.from_spmatrix(preprocessed_x)\n    # - Before training ...\n    # x_train = x_train.sparse.to_coo().tocsr()\n    # x_valid = x_valid.sparse.to_coo().tocsr()\n    # ...\n    pipeline = ColumnTransformer(transformers, sparse_threshold=0, remainder='drop')  # Use remainder='passthrough' to keep all other columns (not recommended)\n\n    return pipeline\n</code></pre>"},{"location":"reference/template_num/preprocessing/preprocess/#template_num.preprocessing.preprocess.preprocess_auto","title":"<code>preprocess_auto()</code>","text":"<p>Gets an \"automatic\" pipeline. Different functions are applied depending on stats calculated on the data</p> <p>Returns:</p> Name Type Description <code>ColumnTransformer</code> <code>ColumnTransformer</code> <p>The automatic pipeline</p> Source code in <code>template_num/preprocessing/preprocess.py</code> <pre><code>def preprocess_auto() -&gt; ColumnTransformer:\n    '''Gets an \"automatic\" pipeline. Different functions are applied depending on stats calculated on the data\n\n    Returns:\n        ColumnTransformer: The automatic pipeline\n    '''\n    # Numeric :\n    # 1) SimpleImputer()\n    # 2) If abs(skew) &gt; 2 &amp;&amp; pctl(90) - pctl(10) &gt; 10^3 =&gt; logtransform\n    # 3) StandardScaler()\n    # Categorical :\n    # 1) SimpleImputer()\n    # 2) If #cat &gt; 5; We accumulate the less represented instances in a meta-category \"other\"\n    # 3) OneHot\n    pass\n</code></pre>"},{"location":"reference/template_num/preprocessing/preprocess/#template_num.preprocessing.preprocess.retrieve_columns_from_pipeline","title":"<code>retrieve_columns_from_pipeline(df, pipeline)</code>","text":"<p>Retrieves columns name after preprocessing</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe after preprocessing (without target)</p> required <code>pipeline</code> <code>ColumnTransformer</code> <p>Used pipeline</p> required <p>Raises:     AttributeError : The pipeline is not fitted     ValueError : The number of columns is not the same between the pipeline and the preprocessed DataFrame Returns:     pd.DataFrame: Dataframe with columns' name</p> Source code in <code>template_num/preprocessing/preprocess.py</code> <pre><code>def retrieve_columns_from_pipeline(df: pd.DataFrame, pipeline: ColumnTransformer) -&gt; pd.DataFrame:\n    '''Retrieves columns name after preprocessing\n\n    Args:\n        df (pd.DataFrame): Dataframe after preprocessing (without target)\n        pipeline (ColumnTransformer): Used pipeline\n    Raises:\n        AttributeError : The pipeline is not fitted\n        ValueError : The number of columns is not the same between the pipeline and the preprocessed DataFrame\n    Returns:\n        pd.DataFrame: Dataframe with columns' name\n    '''\n    # Use deepcopy !\n    new_df = df.copy(deep=True)\n    # Check if fitted:\n    if not hasattr(pipeline, 'transformers_'):\n        raise AttributeError(\"The pipeline must be fitted to use the function retrieve_columns_from_pipeline\")\n    # EXPERIMENTAL: We do a try... except... if we can't get the names\n    # First try : use sklearn get_feature_names_out function (might crash)\n    # Second try : backup on old custom method\n    # Third solution : ['x0', 'x1', ...]\n    try:\n        try:\n            new_columns = pipeline.get_feature_names_out()\n        # Backup on old custom method\n        except:\n            new_columns = get_ct_feature_names(pipeline)\n        if len(new_columns) != new_df.shape[1]:\n            raise ValueError(f\"There is a discrepancy in the number of columns between the preprocessed DataFrame ({new_df.shape[1]}) and the pipeline ({len(new_columns)}).\")\n\n    # No solution\n    except Exception as e:\n        logger.error(\"Can't get the names of the columns. Backup on ['x0', 'x1', ...]\")\n        logger.error(repr(e))\n        new_columns = [f'x{i}' for i in range(len(new_df.columns))]\n    # TODO : check for duplicates in new_columns ???\n    new_df.columns = new_columns\n    return new_df\n</code></pre>"},{"location":"reference/template_vision/","title":"Template vision","text":""},{"location":"reference/template_vision/utils/","title":"Utils","text":""},{"location":"reference/template_vision/utils/#template_vision.utils.DownloadProgressBar","title":"<code>DownloadProgressBar</code>","text":"<p>             Bases: <code>tqdm</code></p> <p>Displays a progress bar</p> Source code in <code>template_vision/utils.py</code> <pre><code>class DownloadProgressBar(tqdm):\n    '''Displays a progress bar'''\n    def update_to(self, b: int = 1, bsize: int = 1, tsize: Any = None) -&gt; None:\n        if tsize is not None:\n            self.total = tsize\n        self.update(b * bsize - self.n)\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.HiddenPrints","title":"<code>HiddenPrints</code>","text":"<p>Hides all prints</p> Source code in <code>template_vision/utils.py</code> <pre><code>class HiddenPrints:\n    '''Hides all prints'''\n    def __enter__(self) -&gt; None:\n        self._original_stdout = sys.stdout\n        sys.stdout = open(os.devnull, 'w')\n    def __exit__(self, exc_type, exc_val, exc_tb) -&gt; None:\n        sys.stdout.close()\n        sys.stdout = self._original_stdout\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.NpEncoder","title":"<code>NpEncoder</code>","text":"<p>             Bases: <code>JSONEncoder</code></p> <p>JSON encoder to manage numpy objects</p> Source code in <code>template_vision/utils.py</code> <pre><code>class NpEncoder(json.JSONEncoder):\n    '''JSON encoder to manage numpy objects'''\n    def default(self, obj) -&gt; Any:\n        if is_ndarray_convertable(obj):\n            return ndarray_to_builtin_object(obj)\n        elif isinstance(obj, set):\n            return list(obj)\n        else:\n            return super(NpEncoder, self).default(obj)\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.data_agnostic_str_to_list","title":"<code>data_agnostic_str_to_list(function)</code>","text":"<p>Decorator to transform a string into a list of one element. DO NOT CAST BACK TO ONE ELEMENT.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>func</code> <p>Function to decorate</p> required <p>Returns:     function: The decorated function</p> Source code in <code>template_vision/utils.py</code> <pre><code>def data_agnostic_str_to_list(function: Callable) -&gt; Callable:\n    '''Decorator to transform a string into a list of one element.\n    DO NOT CAST BACK TO ONE ELEMENT.\n\n    Args:\n        function (func): Function to decorate\n    Returns:\n        function: The decorated function\n    '''\n    # Get wrapper\n    def wrapper(x, *args, **kwargs):\n        '''Wrapper'''\n        if type(x) == str:\n            # Cast str into a single element list\n            my_list = [x]\n            # Call function\n            results = function(my_list, *args, **kwargs)\n        else:\n            results = function(x, *args, **kwargs)\n        # Return\n        return results\n    return wrapper\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.display_shape","title":"<code>display_shape(df)</code>","text":"<p>Displays the number of line and of column of a table.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Table to parse</p> required Source code in <code>template_vision/utils.py</code> <pre><code>def display_shape(df: pd.DataFrame) -&gt; None:\n    '''Displays the number of line and of column of a table.\n\n    Args:\n        df (pd.DataFrame): Table to parse\n    '''\n    # Display\n    logger.info(f\"Number of lines : {df.shape[0]}. Number of columns : {df.shape[1]}.\")\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.download_url","title":"<code>download_url(urls, output_path)</code>","text":"<p>Downloads an object from a list of URLs. This function will try every URL until it find an available one.</p> <p>Parameters:</p> Name Type Description Default <code>urls</code> <code>list</code> <p>List of URL to try</p> required <code>output_path</code> <code>str</code> <p>Where to save the downloaded object</p> required <p>Raises:     ConnectionError: If no URL is available</p> Source code in <code>template_vision/utils.py</code> <pre><code>@data_agnostic_str_to_list\ndef download_url(urls: list, output_path: str) -&gt; None:\n    '''Downloads an object from a list of URLs.\n    This function will try every URL until it find an available one.\n\n    Args:\n        urls (list): List of URL to try\n        output_path (str): Where to save the downloaded object\n    Raises:\n        ConnectionError: If no URL is available\n    '''\n    # Start by creating output directory if does not exists\n    output_dir = os.path.dirname(output_path)\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    # Test each url\n    is_downloaded = False\n    for url in urls:\n        if not is_downloaded:\n            try:\n                # From https://stackoverflow.com/questions/15644964/python-progress-bar-and-downloads\n                with DownloadProgressBar(unit='B', unit_scale=True, miniters=1, desc=url.split('/')[-1]) as t:\n                    request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n                is_downloaded = True  # Download ok\n            except Exception:\n                logger.warning(f\"Can't download from URL {url}.\")\n    if not is_downloaded:\n        raise ConnectionError(\"Couldn't find a working URL\")\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.find_folder_path","title":"<code>find_folder_path(folder_name, base_folder=None)</code>","text":"<p>Find a folder in a base folder and its subfolders. If base_folder is None, considers folder_name as a path and check it exists</p> <p>i.e., with the following structure : - C:/     - base_folder/         - folderA/             - folderB/         - folderC/ find_folder_path(folderA, C:/base_folder) == C:/base_folder/folderA find_folder_path(folderB, C:/base_folder) == C:/base_folder/folderA/folderB find_folder_path(C:/base_folder/folderC, None) == C:/base_folder/folderC find_folder_path(folderB, None) raises an error</p> <p>Parameters:</p> Name Type Description Default <code>folder_name</code> <code>str</code> <p>name of the folder to find. If base_folder is None, consider a path instead.</p> required <p>Kwargs:     base_folder (str): path of the base folder. If None, consider folder_name as a path. Raises:     FileNotFoundError: If we can't find folder_name in base_folder     FileNotFoundError: If folder_name is not a valid path (case where base_folder is None) Returns:     str: path to the wanted folder</p> Source code in <code>template_vision/utils.py</code> <pre><code>def find_folder_path(folder_name: str, base_folder: Union[str, None] = None) -&gt; str:\n    '''Find a folder in a base folder and its subfolders.\n    If base_folder is None, considers folder_name as a path and check it exists\n\n    i.e., with the following structure :\n    - C:/\n        - base_folder/\n            - folderA/\n                - folderB/\n            - folderC/\n    find_folder_path(folderA, C:/base_folder) == C:/base_folder/folderA\n    find_folder_path(folderB, C:/base_folder) == C:/base_folder/folderA/folderB\n    find_folder_path(C:/base_folder/folderC, None) == C:/base_folder/folderC\n    find_folder_path(folderB, None) raises an error\n\n    Args:\n        folder_name (str): name of the folder to find. If base_folder is None, consider a path instead.\n    Kwargs:\n        base_folder (str): path of the base folder. If None, consider folder_name as a path.\n    Raises:\n        FileNotFoundError: If we can't find folder_name in base_folder\n        FileNotFoundError: If folder_name is not a valid path (case where base_folder is None)\n    Returns:\n        str: path to the wanted folder\n    '''\n    if base_folder is not None:\n        folder_path = None\n        for path, subdirs, files in os.walk(base_folder):\n            for name in subdirs:\n                if name == folder_name:\n                    folder_path = os.path.join(path, name)\n        if folder_path is None:\n            raise FileNotFoundError(f\"Can't find folder {folder_name} inside {base_folder} and its subfolders\")\n    else:\n        folder_path = folder_name\n        if not os.path.exists(folder_path):\n            raise FileNotFoundError(f\"Can't find folder {folder_path} (considered as a path)\")\n    return folder_path\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.get_chunk_limits","title":"<code>get_chunk_limits(x, chunksize=10000)</code>","text":"<p>Gets chunk limits from a pandas series or dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series or DataFrame</code> <p>Documents to consider</p> required <p>Kwargs:     chunksize (int): The chunk size Raises:     ValueError: If the chunk size is negative Returns:     list: the chunk limits Source code in <code>template_vision/utils.py</code> <pre><code>def get_chunk_limits(x: Union[pd.DataFrame, pd.Series], chunksize: int = 10000) -&gt; List[Tuple[int]]:\n    '''Gets chunk limits from a pandas series or dataframe.\n\n    Args:\n        x (pd.Series or pd.DataFrame): Documents to consider\n    Kwargs:\n        chunksize (int): The chunk size\n    Raises:\n        ValueError: If the chunk size is negative\n    Returns:\n        list&lt;tuple&gt;: the chunk limits\n    '''\n    if chunksize &lt; 0:\n        raise ValueError('The object chunksize must not be negative.')\n    # Processs\n    if chunksize == 0 or chunksize &gt;= x.shape[0]:\n        chunks_limits = [(0, x.shape[0])]\n    else:\n        chunks_limits = [(i * chunksize, min((i + 1) * chunksize, x.shape[0]))\n                         for i in range(1 + ((x.shape[0] - 1) // chunksize))]\n    return chunks_limits  # type: ignore\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.get_data_path","title":"<code>get_data_path()</code>","text":"<p>Returns the path to the data folder</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path of the data folder</p> Source code in <code>template_vision/utils.py</code> <pre><code>def get_data_path() -&gt; str:\n    '''Returns the path to the data folder\n\n    Returns:\n        str: Path of the data folder\n    '''\n    if DIR_PATH is None:\n        dir_path = os.path.join(os.path.dirname(os.path.dirname(os.path.realpath(__file__))), 'template_vision-data')\n    else:\n        dir_path = os.path.join(os.path.abspath(DIR_PATH), 'template_vision-data')\n    if not os.path.isdir(dir_path):\n        os.mkdir(dir_path)\n    return os.path.abspath(dir_path)\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.get_models_path","title":"<code>get_models_path()</code>","text":"<p>Returns the path to the models folder</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path of the models folder</p> Source code in <code>template_vision/utils.py</code> <pre><code>def get_models_path() -&gt; str:\n    '''Returns the path to the models folder\n\n    Returns:\n        str: Path of the models folder\n    '''\n    if DIR_PATH is None:\n        dir_path = os.path.join(os.path.dirname(os.path.dirname(os.path.realpath(__file__))), 'template_vision-models')\n    else:\n        dir_path = os.path.join(os.path.abspath(DIR_PATH), 'template_vision-models')\n    if not os.path.isdir(dir_path):\n        os.mkdir(dir_path)\n    return os.path.abspath(dir_path)\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.get_package_version","title":"<code>get_package_version()</code>","text":"<p>Returns the current version of the package</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>version of the package</p> Source code in <code>template_vision/utils.py</code> <pre><code>def get_package_version() -&gt; str:\n    '''Returns the current version of the package\n\n    Returns:\n        str: version of the package\n    '''\n    version = importlib.metadata.version('template_vision')\n    return version\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.get_ressources_path","title":"<code>get_ressources_path()</code>","text":"<p>Returns the path to the ressources folder</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path of the ressources folder</p> Source code in <code>template_vision/utils.py</code> <pre><code>def get_ressources_path() -&gt; str:\n    '''Returns the path to the ressources folder\n\n    Returns:\n        str: Path of the ressources folder\n    '''\n    dir_path = os.path.join(os.path.dirname(os.path.dirname(os.path.realpath(__file__))), 'template_vision-ressources')\n    if not os.path.isdir(dir_path):\n        os.mkdir(dir_path)\n    return os.path.abspath(dir_path)\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.is_ndarray_convertable","title":"<code>is_ndarray_convertable(obj)</code>","text":"<p>Returns True if the object is covertable to a builtin type in the same way a np.ndarray is</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>an object to test</p> required <p>Returns:     bool: True if the object is covertable to a list as a np.ndarray is</p> Source code in <code>template_vision/utils.py</code> <pre><code>def is_ndarray_convertable(obj: Any) -&gt; bool:\n    '''Returns True if the object is covertable to a builtin type in the same way a np.ndarray is\n\n    Args:\n        obj (Any): an object to test\n    Returns:\n        bool: True if the object is covertable to a list as a np.ndarray is\n    '''\n    return hasattr(obj, \"dtype\") and hasattr(obj, \"astype\") and hasattr(obj, \"tolist\")\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.ndarray_to_builtin_object","title":"<code>ndarray_to_builtin_object(obj)</code>","text":"<p>Transform a numpy.ndarray like object to a builtin type like int, float or list</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>An object</p> required <p>Raises:     ValueError: Raise a ValueError when obj is not ndarray convertable Returns:     Any: The object converted to a builtin type like int, float or list</p> Source code in <code>template_vision/utils.py</code> <pre><code>def ndarray_to_builtin_object(obj: Any) -&gt; Any:\n    '''Transform a numpy.ndarray like object to a builtin type like int, float or list\n\n    Args:\n        obj (Any): An object\n    Raises:\n        ValueError: Raise a ValueError when obj is not ndarray convertable\n    Returns:\n        Any: The object converted to a builtin type like int, float or list\n    '''\n    if is_ndarray_convertable(obj):\n        if np.issubdtype(obj.dtype, np.integer):\n            return obj.astype(int).tolist()\n        elif np.issubdtype(obj.dtype, np.number):\n            return obj.astype(float).tolist()\n        else:\n            return obj.tolist()\n    else:\n        raise ValueError(f\"{obj} is not ndarray convertable\")\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.read_folder","title":"<code>read_folder(folder_path, images_ext=('.jpg', '.jpeg', '.png'), sep=';', encoding='utf-8', accept_no_metadata=False)</code>","text":"<p>Loads images and classes / bboxes from a directory of images</p> <p>Parameters:</p> Name Type Description Default <code>folder_path</code> <code>str</code> <p>Directory with the images to be loaded - abs path</p> required <p>Kwargs:     images_ext (tuple): Accepted images extensions if automatic detection (i.e. no metadata file)     sep (str): Separator of the metadata file - if exists     encoding (str): Encoding of the metadata file - if exists     accept_no_metadata (bool): If we allow no targets metadata (i.e. returns only file paths, useful for predictions) Returns:     list: List of images path     list: List of classes associated with images if classification task, bboxes if objet detection task     str: Name of the prerprocessing pipeline used     str: Task type ('classification' or 'object_detection')</p> Source code in <code>template_vision/utils.py</code> <pre><code>def read_folder(folder_path: str, images_ext: tuple = ('.jpg', '.jpeg', '.png'),\n                sep: str = ';', encoding: str = 'utf-8',\n                accept_no_metadata: bool = False) -&gt; Tuple[list, list, str, str]:\n    '''Loads images and classes / bboxes from a directory of images\n\n    Args:\n        folder_path (str): Directory with the images to be loaded - abs path\n    Kwargs:\n        images_ext (tuple): Accepted images extensions if automatic detection (i.e. no metadata file)\n        sep (str): Separator of the metadata file - if exists\n        encoding (str): Encoding of the metadata file - if exists\n        accept_no_metadata (bool): If we allow no targets metadata (i.e. returns only file paths, useful for predictions)\n    Returns:\n        list: List of images path\n        list: List of classes associated with images if classification task, bboxes if objet detection task\n        str: Name of the prerprocessing pipeline used\n        str: Task type ('classification' or 'object_detection')\n    '''\n    metadata_object_detection = os.path.join(folder_path, 'metadata_bboxes.csv')\n    # Object detection\n    if os.path.exists(metadata_object_detection):\n        logger.info(\"Object detection task - Loading folder ...\")\n        path_list, bboxes_list, preprocess_str = read_folder_object_detection(folder_path, images_ext=images_ext,\n                                                                              sep=sep, encoding=encoding,\n                                                                              accept_no_metadata=accept_no_metadata)\n        return path_list, bboxes_list, preprocess_str, 'object_detection'\n    # Classifier\n    else:\n        logger.info(\"Classification task - Loading folder ...\")\n        path_list, classes_list, preprocess_str = read_folder_classification(folder_path, images_ext=images_ext,\n                                                                             sep=sep, encoding=encoding,\n                                                                             accept_no_metadata=accept_no_metadata)\n        return path_list, classes_list, preprocess_str, 'classification'\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.read_folder_classification","title":"<code>read_folder_classification(folder_path, images_ext=('.jpg', '.jpeg', '.png'), sep=';', encoding='utf-8', accept_no_metadata=False)</code>","text":"<p>Loads images and classes from a directory of images - classification task</p> <p>Solution 1: usage of a metadata file (metadata.csv) Solution 2: all images at root directory, and prefixed with class names (e.g. class_filename.ext) Solution 3: all images saved in class subdirectories Solution 4: read images from root directory - no targets</p> <p>Parameters:</p> Name Type Description Default <code>folder_path</code> <code>str</code> <p>Directory with the images to be loaded - abs path</p> required <p>Kwargs:     images_ext (tuple): Accepted images extensions if automatic detection (i.e. no metadata file)     sep (str): Separator of the metadata file - if exists     encoding (str): Encoding of the metadata file - if exists     accept_no_metadata (bool): If we allow no targets metadata (i.e. returns only file paths, useful for predictions) Raises:     FileNotFoundError: If folder path does not exists     NotADirectoryError: If the provided path is not a directory     ValueError: If column 'filename' does not exists in the metadata file     ValueError: If column 'class' does not exists in the metadata file and accept_no_metadata is False     RuntimeError: If no loading solution found Returns:     list: List of images path     list: List of classes associated with images     str: Name of the prerprocessing pipeline used</p> Source code in <code>template_vision/utils.py</code> <pre><code>def read_folder_classification(folder_path: str, images_ext: tuple = ('.jpg', '.jpeg', '.png'),\n                               sep: str = ';', encoding: str = 'utf-8',\n                               accept_no_metadata: bool = False) -&gt; Tuple[list, list, str]:\n    '''Loads images and classes from a directory of images - classification task\n\n    Solution 1: usage of a metadata file (metadata.csv)\n    Solution 2: all images at root directory, and prefixed with class names (e.g. class_filename.ext)\n    Solution 3: all images saved in class subdirectories\n    Solution 4: read images from root directory - no targets\n\n    Args:\n        folder_path (str): Directory with the images to be loaded - abs path\n    Kwargs:\n        images_ext (tuple): Accepted images extensions if automatic detection (i.e. no metadata file)\n        sep (str): Separator of the metadata file - if exists\n        encoding (str): Encoding of the metadata file - if exists\n        accept_no_metadata (bool): If we allow no targets metadata (i.e. returns only file paths, useful for predictions)\n    Raises:\n        FileNotFoundError: If folder path does not exists\n        NotADirectoryError: If the provided path is not a directory\n        ValueError: If column 'filename' does not exists in the metadata file\n        ValueError: If column 'class' does not exists in the metadata file and accept_no_metadata is False\n        RuntimeError: If no loading solution found\n    Returns:\n        list: List of images path\n        list: List of classes associated with images\n        str: Name of the prerprocessing pipeline used\n    '''\n    logger.info(f\"Loading folder {folder_path} ...\")\n\n    # Check path exists and it's a directory\n    if not os.path.exists(folder_path):\n        raise FileNotFoundError(f\"The path {folder_path} does not exist\")\n    if not os.path.isdir(folder_path):\n        raise NotADirectoryError(f\"{folder_path} is not a valid directory\")\n\n    # We first check for a preprocessing file\n    preprocess_file = os.path.join(folder_path, 'preprocess_pipeline.conf')\n    if os.path.exists(preprocess_file):\n        logger.info(\"Found a preprocessing file\")\n        with open(preprocess_file, 'r', encoding=encoding) as f:\n            preprocess_str = f.readline()\n    else:\n        logger.info(\"Can't find a preprocessing file, backup on 'no_preprocess'\")\n        preprocess_str = 'no_preprocess'\n\n    # Solution 1: we try to load the directory by reading a metadata file\n    # This file must be named metadata.csv and contain a column `filename`\n    metadata_file = os.path.join(folder_path, 'metadata.csv')\n    if os.path.exists(metadata_file):\n        logger.info(\"Found a metadata file\")\n\n        # Loading metadata file\n        metadata_df = pd.read_csv(metadata_file, sep=sep, encoding=encoding)\n        if 'filename' not in metadata_df.columns:\n            raise ValueError(\"The metadata file must contain a column 'filename'\")\n\n        # Retrieving information (path &amp; classes)\n        path_list = list(metadata_df['filename'].values)\n        path_list = [os.path.join(folder_path, f) for f in path_list]\n        if 'class' in metadata_df.columns:\n            classes_list = [str(cl) for cl in metadata_df['class'].values]\n        elif accept_no_metadata:\n            logger.info(\"Can't retrieve classes (missing 'class' column in metadata file)\")\n            classes_list = None\n        else:\n            raise ValueError(\"The metadata file must contain a column 'class' with argument `accept_no_metadata` at False\")\n\n        # Return here\n        return path_list, classes_list, preprocess_str\n\n    # Solution 2: we check if all files are inside the root directory and if they are all prefixed (i.e. prefix_filename.ext)\n    folder_list = os.listdir(folder_path)\n    folder_list = [f for f in folder_list if f != 'preprocess_pipeline.conf']  # Do not consider preprocessing file\n    # Check if all files are images\n    if all([f.endswith(images_ext) for f in folder_list]):\n        logger.info(\"Try to load images from root directory\")\n        path_list = [os.path.join(folder_path, f) for f in folder_list]\n\n        # Check prefixes\n        if all([len(f.split('_')) &gt; 1 for f in folder_list]) and all([len(f.split('_')[0]) &gt; 0 for f in folder_list]):\n            classes_list = [f.split('_')[0] for f in folder_list]\n        else:\n            logger.info(\"Can't retrieve classes (files are not prefixed)\")\n            classes_list = None\n\n        # Return here\n        return path_list, classes_list, preprocess_str\n\n    # Solution 3: check if images are saved in class subdirectories\n    folders_elements = os.listdir(folder_path)\n    folders_elements = [f for f in folders_elements if f != 'preprocess_pipeline.conf']  # Do not consider preprocessing file\n    # Check if only subdirectories\n    if all([os.path.isdir(os.path.join(folder_path, f)) for f in folders_elements]):\n        # Check if each subdirectories contain images\n        if all([all([f2.endswith(images_ext) for f2 in os.listdir(os.path.join(folder_path, f))]) for f in folders_elements]):\n            logger.info(\"Try to load images from class subdirectories\")\n\n            # Retrieving information (path &amp; classes)\n            path_list = []\n            classes_list = []\n            for folder in folders_elements:\n                tmp_path_list = [os.path.join(folder_path, folder, f) for f in os.listdir(os.path.join(folder_path, folder))]\n                tmp_classes_list = [folder] * len(tmp_path_list)\n                path_list += tmp_path_list\n                classes_list += tmp_classes_list\n            return path_list, classes_list, preprocess_str\n\n    # Solution 4: if accept no metadata, we retrieve all images inside the root directory\n    elif accept_no_metadata:\n        folder_list = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n        folder_list = [f for f in folder_list if f != 'preprocess_pipeline.conf']  # Do not consider preprocessing file\n        folder_list = [f for f in folder_list if f.endswith(images_ext)]  # Keep only images\n        path_list = [os.path.join(folder_path, f) for f in folder_list]  # Get abs paths\n        return path_list, None, preprocess_str  # No targets\n\n    # No more solution, raise error\n    raise RuntimeError(f\"No loading solution found for folder ({folder_path})\")\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.read_folder_object_detection","title":"<code>read_folder_object_detection(folder_path, images_ext=('.jpg', '.jpeg', '.png'), sep=';', encoding='utf-8', accept_no_metadata=False)</code>","text":"<p>Loads images and bboxes from a directory of images - object detection task</p> <p>Solution 1: usage of a metadata file (metadata_bboxes.csv) Solution 2: read images from root directory - no targets</p> <p>Parameters:</p> Name Type Description Default <code>folder_path</code> <code>str</code> <p>Directory with the images to be loaded - abs path</p> required <p>Kwargs:     images_ext (tuple): Accepted images extensions if automatic detection (i.e. no metadata file)     sep (str): Separator of the metadata file - if exists     encoding (str): Encoding of the metadata file - if exists     accept_no_metadata (bool): If we allow no targets metadata (i.e. returns only file paths, useful for predictions) Raises:     FileNotFoundError: If folder path does not exists     NotADirectoryError: If the provided path is not a directory     ValueError: If column 'filename' does not exists in the metadata file     RuntimeError: If no loading solution found Returns:     list: List of images path     list: List of bboxes associated with images     str: Name of the prerprocessing pipeline used</p> Source code in <code>template_vision/utils.py</code> <pre><code>def read_folder_object_detection(folder_path: str, images_ext: tuple = ('.jpg', '.jpeg', '.png'),\n                                 sep: str = ';', encoding: str = 'utf-8',\n                                 accept_no_metadata: bool = False) -&gt; Tuple[list, list, str]:\n    '''Loads images and bboxes from a directory of images - object detection task\n\n    Solution 1: usage of a metadata file (metadata_bboxes.csv)\n    Solution 2: read images from root directory - no targets\n\n    Args:\n        folder_path (str): Directory with the images to be loaded - abs path\n    Kwargs:\n        images_ext (tuple): Accepted images extensions if automatic detection (i.e. no metadata file)\n        sep (str): Separator of the metadata file - if exists\n        encoding (str): Encoding of the metadata file - if exists\n        accept_no_metadata (bool): If we allow no targets metadata (i.e. returns only file paths, useful for predictions)\n    Raises:\n        FileNotFoundError: If folder path does not exists\n        NotADirectoryError: If the provided path is not a directory\n        ValueError: If column 'filename' does not exists in the metadata file\n        RuntimeError: If no loading solution found\n    Returns:\n        list: List of images path\n        list: List of bboxes associated with images\n        str: Name of the prerprocessing pipeline used\n    '''\n    logger.info(f\"Loading folder {folder_path} ...\")\n\n    # Check path exists and it's a directory\n    if not os.path.exists(folder_path):\n        raise FileNotFoundError(f\"The path {folder_path} does not exist\")\n    if not os.path.isdir(folder_path):\n        raise NotADirectoryError(f\"{folder_path} is not a valid directory\")\n\n    # We first check for a preprocessing file\n    preprocess_file = os.path.join(folder_path, 'preprocess_pipeline.conf')\n    if os.path.exists(preprocess_file):\n        logger.info(\"Found a preprocessing file\")\n        with open(preprocess_file, 'r', encoding=encoding) as f:\n            preprocess_str = f.readline()\n    else:\n        logger.info(\"Can't find a preprocessing file, backup on 'no_preprocess'\")\n        preprocess_str = 'no_preprocess'\n\n    # Solution 1: we try to load the directory by reading a metadata file\n    # This file must be named metadata_bboxes.csv and contain a column `filename`\n    metadata_file = os.path.join(folder_path, 'metadata_bboxes.csv')\n    if os.path.exists(metadata_file):\n        logger.info(\"Found a metadata file\")\n\n        # Loading metadata file\n        metadata_df = pd.read_csv(metadata_file, sep=sep, encoding=encoding)\n        if 'filename' not in metadata_df.columns:\n            raise ValueError(\"The metadata file must contain a column 'filename'\")\n\n        # Retrieving information (path &amp; bboxes)\n        filenames = list(metadata_df['filename'].unique())\n        path_list = [os.path.join(folder_path, f) for f in filenames]\n        # Try to read bboxes\n        if all([_ in metadata_df.columns for _ in ['class', 'x1', 'x2', 'y1', 'y2']]):\n            bboxes_list = []\n            for filename in filenames:\n                filtered_bboxes = metadata_df[metadata_df.filename == filename]\n                tmp_bboxes_list = []\n                for i, row in filtered_bboxes.iterrows():\n                    tmp_bboxes_list.append({\n                        'class': str(row['class']),  # We ensure all classes are strings\n                        'x1': row['x1'],\n                        'x2': row['x2'],\n                        'y1': row['y1'],\n                        'y2': row['y2'],\n                    })\n                bboxes_list.append(tmp_bboxes_list)\n        else:\n            logger.info(\"Can't retrieve bboxes\")\n            bboxes_list = None\n\n    # Solution 2: if accept no metadata, we retrieve all images inside the root directory\n    elif accept_no_metadata:\n        folder_list = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n        folder_list = [f for f in folder_list if f != 'preprocess_pipeline.conf']  # Do not consider preprocessing file\n        folder_list = [f for f in folder_list if f.endswith(images_ext)]  # Keep only images\n        path_list = [os.path.join(folder_path, f) for f in folder_list]  # Get abs paths\n        bboxes_list = None  # No targets\n\n    # No solution found, raise error\n    else:\n        raise RuntimeError(f\"No loading solution found for folder ({folder_path})\")\n\n    # Return\n    return path_list, bboxes_list, preprocess_str\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.rebuild_metadata_classification","title":"<code>rebuild_metadata_classification(filenames_list, classes_list)</code>","text":"<p>Rebuilds a metadata file from files names and associated classes - classification task</p> <p>Parameters:</p> Name Type Description Default <code>filenames_list</code> <code>list</code> <p>List of files names (actually a path relative to files parent directory)</p> required <code>classes_list</code> <code>list</code> <p>List of classes</p> required <p>Raises:     ValueError: Both list must be of same length Returns:     pd.DataFrame: The new metadata dataframe</p> Source code in <code>template_vision/utils.py</code> <pre><code>def rebuild_metadata_classification(filenames_list: list, classes_list: list) -&gt; pd.DataFrame:\n    '''Rebuilds a metadata file from files names and associated classes - classification task\n\n    Args:\n        filenames_list (list): List of files names (actually a path relative to files parent directory)\n        classes_list (list): List of classes\n    Raises:\n        ValueError: Both list must be of same length\n    Returns:\n        pd.DataFrame: The new metadata dataframe\n    '''\n    if len(filenames_list) != len(classes_list):\n        raise ValueError(\"Both list 'filenames_list' &amp; 'classes_list' must be of same length\")\n    return pd.DataFrame({'filename': filenames_list, 'class': classes_list})\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.rebuild_metadata_object_detection","title":"<code>rebuild_metadata_object_detection(filenames_list, bboxes_list)</code>","text":"<p>Rebuilds a metadata file from files names and associated bboxes - object detection task</p> <p>Parameters:</p> Name Type Description Default <code>filenames_list</code> <code>list</code> <p>List of files names (actually a path relative to files parent directory)</p> required <code>bboxes_list</code> <code>list</code> <p>List of bboxes</p> required <p>Raises:     ValueError: Both list must be of same length Returns:     pd.DataFrame: The new metadata dataframe</p> Source code in <code>template_vision/utils.py</code> <pre><code>def rebuild_metadata_object_detection(filenames_list: list, bboxes_list: list) -&gt; pd.DataFrame:\n    '''Rebuilds a metadata file from files names and associated bboxes - object detection task\n\n    Args:\n        filenames_list (list): List of files names (actually a path relative to files parent directory)\n        bboxes_list (list): List of bboxes\n    Raises:\n        ValueError: Both list must be of same length\n    Returns:\n        pd.DataFrame: The new metadata dataframe\n    '''\n    if len(filenames_list) != len(bboxes_list):\n        raise ValueError(\"Both list 'filenames_list' &amp; 'bboxes_list' must be of same length\") \n\n    rows = []\n    for filename, bboxes in zip(filenames_list, bboxes_list):\n        for bbox in bboxes:\n            new_row = {'filename': filename, 'class': bbox['class'], 'x1': bbox['x1'], 'x2': bbox['x2'], 'y1': bbox['y1'], 'y2': bbox['y2']}\n            rows.append(new_row)\n\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.trained_needed","title":"<code>trained_needed(function)</code>","text":"<p>Decorator to ensure that a model has been trained.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>func</code> <p>Function to decorate</p> required <p>Returns:     function: The decorated function</p> Source code in <code>template_vision/utils.py</code> <pre><code>def trained_needed(function: Callable) -&gt; Callable:\n    '''Decorator to ensure that a model has been trained.\n\n    Args:\n        function (func): Function to decorate\n    Returns:\n        function: The decorated function\n    '''\n    # Get wrapper\n    def wrapper(self, *args, **kwargs):\n        '''Wrapper'''\n        if not self.trained:\n            raise AttributeError(f\"The function {function.__name__} can't be called as long as the model hasn't been fitted\")\n        else:\n            return function(self, *args, **kwargs)\n    return wrapper\n</code></pre>"},{"location":"reference/template_vision/models_training/","title":"Models training","text":""},{"location":"reference/template_vision/models_training/model_class/","title":"Model class","text":""},{"location":"reference/template_vision/models_training/model_class/#template_vision.models_training.model_class.ModelClass","title":"<code>ModelClass</code>","text":"<p>Parent class for the models</p> Source code in <code>template_vision/models_training/model_class.py</code> <pre><code>class ModelClass:\n    '''Parent class for the models'''\n\n    _default_name = 'none'\n    # Variable annotation : https://www.python.org/dev/peps/pep-0526/\n    # Solves lots of typing errors, cf mypy\n    list_classes: list\n    dict_classes: dict\n\n    # Not implemented :\n    # -&gt; fit\n    # -&gt; predict\n    # -&gt; predict_proba\n    # -&gt; inverse_transform\n    # -&gt; get_and_save_metrics\n\n    def __init__(self, model_dir: Union[str, None] = None, model_name: Union[str, None] = None,\n                 level_save: str = 'HIGH', **kwargs) -&gt; None:\n        '''Initialization of the parent class.\n\n        Kwargs:\n            model_dir (str): Folder where to save the model\n                If None, creates a directory based on the model's name and the date (most common usage)\n            model_name (str): The name of the model\n            level_save (str): Level of saving\n                LOW: stats + configurations + logger keras - /!\\\\ The model can't be reused /!\\\\ -\n                MEDIUM: LOW + hdf5 + pkl + plots\n                HIGH: MEDIUM + predictions\n        Raises:\n            ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\n            NotADirectoryError: If a provided model directory is not a directory (i.e. it's a file)\n        '''\n        if level_save not in ['LOW', 'MEDIUM', 'HIGH']:\n            raise ValueError(f\"The object level_save ({level_save}) is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\")\n\n        # Get logger\n        self.logger = logging.getLogger(__name__)\n\n        # Model type -&gt; 'classifier' or 'object_detector' depending on the model\n        self.model_type = None\n\n        # Model name\n        self.model_name = self._default_name if model_name is None else model_name\n\n        # Model folder\n        if model_dir is None:\n            self.model_dir = self._get_new_model_dir()\n        else:\n            if not os.path.exists(model_dir):\n                os.makedirs(model_dir)\n            if not os.path.isdir(model_dir):\n                raise NotADirectoryError(f\"{model_dir} is not a valid directory\")\n            self.model_dir = os.path.abspath(model_dir)\n\n        # Other options\n        self.level_save = level_save\n\n        # is trained ?\n        self.trained = False\n        self.nb_fit = 0\n\n        # Configuration dict. to be logged. Set on save.\n        self.json_dict: Dict[Any, Any] = {}\n\n    def fit(self, df_train, **kwargs) -&gt; dict:\n        '''Trains the model\n\n        Args:\n            df_train (pd.DataFrame): Train dataset\n                Must contain file_path &amp; file_class columns if classifier\n                Must contain file_path &amp; bboxes columns if object detector\n        Returns:\n            dict: Fit arguments, to be used with transfer learning fine-tuning\n        '''\n        raise NotImplementedError(\"'fit' needs to be overridden\")\n\n    def predict(self, df_test: pd.DataFrame, **kwargs) -&gt; Union[np.ndarray, list]:\n        '''Predictions on test set\n\n        Args:\n            df_test (pd.DataFrame): DataFrame to be predicted, with column file_path\n        Returns:\n            (np.ndarray | list): Array, shape = [n_samples, n_classes] or List of n_samples elements\n        '''\n        raise NotImplementedError(\"'predict' needs to be overridden\")\n\n    def predict_proba(self, df_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n        '''Predicts probabilities on the test dataset\n\n        Args:\n            df_test (pd.DataFrame): DataFrame to be predicted, with column file_path\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n        raise NotImplementedError(\"'predict_proba' needs to be overridden\")\n\n    def inverse_transform(self, y: Union[list, np.ndarray]) -&gt; Union[list, tuple]:\n        '''Gets the final format of prediction\n            - Classification : classes from predictions\n            - Object detections : list of bboxes per image\n\n        Args:\n            y (list | np.ndarray): Array-like\n        Returns:\n            List of classes if classifier\n            List of bboxes if object detector\n        '''\n        raise NotImplementedError(\"'inverse_transform' needs to be overridden\")\n\n    def get_and_save_metrics(self, y_true, y_pred, list_files_x: Union[list, None] = None,\n                             type_data: str = '') -&gt; pd.DataFrame:\n        '''Gets and saves the metrics of a model\n\n        Args:\n            y_true (?): Array-like [n_samples, 1] if classifier\n                # If classifier, class of each image\n                # If object detector, list of list of bboxes per image\n                    bbox format : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ...}\n            y_pred (?): Array-like [n_samples, 1] if classifier\n                # If classifier, class of each image\n                # If object detector, list of list of bboxes per image\n                    bbox format : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ...}\n        Kwargs:\n            list_files_x (list): Input images file paths\n            type_data (str): Type of dataset (validation, test, ...)\n        Returns:\n            pd.DataFrame: The dataframe containing statistics\n        '''\n        raise NotImplementedError(\"'get_and_save_metrics' needs to be overridden\")\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n\n        # Manage paths\n        pkl_path = os.path.join(self.model_dir, f\"{self.model_name}.pkl\")\n        conf_path = os.path.join(self.model_dir, \"configurations.json\")\n\n        # Save model &amp; pipeline preprocessing si level_save &gt; 'LOW'\n        if self.level_save in ['MEDIUM', 'HIGH']:\n            with open(pkl_path, 'wb') as f:\n                pickle.dump(self, f)\n\n        # Save configuration JSON\n        json_dict = {\n            'maintainers': 'Agence DataServices',\n            'gabarit_version': '1.3.4.dev0+local',\n            'date': datetime.now().strftime(\"%d/%m/%Y - %H:%M:%S\"),  # Not the same as the folder's name\n            'package_version': utils.get_package_version(),\n            'model_name': self.model_name,\n            'model_dir': self.model_dir,\n            'model_type': self.model_type,\n            'trained': self.trained,\n            'nb_fit': self.nb_fit,\n            'level_save': self.level_save,\n            'librairie': None,\n        }\n        # Merge json_data if not None\n        if json_data is not None:\n            # Priority given to json_data !\n            json_dict = {**json_dict, **json_data}\n\n        # Add conf to attributes\n        self.json_dict = json_dict\n\n        # Save conf\n        with open(conf_path, 'w', encoding='utf-8') as json_file:\n            json.dump(json_dict, json_file, indent=4, cls=utils.NpEncoder)\n\n        # Now, save a properties file for the model upload\n        self._save_upload_properties(json_dict)\n\n    def _save_upload_properties(self, json_dict: Union[dict, None] = None) -&gt; None:\n        '''Prepares a configuration file for a future export (e.g on an artifactory)\n\n        Kwargs:\n            json_dict: Configurations to save\n        '''\n        if json_dict is None:\n            json_dict = {}\n\n        # Manage paths\n        properties_path = os.path.join(self.model_dir, \"properties.json\")\n        vanilla_model_upload_instructions = os.path.join(utils.get_ressources_path(), 'model_upload_instructions.md')\n        specific_model_upload_instructions = os.path.join(self.model_dir, \"model_upload_instructions.md\")\n\n        # First, we define a list of \"allowed\" properties\n        allowed_properties = [\"maintainers\", \"gabarit_version\", \"date\", \"package_version\", \"model_name\", \"list_classes\",\n                              \"librairie\", \"fit_time\"]\n        # Now we filter these properties\n        final_dict = {k: v for k, v in json_dict.items() if k in allowed_properties}\n        # Save\n        with open(properties_path, 'w', encoding='utf-8') as f:\n            json.dump(final_dict, f, indent=4, cls=utils.NpEncoder)\n\n        # Add instructions to upload a model to a storage solution (e.g. Artifactory)\n        with open(vanilla_model_upload_instructions, 'r', encoding='utf-8') as f:\n            content = f.read()\n        # TODO: to be improved\n        new_content = content.replace('model_dir_path_identifier', os.path.abspath(self.model_dir))\n        with open(specific_model_upload_instructions, 'w', encoding='utf-8') as f:\n            f.write(new_content)\n\n    def _get_new_model_dir(self) -&gt; str:\n        '''Gets a folder where to save the model\n\n        Returns:\n            str: Path to the folder\n        '''\n        models_dir = utils.get_models_path()\n        subfolder = os.path.join(models_dir, self.model_name)\n        folder_name = datetime.now().strftime(f\"{self.model_name}_%Y_%m_%d-%H_%M_%S\")\n        model_dir = os.path.join(subfolder, folder_name)\n        if os.path.isdir(model_dir):\n            time.sleep(1)  # Wait 1 second so that the 'date' changes...\n            return self._get_new_model_dir()  # Get new directory name\n        else:\n            os.makedirs(model_dir)\n        return model_dir\n\n    def display_if_gpu_activated(self) -&gt; None:\n        '''Displays if a GPU is being used'''\n        if self._is_gpu_activated():\n            self.logger.info(\"GPU activated\")\n\n    def _is_gpu_activated(self) -&gt; bool:\n        '''Checks if we use a GPU\n\n        Returns:\n            bool: whether GPU is available or not\n        '''\n        # By default, no GPU\n        return False\n</code></pre>"},{"location":"reference/template_vision/models_training/model_class/#template_vision.models_training.model_class.ModelClass.__init__","title":"<code>__init__(model_dir=None, model_name=None, level_save='HIGH', **kwargs)</code>","text":"<p>Initialization of the parent class.</p> Kwargs <p>model_dir (str): Folder where to save the model     If None, creates a directory based on the model's name and the date (most common usage) model_name (str): The name of the model level_save (str): Level of saving     LOW: stats + configurations + logger keras - /! The model can't be reused /! -     MEDIUM: LOW + hdf5 + pkl + plots     HIGH: MEDIUM + predictions</p> <p>Raises:     ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])     NotADirectoryError: If a provided model directory is not a directory (i.e. it's a file)</p> Source code in <code>template_vision/models_training/model_class.py</code> <pre><code>def __init__(self, model_dir: Union[str, None] = None, model_name: Union[str, None] = None,\n             level_save: str = 'HIGH', **kwargs) -&gt; None:\n    '''Initialization of the parent class.\n\n    Kwargs:\n        model_dir (str): Folder where to save the model\n            If None, creates a directory based on the model's name and the date (most common usage)\n        model_name (str): The name of the model\n        level_save (str): Level of saving\n            LOW: stats + configurations + logger keras - /!\\\\ The model can't be reused /!\\\\ -\n            MEDIUM: LOW + hdf5 + pkl + plots\n            HIGH: MEDIUM + predictions\n    Raises:\n        ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\n        NotADirectoryError: If a provided model directory is not a directory (i.e. it's a file)\n    '''\n    if level_save not in ['LOW', 'MEDIUM', 'HIGH']:\n        raise ValueError(f\"The object level_save ({level_save}) is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\")\n\n    # Get logger\n    self.logger = logging.getLogger(__name__)\n\n    # Model type -&gt; 'classifier' or 'object_detector' depending on the model\n    self.model_type = None\n\n    # Model name\n    self.model_name = self._default_name if model_name is None else model_name\n\n    # Model folder\n    if model_dir is None:\n        self.model_dir = self._get_new_model_dir()\n    else:\n        if not os.path.exists(model_dir):\n            os.makedirs(model_dir)\n        if not os.path.isdir(model_dir):\n            raise NotADirectoryError(f\"{model_dir} is not a valid directory\")\n        self.model_dir = os.path.abspath(model_dir)\n\n    # Other options\n    self.level_save = level_save\n\n    # is trained ?\n    self.trained = False\n    self.nb_fit = 0\n\n    # Configuration dict. to be logged. Set on save.\n    self.json_dict: Dict[Any, Any] = {}\n</code></pre>"},{"location":"reference/template_vision/models_training/model_class/#template_vision.models_training.model_class.ModelClass.display_if_gpu_activated","title":"<code>display_if_gpu_activated()</code>","text":"<p>Displays if a GPU is being used</p> Source code in <code>template_vision/models_training/model_class.py</code> <pre><code>def display_if_gpu_activated(self) -&gt; None:\n    '''Displays if a GPU is being used'''\n    if self._is_gpu_activated():\n        self.logger.info(\"GPU activated\")\n</code></pre>"},{"location":"reference/template_vision/models_training/model_class/#template_vision.models_training.model_class.ModelClass.fit","title":"<code>fit(df_train, **kwargs)</code>","text":"<p>Trains the model</p> <p>Parameters:</p> Name Type Description Default <code>df_train</code> <code>DataFrame</code> <p>Train dataset Must contain file_path &amp; file_class columns if classifier Must contain file_path &amp; bboxes columns if object detector</p> required <p>Returns:     dict: Fit arguments, to be used with transfer learning fine-tuning</p> Source code in <code>template_vision/models_training/model_class.py</code> <pre><code>def fit(self, df_train, **kwargs) -&gt; dict:\n    '''Trains the model\n\n    Args:\n        df_train (pd.DataFrame): Train dataset\n            Must contain file_path &amp; file_class columns if classifier\n            Must contain file_path &amp; bboxes columns if object detector\n    Returns:\n        dict: Fit arguments, to be used with transfer learning fine-tuning\n    '''\n    raise NotImplementedError(\"'fit' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_vision/models_training/model_class/#template_vision.models_training.model_class.ModelClass.get_and_save_metrics","title":"<code>get_and_save_metrics(y_true, y_pred, list_files_x=None, type_data='')</code>","text":"<p>Gets and saves the metrics of a model</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>?</code> <p>Array-like [n_samples, 1] if classifier</p> required <code>y_pred</code> <code>?</code> <p>Array-like [n_samples, 1] if classifier</p> required <p>Kwargs:     list_files_x (list): Input images file paths     type_data (str): Type of dataset (validation, test, ...) Returns:     pd.DataFrame: The dataframe containing statistics</p> Source code in <code>template_vision/models_training/model_class.py</code> <pre><code>def get_and_save_metrics(self, y_true, y_pred, list_files_x: Union[list, None] = None,\n                         type_data: str = '') -&gt; pd.DataFrame:\n    '''Gets and saves the metrics of a model\n\n    Args:\n        y_true (?): Array-like [n_samples, 1] if classifier\n            # If classifier, class of each image\n            # If object detector, list of list of bboxes per image\n                bbox format : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ...}\n        y_pred (?): Array-like [n_samples, 1] if classifier\n            # If classifier, class of each image\n            # If object detector, list of list of bboxes per image\n                bbox format : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ...}\n    Kwargs:\n        list_files_x (list): Input images file paths\n        type_data (str): Type of dataset (validation, test, ...)\n    Returns:\n        pd.DataFrame: The dataframe containing statistics\n    '''\n    raise NotImplementedError(\"'get_and_save_metrics' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_vision/models_training/model_class/#template_vision.models_training.model_class.ModelClass.get_and_save_metrics--if-classifier-class-of-each-image","title":"If classifier, class of each image","text":""},{"location":"reference/template_vision/models_training/model_class/#template_vision.models_training.model_class.ModelClass.get_and_save_metrics--if-object-detector-list-of-list-of-bboxes-per-image","title":"If object detector, list of list of bboxes per image","text":"<pre><code>bbox format : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ...}\n</code></pre>"},{"location":"reference/template_vision/models_training/model_class/#template_vision.models_training.model_class.ModelClass.get_and_save_metrics--if-classifier-class-of-each-image","title":"If classifier, class of each image","text":""},{"location":"reference/template_vision/models_training/model_class/#template_vision.models_training.model_class.ModelClass.get_and_save_metrics--if-object-detector-list-of-list-of-bboxes-per-image","title":"If object detector, list of list of bboxes per image","text":"<pre><code>bbox format : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ...}\n</code></pre>"},{"location":"reference/template_vision/models_training/model_class/#template_vision.models_training.model_class.ModelClass.inverse_transform","title":"<code>inverse_transform(y)</code>","text":"<p>Gets the final format of prediction     - Classification : classes from predictions     - Object detections : list of bboxes per image</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>list | ndarray</code> <p>Array-like</p> required <p>Returns:     List of classes if classifier     List of bboxes if object detector</p> Source code in <code>template_vision/models_training/model_class.py</code> <pre><code>def inverse_transform(self, y: Union[list, np.ndarray]) -&gt; Union[list, tuple]:\n    '''Gets the final format of prediction\n        - Classification : classes from predictions\n        - Object detections : list of bboxes per image\n\n    Args:\n        y (list | np.ndarray): Array-like\n    Returns:\n        List of classes if classifier\n        List of bboxes if object detector\n    '''\n    raise NotImplementedError(\"'inverse_transform' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_vision/models_training/model_class/#template_vision.models_training.model_class.ModelClass.predict","title":"<code>predict(df_test, **kwargs)</code>","text":"<p>Predictions on test set</p> <p>Parameters:</p> Name Type Description Default <code>df_test</code> <code>DataFrame</code> <p>DataFrame to be predicted, with column file_path</p> required <p>Returns:     (np.ndarray | list): Array, shape = [n_samples, n_classes] or List of n_samples elements</p> Source code in <code>template_vision/models_training/model_class.py</code> <pre><code>def predict(self, df_test: pd.DataFrame, **kwargs) -&gt; Union[np.ndarray, list]:\n    '''Predictions on test set\n\n    Args:\n        df_test (pd.DataFrame): DataFrame to be predicted, with column file_path\n    Returns:\n        (np.ndarray | list): Array, shape = [n_samples, n_classes] or List of n_samples elements\n    '''\n    raise NotImplementedError(\"'predict' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_vision/models_training/model_class/#template_vision.models_training.model_class.ModelClass.predict_proba","title":"<code>predict_proba(df_test, **kwargs)</code>","text":"<p>Predicts probabilities on the test dataset</p> <p>Parameters:</p> Name Type Description Default <code>df_test</code> <code>DataFrame</code> <p>DataFrame to be predicted, with column file_path</p> required <p>Returns:     (np.ndarray): Array, shape = [n_samples, n_classes]</p> Source code in <code>template_vision/models_training/model_class.py</code> <pre><code>def predict_proba(self, df_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n    '''Predicts probabilities on the test dataset\n\n    Args:\n        df_test (pd.DataFrame): DataFrame to be predicted, with column file_path\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n    raise NotImplementedError(\"'predict_proba' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_vision/models_training/model_class/#template_vision.models_training.model_class.ModelClass.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_vision/models_training/model_class.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n\n    # Manage paths\n    pkl_path = os.path.join(self.model_dir, f\"{self.model_name}.pkl\")\n    conf_path = os.path.join(self.model_dir, \"configurations.json\")\n\n    # Save model &amp; pipeline preprocessing si level_save &gt; 'LOW'\n    if self.level_save in ['MEDIUM', 'HIGH']:\n        with open(pkl_path, 'wb') as f:\n            pickle.dump(self, f)\n\n    # Save configuration JSON\n    json_dict = {\n        'maintainers': 'Agence DataServices',\n        'gabarit_version': '1.3.4.dev0+local',\n        'date': datetime.now().strftime(\"%d/%m/%Y - %H:%M:%S\"),  # Not the same as the folder's name\n        'package_version': utils.get_package_version(),\n        'model_name': self.model_name,\n        'model_dir': self.model_dir,\n        'model_type': self.model_type,\n        'trained': self.trained,\n        'nb_fit': self.nb_fit,\n        'level_save': self.level_save,\n        'librairie': None,\n    }\n    # Merge json_data if not None\n    if json_data is not None:\n        # Priority given to json_data !\n        json_dict = {**json_dict, **json_data}\n\n    # Add conf to attributes\n    self.json_dict = json_dict\n\n    # Save conf\n    with open(conf_path, 'w', encoding='utf-8') as json_file:\n        json.dump(json_dict, json_file, indent=4, cls=utils.NpEncoder)\n\n    # Now, save a properties file for the model upload\n    self._save_upload_properties(json_dict)\n</code></pre>"},{"location":"reference/template_vision/models_training/model_keras/","title":"Model keras","text":""},{"location":"reference/template_vision/models_training/model_keras/#template_vision.models_training.model_keras.ModelKeras","title":"<code>ModelKeras</code>","text":"<p>             Bases: <code>ModelClass</code></p> <p>Generic model for Keras NN</p> Source code in <code>template_vision/models_training/model_keras.py</code> <pre><code>class ModelKeras(ModelClass):\n    '''Generic model for Keras NN'''\n\n    _default_name = 'model_keras'\n    # Not implemented :\n    # -&gt; _get_model\n    # -&gt; reload_from_standalone\n\n    # Should pby be overridden :\n    # -&gt; _get_preprocess_input\n\n    def __init__(self, batch_size: int = 64, epochs: int = 99, validation_split: float = 0.2, patience: int = 5,\n                 width: int = 224, height: int = 224, depth: int = 3, color_mode: str = 'rgb',\n                 in_memory: bool = False, data_augmentation_params: dict = {},\n                 nb_train_generator_images_to_save: int = 20,\n                 keras_params: dict = {}, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelClass for more arguments)\n\n        Kwargs:\n            batch_size (int): Batch size\n            epochs (int): Number of epochs\n            validation_split (float): Percentage for the validation set split\n                Only used if no input validation set when fitting\n            patience (int): Early stopping patience\n            width (int): NN input width (images are resized)\n            height (int): NN input height (images are resized)\n            depth (int): NN input depth\n            color_mode (str): NN input color mode\n            in_memory (bool): If all images should be loaded in memory, otherwise it uses a generator\n                /!\\\\ OOM errors can happen really quickly (depends on the dataset size)\n                /!\\\\ Data augmentation impossible if `in_memory` is set to True\n            data_augmentation_params (dict): Dictionnary of parameters to be used with the data augmentation\n                cf. https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n                /!\\\\ Not used if `in_memory` is set to True\n            nb_train_generator_images_to_save (int): If &gt; 0, save some input generated images\n                If helps with to understand what goes in your NN\n            keras_params (dict): Parameters used by Keras models.\n                e.g. learning_rate, nb_lstm_units, etc...\n                The purpose of this dictionary is for the user to use it as they wants in the _get_model function\n                This parameter was initially added in order to do an hyperparameters search\n        Raises:\n            ValueError: If `in_memory` is set to True and `data_augmentation_params` is not empty\n        '''\n        # TODO: learning rate should be an attribute !\n\n        # Check for errors\n        if in_memory and len(data_augmentation_params) &gt; 0:\n            raise ValueError(\"Data augmentation is impossible for 'in_memory' mode\")\n\n        # Init.\n        super().__init__(**kwargs)\n\n        # Fix tensorflow GPU\n        gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n        for device in gpu_devices:\n            tf.config.experimental.set_memory_growth(device, True)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Param. model\n        self.batch_size = batch_size\n        self.epochs = epochs\n        self.validation_split = validation_split\n        self.patience = patience\n\n        # Params. generator\n        self.width = width\n        self.height = height\n        self.depth = depth\n        self.color_mode = color_mode\n        self.in_memory = in_memory\n        self.data_augmentation_params = data_augmentation_params.copy()\n\n        # Warnings if depth does not match with color_mode\n        if self.color_mode == 'rgb' and self.depth != 3:\n            self.logger.warning(f\"`color_mode` parameter is 'rgb', but `depth` parameteris not equal to 3 ({self.depth})\")\n            self.logger.warning(\"We continue, but this can lead to errors during the training\")\n        if self.color_mode == 'rgba' and self.depth != 4:\n            self.logger.warning(f\"`color_mode` parameter is 'rgba', but `depth` parameteris not equal to 4 ({self.depth})\")\n            self.logger.warning(\"We continue, but this can lead to errors during the training\")\n\n        # TODO: add Test time augmentation ?\n\n        # Misc.\n        self.nb_train_generator_images_to_save = nb_train_generator_images_to_save\n\n        # Model set on fit\n        self.model: Any = None\n\n        # Set preprocess input\n        self.preprocess_input = self._get_preprocess_input()\n\n        # Keras params\n        self.keras_params = keras_params.copy()\n\n        # Keras custom objects : we get the ones specified in utils_deep_keras\n        self.custom_objects = utils_deep_keras.custom_objects\n\n    def fit(self, df_train: pd.DataFrame, df_valid: Union[pd.DataFrame, None] = None, with_shuffle: bool = True, **kwargs) -&gt; dict:\n        '''Fits the model\n\n        Args:\n            df_train (pd.DataFrame): Train dataset\n                Must contain file_path &amp; file_class columns if classifier\n                Must contain file_path &amp; bboxes columns if object detector\n        Kwargs:\n            df_valid (pd.DataFrame): Validation dataset\n                Must contain file_path &amp; file_class columns if classifier\n                Must contain file_path &amp; bboxes columns if object detector\n            with_shuffle (boolean): If the train dataset must be shuffled\n                This should be used if the input dataset is not shuffled &amp; no validation set as the split_validation takes the lines in order.\n                Thus, the validation set might get classes which are not in the train set ...\n        Raises:\n            NotImplementedError: If the model is not `classifier` nor `object_detector`\n        Returns:\n            dict: Fit arguments, to be used with transfer learning fine-tuning\n        '''\n        if self.model_type == 'classifier':\n            return self._fit_classifier(df_train, df_valid=df_valid, with_shuffle=with_shuffle, **kwargs)\n        elif self.model_type == 'object_detector':\n            return self._fit_object_detector(df_train, df_valid=df_valid, with_shuffle=with_shuffle, **kwargs)\n        else:\n            raise NotImplementedError(\"Only `classifier` and `object_detector` model type are supported.\")\n\n    def _fit_classifier(self, df_train: pd.DataFrame, df_valid: pd.DataFrame = None, with_shuffle: bool = True, **kwargs) -&gt; dict:\n        '''Fits the model - classifier\n\n        Args:\n            df_train (pd.DataFrame): Train dataset\n                Must contain file_path &amp; file_class columns\n        Kwargs:\n            df_valid (pd.DataFrame): Validation dataset\n                Must contain file_path &amp; file_class columns\n            with_shuffle (boolean): If the train dataset must be shuffled\n                This should be used if the input dataset is not shuffled &amp; no validation set as the split_validation takes the lines in order.\n                Thus, the validation set might get classes which are not in the train set ...\n        Raises:\n            ValueError: If the model is not of type `classifier`\n            ValueError: If already trained and new dataset does not match model's classes\n        Returns:\n            dict: Fit arguments, to be used with transfer learning fine-tuning\n        '''\n        if self.model_type != 'classifier':\n            raise ValueError(f\"`_fit_classifier` function does not support model type {self.model_type}\")\n\n        ##############################################\n        # Manage retrain\n        ##############################################\n\n        # If a model has already been fitted, we make a new folder in order not to overwrite the existing one !\n        # And we save the old conf\n        if self.trained:\n            # Get src files to save\n            src_files = [os.path.join(self.model_dir, \"configurations.json\")]\n            if self.nb_fit &gt; 1:\n                for i in range(1, self.nb_fit):\n                    src_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n            # Change model dir\n            self.model_dir = self._get_new_model_dir()\n            # Get dst files\n            dst_files = [os.path.join(self.model_dir, f\"configurations_fit_{self.nb_fit}.json\")]\n            if self.nb_fit &gt; 1:\n                for i in range(1, self.nb_fit):\n                    dst_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n            # Copies\n            for src, dst in zip(src_files, dst_files):\n                try:\n                    shutil.copyfile(src, dst)\n                except Exception as e:\n                    self.logger.error(f\"Impossible to copy {src} to {dst}\")\n                    self.logger.error(\"We still continue ...\")\n                    self.logger.error(repr(e))\n\n        ##############################################\n        # Prepare dataset\n        # Also extract list of classes\n        ##############################################\n\n        # Extract list of classes from df_train\n        list_classes = sorted(list(df_train['file_class'].unique()))\n        # Also set dict_classes\n        dict_classes = {i: col for i, col in enumerate(list_classes)}\n\n        # Validate classes if already trained, else set them\n        if self.trained:\n            if self.list_classes != list_classes:\n                raise ValueError(\"Error: the new dataset does not match with the already fitted model\")\n            if self.dict_classes != dict_classes:\n                raise ValueError(\"Error: the new dataset does not match with the already fitted model\")\n        else:\n            self.list_classes = list_classes\n            self.dict_classes = dict_classes\n\n        # Shuffle training dataset if wanted\n        # It is advised as validation_split from keras does not shufle the data\n        # Hence, for classificationt task, we might have classes in the validation data that we never met in the training data\n        if with_shuffle:\n            df_train = df_train.sample(frac=1.).reset_index(drop=True)\n\n        if df_valid is None:\n            self.logger.warning(f\"Warning, no validation set. The training set will be splitted (validation fraction = {self.validation_split})\")\n\n        ##############################################\n        # We save some preprocessed / augmented input images examples\n        ##############################################\n\n        # Save some examples\n        if self.nb_train_generator_images_to_save &gt; 0:\n            self.logger.info(\"Retrieving a generator to save some preprocessed / augmented input images examples\")\n            # 1. Retrieve a generator (if in_memory, use 'valid' to avoid augmentation)\n            if not self.in_memory:\n                tmp_gen = self._get_generator(df_train, data_type='train', batch_size=1)\n            else:\n                tmp_gen = self._get_generator(df_train, data_type='valid', batch_size=1)\n            # 2. Retrieve generated images one by one\n            images = [tmp_gen.next()[0][0] for i in range(self.nb_train_generator_images_to_save)]\n            # 3. Remove negative pixels\n            min_pixel = min([np.min(_) for _ in images])\n            if min_pixel &lt; 0:\n                images = [arr - min_pixel for arr in images]\n            # 4. Rescale and scale uint8\n            max_pixel = max([np.max(_) for _ in images])\n            images = [(arr * 255 / max_pixel).astype('uint8') for arr in images]\n            # 5. Cast back to image format\n            images = [Image.fromarray(arr, 'RGBA' if arr.shape[-1] == 4 else 'RGB') for arr in images]\n            # 6. Save\n            save_dir = os.path.join(self.model_dir, f'examples_fit_{self.nb_fit + 1}')\n            if not os.path.exists(save_dir):\n                os.makedirs(save_dir)\n            for i, im in enumerate(images):\n                im_path = os.path.join(save_dir, f'example_{i}.png')\n                im.save(im_path, format='PNG')\n\n        ##############################################\n        # Get generators if not in_memory, else get full data\n        # Finally fit the model\n        ##############################################\n\n        if not self.in_memory:\n            self.logger.info(\"Loading data via generators\")\n\n            # Create generators\n            if df_valid is not None:\n                self.logger.info(\"Retrieving a generator for the training set\")\n                train_generator = self._get_generator(df_train, data_type='train', batch_size=min(self.batch_size, len(df_train)))\n                self.logger.info(\"Retrieving a generator for the validation set\")\n                valid_generator = self._get_generator(df_valid, data_type='valid', batch_size=min(self.batch_size, len(df_valid)))\n                # Set dataset related args\n                steps_per_epoch_arg = len(df_train) // min(self.batch_size, len(df_train))\n                validation_steps_arg = len(df_valid) // min(self.batch_size, len(df_valid))\n            else:\n                # If no validation, we'll split the training set using validation_split attribute\n                df_train_split, df_valid_split = train_test_split(df_train, test_size=self.validation_split)\n                self.logger.info(\"Retrieving a generator for the training set\")\n                train_generator = self._get_generator(df_train_split, data_type='train', batch_size=min(self.batch_size, len(df_train_split)))\n                self.logger.info(\"Retrieving a generator for the validation set\")\n                valid_generator = self._get_generator(df_valid_split, data_type='valid', batch_size=min(self.batch_size, len(df_valid_split)))\n                # Set dataset related args\n                steps_per_epoch_arg = len(df_train_split) // min(self.batch_size, len(df_train_split))\n                validation_steps_arg = len(df_valid_split) // min(self.batch_size, len(df_valid_split))\n\n            # Get fit arguments\n            x_arg = train_generator\n            y_arg = None\n            batch_size_arg = None\n            # validation_data does work with generators (TensorFlow doc is not up to date)\n            validation_data_arg = valid_generator\n            validation_split_arg = None\n\n        # Load in memory - Can easily lead to OOM issues\n        else:\n            self.logger.info(\"Loading data in memory\")\n\n            # We retrieve all the data\n            # Trick: we still use generators to have the correct preprocessing\n            # -&gt; data_type = valid (no shuffle, no data augmentation)\n            train_generator = self._get_generator(df_train, data_type='valid', batch_size=len(df_train))\n            x_train, y_train = train_generator.next()\n            if df_valid is not None:\n                valid_generator = self._get_generator(df_valid, data_type='valid', batch_size=min(self.batch_size, len(df_valid)))\n                x_val, y_val = valid_generator.next()\n                validation_data = (x_val, y_val)\n            else:\n                validation_data = None\n\n            # Get fit arguments\n            x_arg = x_train\n            y_arg = y_train\n            batch_size_arg = self.batch_size\n            steps_per_epoch_arg = None\n            validation_data_arg = validation_data  # Can be None if no validation set\n            validation_steps_arg = None\n            validation_split_arg = self.validation_split if validation_data is None else None\n\n        # Get model (if already fitted, _get_model returns instance model)\n        self.model = self._get_model()\n\n        # Get callbacks (early stopping &amp; checkpoint)\n        callbacks = self._get_callbacks()\n\n        # Fit\n        # We use a try...except in order to save the model if an error arises\n        # after more than a minute into training\n        start_time = time.time()\n        try:\n            fit_arguments = {\n                'x': x_arg,\n                'y': y_arg,\n                'batch_size': batch_size_arg,\n                'steps_per_epoch': steps_per_epoch_arg,\n                'validation_data': validation_data_arg,\n                'validation_split': validation_split_arg,\n                'validation_steps': validation_steps_arg,\n            }\n            fit_history = self.model.fit(  # type: ignore\n                epochs=self.epochs,\n                callbacks=callbacks,\n                verbose=1,\n                **fit_arguments,\n            )\n        except (RuntimeError, SystemError, SystemExit, EnvironmentError, KeyboardInterrupt, tf.errors.ResourceExhaustedError, tf.errors.InternalError,\n                tf.errors.UnavailableError, tf.errors.UnimplementedError, tf.errors.UnknownError, Exception) as e:\n            # Steps:\n            # 1. Display tensorflow error\n            # 2. Check if more than one minute elapsed &amp; not several iterations &amp; existence best.hdf5\n            # 3. Reload best model\n            # 4. We consider that a fit occured (trained = True, nb_fit += 1)\n            # 5. Save &amp; create a warning file\n            # 6. Display error messages\n            # 7. Raise an error\n\n            # 1.\n            self.logger.error(repr(e))\n\n            # 2.\n            best_path = os.path.join(self.model_dir, 'best.hdf5')\n            time_spent = time.time() - start_time\n            if time_spent &gt;= 60 and os.path.exists(best_path):\n                # 3.\n                self.model = load_model(best_path, custom_objects=self.custom_objects)\n                # 4.\n                self.trained = True\n                self.nb_fit += 1\n                # 5.\n                self.save()\n                with open(os.path.join(self.model_dir, \"0_MODEL_INCOMPLETE\"), 'w'):\n                    pass\n                with open(os.path.join(self.model_dir, \"1_TRAINING_NEEDS_TO_BE_RESUMED\"), 'w'):\n                    pass\n                # 6.\n                self.logger.error(\"[EXPERIMENTAL] Error during model training\")\n                self.logger.error(f\"[EXPERIMENTAL] The error happened after {round(time_spent, 2)}s of training\")\n                self.logger.error(\"[EXPERIMENTAL] A saving of the model is done but this model won't be usable as is.\")\n                self.logger.error(f\"[EXPERIMENTAL] In order to resume the training, we have to specify this model ({ntpath.basename(self.model_dir)}) in the file 2_training.py\")\n                self.logger.error(\"[EXPERIMENTAL] Warning, the preprocessing is not saved in the configuration file\")\n                self.logger.error(\"[EXPERIMENTAL] Warning, the best model might be corrupted in some cases\")\n            # 7.\n            raise RuntimeError(\"Error during model training\")\n\n        # Print accuracy &amp; loss if level_save &gt; 'LOW'\n        if self.level_save in ['MEDIUM', 'HIGH']:\n            self._plot_metrics_and_loss(fit_history)\n            # Reload best model\n            self.model = load_model(\n                os.path.join(self.model_dir, 'best.hdf5'),\n                custom_objects=self.custom_objects\n            )\n\n        # Set trained\n        self.trained = True\n        self.nb_fit += 1\n\n        # Return fit arguments. This is useful for transfer learning algorithms\n        return fit_arguments\n\n    def _fit_object_detector(self, df_train: pd.DataFrame, df_valid: pd.DataFrame = None, with_shuffle: bool = True, **kwargs) -&gt; dict:\n        '''Fits the model - object detector\n\n        Args:\n            df_train (pd.DataFrame): Train dataset\n                Must contain file_path &amp; bboxes columns\n        Kwargs:\n            df_valid (pd.DataFrame): Validation dataset\n                Must contain file_path &amp; bboxes columns\n            with_shuffle (boolean): If the train dataset must be shuffled\n        Raises:\n            ValueError: If the model is not of type `object_detector`\n        '''\n        raise NotImplementedError(\"'_fit_object_detector' needs to be overridden\")\n\n    @utils.trained_needed\n    def predict(self, df_test: pd.DataFrame, return_proba: bool = False, batch_size: Union[int, None] = None) -&gt; Union[np.ndarray, list]:\n        '''Predictions on test set\n\n        Args:\n            df_test (pd.DataFrame): DataFrame to be predicted, with column file_path\n        Kwargs:\n            return_proba (bool): If the function should return the probabilities instead of the classes -- classifier only\n            batch_size (int): Batch size to be used -- classifier only\n        Raises:\n            NotImplementedError: If the model is not `classifier` nor `object_detector`\n        Returns:\n            (np.ndarray | list): Array, shape = [n_samples, n_classes] or List of n_samples elements\n        '''\n        if self.model_type == 'classifier':\n            return self._predict_classifier(df_test, return_proba=return_proba, batch_size=batch_size)\n        elif self.model_type == 'object_detector':\n            return self._predict_object_detector(df_test)\n        else:\n            raise NotImplementedError(\"Only 'classifier' and 'object_detector' model type are supported\")\n\n    @utils.trained_needed\n    def _predict_classifier(self, df_test, return_proba: bool = False, batch_size: int = None) -&gt; np.ndarray:\n        '''Predictions on test set\n\n        Args:\n            df_test (pd.DataFrame): DataFrame to be predicted, with column file_path\n        Kwargs:\n            return_proba (bool): If the function should return the probabilities instead of the classes\n            batch_size (int): Batch size to be used\n        Raises:\n            ValueError: If the model is not a classifier\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n        if self.model_type != 'classifier':\n            raise ValueError(f\"`_predict_classifier` function does not support model type {self.model_type}\")\n\n        # Backup on training batch size if no batch size defined\n        if batch_size is None:\n            batch_size = self.batch_size\n\n        # Get generator or fulldata if in_memory\n        if not self.in_memory:\n            self.logger.info(\"Retrieving a generator for test data\")\n            test_generator = self._get_generator(df_test, data_type='test', batch_size=min(batch_size, len(df_test)))\n            # Get predict arguments\n            x_arg = test_generator\n            batch_size_arg = None\n        else:\n            self.logger.info(\"Retrieving a all test data in memory\")\n            test_generator = self._get_generator(df_test, data_type='test', batch_size=len(df_test))\n            x_test, _ = test_generator.next()\n            # Get predict arguments\n            x_arg = x_test\n            batch_size_arg = batch_size\n\n        # Predict\n        predicted_proba = self.model.predict(  # type: ignore\n            x_arg,\n            batch_size=batch_size_arg,\n            steps=None,\n            workers=8,  # TODO : Check if this is ok if there are less CPUs\n            verbose=1\n        )\n\n        # We return the probabilities if wanted\n        if return_proba:\n            return predicted_proba\n\n        # Finally, we get the classes predictions\n        return self.get_classes_from_proba(predicted_proba)  # type: ignore\n\n    @utils.trained_needed\n    def _predict_object_detector(self, df_test: pd.DataFrame, **kwargs) -&gt; list:\n        '''Predictions on test set - works only with batch size = 1\n\n        Args:\n            df_test (pd.DataFrame): DataFrame to be predicted, with column file_path\n        Raises:\n            ValueError: If the model is not an object detector\n        Returns:\n            list: List of list of bboxes (one list per image)\n        '''\n        raise NotImplementedError(\"'_predict_object_detector' needs to be overridden\")\n\n    @utils.trained_needed\n    def predict_proba(self, df_test, batch_size: int = None) -&gt; np.ndarray:\n        '''Predicts probabilities on the test dataset\n\n        Args:\n            df_test (pd.DataFrame): DataFrame to be predicted, with column file_path\n        Kwargs:\n            batch_size (int): Batch size to be used\n        Raises:\n            ValueError: If the model is not a classifier\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n        if self.model_type != 'classifier':\n            raise ValueError(f\"`predict_proba` function does not support model type {self.model_type}\")\n\n        # We reuse the predict function\n        return self.predict(df_test, return_proba=True, batch_size=batch_size)\n\n    def _get_generator(self, df: pd.DataFrame, data_type: str, batch_size: int, **kwargs) -&gt; ImageDataGenerator:\n        '''Gets image generator from a list of files\n\n        Args:\n            df (pd.DataFrame): DataFrame with files to be loaded\n            data_type (str): 'train', 'valid' or 'test'\n            batch_size (int): Batch size to be used\n        Raises:\n            NotImplementedError: If the model type is not supported\n        '''\n        if self.model_type == 'classifier':\n            return self._get_generator_classifier(df, data_type, batch_size)\n        else:\n            raise NotImplementedError(f\"`_get_generator` needs to be overridden for model type {self.model_type}\")\n\n    def _get_generator_classifier(self, df: pd.DataFrame, data_type: str, batch_size: int, **kwarg) -&gt; ImageDataGenerator:\n        '''Gets image generator from a list of files - classifier version\n\n        Args:\n            df (pd.DataFrame): DataFrame with files to be loaded\n            data_type (str): 'train', 'valid' or 'test'\n            batch_size (int): Batch size to be used\n        Raises:\n            ValueError: If the model is not a classifier\n            ValueError: If data_type is not in ['train', 'valid', 'test']\n            AttributeError: If list_classes attribute is not defined\n        '''\n        if self.model_type != 'classifier':\n            raise ValueError(f\"`_get_generator_classifier` function does not support model type {self.model_type}\")\n        if data_type not in ['train', 'valid', 'test']:\n            raise ValueError(f\"{data_type} is not a valid option for argument data_type (['train', 'valid', 'test'])\")\n        if self.list_classes is None:\n            raise AttributeError(\"Cannot get an image generator if list_classes is not set.\")\n\n        # Copy\n        df = df.copy(deep=True)\n        # Set data_gen (no augmentation if validation/test)\n        if data_type == 'train':\n            data_generator = ImageDataGenerator(preprocessing_function=self.preprocess_input, **self.data_augmentation_params)\n        else:\n            data_generator = ImageDataGenerator(preprocessing_function=self.preprocess_input)\n\n        # Get generator\n        shuffle = True if data_type == 'train' else False  # DO NOT SHUFFLE IF VALID OR TEST !\n        if data_type != 'test':\n            generator = data_generator.flow_from_dataframe(df, directory=None, x_col='file_path', y_col='file_class', classes=self.list_classes,\n                                                           target_size=(self.width, self.height), color_mode=self.color_mode, class_mode='categorical',\n                                                           batch_size=batch_size, shuffle=shuffle)\n        # For the test dataset, we create a fake DataFrame with a unique class\n        else:\n            df['fake_class_col'] = 'all_classes'\n            generator = data_generator.flow_from_dataframe(df, directory=None, x_col='file_path', y_col='fake_class_col', classes=['all_classes'],\n                                                           target_size=(self.width, self.height), color_mode=self.color_mode, class_mode='categorical',\n                                                           batch_size=batch_size, shuffle=False)\n\n        return generator\n\n    def _get_preprocess_input(self) -&gt; Union[Callable, None]:\n        '''Gets the preprocessing to be used before feeding images to the NN\n        Needs to be overridden by child classes\n\n        Returns:\n            (Callable | None): Preprocessing function\n        '''\n        return None\n\n    def _get_model(self) -&gt; Any:\n        '''Gets a model structure - returns the instance model instead if already defined\n\n        Returns:\n            (Model): a Keras model\n        '''\n        raise NotImplementedError(\"'_get_model' needs to be overridden\")\n\n    def _get_callbacks(self, *args) -&gt; list:\n        '''Gets model callbacks\n\n        Returns:\n            list: List of callbacks\n        '''\n        # Get classic callbacks\n        callbacks = [EarlyStopping(monitor='val_loss', patience=self.patience, restore_best_weights=True)]\n        if self.level_save in ['MEDIUM', 'HIGH']:\n            callbacks.append(\n                ModelCheckpoint(\n                    filepath=os.path.join(self.model_dir, 'best.hdf5'), monitor='val_loss', save_best_only=True, mode='auto'\n                )\n            )\n        callbacks.append(CSVLogger(filename=os.path.join(self.model_dir, 'logger.csv'), separator=';', append=False))\n        callbacks.append(TerminateOnNaN())\n\n        # Get LearningRateScheduler\n        scheduler = self._get_learning_rate_scheduler()\n        if scheduler is not None:\n            callbacks.append(LearningRateScheduler(scheduler))\n\n        # Manage tensorboard\n        if self.level_save in ['HIGH']:\n            # Get log directory\n            models_path = utils.get_models_path()\n            tensorboard_dir = os.path.join(models_path, 'tensorboard_logs')\n            # We add a prefix so that the function load_model works correctly (it looks for a sub-folder with model name)\n            log_dir = os.path.join(tensorboard_dir, f\"tensorboard_{ntpath.basename(self.model_dir)}\")\n            if not os.path.exists(log_dir):\n                os.makedirs(log_dir)\n\n            # TODO: check if this class does not slow proccesses\n            # -&gt; For now: comment\n            # Create custom class to monitore LR changes\n            # https://stackoverflow.com/questions/49127214/keras-how-to-output-learning-rate-onto-tensorboard\n            # class LRTensorBoard(TensorBoard):\n            #     def __init__(self, log_dir, **kwargs) -&gt; None:  # add other arguments to __init__ if you need\n            #         super().__init__(log_dir=log_dir, **kwargs)\n            #\n            #     def on_epoch_end(self, epoch, logs=None):\n            #         logs.update({'lr': K.eval(self.model.optimizer.lr)})\n            #         super().on_epoch_end(epoch, logs)\n\n            callbacks.append(TensorBoard(log_dir=log_dir, write_grads=False, write_images=False))\n            self.logger.info(f\"To start tensorboard: python -m tensorboard.main --logdir {tensorboard_dir} --samples_per_plugin images=10\")\n            # We use samples_per_plugin to avoid a rare issue between matplotlib and tensorboard\n            # https://stackoverflow.com/questions/27147300/matplotlib-tcl-asyncdelete-async-handler-deleted-by-the-wrong-thread\n\n        return callbacks\n\n    def _get_learning_rate_scheduler(self) -&gt; Union[Callable, None]:\n        '''Fonction to define a Learning Rate Scheduler\n           -&gt; if it returns None, no scheduler will be used. (def.)\n           -&gt; This function will be save directly in the model configuration file\n           -&gt; This can be overridden at runing time\n\n        Returns:\n            (Callable | None): A learning rate Scheduler\n        '''\n        # e.g.\n        # def scheduler(epoch):\n        #     lim_epoch = 75\n        #     if epoch &lt; lim_epoch:\n        #         return 0.01\n        #     else:\n        #         return max(0.001, 0.01 * math.exp(0.01 * (lim_epoch - epoch)))\n        scheduler = None\n        return scheduler\n\n    def _plot_metrics_and_loss(self, fit_history, **kwargs) -&gt; None:\n        '''Plots available metrics and losses\n\n        Args:\n            fit_history (?) : fit history\n        '''\n        # Manage dir\n        plots_path = os.path.join(self.model_dir, 'plots')\n        if not os.path.exists(plots_path):\n            os.makedirs(plots_path)\n\n        # Get a dictionnary of possible metrics/loss plots\n        metrics_dir = {\n            'acc': ['Accuracy', 'accuracy'],\n            'loss': ['Loss', 'loss'],\n            'categorical_accuracy': ['Categorical accuracy', 'categorical_accuracy'],\n            'f1': ['F1-score', 'f1_score'],\n            'precision': ['Precision', 'precision'],\n            'recall': ['Recall', 'recall'],\n        }\n\n        # Plot each available metric\n        for metric in fit_history.history.keys():\n            if metric in metrics_dir.keys():\n                title = metrics_dir[metric][0]\n                filename = metrics_dir[metric][1]\n                plt.figure(figsize=(10, 8))\n                plt.plot(fit_history.history[metric])\n                plt.plot(fit_history.history[f'val_{metric}'])\n                plt.title(f\"Model {title}\")\n                plt.ylabel(title)\n                plt.xlabel('Epoch')\n                plt.legend(['Train', 'Validation'], loc='upper left')\n                # Save\n                filename = f\"{filename}.jpeg\"\n                plt.savefig(os.path.join(plots_path, filename))\n\n                # Close figures\n                plt.close('all')\n\n    def _save_model_png(self, model) -&gt; None:\n        '''Tries to save the structure of the model in png format\n        Graphviz necessary\n\n        Args:\n            model (?): model to plot\n        '''\n        # Check if graphiz is intalled\n        # TODO : to be improved !\n        graphiz_path = 'C:/Program Files (x86)/Graphviz2.38/bin/'\n        if os.path.isdir(graphiz_path):\n            os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n            img_path = os.path.join(self.model_dir, 'model.png')\n            plot_model(model, to_file=img_path)\n\n    @no_type_check  # We do not check the type, because it is complicated with managing custom_objects_str\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save configuration JSON\n        if json_data is None:\n            json_data = {}\n\n        json_data['librairie'] = 'keras'\n        json_data['batch_size'] = self.batch_size\n        json_data['epochs'] = self.epochs\n        json_data['validation_split'] = self.validation_split\n        json_data['patience'] = self.patience\n        json_data['width'] = self.width\n        json_data['height'] = self.height\n        json_data['depth'] = self.depth\n        json_data['color_mode'] = self.color_mode\n        json_data['in_memory'] = self.in_memory\n        json_data['data_augmentation_params'] = self.data_augmentation_params\n        json_data['nb_train_generator_images_to_save'] = self.nb_train_generator_images_to_save\n        json_data['keras_params'] = self.keras_params\n        if self.model is not None:\n            json_data['keras_model'] = json.loads(self.model.to_json())\n        else:\n            json_data['keras_model'] = None\n\n        # Add _get_model code if not in json_data\n        if '_get_model' not in json_data.keys():\n            json_data['_get_model'] = pickle.source.getsourcelines(self._get_model)[0]\n        # Add _get_preprocess_input code if not in json_data\n        if '_get_preprocess_input' not in json_data.keys():\n            json_data['_get_preprocess_input'] = pickle.source.getsourcelines(self._get_preprocess_input)[0]\n        # Save preprocess_input to a .pkl file if level_save &gt; LOW\n        pkl_path = os.path.join(self.model_dir, \"preprocess_input.pkl\")\n        if self.level_save in ['MEDIUM', 'HIGH']:\n            with open(pkl_path, 'wb') as f:\n                pickle.dump(self.preprocess_input, f)\n        # Add _get_learning_rate_scheduler code if not in json_data\n        if '_get_learning_rate_scheduler' not in json_data.keys():\n            json_data['_get_learning_rate_scheduler'] = pickle.source.getsourcelines(self._get_learning_rate_scheduler)[0]\n        # Add custom_objects code if not in json_data\n        if 'custom_objects' not in json_data.keys():\n            custom_objects_str = self.custom_objects.copy()\n            for key in custom_objects_str.keys():\n                if callable(custom_objects_str[key]):\n                    # Nominal case\n                    if not type(custom_objects_str[key]) == functools.partial:\n                        custom_objects_str[key] = pickle.source.getsourcelines(custom_objects_str[key])[0]\n                    # Manage partials\n                    else:\n                        custom_objects_str[key] = {\n                            'type': 'partial',\n                            'args': custom_objects_str[key].args,\n                            'function': pickle.source.getsourcelines(custom_objects_str[key].func)[0],\n                        }\n            json_data['custom_objects'] = custom_objects_str\n\n        # Save strategy :\n        # - best.hdf5 already saved in fit()\n        # - can't pickle keras model, so we drop it, save, and reload it\n        keras_model = self.model\n        self.model = None\n        super().save(json_data=json_data)\n        self.model = keras_model\n\n    def reload_model(self, hdf5_path: str) -&gt; Any:\n        '''Loads a Keras model from a HDF5 file\n\n        Args:\n            hdf5_path (str): Path to the hdf5 file\n        Returns:\n            ?: Keras model\n        '''\n        # Fix tensorflow GPU if not already done (useful if we reload a model)\n        try:\n            gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n            for device in gpu_devices:\n                tf.config.experimental.set_memory_growth(device, True)\n        except Exception:\n            pass\n\n        # We check if we already have the custom objects\n        if hasattr(self, 'custom_objects') and self.custom_objects is not None:\n            custom_objects = self.custom_objects\n        else:\n            self.logger.warning(\"Can't find the attribute 'custom_objects' in the model to be reloaded\")\n            self.logger.warning(\"Backup on the default custom_objects of utils_deep_keras\")\n            custom_objects = utils_deep_keras.custom_objects\n\n        # Loading of the model\n        keras_model = load_model(hdf5_path, custom_objects=custom_objects)\n\n        # Set trained to true if not already true\n        if not self.trained:\n            self.trained = True\n            self.nb_fit = 1\n\n        # Return\n        return keras_model\n\n    def reload_from_standalone(self, **kwargs) -&gt; None:\n        '''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Needs to be overridden /!\\\\ -\n        '''\n        raise NotImplementedError(\"'reload' needs to be overridden\")\n\n    def _is_gpu_activated(self) -&gt; bool:\n        '''Checks if a GPU is used\n\n        Returns:\n            bool: whether GPU is available or not\n        '''\n        # Check for available GPU devices\n        physical_devices = tf.config.list_physical_devices('GPU')\n        if len(physical_devices) &gt; 0:\n            return True\n        else:\n            return False\n</code></pre>"},{"location":"reference/template_vision/models_training/model_keras/#template_vision.models_training.model_keras.ModelKeras.__init__","title":"<code>__init__(batch_size=64, epochs=99, validation_split=0.2, patience=5, width=224, height=224, depth=3, color_mode='rgb', in_memory=False, data_augmentation_params={}, nb_train_generator_images_to_save=20, keras_params={}, **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass for more arguments)</p> Kwargs <p>batch_size (int): Batch size epochs (int): Number of epochs validation_split (float): Percentage for the validation set split     Only used if no input validation set when fitting patience (int): Early stopping patience width (int): NN input width (images are resized) height (int): NN input height (images are resized) depth (int): NN input depth color_mode (str): NN input color mode in_memory (bool): If all images should be loaded in memory, otherwise it uses a generator     /! OOM errors can happen really quickly (depends on the dataset size)     /! Data augmentation impossible if <code>in_memory</code> is set to True data_augmentation_params (dict): Dictionnary of parameters to be used with the data augmentation     cf. https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator     /! Not used if <code>in_memory</code> is set to True nb_train_generator_images_to_save (int): If &gt; 0, save some input generated images     If helps with to understand what goes in your NN keras_params (dict): Parameters used by Keras models.     e.g. learning_rate, nb_lstm_units, etc...     The purpose of this dictionary is for the user to use it as they wants in the _get_model function     This parameter was initially added in order to do an hyperparameters search</p> <p>Raises:     ValueError: If <code>in_memory</code> is set to True and <code>data_augmentation_params</code> is not empty</p> Source code in <code>template_vision/models_training/model_keras.py</code> <pre><code>def __init__(self, batch_size: int = 64, epochs: int = 99, validation_split: float = 0.2, patience: int = 5,\n             width: int = 224, height: int = 224, depth: int = 3, color_mode: str = 'rgb',\n             in_memory: bool = False, data_augmentation_params: dict = {},\n             nb_train_generator_images_to_save: int = 20,\n             keras_params: dict = {}, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelClass for more arguments)\n\n    Kwargs:\n        batch_size (int): Batch size\n        epochs (int): Number of epochs\n        validation_split (float): Percentage for the validation set split\n            Only used if no input validation set when fitting\n        patience (int): Early stopping patience\n        width (int): NN input width (images are resized)\n        height (int): NN input height (images are resized)\n        depth (int): NN input depth\n        color_mode (str): NN input color mode\n        in_memory (bool): If all images should be loaded in memory, otherwise it uses a generator\n            /!\\\\ OOM errors can happen really quickly (depends on the dataset size)\n            /!\\\\ Data augmentation impossible if `in_memory` is set to True\n        data_augmentation_params (dict): Dictionnary of parameters to be used with the data augmentation\n            cf. https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n            /!\\\\ Not used if `in_memory` is set to True\n        nb_train_generator_images_to_save (int): If &gt; 0, save some input generated images\n            If helps with to understand what goes in your NN\n        keras_params (dict): Parameters used by Keras models.\n            e.g. learning_rate, nb_lstm_units, etc...\n            The purpose of this dictionary is for the user to use it as they wants in the _get_model function\n            This parameter was initially added in order to do an hyperparameters search\n    Raises:\n        ValueError: If `in_memory` is set to True and `data_augmentation_params` is not empty\n    '''\n    # TODO: learning rate should be an attribute !\n\n    # Check for errors\n    if in_memory and len(data_augmentation_params) &gt; 0:\n        raise ValueError(\"Data augmentation is impossible for 'in_memory' mode\")\n\n    # Init.\n    super().__init__(**kwargs)\n\n    # Fix tensorflow GPU\n    gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n    for device in gpu_devices:\n        tf.config.experimental.set_memory_growth(device, True)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Param. model\n    self.batch_size = batch_size\n    self.epochs = epochs\n    self.validation_split = validation_split\n    self.patience = patience\n\n    # Params. generator\n    self.width = width\n    self.height = height\n    self.depth = depth\n    self.color_mode = color_mode\n    self.in_memory = in_memory\n    self.data_augmentation_params = data_augmentation_params.copy()\n\n    # Warnings if depth does not match with color_mode\n    if self.color_mode == 'rgb' and self.depth != 3:\n        self.logger.warning(f\"`color_mode` parameter is 'rgb', but `depth` parameteris not equal to 3 ({self.depth})\")\n        self.logger.warning(\"We continue, but this can lead to errors during the training\")\n    if self.color_mode == 'rgba' and self.depth != 4:\n        self.logger.warning(f\"`color_mode` parameter is 'rgba', but `depth` parameteris not equal to 4 ({self.depth})\")\n        self.logger.warning(\"We continue, but this can lead to errors during the training\")\n\n    # TODO: add Test time augmentation ?\n\n    # Misc.\n    self.nb_train_generator_images_to_save = nb_train_generator_images_to_save\n\n    # Model set on fit\n    self.model: Any = None\n\n    # Set preprocess input\n    self.preprocess_input = self._get_preprocess_input()\n\n    # Keras params\n    self.keras_params = keras_params.copy()\n\n    # Keras custom objects : we get the ones specified in utils_deep_keras\n    self.custom_objects = utils_deep_keras.custom_objects\n</code></pre>"},{"location":"reference/template_vision/models_training/model_keras/#template_vision.models_training.model_keras.ModelKeras.fit","title":"<code>fit(df_train, df_valid=None, with_shuffle=True, **kwargs)</code>","text":"<p>Fits the model</p> <p>Parameters:</p> Name Type Description Default <code>df_train</code> <code>DataFrame</code> <p>Train dataset Must contain file_path &amp; file_class columns if classifier Must contain file_path &amp; bboxes columns if object detector</p> required <p>Kwargs:     df_valid (pd.DataFrame): Validation dataset         Must contain file_path &amp; file_class columns if classifier         Must contain file_path &amp; bboxes columns if object detector     with_shuffle (boolean): If the train dataset must be shuffled         This should be used if the input dataset is not shuffled &amp; no validation set as the split_validation takes the lines in order.         Thus, the validation set might get classes which are not in the train set ... Raises:     NotImplementedError: If the model is not <code>classifier</code> nor <code>object_detector</code> Returns:     dict: Fit arguments, to be used with transfer learning fine-tuning</p> Source code in <code>template_vision/models_training/model_keras.py</code> <pre><code>def fit(self, df_train: pd.DataFrame, df_valid: Union[pd.DataFrame, None] = None, with_shuffle: bool = True, **kwargs) -&gt; dict:\n    '''Fits the model\n\n    Args:\n        df_train (pd.DataFrame): Train dataset\n            Must contain file_path &amp; file_class columns if classifier\n            Must contain file_path &amp; bboxes columns if object detector\n    Kwargs:\n        df_valid (pd.DataFrame): Validation dataset\n            Must contain file_path &amp; file_class columns if classifier\n            Must contain file_path &amp; bboxes columns if object detector\n        with_shuffle (boolean): If the train dataset must be shuffled\n            This should be used if the input dataset is not shuffled &amp; no validation set as the split_validation takes the lines in order.\n            Thus, the validation set might get classes which are not in the train set ...\n    Raises:\n        NotImplementedError: If the model is not `classifier` nor `object_detector`\n    Returns:\n        dict: Fit arguments, to be used with transfer learning fine-tuning\n    '''\n    if self.model_type == 'classifier':\n        return self._fit_classifier(df_train, df_valid=df_valid, with_shuffle=with_shuffle, **kwargs)\n    elif self.model_type == 'object_detector':\n        return self._fit_object_detector(df_train, df_valid=df_valid, with_shuffle=with_shuffle, **kwargs)\n    else:\n        raise NotImplementedError(\"Only `classifier` and `object_detector` model type are supported.\")\n</code></pre>"},{"location":"reference/template_vision/models_training/model_keras/#template_vision.models_training.model_keras.ModelKeras.predict","title":"<code>predict(df_test, return_proba=False, batch_size=None)</code>","text":"<p>Predictions on test set</p> <p>Parameters:</p> Name Type Description Default <code>df_test</code> <code>DataFrame</code> <p>DataFrame to be predicted, with column file_path</p> required <p>Kwargs:     return_proba (bool): If the function should return the probabilities instead of the classes -- classifier only     batch_size (int): Batch size to be used -- classifier only Raises:     NotImplementedError: If the model is not <code>classifier</code> nor <code>object_detector</code> Returns:     (np.ndarray | list): Array, shape = [n_samples, n_classes] or List of n_samples elements</p> Source code in <code>template_vision/models_training/model_keras.py</code> <pre><code>@utils.trained_needed\ndef predict(self, df_test: pd.DataFrame, return_proba: bool = False, batch_size: Union[int, None] = None) -&gt; Union[np.ndarray, list]:\n    '''Predictions on test set\n\n    Args:\n        df_test (pd.DataFrame): DataFrame to be predicted, with column file_path\n    Kwargs:\n        return_proba (bool): If the function should return the probabilities instead of the classes -- classifier only\n        batch_size (int): Batch size to be used -- classifier only\n    Raises:\n        NotImplementedError: If the model is not `classifier` nor `object_detector`\n    Returns:\n        (np.ndarray | list): Array, shape = [n_samples, n_classes] or List of n_samples elements\n    '''\n    if self.model_type == 'classifier':\n        return self._predict_classifier(df_test, return_proba=return_proba, batch_size=batch_size)\n    elif self.model_type == 'object_detector':\n        return self._predict_object_detector(df_test)\n    else:\n        raise NotImplementedError(\"Only 'classifier' and 'object_detector' model type are supported\")\n</code></pre>"},{"location":"reference/template_vision/models_training/model_keras/#template_vision.models_training.model_keras.ModelKeras.predict_proba","title":"<code>predict_proba(df_test, batch_size=None)</code>","text":"<p>Predicts probabilities on the test dataset</p> <p>Parameters:</p> Name Type Description Default <code>df_test</code> <code>DataFrame</code> <p>DataFrame to be predicted, with column file_path</p> required <p>Kwargs:     batch_size (int): Batch size to be used Raises:     ValueError: If the model is not a classifier Returns:     (np.ndarray): Array, shape = [n_samples, n_classes]</p> Source code in <code>template_vision/models_training/model_keras.py</code> <pre><code>@utils.trained_needed\ndef predict_proba(self, df_test, batch_size: int = None) -&gt; np.ndarray:\n    '''Predicts probabilities on the test dataset\n\n    Args:\n        df_test (pd.DataFrame): DataFrame to be predicted, with column file_path\n    Kwargs:\n        batch_size (int): Batch size to be used\n    Raises:\n        ValueError: If the model is not a classifier\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n    if self.model_type != 'classifier':\n        raise ValueError(f\"`predict_proba` function does not support model type {self.model_type}\")\n\n    # We reuse the predict function\n    return self.predict(df_test, return_proba=True, batch_size=batch_size)\n</code></pre>"},{"location":"reference/template_vision/models_training/model_keras/#template_vision.models_training.model_keras.ModelKeras.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Needs to be overridden /! -</p> Source code in <code>template_vision/models_training/model_keras.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n    '''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Needs to be overridden /!\\\\ -\n    '''\n    raise NotImplementedError(\"'reload' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_vision/models_training/model_keras/#template_vision.models_training.model_keras.ModelKeras.reload_model","title":"<code>reload_model(hdf5_path)</code>","text":"<p>Loads a Keras model from a HDF5 file</p> <p>Parameters:</p> Name Type Description Default <code>hdf5_path</code> <code>str</code> <p>Path to the hdf5 file</p> required <p>Returns:     ?: Keras model</p> Source code in <code>template_vision/models_training/model_keras.py</code> <pre><code>def reload_model(self, hdf5_path: str) -&gt; Any:\n    '''Loads a Keras model from a HDF5 file\n\n    Args:\n        hdf5_path (str): Path to the hdf5 file\n    Returns:\n        ?: Keras model\n    '''\n    # Fix tensorflow GPU if not already done (useful if we reload a model)\n    try:\n        gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n        for device in gpu_devices:\n            tf.config.experimental.set_memory_growth(device, True)\n    except Exception:\n        pass\n\n    # We check if we already have the custom objects\n    if hasattr(self, 'custom_objects') and self.custom_objects is not None:\n        custom_objects = self.custom_objects\n    else:\n        self.logger.warning(\"Can't find the attribute 'custom_objects' in the model to be reloaded\")\n        self.logger.warning(\"Backup on the default custom_objects of utils_deep_keras\")\n        custom_objects = utils_deep_keras.custom_objects\n\n    # Loading of the model\n    keras_model = load_model(hdf5_path, custom_objects=custom_objects)\n\n    # Set trained to true if not already true\n    if not self.trained:\n        self.trained = True\n        self.nb_fit = 1\n\n    # Return\n    return keras_model\n</code></pre>"},{"location":"reference/template_vision/models_training/model_keras/#template_vision.models_training.model_keras.ModelKeras.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_vision/models_training/model_keras.py</code> <pre><code>@no_type_check  # We do not check the type, because it is complicated with managing custom_objects_str\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save configuration JSON\n    if json_data is None:\n        json_data = {}\n\n    json_data['librairie'] = 'keras'\n    json_data['batch_size'] = self.batch_size\n    json_data['epochs'] = self.epochs\n    json_data['validation_split'] = self.validation_split\n    json_data['patience'] = self.patience\n    json_data['width'] = self.width\n    json_data['height'] = self.height\n    json_data['depth'] = self.depth\n    json_data['color_mode'] = self.color_mode\n    json_data['in_memory'] = self.in_memory\n    json_data['data_augmentation_params'] = self.data_augmentation_params\n    json_data['nb_train_generator_images_to_save'] = self.nb_train_generator_images_to_save\n    json_data['keras_params'] = self.keras_params\n    if self.model is not None:\n        json_data['keras_model'] = json.loads(self.model.to_json())\n    else:\n        json_data['keras_model'] = None\n\n    # Add _get_model code if not in json_data\n    if '_get_model' not in json_data.keys():\n        json_data['_get_model'] = pickle.source.getsourcelines(self._get_model)[0]\n    # Add _get_preprocess_input code if not in json_data\n    if '_get_preprocess_input' not in json_data.keys():\n        json_data['_get_preprocess_input'] = pickle.source.getsourcelines(self._get_preprocess_input)[0]\n    # Save preprocess_input to a .pkl file if level_save &gt; LOW\n    pkl_path = os.path.join(self.model_dir, \"preprocess_input.pkl\")\n    if self.level_save in ['MEDIUM', 'HIGH']:\n        with open(pkl_path, 'wb') as f:\n            pickle.dump(self.preprocess_input, f)\n    # Add _get_learning_rate_scheduler code if not in json_data\n    if '_get_learning_rate_scheduler' not in json_data.keys():\n        json_data['_get_learning_rate_scheduler'] = pickle.source.getsourcelines(self._get_learning_rate_scheduler)[0]\n    # Add custom_objects code if not in json_data\n    if 'custom_objects' not in json_data.keys():\n        custom_objects_str = self.custom_objects.copy()\n        for key in custom_objects_str.keys():\n            if callable(custom_objects_str[key]):\n                # Nominal case\n                if not type(custom_objects_str[key]) == functools.partial:\n                    custom_objects_str[key] = pickle.source.getsourcelines(custom_objects_str[key])[0]\n                # Manage partials\n                else:\n                    custom_objects_str[key] = {\n                        'type': 'partial',\n                        'args': custom_objects_str[key].args,\n                        'function': pickle.source.getsourcelines(custom_objects_str[key].func)[0],\n                    }\n        json_data['custom_objects'] = custom_objects_str\n\n    # Save strategy :\n    # - best.hdf5 already saved in fit()\n    # - can't pickle keras model, so we drop it, save, and reload it\n    keras_model = self.model\n    self.model = None\n    super().save(json_data=json_data)\n    self.model = keras_model\n</code></pre>"},{"location":"reference/template_vision/models_training/utils_deep_keras/","title":"Utils deep keras","text":""},{"location":"reference/template_vision/models_training/utils_deep_keras/#template_vision.models_training.utils_deep_keras.f1","title":"<code>f1(y_true, y_pred)</code>","text":"<p>f1 score, to use as custom metrics</p> <ul> <li>/! To use with a big batch size /! -</li> </ul> From <p>https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth values</p> required <code>y_pred</code> <p>The predicted values</p> required <p>Returns:     float: metric</p> Source code in <code>template_vision/models_training/utils_deep_keras.py</code> <pre><code>def f1(y_true, y_pred) -&gt; float:\n    '''f1 score, to use as custom metrics\n\n    - /!\\\\ To use with a big batch size /!\\\\ -\n\n    From:\n        https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n        https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras\n\n    Args:\n        y_true: Ground truth values\n        y_pred: The predicted values\n    Returns:\n        float: metric\n    '''\n    # Round pred to 0 &amp; 1\n    y_pred = K.round(y_pred)\n    y_true = K.cast(y_true, 'float32')  # Fix : TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.\n\n    ground_positives = K.sum(y_true, axis=0) + K.epsilon()  # We add an epsilon -&gt; manage the case where a class is absent in the batch\n\n    y_pred = K.round(y_pred)\n    tp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n    # tn = K.sum(K.cast((1 - y_true) * (1 - y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\n\n    p = tp / (tp + fp + K.epsilon())\n    r = tp / (tp + fn + K.epsilon())\n\n    f1 = 2 * p * r / (p + r + K.epsilon())\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n\n    weighted_f1 = f1 * ground_positives / K.sum(ground_positives)\n    weighted_f1 = K.sum(weighted_f1)\n\n    return weighted_f1\n</code></pre>"},{"location":"reference/template_vision/models_training/utils_deep_keras/#template_vision.models_training.utils_deep_keras.f1_loss","title":"<code>f1_loss(y_true, y_pred)</code>","text":"<p>f1 loss, to use as custom loss</p> <ul> <li>/! To use with a big batch size /! -</li> </ul> From <p>https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth values</p> required <code>y_pred</code> <p>The predicted values</p> required <p>Returns:     float: metric</p> Source code in <code>template_vision/models_training/utils_deep_keras.py</code> <pre><code>def f1_loss(y_true, y_pred) -&gt; float:\n    '''f1 loss, to use as custom loss\n\n    - /!\\\\ To use with a big batch size /!\\\\ -\n\n    From:\n        https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n        https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras\n\n    Args:\n        y_true: Ground truth values\n        y_pred: The predicted values\n    Returns:\n        float: metric\n    '''\n    # TODO : Find a mean of rounding y_pred\n    # TODO : Problem : models will quickly converge on probabilities 1.0 &amp; 0.0 to optimize this loss....\n    # We can't round here :(\n    # Please make sure that all of your ops have a gradient defined (i.e. are differentiable).\n    # Common ops without gradient: K.argmax, K.round, K.eval.\n    y_true = K.cast(y_true, 'float32')  # Fix : TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.\n\n    ground_positives = K.sum(y_true, axis=0) + K.epsilon()  # We add an epsilon -&gt; manage the case where a class is absent in the batch\n\n    tp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n    # tn = K.sum(K.cast((1 - y_true) * (1 - y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\n\n    p = tp / (tp + fp + K.epsilon())\n    r = tp / (tp + fn + K.epsilon())\n\n    f1 = 2 * p * r / (p + r + K.epsilon())\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n\n    weighted_f1 = f1 * ground_positives / K.sum(ground_positives)\n    weighted_f1 = K.sum(weighted_f1)\n\n    return 1 - weighted_f1\n</code></pre>"},{"location":"reference/template_vision/models_training/utils_deep_keras/#template_vision.models_training.utils_deep_keras.fb_loss","title":"<code>fb_loss(b, y_true, y_pred)</code>","text":"<p>fB loss, to use as custom loss</p> <ul> <li>/! To use with a big batch size /! -</li> </ul> From <p>https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>float</code> <p>importance recall in the calculation of the fB score</p> required <code>y_true</code> <p>Ground truth values</p> required <code>y_pred</code> <p>The predicted values</p> required <p>Returns:     float: metric</p> Source code in <code>template_vision/models_training/utils_deep_keras.py</code> <pre><code>def fb_loss(b: float, y_true, y_pred) -&gt; float:\n    '''fB loss, to use as custom loss\n\n    - /!\\\\ To use with a big batch size /!\\\\ -\n\n    From:\n        https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n        https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras\n\n    Args:\n        b (float): importance recall in the calculation of the fB score\n        y_true: Ground truth values\n        y_pred: The predicted values\n    Returns:\n        float: metric\n    '''\n    # TODO : Find a mean of rounding y_pred\n    # TODO : Problem : models will quickly converge on probabilities 1.0 &amp; 0.0 to optimize this loss....\n    # We can't round here :(\n    # Please make sure that all of your ops have a gradient defined (i.e. are differentiable).\n    # Common ops without gradient: K.argmax, K.round, K.eval.\n    y_true = K.cast(y_true, 'float32')  # Fix : TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.\n\n    ground_positives = K.sum(y_true, axis=0) + K.epsilon()  # We add an epsilon -&gt; manage the case where a class is absent in the batch\n\n    tp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n    # tn = K.sum(K.cast((1 - y_true) * (1 - y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\n\n    p = tp / (tp + fp + K.epsilon())\n    r = tp / (tp + fn + K.epsilon())\n\n    fb = (1 + b**2) * p * r / ((p * b**2) + r + K.epsilon())\n    fb = tf.where(tf.math.is_nan(fb), tf.zeros_like(fb), fb)\n\n    weighted_fb = fb * ground_positives / K.sum(ground_positives)\n    weighted_fb = K.sum(weighted_fb)\n\n    return 1 - weighted_fb\n</code></pre>"},{"location":"reference/template_vision/models_training/utils_deep_keras/#template_vision.models_training.utils_deep_keras.get_fb_loss","title":"<code>get_fb_loss(b=2.0)</code>","text":"<p>Gets a fB-score loss</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>float</code> <p>importance recall in the calculation of the fB score</p> <code>2.0</code> <p>Returns:     Callable: fb_loss</p> Source code in <code>template_vision/models_training/utils_deep_keras.py</code> <pre><code>def get_fb_loss(b: float = 2.0) -&gt; Callable:\n    ''' Gets a fB-score loss\n\n    Args:\n        b (float): importance recall in the calculation of the fB score\n    Returns:\n        Callable: fb_loss\n    '''\n    # - /!\\ Utilisation partial obligatoire pour pouvoir pickle des fonctions dynamiques ! /!\\ -\n    fn = partial(fb_loss, b)\n    # FIX:  AttributeError: 'functools.partial' object has no attribute '__name__'\n    fn.__name__ = 'fb_loss'  # type: ignore\n    return fn\n</code></pre>"},{"location":"reference/template_vision/models_training/utils_deep_keras/#template_vision.models_training.utils_deep_keras.get_weighted_binary_crossentropy","title":"<code>get_weighted_binary_crossentropy(pos_weight=10.0)</code>","text":"<p>Gets a \"weighted binary crossentropy\" loss From https://stats.stackexchange.com/questions/261128/neural-network-for-multi-label-classification-with-large-number-of-classes-outpu TO BE ADDED IN custom_objects : 'weighted_binary_crossentropy': utils_deep_keras.get_weighted_binary_crossentropy(pos_weight=...)</p> <p>Parameters:</p> Name Type Description Default <code>pos_weight</code> <code>float</code> <p>Weight of the positive class, to be tuned</p> <code>10.0</code> <p>Returns:     Callable: Weighted binary crossentropy loss</p> Source code in <code>template_vision/models_training/utils_deep_keras.py</code> <pre><code>def get_weighted_binary_crossentropy(pos_weight: float = 10.0) -&gt; Callable:\n    ''' Gets a \"weighted binary crossentropy\" loss\n    From https://stats.stackexchange.com/questions/261128/neural-network-for-multi-label-classification-with-large-number-of-classes-outpu\n    TO BE ADDED IN custom_objects : 'weighted_binary_crossentropy': utils_deep_keras.get_weighted_binary_crossentropy(pos_weight=...)\n\n    Args:\n        pos_weight (float): Weight of the positive class, to be tuned\n    Returns:\n        Callable: Weighted binary crossentropy loss\n    '''\n    # - /!\\ Use of partial mandatory in order to be able to pickle dynamical functions ! /!\\ -\n    fn = partial(weighted_binary_crossentropy, pos_weight)\n    # FIX:  AttributeError: 'functools.partial' object has no attribute '__name__'\n    fn.__name__ = 'weighted_binary_crossentropy'  # type: ignore\n    return fn\n</code></pre>"},{"location":"reference/template_vision/models_training/utils_deep_keras/#template_vision.models_training.utils_deep_keras.precision","title":"<code>precision(y_true, y_pred)</code>","text":"<p>Precision, to use as custom metrics</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth values</p> required <code>y_pred</code> <p>The predicted values</p> required <p>Returns:     float: metric</p> Source code in <code>template_vision/models_training/utils_deep_keras.py</code> <pre><code>def precision(y_true, y_pred) -&gt; float:\n    '''Precision, to use as custom metrics\n\n    Args:\n        y_true: Ground truth values\n        y_pred: The predicted values\n    Returns:\n        float: metric\n    '''\n    y_pred = K.round(y_pred)\n    y_true = K.cast(y_true, 'float32')  # Fix : TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.\n\n    ground_positives = K.sum(y_true, axis=0) + K.epsilon()  # We add an epsilon -&gt; manage the case where a class is absent in the batch\n\n    tp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n    fp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\n\n    precision = tp / (tp + fp + K.epsilon())\n    precision = tf.where(tf.math.is_nan(precision), tf.zeros_like(precision), precision)\n\n    weighted_precision = precision * ground_positives / K.sum(ground_positives)\n    weighted_precision = K.sum(weighted_precision)\n\n    return weighted_precision\n</code></pre>"},{"location":"reference/template_vision/models_training/utils_deep_keras/#template_vision.models_training.utils_deep_keras.recall","title":"<code>recall(y_true, y_pred)</code>","text":"<p>Recall to use as a custom metrics</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth values</p> required <code>y_pred</code> <p>The predicted values</p> required <p>Returns:     float: metric</p> Source code in <code>template_vision/models_training/utils_deep_keras.py</code> <pre><code>def recall(y_true, y_pred) -&gt; float:\n    '''Recall to use as a custom metrics\n\n    Args:\n        y_true: Ground truth values\n        y_pred: The predicted values\n    Returns:\n        float: metric\n    '''\n    y_pred = K.round(y_pred)\n    y_true = K.cast(y_true, 'float32')  # Fix : TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.\n\n    ground_positives = K.sum(y_true, axis=0) + K.epsilon()  # We add an epsilon -&gt; manage the case where a class is absent in the batch\n\n    tp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\n\n    recall = tp / (tp + fn + K.epsilon())\n    recall = tf.where(tf.math.is_nan(recall), tf.zeros_like(recall), recall)\n\n    weighted_recall = recall * ground_positives / K.sum(ground_positives)\n    weighted_recall = K.sum(weighted_recall)\n\n    return weighted_recall\n</code></pre>"},{"location":"reference/template_vision/models_training/utils_deep_keras/#template_vision.models_training.utils_deep_keras.weighted_binary_crossentropy","title":"<code>weighted_binary_crossentropy(pos_weight, target, output)</code>","text":"<p>Weighted binary crossentropy between an output tensor and a target tensor. pos_weight is used as a multiplier for the positive targets.</p> <p>Combination of the following functions: * keras.losses.binary_crossentropy * keras.backend.tensorflow_backend.binary_crossentropy * tf.nn.weighted_cross_entropy_with_logits</p> <p>Parameters:</p> Name Type Description Default <code>pos_weight</code> <code>float</code> <p>poid classe positive, to be tuned</p> required <code>target</code> <p>Target tensor</p> required <code>output</code> <p>Output tensor</p> required <p>Returns:     float: metric</p> Source code in <code>template_vision/models_training/utils_deep_keras.py</code> <pre><code>def weighted_binary_crossentropy(pos_weight: float, target, output) -&gt; float:\n    '''Weighted binary crossentropy between an output tensor\n    and a target tensor. pos_weight is used as a multiplier\n    for the positive targets.\n\n    Combination of the following functions:\n    * keras.losses.binary_crossentropy\n    * keras.backend.tensorflow_backend.binary_crossentropy\n    * tf.nn.weighted_cross_entropy_with_logits\n\n    Args:\n        pos_weight (float): poid classe positive, to be tuned\n        target: Target tensor\n        output: Output tensor\n    Returns:\n        float: metric\n    '''\n    target = K.cast(target, 'float32')\n    output = K.cast(output, 'float32')\n    # transform back to logits\n    _epsilon = tf.convert_to_tensor(K.epsilon(), output.dtype.base_dtype)\n    output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n    output = tf.math.log(output / (1 - output))\n    # compute weighted loss\n    loss = tf.nn.weighted_cross_entropy_with_logits(target, output, pos_weight=pos_weight)\n    return tf.reduce_mean(loss, axis=-1)\n</code></pre>"},{"location":"reference/template_vision/models_training/utils_models/","title":"Utils models","text":""},{"location":"reference/template_vision/models_training/utils_models/#template_vision.models_training.utils_models.display_train_test_shape","title":"<code>display_train_test_shape(df_train, df_test, df_shape=None)</code>","text":"<p>Displays the size of a train/test split</p> <p>Parameters:</p> Name Type Description Default <code>df_train</code> <code>DataFrame</code> <p>Train dataset</p> required <code>df_test</code> <code>DataFrame</code> <p>Test dataset</p> required <p>Kwargs:     df_shape (int): Size of the initial dataset Raises:     ValueError: If the object df_shape is not positive</p> Source code in <code>template_vision/models_training/utils_models.py</code> <pre><code>def display_train_test_shape(df_train: pd.DataFrame, df_test: pd.DataFrame, df_shape: Union[int, None] = None) -&gt; None:\n    '''Displays the size of a train/test split\n\n    Args:\n        df_train (pd.DataFrame): Train dataset\n        df_test (pd.DataFrame): Test dataset\n    Kwargs:\n        df_shape (int): Size of the initial dataset\n    Raises:\n        ValueError: If the object df_shape is not positive\n    '''\n    if df_shape is not None and df_shape &lt; 1:\n        raise ValueError(\"The object df_shape must be positive\")\n\n    # Process\n    if df_shape is None:\n        df_shape = df_train.shape[0] + df_test.shape[0]\n    logger.info(f\"There are {df_train.shape[0]} lines in the train dataset and {df_test.shape[0]} in the test dataset.\")\n    logger.info(f\"{round(100 * df_train.shape[0] / df_shape, 2)}% of data are in the train set\")\n    logger.info(f\"{round(100 * df_test.shape[0] / df_shape, 2)}% of data are in the test set\")\n</code></pre>"},{"location":"reference/template_vision/models_training/utils_models/#template_vision.models_training.utils_models.load_model","title":"<code>load_model(model_dir, is_path=False)</code>","text":"<p>Loads a model from a path</p> <p>Parameters:</p> Name Type Description Default <code>model_dir</code> <code>str</code> <p>Name of the folder containing the model (e.g. model_autres_2019_11_07-13_43_19)</p> required <p>Kwargs:     is_path (bool): If folder path instead of name (permits to load model from elsewhere) Returns:     ?: Model     dict: Model configurations</p> Source code in <code>template_vision/models_training/utils_models.py</code> <pre><code>def load_model(model_dir: str, is_path: bool = False) -&gt; Tuple[Any, dict]:\n    '''Loads a model from a path\n\n    Args:\n        model_dir (str): Name of the folder containing the model (e.g. model_autres_2019_11_07-13_43_19)\n    Kwargs:\n        is_path (bool): If folder path instead of name (permits to load model from elsewhere)\n    Returns:\n        ?: Model\n        dict: Model configurations\n    '''\n    # Find model path\n    base_folder = None if is_path else utils.get_models_path()\n    model_path = utils.find_folder_path(model_dir, base_folder)\n\n    # Get configs\n    configuration_path = os.path.join(model_path, 'configurations.json')\n    with open(configuration_path, 'r', encoding='utf-8') as f:\n        configs = json.load(f)\n    # Can't set int as keys in json, so need to cast it after reloading\n    # dict_classes keys are always ints\n    if 'dict_classes' in configs.keys() and configs['dict_classes'] is not None:\n        configs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n\n    # Load model\n    pkl_path = os.path.join(model_path, f\"{configs['model_name']}.pkl\")\n    with open(pkl_path, 'rb') as f:\n        model = pickle.load(f)\n\n    # Change model_dir if diff\n    if model_path != model.model_dir:\n        model.model_dir = model_path\n        configs['model_dir'] = model_path\n\n    # Load specifics\n    hdf5_path = os.path.join(model_path, 'best.hdf5')\n\n    # TODO : we should probably have a single function `load_self` and let the model manage it's reload\n    # Check for keras model\n    if os.path.exists(hdf5_path):\n        # If a specific reload function has been defined (e.g. faster RCNN), we use it\n        if hasattr(model, 'reload_models_from_hdf5'):\n            model.reload_models_from_hdf5(hdf5_path)\n        else:\n            model.model = model.reload_model(hdf5_path)\n\n    # Display if GPU is being used\n    model.display_if_gpu_activated()\n\n    # Return model &amp; configs\n    return model, configs\n</code></pre>"},{"location":"reference/template_vision/models_training/utils_models/#template_vision.models_training.utils_models.normal_split","title":"<code>normal_split(df, test_size=0.25, seed=None)</code>","text":"<p>Splits a DataFrame into train and test sets</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe containing the data</p> required <p>Kwargs:     test_size (float): Proportion representing the size of the expected test set     seed (int): random seed Raises:     ValueError: If the object test_size is not between 0 and 1 Returns:     DataFrame: Train dataframe     DataFrame: Test dataframe</p> Source code in <code>template_vision/models_training/utils_models.py</code> <pre><code>def normal_split(df: pd.DataFrame, test_size: float = 0.25, seed: Union[int, None] = None) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    '''Splits a DataFrame into train and test sets\n\n    Args:\n        df (pd.DataFrame): Dataframe containing the data\n    Kwargs:\n        test_size (float): Proportion representing the size of the expected test set\n        seed (int): random seed\n    Raises:\n        ValueError: If the object test_size is not between 0 and 1\n    Returns:\n        DataFrame: Train dataframe\n        DataFrame: Test dataframe\n    '''\n    if not 0 &lt;= test_size &lt;= 1:\n        raise ValueError('The object test_size must be between 0 and 1')\n\n    # Normal split\n    logger.info(\"Normal split\")\n    df_train, df_test = train_test_split(df, test_size=test_size, random_state=seed)\n\n    # Display\n    display_train_test_shape(df_train, df_test, df_shape=df.shape[0])\n\n    # Return\n    return df_train, df_test\n</code></pre>"},{"location":"reference/template_vision/models_training/utils_models/#template_vision.models_training.utils_models.predict","title":"<code>predict(data_input, model, model_conf, return_proba=False, **kwargs)</code>","text":"<p>Gets predictions of a model on images</p> <p>Parameters:</p> Name Type Description Default <code>data_input</code> <code>str | list&lt;str&gt; | np.ndarray</code> <p>New content to be predicted - str: abs. path to an image - list: list of abs. path to an image - np.ndarray: an already loaded image     Possibility to have several images if 4 dim (i.e (nb_images, width, height, channels)) - pd.DataFrame: Dataframe with a column file_path (abs. paths to images) required <code>model</code> <code>ModelClass</code> <p>Model to use</p> required <code>model_conf</code> <code>dict</code> <p>Model configurations</p> required <p>Kwargs:     return_proba (bool): If probabilities must be return instead Raises:     NotImplementedError: If model is object detection task     FileNotFoundError: If the input file does not exist (input type == str)     FileNotFoundError: If one of the input files does not exist (input type == list)     ValueError: If the input image format is not compatible (input type == np.ndarray)     ValueError: If the input array is not compatible (input type == np.ndarray)     ValueError: If the input DataFrame does not contains a 'file_path' column (input type == pd.DataFrame)     ValueError: If the input type is not a valid type option Returns:     List[str], np.ndarray: predictions or probabilities         - If return_proba -&gt; np.ndarray         - Else List[str]</p> Source code in <code>template_vision/models_training/utils_models.py</code> <pre><code>def predict(data_input: Union[str, List[str], np.ndarray, pd.DataFrame], model, model_conf: dict,\n            return_proba: bool = False, **kwargs) -&gt; Union[List[str], np.ndarray]:\n    '''Gets predictions of a model on images\n\n    Args:\n        data_input (str | list&lt;str&gt; | np.ndarray): New content to be predicted\n            - str: abs. path to an image\n            - list&lt;str&gt;: list of abs. path to an image\n            - np.ndarray: an already loaded image\n                Possibility to have several images if 4 dim (i.e (nb_images, width, height, channels))\n            - pd.DataFrame: Dataframe with a column file_path (abs. paths to images)\n        model (ModelClass): Model to use\n        model_conf (dict): Model configurations\n    Kwargs:\n        return_proba (bool): If probabilities must be return instead\n    Raises:\n        NotImplementedError: If model is object detection task\n        FileNotFoundError: If the input file does not exist (input type == str)\n        FileNotFoundError: If one of the input files does not exist (input type == list)\n        ValueError: If the input image format is not compatible (input type == np.ndarray)\n        ValueError: If the input array is not compatible (input type == np.ndarray)\n        ValueError: If the input DataFrame does not contains a 'file_path' column (input type == pd.DataFrame)\n        ValueError: If the input type is not a valid type option\n    Returns:\n        List[str], np.ndarray: predictions or probabilities\n            - If return_proba -&gt; np.ndarray\n            - Else List[str]\n    '''\n    # TODO\n    # TODO\n    # TODO: Make this works with object_detector !!!\n    # TODO\n    # TODO\n    if model.model_type == 'object_detector':\n        raise NotImplementedError(\"`predict` is not yet implemented for object detection task\")\n\n    ##############################################\n    # Retrieve data - PIL format (list)\n    ##############################################\n\n    # Type 1: absolute path\n    if isinstance(data_input, str):\n        if not os.path.exists(data_input):\n            raise FileNotFoundError(f\"The file {data_input} does not exist\")\n        images = [Image.open(data_input)]\n\n    # Type 2: list of absolute paths\n    elif isinstance(data_input, list):\n        if not all([os.path.exists(_) for _ in data_input]):\n            raise FileNotFoundError(\"At least one of the input path does not exist\")\n        images = [Image.open(_) for _ in data_input]\n\n    # Type 3: numpy array\n    elif isinstance(data_input, np.ndarray):\n        # If only one image (shape = 3), exapnd a 4th image\n        if len(data_input.shape) == 3:\n            data_input = np.expand_dims(data_input, 0)\n        # Consider input as image list\n        if len(data_input.shape) == 4:\n            images = []\n            for i in range(data_input.shape[0]):\n                np_image = data_input[i]\n                # RGB\n                if np_image.shape[-1] == 3:\n                    images.append(Image.fromarray(np_image, 'RGB'))\n                elif np_image.shape[-1] == 4:\n                    images.append(Image.fromarray(np_image, 'RGBA'))\n                else:\n                    raise ValueError(f\"Input image format ({np_image.shape}) is not compatible\")\n        else:\n            raise ValueError(f\"Input array format ({type(data_input)}) is not valid\")\n\n    # Type 4: pd.DataFrame\n    elif isinstance(data_input, pd.DataFrame):\n        if 'file_path' not in data_input.columns:\n            raise ValueError(\"The input DataFrame does not contains a 'file_path' column (mandatory)\")\n        file_paths = list(data_input['file_path'].values)\n        if not all([os.path.exists(_) for _ in file_paths]):\n            raise FileNotFoundError(\"At least one of the input path does not exist\")\n        images = [Image.open(_) for _ in file_paths]\n\n    # No solution\n    else:\n        raise ValueError(f\"Input type ({type(data_input)}) is not a valid type option.\")\n\n    ##############################################\n    # Apply preprocessing\n    ##############################################\n\n    # Get preprocessor\n    if 'preprocess_str' in model_conf.keys():\n        preprocess_str = model_conf['preprocess_str']\n    else:\n        preprocess_str = \"no_preprocess\"\n    preprocessor = preprocess.get_preprocessor(preprocess_str)\n\n    # Preprocess\n    images_preprocessed = preprocessor(images)\n\n    ##############################################\n    # Save all preprocessed images in a temporary directory\n    ##############################################\n\n    # We'll create a temporary folder to save preprocessed images\n    with tempfile.TemporaryDirectory(dir=utils.get_data_path()) as tmp_folder:\n        # Save images\n        images_path = []\n        for i, img in enumerate(images_preprocessed):\n            img_path = os.path.join(tmp_folder, f\"image_{i}.png\")\n            img.save(img_path, format='PNG')\n            images_path.append(img_path)\n\n        # Get predictions\n        df = pd.DataFrame({'file_path': images_path})\n        predictions, probas = model.predict_with_proba(df)\n\n    # Getting out of the context, all temporary data is deleted\n\n    ##############################################\n    # Return result\n    ##############################################\n    if return_proba:\n        return probas\n    else:\n        return model.inverse_transform(predictions)\n</code></pre>"},{"location":"reference/template_vision/models_training/utils_models/#template_vision.models_training.utils_models.predict_with_proba","title":"<code>predict_with_proba(data_input, model, model_conf)</code>","text":"<p>Gets probabilities predictions of a model on a dataset</p> <p>Parameters:</p> Name Type Description Default <code>data_input</code> <code>str | list&lt;str&gt; | np.ndarray</code> <p>New content to be predicted - str: abs. path to an image - list: list of abs. path to an image - np.ndarray: an already loaded image     Possibility to have several images if 4 dim (i.e (nb_images, width, height, channels)) - pd.DataFrame: Dataframe with a column file_path (abs. paths to images) required <code>model</code> <code>ModelClass</code> <p>Model to use</p> required <code>model_conf</code> <code>dict</code> <p>Model configurations</p> required <p>Raises:     NotImplementedError: If model is object detection task     ValueError : If predict does not return an np.ndarray Returns:     Union[List[str], List[float]]: predictions, probabilities</p> Source code in <code>template_vision/models_training/utils_models.py</code> <pre><code>def predict_with_proba(data_input: Union[str, List[str], np.ndarray, pd.DataFrame], model,\n                       model_conf: dict) -&gt; Tuple[List[str], List[float]]:\n    '''Gets probabilities predictions of a model on a dataset\n\n    Args:\n        data_input (str | list&lt;str&gt; | np.ndarray): New content to be predicted\n            - str: abs. path to an image\n            - list&lt;str&gt;: list of abs. path to an image\n            - np.ndarray: an already loaded image\n                Possibility to have several images if 4 dim (i.e (nb_images, width, height, channels))\n            - pd.DataFrame: Dataframe with a column file_path (abs. paths to images)\n        model (ModelClass): Model to use\n        model_conf (dict): Model configurations\n    Raises:\n        NotImplementedError: If model is object detection task\n        ValueError : If predict does not return an np.ndarray\n    Returns:\n        Union[List[str], List[float]]: predictions, probabilities\n    '''\n    if model.model_type == 'object_detector':\n        raise NotImplementedError(\"`predict_with_proba` is not yet implemented for object detection task\")\n\n    # Get probas\n    probas = predict(data_input, model, model_conf, return_proba=True)\n\n    # Check type\n    if type(probas) != np.ndarray:\n        raise ValueError(\"Internal error - probas should be an np.ndarray.\")\n\n    # Manage cases with only one element\n    predictions = model.get_classes_from_proba(probas)\n    predictions = model.inverse_transform(predictions)\n    max_probas = list(probas.max(axis=1))\n\n    # Return\n    return predictions, max_probas\n</code></pre>"},{"location":"reference/template_vision/models_training/utils_models/#template_vision.models_training.utils_models.remove_small_classes","title":"<code>remove_small_classes(df, col, min_rows=2)</code>","text":"<p>Deletes the classes with small numbers of elements</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe containing the data</p> required <code>col</code> <code>str | int</code> <p>Columns containing the classes</p> required <p>Kwargs:     min_rows (int): Minimal number of lines in the training set (default: 2) Raises:     ValueError: If the object min_rows is not positive Returns:     pd.DataFrame: New dataset</p> Source code in <code>template_vision/models_training/utils_models.py</code> <pre><code>def remove_small_classes(df: pd.DataFrame, col: Union[str, int], min_rows: int = 2) -&gt; pd.DataFrame:\n    '''Deletes the classes with small numbers of elements\n\n    Args:\n        df (pd.DataFrame): Dataframe containing the data\n        col (str | int): Columns containing the classes\n    Kwargs:\n        min_rows (int): Minimal number of lines in the training set (default: 2)\n    Raises:\n        ValueError: If the object min_rows is not positive\n    Returns:\n        pd.DataFrame: New dataset\n    '''\n    if min_rows &lt; 1:\n        raise ValueError(\"The object min_rows must be positive\")\n\n    # Looking for classes with less than min_rows lines\n    v_count = df[col].value_counts()\n    classes_to_remove = list(v_count[v_count &lt; min_rows].index.values)\n    for cl in classes_to_remove:\n        logger.warning(f\"/!\\\\ /!\\\\ /!\\\\ Class {cl} has less than {min_rows} lines in the training set.\")\n        logger.warning(\"/!\\\\ /!\\\\ /!\\\\ This class is automatically removed from the dataset.\")\n    return df[~df[col].isin(classes_to_remove)]\n</code></pre>"},{"location":"reference/template_vision/models_training/utils_models/#template_vision.models_training.utils_models.stratified_split","title":"<code>stratified_split(df, col, test_size=0.25, seed=None)</code>","text":"<p>Splits a DataFrame into train and test sets - Stratified strategy</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe containing the data</p> required <code>col</code> <code>str or int</code> <p>column on which to do the stratified split</p> required <p>Kwargs:     test_size (float): Proportion representing the size of the expected test set     seed (int): Random seed Raises:     ValueError: If the object test_size is not between 0 and 1 Returns:     DataFrame: Train dataframe     DataFrame: Test dataframe</p> Source code in <code>template_vision/models_training/utils_models.py</code> <pre><code>def stratified_split(df: pd.DataFrame, col: Union[str, int], test_size: float = 0.25,\n                     seed: Union[int, None] = None) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    '''Splits a DataFrame into train and test sets - Stratified strategy\n\n    Args:\n        df (pd.DataFrame): Dataframe containing the data\n        col (str or int): column on which to do the stratified split\n    Kwargs:\n        test_size (float): Proportion representing the size of the expected test set\n        seed (int): Random seed\n    Raises:\n        ValueError: If the object test_size is not between 0 and 1\n    Returns:\n        DataFrame: Train dataframe\n        DataFrame: Test dataframe\n    '''\n    if not 0 &lt;= test_size &lt;= 1:\n        raise ValueError('The object test_size must be between 0 and 1')\n\n    # Stratified split\n    logger.info(\"Stratified split\")\n    df = remove_small_classes(df, col, min_rows=math.ceil(1 / test_size))  # minimum lines number per category to split\n    df_train, df_test = train_test_split(df, stratify=df[col], test_size=test_size, random_state=seed)\n\n    # Display\n    display_train_test_shape(df_train, df_test, df_shape=df.shape[0])\n\n    # Return\n    return df_train, df_test\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/","title":"Classifiers","text":""},{"location":"reference/template_vision/models_training/classifiers/model_classifier/","title":"Model classifier","text":""},{"location":"reference/template_vision/models_training/classifiers/model_classifier/#template_vision.models_training.classifiers.model_classifier.ModelClassifierMixin","title":"<code>ModelClassifierMixin</code>","text":"<p>Parent class (Mixin) for classifier models</p> Source code in <code>template_vision/models_training/classifiers/model_classifier.py</code> <pre><code>class ModelClassifierMixin:\n    '''Parent class (Mixin) for classifier models'''\n\n    # Not implemented :\n    # -&gt; predict : To be implementd by the parent class when using this mixin\n\n    def __init__(self, level_save: str = 'HIGH', **kwargs) -&gt; None:\n        '''Initialization of the class\n\n        Kwargs:\n            level_save (str): Level of saving\n                LOW: stats + configurations + logger keras - /!\\\\ The model can't be reused /!\\\\ -\n                MEDIUM: LOWlevel_save + hdf5 + pkl + plots\n                HIGH: MEDIUM + predictions\n        Raises:\n            ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\n        '''\n        super().__init__(level_save=level_save, **kwargs)  # forwards level_save &amp; all unused arguments\n\n        if level_save not in ['LOW', 'MEDIUM', 'HIGH']:\n            raise ValueError(f\"The object level_save ({level_save}) is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\")\n\n        # Get logger\n        self.logger = logging.getLogger(__name__)\n\n        # Model type\n        self.model_type = 'classifier'\n\n        # Classes list to use (set on fit)\n        self.list_classes = None\n        self.dict_classes = None\n\n        # Other options\n        self.level_save = level_save\n\n    @utils.trained_needed\n    def predict_with_proba(self, df_test: pd.DataFrame) -&gt; Tuple[np.ndarray, np.ndarray]:\n        '''Predictions on test set with probabilities\n\n        Args:\n            df_test (pd.DataFrame): DataFrame to be predicted, with column file_path\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n        # Process\n        predicted_proba = self.predict(df_test, return_proba=True)\n        predicted_class = self.get_classes_from_proba(predicted_proba)\n        return predicted_class, predicted_proba\n\n    @utils.trained_needed\n    def get_predict_position(self, df_test: pd.DataFrame, y_true) -&gt; np.ndarray:\n        '''Gets the order of predictions of y_true.\n        Positions start at 1 (not 0)\n\n        Args:\n            df_test (pd.DataFrame): DataFrame to be predicted, with column file_path\n            y_true (?): Array-like, shape = [n_samples, n_features] - Classes\n        Returns:\n            (?): Array, shape = [n_samples]\n        '''\n        # Process\n        # Cast as pd.Series\n        y_true = pd.Series(y_true)\n        # Get predicted probabilities\n        predicted_proba = self.predict(df_test, return_proba=True)\n        # Get position\n        order = predicted_proba.argsort()\n        ranks = len(self.list_classes) - order.argsort()\n        df_probas = pd.DataFrame(ranks, columns=self.list_classes)\n        predict_positions = np.array([df_probas.loc[i, cl] if cl in df_probas.columns else -1 for i, cl in enumerate(y_true)])\n        return predict_positions\n\n    def get_classes_from_proba(self, predicted_proba: np.ndarray) -&gt; np.ndarray:\n        '''Gets the classes from probabilities\n\n        Args:\n            predicted_proba (np.ndarray): The probabilities predicted by the model, shape = [n_samples, n_classes]\n        Returns:\n            predicted_class (np.ndarray): Shape = [n_samples]\n        '''\n        predicted_class = np.vectorize(lambda x: self.dict_classes[x])(predicted_proba.argmax(axis=-1))\n        return predicted_class\n\n    def get_top_n_from_proba(self, predicted_proba: np.ndarray, n: int = 5) -&gt; Tuple[list, list]:\n        '''Gets the Top n predictions from probabilities\n\n        Args:\n            predicted_proba (np.ndarray): Predicted probabilities = [n_samples, n_classes]\n        kwargs:\n            n (int): Number of classes to return\n        Raises:\n            ValueError: If the number of classes to return is greater than the number of classes of the model\n        Returns:\n            top_n (list): Top n predicted classes\n            top_n_proba (list): Top n probabilities (corresponding to the top_n list of classes)\n        '''\n        if self.list_classes is not None and n &gt; len(self.list_classes):\n            raise ValueError(\"The number of classes to return is greater than the number of classes of the model\")\n        # Process\n        idx = predicted_proba.argsort()[:, -n:][:, ::-1]\n        top_n_proba = list(np.take_along_axis(predicted_proba, idx, axis=1))\n        top_n = list(np.vectorize(lambda x: self.dict_classes[x])(idx))\n        return top_n, top_n_proba\n\n    def inverse_transform(self, y: Union[list, np.ndarray]) -&gt; Union[list, tuple]:\n        '''Gets a list of classes from the predictions (mainly useful for multi-labels)\n\n        Args:\n            y (list | np.ndarray): Array-like, shape = [n_samples, n_classes], arrays of 0s and 1s\n        Returns:\n            (?): List of classes\n        '''\n        return list(y) if type(y) == np.ndarray else y\n\n    def get_and_save_metrics(self, y_true, y_pred, list_files_x: Union[list, None] = None,\n                             type_data: str = '') -&gt; pd.DataFrame:\n        '''Gets and saves the metrics of a model\n\n        Args:\n            y_true (?): Array-like [n_samples, 1] if classifier\n                # If classifier, class of each image\n                # If object detector, list of list of bboxes per image\n                    bbox format : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ...}\n            y_pred (?): Array-like [n_samples, 1] if classifier\n                # If classifier, class of each image\n                # If object detector, list of list of bboxes per image\n                    bbox format : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ...}\n        Kwargs:\n            list_files_x (list): Input images file paths\n            type_data (str): Type of dataset (validation, test, ...)\n        Returns:\n            pd.DataFrame: The d\n        '''\n        # Cast to np.array\n        y_true = np.array(y_true)\n        y_pred = np.array(y_pred)\n\n        # Check shapes\n        if len(y_true.shape) == 2 and y_true.shape[1] == 1:\n            y_true = np.ravel(y_true)\n        if len(y_pred.shape) == 2 and y_pred.shape[1] == 1:\n            y_pred = np.ravel(y_pred)\n\n        # Save a predictionn file if wanted\n        if self.level_save == 'HIGH':\n            # Inverse transform\n            y_true_df = list(self.inverse_transform(y_true))\n            y_pred_df = list(self.inverse_transform(y_pred))\n\n            # Concat in a dataframe\n            df = pd.DataFrame({'y_true': y_true_df, 'y_pred': y_pred_df})\n            # Ajout colonne file_path si possible\n            if list_files_x is not None:\n                df['file_path'] = list_files_x\n            # Add a matched column\n            df['matched'] = (df['y_true'] == df['y_pred']).astype(int)\n\n            #  Save predictions\n            file_path = os.path.join(self.model_dir, f\"predictions{'_' + type_data if len(type_data) &gt; 0 else ''}.csv\")\n            df.sort_values('matched', ascending=True).to_csv(file_path, sep=';', index=None, encoding='utf-8')\n\n        # Gets global f1 score / acc_tot / trues / falses / precision / recall / support\n        f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n        trues = np.sum(y_true == y_pred)\n        falses = np.sum(y_true != y_pred)\n        acc_tot = accuracy_score(y_true, y_pred)\n        precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n        recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n        labels_tmp, counts_tmp = np.unique(y_true, return_counts=True)\n        support = [0.] * len(self.list_classes) + [1.0]\n        for i, cl in enumerate(self.list_classes):\n            if cl in labels_tmp:\n                idx_tmp = list(labels_tmp).index(cl)\n                support[i] = counts_tmp[idx_tmp] / y_pred.shape[0]\n\n        # Global Statistics\n        self.logger.info('-- * * * * * * * * * * * * * * --')\n        self.logger.info(f\"Statistics f1-score{' ' + type_data if len(type_data) &gt; 0 else ''}\")\n        self.logger.info('--------------------------------')\n        self.logger.info(f\"Total accuracy : {round(acc_tot * 100, 2)}% \\t Trues: {trues} \\t Falses: {falses}\")\n        self.logger.info(f\"F1-score (weighted) : {round(f1_weighted, 5)}\")\n        self.logger.info(f\"Precision (weighted) : {round(precision_weighted, 5)}\")\n        self.logger.info(f\"Recall (weighted) : {round(recall_weighted, 5)}\")\n        self.logger.info('--------------------------------')\n\n        # Metrics file\n        dict_df_stats = {}\n\n        # Add metrics\n        labels = self.list_classes\n        # Plot confusion matrices if level_save &gt; LOW\n        if self.level_save in ['MEDIUM', 'HIGH']:\n            if len(labels) &gt; 50:\n                self.logger.warning(\n                    f\"Warning, there are {len(labels)} categories to plot in the confusion matrix.\\n\"\n                    \"Heavy chances of slowness/display bugs/crashes...\\n\"\n                    \"SKIP the plots\"\n                )\n            else:\n                # Global statistics\n                c_mat = confusion_matrix(y_true, y_pred, labels=labels)\n                self._plot_confusion_matrix(c_mat, labels, type_data=type_data, normalized=False)\n                self._plot_confusion_matrix(c_mat, labels, type_data=type_data, normalized=True)\n\n        # Get statistics per class\n        for i, label in enumerate(labels):\n            label_str = str(label)  # Fix : If label is an int, can cause some problems (e.g. only zeroes in the confusion matrix)\n            none_class = 'None' if label_str != 'None' else 'others'  # Check that the class is not already 'None'\n            y_true_tmp = [label_str if _ == label else none_class for _ in y_true]\n            y_pred_tmp = [label_str if _ == label else none_class for _ in y_pred]\n            c_mat_tmp = confusion_matrix(y_true_tmp, y_pred_tmp, labels=[none_class, label_str])\n            dict_df_stats[i] = self._update_info_from_c_mat(c_mat_tmp, label, log_info=False)\n\n        # Add global statistics\n        dict_df_stats[i+1] = {\n            'Label': 'All',\n            'F1-Score': f1_weighted,\n            'Accuracy': acc_tot,\n            'Precision': precision_weighted,\n            'Recall': recall_weighted,\n            'Trues': trues,\n            'Falses': falses,\n            'True positive': None,\n            'True negative': None,\n            'False positive': None,\n            'False negative': None,\n            'Condition positive': None,\n            'Condition negative': None,\n            'Predicted positive': None,\n            'Predicted negative': None,\n        }\n        df_stats = pd.DataFrame.from_dict(dict_df_stats, orient='index')\n\n        # Add support\n        df_stats['Support'] = support\n\n        # Save .csv\n        file_path = os.path.join(self.model_dir, f\"f1{'_' + type_data if len(type_data) &gt; 0 else ''}@{f1_weighted}.csv\")\n        df_stats.to_csv(file_path, sep=';', index=False, encoding='utf-8')\n\n        # Save accuracy\n        acc_path = os.path.join(self.model_dir, f\"acc{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(acc_tot, 5)}\")\n        with open(acc_path, 'w'):\n            pass\n\n        return df_stats\n\n    def get_metrics_simple_monolabel(self, y_true, y_pred) -&gt; pd.DataFrame:\n        '''Gets metrics on mono-label predictions\n        Same as the method get_and_save_metrics but without all the fluff (save, etc.)\n\n        Args:\n            y_true (?): Array-like, shape = [n_samples,]\n            y_pred (?): Array-like, shape = [n_samples,]\n        Returns:\n            pd.DataFrame: The dataframe containing statistics\n        '''\n        # Cast to np.array\n        y_true = np.array(y_true)\n        y_pred = np.array(y_pred)\n\n        # Check shapes\n        if len(y_true.shape) == 2 and y_true.shape[1] == 1:\n            y_true = np.ravel(y_true)\n        if len(y_pred.shape) == 2 and y_pred.shape[1] == 1:\n            y_pred = np.ravel(y_pred)\n\n        # Gets global f1 score / acc_tot / trues / falses / precision / recall / support\n        f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n        trues = np.sum(y_true == y_pred)\n        falses = np.sum(y_true != y_pred)\n        acc_tot = accuracy_score(y_true, y_pred)\n        precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n        recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n        labels_tmp, counts_tmp = np.unique(y_true, return_counts=True)\n        support = [0.] * len(self.list_classes) + [1.0]\n        for i, cl in enumerate(self.list_classes):\n            if cl in labels_tmp:\n                idx_tmp = list(labels_tmp).index(cl)\n                support[i] = counts_tmp[idx_tmp] / y_pred.shape[0]\n\n        # DataFrame metrics\n        dict_df_stats = {}\n\n        # Get statistics per class\n        labels = self.list_classes\n        for i, label in enumerate(labels):\n            label_str = str(label)  # Fix : If label is an int, can cause some problems (e.g. only zeroes in the confusion matrix)\n            none_class = 'None' if label_str != 'None' else 'others'  # Check that the class is not already 'None'\n            y_true_tmp = [label_str if _ == label else none_class for _ in y_true]\n            y_pred_tmp = [label_str if _ == label else none_class for _ in y_pred]\n            c_mat_tmp = confusion_matrix(y_true_tmp, y_pred_tmp, labels=[none_class, label_str])\n            dict_df_stats[i] = self._update_info_from_c_mat(c_mat_tmp, label, log_info=False)\n\n        # Add global statistics\n        dict_df_stats[i+1] = {\n            'Label': 'All',\n            'F1-Score': f1_weighted,\n            'Accuracy': acc_tot,\n            'Precision': precision_weighted,\n            'Recall': recall_weighted,\n            'Trues': trues,\n            'Falses': falses,\n            'True positive': None,\n            'True negative': None,\n            'False positive': None,\n            'False negative': None,\n            'Condition positive': None,\n            'Condition negative': None,\n            'Predicted positive': None,\n            'Predicted negative': None,\n        }\n        df_stats = pd.DataFrame.from_dict(dict_df_stats, orient='index')\n\n        # Add support\n        df_stats['Support'] = support\n\n        # Return dataframe\n        return df_stats\n\n    def _update_info_from_c_mat(self, c_mat: np.ndarray, label: str, log_info: bool = True) -&gt; dict:\n        '''Updates a dataframe for the method get_and_save_metrics, given a confusion matrix\n\n        Args:\n            c_mat (np.ndarray): Confusion matrix\n            label (str): Label to use\n        Kwargs:\n            log_info (bool): If the statistics must be logged\n        Returns:\n            dict: Dictionary with the information for the update of the dataframe\n        '''\n        # Extract all needed info from c_mat\n        true_negative = c_mat[0][0]\n        true_positive = c_mat[1][1]\n        false_negative = c_mat[1][0]\n        false_positive = c_mat[0][1]\n        condition_positive = false_negative + true_positive\n        condition_negative = false_positive + true_negative\n        predicted_positive = false_positive + true_positive\n        predicted_negative = false_negative + true_negative\n        trues_cat = true_negative + true_positive\n        falses_cat = false_negative + false_positive\n        accuracy = (true_negative + true_positive) / (true_negative + true_positive + false_negative + false_positive)\n        precision = 0 if predicted_positive == 0 else true_positive / predicted_positive\n        recall = 0 if condition_positive == 0 else true_positive / condition_positive\n        f1 = 0 if precision + recall == 0 else 2 * precision * recall / (precision + recall)\n\n        # Display some info\n        if log_info:\n            self.logger.info(\n                f\"F1-score: {round(f1, 5)}  \\t Precision: {round(100 * precision, 2)}% \\t\"\n                f\"Recall: {round(100 * recall, 2)}% \\t Trues: {trues_cat} \\t Falses: {falses_cat} \\t\\t --- {label} \"\n            )\n\n        # Return result\n        return {\n            'Label': f'{label}',\n            'F1-Score': f1,\n            'Accuracy': accuracy,\n            'Precision': precision,\n            'Recall': recall,\n            'Trues': trues_cat,\n            'Falses': falses_cat,\n            'True positive': true_positive,\n            'True negative': true_negative,\n            'False positive': false_positive,\n            'False negative': false_negative,\n            'Condition positive': condition_positive,\n            'Condition negative': condition_negative,\n            'Predicted positive': predicted_positive,\n            'Predicted negative': predicted_negative,\n        }\n\n    def _plot_confusion_matrix(self, c_mat: np.ndarray, labels: list, type_data: str = '',\n                               normalized: bool = False, subdir: Union[str, None] = None) -&gt; None:\n        '''Plots a confusion matrix\n\n        Args:\n            c_mat (np.ndarray): Confusion matrix\n            labels (list): Labels to plot\n        Kwargs:\n            type_data (str): Type of dataset (validation, test, ...)\n            normalized (bool): If the confusion matrix should be normalized\n            subdir (str): Sub-directory for writing the plot\n        '''\n\n        # Get title\n        if normalized:\n            title = f\"Normalized confusion matrix{' - ' + type_data if len(type_data) &gt; 0 else ''}\"\n        else:\n            title = f\"Confusion matrix, without normalization{' - ' + type_data if len(type_data) &gt; 0 else ''}\"\n\n        # Init. plot\n        width = round(10 + 0.5 * len(c_mat))\n        height = round(4 / 5 * width)\n        fig, ax = plt.subplots(figsize=(width, height))\n\n        # Plot\n        if normalized:\n            c_mat = c_mat.astype('float') / c_mat.sum(axis=1)[:, np.newaxis]\n            sns.heatmap(c_mat, annot=True, fmt=\".2f\", cmap=plt.cm.Blues, ax=ax) # type: ignore\n        else:\n            sns.heatmap(c_mat, annot=True, fmt=\"d\", cmap=plt.cm.Blues, ax=ax) # type: ignore\n\n        # labels, title and ticks\n        ax.set_xlabel('Predicted classes', fontsize=height * 2)\n        ax.set_ylabel('Real classes', fontsize=height * 2)\n        ax.set_title(title, fontsize=width * 2)\n        ax.xaxis.set_ticklabels(labels)\n        ax.yaxis.set_ticklabels(labels)\n        plt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\n        plt.setp(ax.get_yticklabels(), rotation=30, horizontalalignment='right')\n        plt.tight_layout()\n\n        # Save\n        plots_path = os.path.join(self.model_dir, 'plots')\n        if subdir is not None:  # Add subdir\n            plots_path = os.path.join(plots_path, subdir)\n        file_name = f\"{type_data + '_' if len(type_data) &gt; 0 else ''}confusion_matrix{'_normalized' if normalized else ''}.png\"\n        if not os.path.exists(plots_path):\n            os.makedirs(plots_path)\n        plt.savefig(os.path.join(plots_path, file_name))\n\n        # Close figures\n        plt.close('all')\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save model\n        if json_data is None:\n            json_data = {}\n\n        json_data['list_classes'] = self.list_classes\n        json_data['dict_classes'] = self.dict_classes\n\n        # Save\n        super().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_classifier/#template_vision.models_training.classifiers.model_classifier.ModelClassifierMixin.__init__","title":"<code>__init__(level_save='HIGH', **kwargs)</code>","text":"<p>Initialization of the class</p> Kwargs <p>level_save (str): Level of saving     LOW: stats + configurations + logger keras - /! The model can't be reused /! -     MEDIUM: LOWlevel_save + hdf5 + pkl + plots     HIGH: MEDIUM + predictions</p> <p>Raises:     ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])</p> Source code in <code>template_vision/models_training/classifiers/model_classifier.py</code> <pre><code>def __init__(self, level_save: str = 'HIGH', **kwargs) -&gt; None:\n    '''Initialization of the class\n\n    Kwargs:\n        level_save (str): Level of saving\n            LOW: stats + configurations + logger keras - /!\\\\ The model can't be reused /!\\\\ -\n            MEDIUM: LOWlevel_save + hdf5 + pkl + plots\n            HIGH: MEDIUM + predictions\n    Raises:\n        ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\n    '''\n    super().__init__(level_save=level_save, **kwargs)  # forwards level_save &amp; all unused arguments\n\n    if level_save not in ['LOW', 'MEDIUM', 'HIGH']:\n        raise ValueError(f\"The object level_save ({level_save}) is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\")\n\n    # Get logger\n    self.logger = logging.getLogger(__name__)\n\n    # Model type\n    self.model_type = 'classifier'\n\n    # Classes list to use (set on fit)\n    self.list_classes = None\n    self.dict_classes = None\n\n    # Other options\n    self.level_save = level_save\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_classifier/#template_vision.models_training.classifiers.model_classifier.ModelClassifierMixin.get_and_save_metrics","title":"<code>get_and_save_metrics(y_true, y_pred, list_files_x=None, type_data='')</code>","text":"<p>Gets and saves the metrics of a model</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>?</code> <p>Array-like [n_samples, 1] if classifier</p> required <code>y_pred</code> <code>?</code> <p>Array-like [n_samples, 1] if classifier</p> required <p>Kwargs:     list_files_x (list): Input images file paths     type_data (str): Type of dataset (validation, test, ...) Returns:     pd.DataFrame: The d</p> Source code in <code>template_vision/models_training/classifiers/model_classifier.py</code> <pre><code>def get_and_save_metrics(self, y_true, y_pred, list_files_x: Union[list, None] = None,\n                         type_data: str = '') -&gt; pd.DataFrame:\n    '''Gets and saves the metrics of a model\n\n    Args:\n        y_true (?): Array-like [n_samples, 1] if classifier\n            # If classifier, class of each image\n            # If object detector, list of list of bboxes per image\n                bbox format : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ...}\n        y_pred (?): Array-like [n_samples, 1] if classifier\n            # If classifier, class of each image\n            # If object detector, list of list of bboxes per image\n                bbox format : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ...}\n    Kwargs:\n        list_files_x (list): Input images file paths\n        type_data (str): Type of dataset (validation, test, ...)\n    Returns:\n        pd.DataFrame: The d\n    '''\n    # Cast to np.array\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Check shapes\n    if len(y_true.shape) == 2 and y_true.shape[1] == 1:\n        y_true = np.ravel(y_true)\n    if len(y_pred.shape) == 2 and y_pred.shape[1] == 1:\n        y_pred = np.ravel(y_pred)\n\n    # Save a predictionn file if wanted\n    if self.level_save == 'HIGH':\n        # Inverse transform\n        y_true_df = list(self.inverse_transform(y_true))\n        y_pred_df = list(self.inverse_transform(y_pred))\n\n        # Concat in a dataframe\n        df = pd.DataFrame({'y_true': y_true_df, 'y_pred': y_pred_df})\n        # Ajout colonne file_path si possible\n        if list_files_x is not None:\n            df['file_path'] = list_files_x\n        # Add a matched column\n        df['matched'] = (df['y_true'] == df['y_pred']).astype(int)\n\n        #  Save predictions\n        file_path = os.path.join(self.model_dir, f\"predictions{'_' + type_data if len(type_data) &gt; 0 else ''}.csv\")\n        df.sort_values('matched', ascending=True).to_csv(file_path, sep=';', index=None, encoding='utf-8')\n\n    # Gets global f1 score / acc_tot / trues / falses / precision / recall / support\n    f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n    trues = np.sum(y_true == y_pred)\n    falses = np.sum(y_true != y_pred)\n    acc_tot = accuracy_score(y_true, y_pred)\n    precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n    recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n    labels_tmp, counts_tmp = np.unique(y_true, return_counts=True)\n    support = [0.] * len(self.list_classes) + [1.0]\n    for i, cl in enumerate(self.list_classes):\n        if cl in labels_tmp:\n            idx_tmp = list(labels_tmp).index(cl)\n            support[i] = counts_tmp[idx_tmp] / y_pred.shape[0]\n\n    # Global Statistics\n    self.logger.info('-- * * * * * * * * * * * * * * --')\n    self.logger.info(f\"Statistics f1-score{' ' + type_data if len(type_data) &gt; 0 else ''}\")\n    self.logger.info('--------------------------------')\n    self.logger.info(f\"Total accuracy : {round(acc_tot * 100, 2)}% \\t Trues: {trues} \\t Falses: {falses}\")\n    self.logger.info(f\"F1-score (weighted) : {round(f1_weighted, 5)}\")\n    self.logger.info(f\"Precision (weighted) : {round(precision_weighted, 5)}\")\n    self.logger.info(f\"Recall (weighted) : {round(recall_weighted, 5)}\")\n    self.logger.info('--------------------------------')\n\n    # Metrics file\n    dict_df_stats = {}\n\n    # Add metrics\n    labels = self.list_classes\n    # Plot confusion matrices if level_save &gt; LOW\n    if self.level_save in ['MEDIUM', 'HIGH']:\n        if len(labels) &gt; 50:\n            self.logger.warning(\n                f\"Warning, there are {len(labels)} categories to plot in the confusion matrix.\\n\"\n                \"Heavy chances of slowness/display bugs/crashes...\\n\"\n                \"SKIP the plots\"\n            )\n        else:\n            # Global statistics\n            c_mat = confusion_matrix(y_true, y_pred, labels=labels)\n            self._plot_confusion_matrix(c_mat, labels, type_data=type_data, normalized=False)\n            self._plot_confusion_matrix(c_mat, labels, type_data=type_data, normalized=True)\n\n    # Get statistics per class\n    for i, label in enumerate(labels):\n        label_str = str(label)  # Fix : If label is an int, can cause some problems (e.g. only zeroes in the confusion matrix)\n        none_class = 'None' if label_str != 'None' else 'others'  # Check that the class is not already 'None'\n        y_true_tmp = [label_str if _ == label else none_class for _ in y_true]\n        y_pred_tmp = [label_str if _ == label else none_class for _ in y_pred]\n        c_mat_tmp = confusion_matrix(y_true_tmp, y_pred_tmp, labels=[none_class, label_str])\n        dict_df_stats[i] = self._update_info_from_c_mat(c_mat_tmp, label, log_info=False)\n\n    # Add global statistics\n    dict_df_stats[i+1] = {\n        'Label': 'All',\n        'F1-Score': f1_weighted,\n        'Accuracy': acc_tot,\n        'Precision': precision_weighted,\n        'Recall': recall_weighted,\n        'Trues': trues,\n        'Falses': falses,\n        'True positive': None,\n        'True negative': None,\n        'False positive': None,\n        'False negative': None,\n        'Condition positive': None,\n        'Condition negative': None,\n        'Predicted positive': None,\n        'Predicted negative': None,\n    }\n    df_stats = pd.DataFrame.from_dict(dict_df_stats, orient='index')\n\n    # Add support\n    df_stats['Support'] = support\n\n    # Save .csv\n    file_path = os.path.join(self.model_dir, f\"f1{'_' + type_data if len(type_data) &gt; 0 else ''}@{f1_weighted}.csv\")\n    df_stats.to_csv(file_path, sep=';', index=False, encoding='utf-8')\n\n    # Save accuracy\n    acc_path = os.path.join(self.model_dir, f\"acc{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(acc_tot, 5)}\")\n    with open(acc_path, 'w'):\n        pass\n\n    return df_stats\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_classifier/#template_vision.models_training.classifiers.model_classifier.ModelClassifierMixin.get_and_save_metrics--if-classifier-class-of-each-image","title":"If classifier, class of each image","text":""},{"location":"reference/template_vision/models_training/classifiers/model_classifier/#template_vision.models_training.classifiers.model_classifier.ModelClassifierMixin.get_and_save_metrics--if-object-detector-list-of-list-of-bboxes-per-image","title":"If object detector, list of list of bboxes per image","text":"<pre><code>bbox format : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ...}\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_classifier/#template_vision.models_training.classifiers.model_classifier.ModelClassifierMixin.get_and_save_metrics--if-classifier-class-of-each-image","title":"If classifier, class of each image","text":""},{"location":"reference/template_vision/models_training/classifiers/model_classifier/#template_vision.models_training.classifiers.model_classifier.ModelClassifierMixin.get_and_save_metrics--if-object-detector-list-of-list-of-bboxes-per-image","title":"If object detector, list of list of bboxes per image","text":"<pre><code>bbox format : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ...}\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_classifier/#template_vision.models_training.classifiers.model_classifier.ModelClassifierMixin.get_classes_from_proba","title":"<code>get_classes_from_proba(predicted_proba)</code>","text":"<p>Gets the classes from probabilities</p> <p>Parameters:</p> Name Type Description Default <code>predicted_proba</code> <code>ndarray</code> <p>The probabilities predicted by the model, shape = [n_samples, n_classes]</p> required <p>Returns:     predicted_class (np.ndarray): Shape = [n_samples]</p> Source code in <code>template_vision/models_training/classifiers/model_classifier.py</code> <pre><code>def get_classes_from_proba(self, predicted_proba: np.ndarray) -&gt; np.ndarray:\n    '''Gets the classes from probabilities\n\n    Args:\n        predicted_proba (np.ndarray): The probabilities predicted by the model, shape = [n_samples, n_classes]\n    Returns:\n        predicted_class (np.ndarray): Shape = [n_samples]\n    '''\n    predicted_class = np.vectorize(lambda x: self.dict_classes[x])(predicted_proba.argmax(axis=-1))\n    return predicted_class\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_classifier/#template_vision.models_training.classifiers.model_classifier.ModelClassifierMixin.get_metrics_simple_monolabel","title":"<code>get_metrics_simple_monolabel(y_true, y_pred)</code>","text":"<p>Gets metrics on mono-label predictions Same as the method get_and_save_metrics but without all the fluff (save, etc.)</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>?</code> <p>Array-like, shape = [n_samples,]</p> required <code>y_pred</code> <code>?</code> <p>Array-like, shape = [n_samples,]</p> required <p>Returns:     pd.DataFrame: The dataframe containing statistics</p> Source code in <code>template_vision/models_training/classifiers/model_classifier.py</code> <pre><code>def get_metrics_simple_monolabel(self, y_true, y_pred) -&gt; pd.DataFrame:\n    '''Gets metrics on mono-label predictions\n    Same as the method get_and_save_metrics but without all the fluff (save, etc.)\n\n    Args:\n        y_true (?): Array-like, shape = [n_samples,]\n        y_pred (?): Array-like, shape = [n_samples,]\n    Returns:\n        pd.DataFrame: The dataframe containing statistics\n    '''\n    # Cast to np.array\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Check shapes\n    if len(y_true.shape) == 2 and y_true.shape[1] == 1:\n        y_true = np.ravel(y_true)\n    if len(y_pred.shape) == 2 and y_pred.shape[1] == 1:\n        y_pred = np.ravel(y_pred)\n\n    # Gets global f1 score / acc_tot / trues / falses / precision / recall / support\n    f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n    trues = np.sum(y_true == y_pred)\n    falses = np.sum(y_true != y_pred)\n    acc_tot = accuracy_score(y_true, y_pred)\n    precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n    recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n    labels_tmp, counts_tmp = np.unique(y_true, return_counts=True)\n    support = [0.] * len(self.list_classes) + [1.0]\n    for i, cl in enumerate(self.list_classes):\n        if cl in labels_tmp:\n            idx_tmp = list(labels_tmp).index(cl)\n            support[i] = counts_tmp[idx_tmp] / y_pred.shape[0]\n\n    # DataFrame metrics\n    dict_df_stats = {}\n\n    # Get statistics per class\n    labels = self.list_classes\n    for i, label in enumerate(labels):\n        label_str = str(label)  # Fix : If label is an int, can cause some problems (e.g. only zeroes in the confusion matrix)\n        none_class = 'None' if label_str != 'None' else 'others'  # Check that the class is not already 'None'\n        y_true_tmp = [label_str if _ == label else none_class for _ in y_true]\n        y_pred_tmp = [label_str if _ == label else none_class for _ in y_pred]\n        c_mat_tmp = confusion_matrix(y_true_tmp, y_pred_tmp, labels=[none_class, label_str])\n        dict_df_stats[i] = self._update_info_from_c_mat(c_mat_tmp, label, log_info=False)\n\n    # Add global statistics\n    dict_df_stats[i+1] = {\n        'Label': 'All',\n        'F1-Score': f1_weighted,\n        'Accuracy': acc_tot,\n        'Precision': precision_weighted,\n        'Recall': recall_weighted,\n        'Trues': trues,\n        'Falses': falses,\n        'True positive': None,\n        'True negative': None,\n        'False positive': None,\n        'False negative': None,\n        'Condition positive': None,\n        'Condition negative': None,\n        'Predicted positive': None,\n        'Predicted negative': None,\n    }\n    df_stats = pd.DataFrame.from_dict(dict_df_stats, orient='index')\n\n    # Add support\n    df_stats['Support'] = support\n\n    # Return dataframe\n    return df_stats\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_classifier/#template_vision.models_training.classifiers.model_classifier.ModelClassifierMixin.get_predict_position","title":"<code>get_predict_position(df_test, y_true)</code>","text":"<p>Gets the order of predictions of y_true. Positions start at 1 (not 0)</p> <p>Parameters:</p> Name Type Description Default <code>df_test</code> <code>DataFrame</code> <p>DataFrame to be predicted, with column file_path</p> required <code>y_true</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features] - Classes</p> required <p>Returns:     (?): Array, shape = [n_samples]</p> Source code in <code>template_vision/models_training/classifiers/model_classifier.py</code> <pre><code>@utils.trained_needed\ndef get_predict_position(self, df_test: pd.DataFrame, y_true) -&gt; np.ndarray:\n    '''Gets the order of predictions of y_true.\n    Positions start at 1 (not 0)\n\n    Args:\n        df_test (pd.DataFrame): DataFrame to be predicted, with column file_path\n        y_true (?): Array-like, shape = [n_samples, n_features] - Classes\n    Returns:\n        (?): Array, shape = [n_samples]\n    '''\n    # Process\n    # Cast as pd.Series\n    y_true = pd.Series(y_true)\n    # Get predicted probabilities\n    predicted_proba = self.predict(df_test, return_proba=True)\n    # Get position\n    order = predicted_proba.argsort()\n    ranks = len(self.list_classes) - order.argsort()\n    df_probas = pd.DataFrame(ranks, columns=self.list_classes)\n    predict_positions = np.array([df_probas.loc[i, cl] if cl in df_probas.columns else -1 for i, cl in enumerate(y_true)])\n    return predict_positions\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_classifier/#template_vision.models_training.classifiers.model_classifier.ModelClassifierMixin.get_top_n_from_proba","title":"<code>get_top_n_from_proba(predicted_proba, n=5)</code>","text":"<p>Gets the Top n predictions from probabilities</p> <p>Parameters:</p> Name Type Description Default <code>predicted_proba</code> <code>ndarray</code> <p>Predicted probabilities = [n_samples, n_classes]</p> required <p>kwargs:     n (int): Number of classes to return Raises:     ValueError: If the number of classes to return is greater than the number of classes of the model Returns:     top_n (list): Top n predicted classes     top_n_proba (list): Top n probabilities (corresponding to the top_n list of classes)</p> Source code in <code>template_vision/models_training/classifiers/model_classifier.py</code> <pre><code>def get_top_n_from_proba(self, predicted_proba: np.ndarray, n: int = 5) -&gt; Tuple[list, list]:\n    '''Gets the Top n predictions from probabilities\n\n    Args:\n        predicted_proba (np.ndarray): Predicted probabilities = [n_samples, n_classes]\n    kwargs:\n        n (int): Number of classes to return\n    Raises:\n        ValueError: If the number of classes to return is greater than the number of classes of the model\n    Returns:\n        top_n (list): Top n predicted classes\n        top_n_proba (list): Top n probabilities (corresponding to the top_n list of classes)\n    '''\n    if self.list_classes is not None and n &gt; len(self.list_classes):\n        raise ValueError(\"The number of classes to return is greater than the number of classes of the model\")\n    # Process\n    idx = predicted_proba.argsort()[:, -n:][:, ::-1]\n    top_n_proba = list(np.take_along_axis(predicted_proba, idx, axis=1))\n    top_n = list(np.vectorize(lambda x: self.dict_classes[x])(idx))\n    return top_n, top_n_proba\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_classifier/#template_vision.models_training.classifiers.model_classifier.ModelClassifierMixin.inverse_transform","title":"<code>inverse_transform(y)</code>","text":"<p>Gets a list of classes from the predictions (mainly useful for multi-labels)</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>list | ndarray</code> <p>Array-like, shape = [n_samples, n_classes], arrays of 0s and 1s</p> required <p>Returns:     (?): List of classes</p> Source code in <code>template_vision/models_training/classifiers/model_classifier.py</code> <pre><code>def inverse_transform(self, y: Union[list, np.ndarray]) -&gt; Union[list, tuple]:\n    '''Gets a list of classes from the predictions (mainly useful for multi-labels)\n\n    Args:\n        y (list | np.ndarray): Array-like, shape = [n_samples, n_classes], arrays of 0s and 1s\n    Returns:\n        (?): List of classes\n    '''\n    return list(y) if type(y) == np.ndarray else y\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_classifier/#template_vision.models_training.classifiers.model_classifier.ModelClassifierMixin.predict_with_proba","title":"<code>predict_with_proba(df_test)</code>","text":"<p>Predictions on test set with probabilities</p> <p>Parameters:</p> Name Type Description Default <code>df_test</code> <code>DataFrame</code> <p>DataFrame to be predicted, with column file_path</p> required <p>Returns:     (np.ndarray): Array, shape = [n_samples, n_classes]     (np.ndarray): Array, shape = [n_samples, n_classes]</p> Source code in <code>template_vision/models_training/classifiers/model_classifier.py</code> <pre><code>@utils.trained_needed\ndef predict_with_proba(self, df_test: pd.DataFrame) -&gt; Tuple[np.ndarray, np.ndarray]:\n    '''Predictions on test set with probabilities\n\n    Args:\n        df_test (pd.DataFrame): DataFrame to be predicted, with column file_path\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n    # Process\n    predicted_proba = self.predict(df_test, return_proba=True)\n    predicted_class = self.get_classes_from_proba(predicted_proba)\n    return predicted_class, predicted_proba\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_classifier/#template_vision.models_training.classifiers.model_classifier.ModelClassifierMixin.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_vision/models_training/classifiers/model_classifier.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save model\n    if json_data is None:\n        json_data = {}\n\n    json_data['list_classes'] = self.list_classes\n    json_data['dict_classes'] = self.dict_classes\n\n    # Save\n    super().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_cnn_classifier/","title":"Model cnn classifier","text":""},{"location":"reference/template_vision/models_training/classifiers/model_cnn_classifier/#template_vision.models_training.classifiers.model_cnn_classifier.ModelCnnClassifier","title":"<code>ModelCnnClassifier</code>","text":"<p>             Bases: <code>ModelClassifierMixin</code>, <code>ModelKeras</code></p> <p>CNN model for classification</p> Source code in <code>template_vision/models_training/classifiers/model_cnn_classifier.py</code> <pre><code>class ModelCnnClassifier(ModelClassifierMixin, ModelKeras):\n    '''CNN model for classification'''\n\n    _default_name = 'model_cnn_classifier'\n\n    def __init__(self, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelClass, ModelKeras &amp; ModelClassifierMixin for more arguments)'''\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n    def _get_model(self) -&gt; Model:\n        '''Gets a model structure - returns the instance model instead if already defined\n\n        Returns:\n            (Model): a Keras model\n        '''\n        # Return model if already set\n        if self.model is not None:\n            return self.model\n\n        # Get input/output dimensions\n        input_shape = (self.width, self.height, self.depth)\n        num_classes = len(self.list_classes)\n\n        # Process\n        input_layer = Input(shape=input_shape)\n\n        # Feature extraction\n\n        x = Conv2D(16, 3, padding='same', activation=None, kernel_initializer=\"he_uniform\")(input_layer)\n        x = BatchNormalization(momentum=0.99)(x)\n        x = ELU(alpha=1.0)(x)\n        x = AveragePooling2D(2, strides=2, padding='valid')(x)\n\n        x = Conv2D(32, 3, padding='same', activation=None, kernel_initializer=\"he_uniform\")(x)\n        x = BatchNormalization(momentum=0.99)(x)\n        x = ELU(alpha=1.0)(x)\n        x = AveragePooling2D(2, strides=2, padding='valid')(x)\n\n        x = Conv2D(48, 3, padding='same', activation=None, kernel_initializer=\"he_uniform\")(x)\n        x = BatchNormalization(momentum=0.99)(x)\n        x = ELU(alpha=1.0)(x)\n        x = AveragePooling2D(2, strides=2, padding='valid')(x)\n\n        x = Conv2D(32, 3, padding='same', activation=None, kernel_initializer=\"he_uniform\")(x)\n        x = BatchNormalization(momentum=0.99)(x)\n        x = ELU(alpha=1.0)(x)\n        x = AveragePooling2D(2, strides=2, padding='valid')(x)\n\n        # Flatten\n\n        x = Flatten()(x)\n\n        # Classification\n\n        x = Dense(64, activation=None, kernel_initializer=\"he_uniform\")(x)\n        x = BatchNormalization(momentum=0.99)(x)\n        x = ELU(alpha=1.0)(x)\n        x = Dropout(0.2)(x)\n\n        x = Dense(128, activation=None, kernel_initializer=\"he_uniform\")(x)\n        x = BatchNormalization(momentum=0.99)(x)\n        x = ELU(alpha=1.0)(x)\n        x = Dropout(0.2)(x)\n\n        x = Dense(128, activation=None, kernel_initializer=\"he_uniform\")(x)\n        x = BatchNormalization(momentum=0.99)(x)\n        x = ELU(alpha=1.0)(x)\n        x = Dropout(0.2)(x)\n\n        # Last layer\n        activation = 'softmax'\n        out = Dense(num_classes, activation=activation, kernel_initializer='glorot_uniform')(x)\n\n        # Set model\n        model = Model(inputs=input_layer, outputs=[out])\n\n        # Set optimizer\n        lr = self.keras_params['learning_rate'] if 'learning_rate' in self.keras_params.keys() else 0.001\n        decay = self.keras_params['decay'] if 'decay' in self.keras_params.keys() else 0.0\n        self.logger.info(f\"Learning rate: {lr}\")\n        self.logger.info(f\"Decay: {decay}\")\n        optimizer = Adam(lr=lr, decay=decay)\n\n        # Set loss &amp; metrics\n        loss = 'categorical_crossentropy'\n        metrics: List[Union[str, Callable]] = ['accuracy']\n\n        # Compile model\n        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n        if self.logger.getEffectiveLevel() &lt; logging.ERROR:\n            model.summary()\n\n        # Try to save model as png if level_save &gt; 'LOW'\n        if self.level_save in ['MEDIUM', 'HIGH']:\n            self._save_model_png(model)\n\n        # Return\n        return model\n\n    def _get_preprocess_input(self) -&gt; Union[Callable, None]:\n        '''Gets the preprocessing to be used before feeding images to the NN\n\n        Returns:\n            (Callable | None): Preprocessing function\n        '''\n        return preprocess_input\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save configuration JSON\n        if json_data is None:\n            json_data = {}\n\n        # Save\n        super().save(json_data=json_data)\n\n    def reload_from_standalone(self, **kwargs) -&gt; None:\n        '''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            hdf5_path (str): Path to hdf5 file\n            preprocess_input_path (str): Path to preprocess input file\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If hdf5_path is None\n            ValueError: If preprocess_input_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object hdf5_path is not an existing file\n            FileNotFoundError: If the object preprocess_input_path is not an existing file\n        '''\n        # Retrieve args\n        configuration_path = kwargs.get('configuration_path', None)\n        hdf5_path = kwargs.get('hdf5_path', None)\n        preprocess_input_path = kwargs.get('preprocess_input_path', None)\n\n        # Checks\n        if configuration_path is None:\n            raise ValueError(\"The argument configuration_path can't be None\")\n        if hdf5_path is None:\n            raise ValueError(\"The argument hdf5_path can't be None\")\n        if preprocess_input_path is None:\n            raise ValueError(\"The argument preprocess_input_path can't be None\")\n        if not os.path.exists(configuration_path):\n            raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n        if not os.path.exists(hdf5_path):\n            raise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\n        if not os.path.exists(preprocess_input_path):\n            raise FileNotFoundError(f\"The file {preprocess_input_path} does not exist\")\n\n        # Load confs\n        with open(configuration_path, 'r', encoding='utf-8') as f:\n            configs = json.load(f)\n        # Can't set int as keys in json, so need to cast it after reloading\n        # dict_classes keys are always ints\n        if 'dict_classes' in configs.keys():\n            configs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n        elif 'list_classes' in configs.keys():\n            configs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n\n        # Set class vars\n        # self.model_name = # Keep the created name\n        # self.model_dir = # Keep the created folder\n        self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n        self.trained = configs.get('trained', True)  # Consider trained by default\n\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['model_type', 'list_classes', 'dict_classes', 'level_save',\n                          'batch_size', 'epochs', 'validation_split', 'patience',\n                          'width', 'height', 'depth', 'color_mode', 'in_memory',\n                          'data_augmentation_params', 'nb_train_generator_images_to_save',\n                          'keras_params']:\n            setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n        # Reload model\n        self.model = load_model_keras(hdf5_path, custom_objects=self.custom_objects)\n\n        # Reload preprocess_input\n        with open(preprocess_input_path, 'rb') as f:\n            self.preprocess_input = pickle.load(f)\n\n        # Save best hdf5 in new folder\n        new_hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\n        shutil.copyfile(hdf5_path, new_hdf5_path)\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_cnn_classifier/#template_vision.models_training.classifiers.model_cnn_classifier.ModelCnnClassifier.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialization of the class (see ModelClass, ModelKeras &amp; ModelClassifierMixin for more arguments)</p> Source code in <code>template_vision/models_training/classifiers/model_cnn_classifier.py</code> <pre><code>def __init__(self, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelClass, ModelKeras &amp; ModelClassifierMixin for more arguments)'''\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_cnn_classifier/#template_vision.models_training.classifiers.model_cnn_classifier.ModelCnnClassifier.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file hdf5_path (str): Path to hdf5 file preprocess_input_path (str): Path to preprocess input file</p> <p>Raises:     ValueError: If configuration_path is None     ValueError: If hdf5_path is None     ValueError: If preprocess_input_path is None     FileNotFoundError: If the object configuration_path is not an existing file     FileNotFoundError: If the object hdf5_path is not an existing file     FileNotFoundError: If the object preprocess_input_path is not an existing file</p> Source code in <code>template_vision/models_training/classifiers/model_cnn_classifier.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n    '''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        hdf5_path (str): Path to hdf5 file\n        preprocess_input_path (str): Path to preprocess input file\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If hdf5_path is None\n        ValueError: If preprocess_input_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object hdf5_path is not an existing file\n        FileNotFoundError: If the object preprocess_input_path is not an existing file\n    '''\n    # Retrieve args\n    configuration_path = kwargs.get('configuration_path', None)\n    hdf5_path = kwargs.get('hdf5_path', None)\n    preprocess_input_path = kwargs.get('preprocess_input_path', None)\n\n    # Checks\n    if configuration_path is None:\n        raise ValueError(\"The argument configuration_path can't be None\")\n    if hdf5_path is None:\n        raise ValueError(\"The argument hdf5_path can't be None\")\n    if preprocess_input_path is None:\n        raise ValueError(\"The argument preprocess_input_path can't be None\")\n    if not os.path.exists(configuration_path):\n        raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n    if not os.path.exists(hdf5_path):\n        raise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\n    if not os.path.exists(preprocess_input_path):\n        raise FileNotFoundError(f\"The file {preprocess_input_path} does not exist\")\n\n    # Load confs\n    with open(configuration_path, 'r', encoding='utf-8') as f:\n        configs = json.load(f)\n    # Can't set int as keys in json, so need to cast it after reloading\n    # dict_classes keys are always ints\n    if 'dict_classes' in configs.keys():\n        configs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n    elif 'list_classes' in configs.keys():\n        configs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n\n    # Set class vars\n    # self.model_name = # Keep the created name\n    # self.model_dir = # Keep the created folder\n    self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n    self.trained = configs.get('trained', True)  # Consider trained by default\n\n    # Try to read the following attributes from configs and, if absent, keep the current one\n    for attribute in ['model_type', 'list_classes', 'dict_classes', 'level_save',\n                      'batch_size', 'epochs', 'validation_split', 'patience',\n                      'width', 'height', 'depth', 'color_mode', 'in_memory',\n                      'data_augmentation_params', 'nb_train_generator_images_to_save',\n                      'keras_params']:\n        setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n    # Reload model\n    self.model = load_model_keras(hdf5_path, custom_objects=self.custom_objects)\n\n    # Reload preprocess_input\n    with open(preprocess_input_path, 'rb') as f:\n        self.preprocess_input = pickle.load(f)\n\n    # Save best hdf5 in new folder\n    new_hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\n    shutil.copyfile(hdf5_path, new_hdf5_path)\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_cnn_classifier/#template_vision.models_training.classifiers.model_cnn_classifier.ModelCnnClassifier.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_vision/models_training/classifiers/model_cnn_classifier.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save configuration JSON\n    if json_data is None:\n        json_data = {}\n\n    # Save\n    super().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_transfer_learning_classifier/","title":"Model transfer learning classifier","text":""},{"location":"reference/template_vision/models_training/classifiers/model_transfer_learning_classifier/#template_vision.models_training.classifiers.model_transfer_learning_classifier.ModelTransferLearningClassifier","title":"<code>ModelTransferLearningClassifier</code>","text":"<p>             Bases: <code>ModelClassifierMixin</code>, <code>ModelKeras</code></p> <p>CNN model with transfer learning for classification</p> Source code in <code>template_vision/models_training/classifiers/model_transfer_learning_classifier.py</code> <pre><code>class ModelTransferLearningClassifier(ModelClassifierMixin, ModelKeras):\n    '''CNN model with transfer learning for classification'''\n\n    _default_name = 'model_transfer_learning_classifier'\n\n    def __init__(self, with_fine_tune: bool = True, second_epochs: int = 10, second_lr: float = 1e-5, second_patience: int = 5, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelClass, ModelKeras &amp; ModelClassifierMixin for more arguments)\n\n        Kwargs:\n            with_fine_tune (bool): If a fine-tuning step should be performed after first training\n            second_epochs (int): Number of epochs for the fine-tuning step\n            second_lr (float): Learning rate for the fine-tuning step\n            second_patience (int): Patience for the fine-tuning step\n        '''\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Params\n        self.with_fine_tune = with_fine_tune\n        self.second_epochs = second_epochs\n        self.second_lr = second_lr\n        self.second_patience = second_patience\n\n    def _fit_classifier(self, df_train: pd.DataFrame, df_valid: pd.DataFrame = None, with_shuffle: bool = True, **kwargs) -&gt; dict:\n        '''Fits the model - overrides parent function\n\n        Args:\n            df_train (pd.DataFrame): Train dataset\n                Must contain file_path &amp; file_class columns\n        Kwargs:\n            df_valid (pd.DataFrame): Validation dataset\n                Must contain file_path &amp; file_class columns\n            with_shuffle (boolean): If the train dataset must be shuffled\n                This should be used if the input dataset is not shuffled &amp; no validation set as the split_validation takes the lines in order.\n                Thus, the validation set might get classes which are not in the train set ...\n        Returns:\n            dict: fit arguments\n        '''\n        # First fit\n        self.logger.info(\"Transfer Learning - Premier entrainement\")\n        fit_arguments = super()._fit_classifier(df_train, df_valid=df_valid, with_shuffle=with_shuffle, **kwargs)\n\n        # Fine tune if wanted\n        if self.with_fine_tune and self.second_epochs &gt; 0:\n            # Unfreeze all layers\n            for layer in self.model.layers:  # type: ignore\n                layer.trainable = True\n\n            # /!\\ Recompile, otherwise unfreeze is not taken into account ! /!\\\n            # Cf. https://keras.io/guides/transfer_learning/#fine-tuning\n            self._compile_model(self.model, lr=self.second_lr)  # Use new LR !\n            if self.logger.getEffectiveLevel() &lt; logging.ERROR:\n                self.model.summary()  # type: ignore\n\n            # Get new callbacks\n            new_callbacks = self._get_second_callbacks()\n\n            # Second fit\n            self.logger.info(\"Transfer Learning - Fine-tuning\")\n            fit_history = self.model.fit(  # type: ignore\n                epochs=self.second_epochs,\n                callbacks=new_callbacks,\n                verbose=1,\n                workers=8,  # TODO : Check if this is ok if there are less CPUs\n                **fit_arguments,\n            )\n\n            # Print accuracy &amp; loss if level_save &gt; 'LOW'\n            if self.level_save in ['MEDIUM', 'HIGH']:\n                # Rename first fit plots dir\n                original_plots_path = os.path.join(self.model_dir, 'plots')\n                renamed_plots_path = os.path.join(self.model_dir, 'plots_initial_fit')\n                shutil.move(original_plots_path, renamed_plots_path)\n                # Plot new fit graphs\n                self._plot_metrics_and_loss(fit_history)\n                # Reload best model\n                self.model = load_model_keras(\n                    os.path.join(self.model_dir, 'best.hdf5'),\n                    custom_objects=self.custom_objects\n                )\n\n        return fit_arguments\n\n    def _get_model(self) -&gt; Model:\n        '''Gets a model structure - returns the instance model instead if already defined\n\n        Returns:\n            (Model): a Keras model\n        '''\n        # Return model if already set\n        if self.model is not None:\n            return self.model\n\n        # The base model will be loaded by keras's internal functions\n        # Keras uses the `get_file` function to load all files from a cache directory (or from the internet)\n        # Per default, all keras's application try to load models files from the keras's cache directory (.keras)\n        # However, these application do not have a parameter to change the default directory, but we want all data\n        # inside the project's data directory (especially as we do not always have access to the internet).\n        # To do so, we change keras's internal function `get_file` to use a directory inside our package as the cache dir.\n        # This allows us to predownload a model from anysource.\n        # IMPORTANT : we need to reset the `get_file` function at the end of this function\n\n        # Monkey patching : https://stackoverflow.com/questions/5626193/what-is-monkey-patching\n        cache_dir = os.path.join(utils.get_data_path(), 'cache_keras')\n        if not os.path.exists(cache_dir):\n            os.makedirs(cache_dir)\n        old_get_file = data_utils.get_file\n        data_utils.get_file = partial(data_utils.get_file, cache_dir=cache_dir)\n\n        # Check if the base model exists, otherwise try to download it\n        # VGG 16\n        # base_model_name = 'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n        # base_model_path = os.path.join(utils.get_data_path(), 'cache_keras', 'models', base_model_name)\n        # base_model_backup_urls = [\n            # 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n        # ]\n        # EfficientNetB6\n        base_model_name = 'efficientnetb6_notop.h5'\n        base_model_path = os.path.join(utils.get_data_path(), 'cache_keras', 'models', base_model_name)\n        base_model_backup_urls = [\n            'https://storage.googleapis.com/keras-applications/efficientnetb6_notop.h5',\n        ]\n        # Check model presence\n        if not os.path.exists(base_model_path):\n            try:\n                utils.download_url(base_model_backup_urls, base_model_path)\n            except Exception:\n                # If we can't download it, we let the function crash alone\n                self.logger.warning(\"Can't find / download the base model for transfer learning application.\")\n\n        # Get input/output dimensions\n        input_shape = (self.width, self.height, self.depth)\n        num_classes = len(self.list_classes)\n\n        # Process\n        input_layer = Input(shape=input_shape)\n\n        # Example VGG16 - to be used with tensorflow.keras.applications.vgg16.preprocess_input - cf _get_preprocess_input\n        # We must use training=False to use the batch norm layers in inference mode\n        # (cf. https://keras.io/guides/transfer_learning/)\n        # base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n        # base_model.trainable = False  # We disable the first layers\n        # x = base_model(input_layer, training=False)\n        # x = Flatten()(x)\n\n        # Example EfficientNetB6 - to be used with tensorflow.keras.applications.efficientnet.preprocess_input - cf _get_preprocess_input\n        # We must use training=False to use the batch norm layers in inference mode\n        # (cf. https://keras.io/guides/transfer_learning/)\n        base_model = EfficientNetB6(weights='imagenet', include_top=False, input_shape=input_shape)\n        base_model.trainable = False  # We disable the first layers\n        x = base_model(input_layer, training=False)\n        x = GlobalAveragePooling2D()(x)\n\n        # Last layer\n        activation = 'softmax'\n        out = Dense(num_classes, activation=activation, kernel_initializer='glorot_uniform')(x)\n\n        # Set model\n        model = Model(inputs=input_layer, outputs=[out])\n\n        # Get lr &amp; compile\n        lr = self.keras_params['learning_rate'] if 'learning_rate' in self.keras_params.keys() else 0.001\n        self._compile_model(model, lr=lr)\n\n        # Display model\n        if self.logger.getEffectiveLevel() &lt; logging.ERROR:\n            model.summary()\n\n        # Try to save model as png if level_save &gt; 'LOW'\n        if self.level_save in ['MEDIUM', 'HIGH']:\n            self._save_model_png(model)\n\n        # We reset the `get_file` function (cf. explanations)\n        data_utils.get_file = old_get_file\n\n        # Return\n        return model\n\n    def _compile_model(self, model: Model, lr: float) -&gt; None:\n        '''Compiles the model. This is usually done in _get_model, but adding a function here\n        helps to simplify the fine-tuning step.\n\n        Args:\n            model (Model): Model to be compiled\n            lr (float): Learning rate to be used\n        '''\n        # Set optimizer\n        decay = self.keras_params['decay'] if 'decay' in self.keras_params.keys() else 0.0\n        self.logger.info(f\"Learning rate: {lr}\")\n        self.logger.info(f\"Decay: {decay}\")\n        optimizer = Adam(lr=lr, decay=decay)\n\n        # Set loss &amp; metrics\n        loss = 'categorical_crossentropy'\n        metrics: List[Union[str, Callable]] = ['accuracy']\n\n        # Compile model\n        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n\n    def _get_preprocess_input(self) -&gt; Union[Callable, None]:\n        '''Gets the preprocessing to be used before feeding images to the NN\n\n        Returns:\n            (Callable | None): Preprocessing function\n        '''\n        # Preprocessing VGG 16\n        # return vgg16_preprocess_input\n        # Preprocessing efficient net\n        return enet_preprocess_input\n\n    def _get_second_callbacks(self) -&gt; list:\n        '''Gets model callbacks - second fit\n\n        Returns:\n            list: List of callbacks\n        '''\n        # We start by renaming 'best.hdf5' &amp; 'logger.csv'\n        if os.path.exists(os.path.join(self.model_dir, 'best.hdf5')):\n            os.rename(os.path.join(self.model_dir, 'best.hdf5'), os.path.join(self.model_dir, 'best_initial_fit.hdf5'))\n        if os.path.exists(os.path.join(self.model_dir, 'logger.csv')):\n            os.rename(os.path.join(self.model_dir, 'logger.csv'), os.path.join(self.model_dir, 'logger_initial_fit.csv'))\n\n        # Get classic callbacks\n        callbacks = [EarlyStopping(monitor='val_loss', patience=self.second_patience, restore_best_weights=True)]\n        if self.level_save in ['MEDIUM', 'HIGH']:\n            callbacks.append(\n                ModelCheckpoint(\n                    filepath=os.path.join(self.model_dir, 'best.hdf5'), monitor='val_loss', save_best_only=True, mode='auto'\n                )\n            )\n        callbacks.append(CSVLogger(filename=os.path.join(self.model_dir, 'logger.csv'), separator=';', append=False))\n        callbacks.append(TerminateOnNaN())\n\n        # Get LearningRateScheduler\n        scheduler = self._get_second_learning_rate_scheduler()\n        if scheduler is not None:\n            callbacks.append(LearningRateScheduler(scheduler))\n\n        # Manage tensorboard\n        if self.level_save in ['HIGH']:\n            # Get log directory\n            models_path = utils.get_models_path()\n            tensorboard_dir = os.path.join(models_path, 'tensorboard_logs_second_fit')\n            # We add a prefix so that the function load_model works correctly (it looks for a sub-folder with model name)\n            log_dir = os.path.join(tensorboard_dir, f\"tensorboard_{ntpath.basename(self.model_dir)}\")\n            if not os.path.exists(log_dir):\n                os.makedirs(log_dir)\n\n            # TODO: check if this class does not slow proccesses\n            # -&gt; For now: comment\n            # Create custom class to monitore LR changes\n            # https://stackoverflow.com/questions/49127214/keras-how-to-output-learning-rate-onto-tensorboard\n            # class LRTensorBoard(TensorBoard):\n            #     def __init__(self, log_dir, **kwargs) -&gt; None:  # add other arguments to __init__ if you need\n            #         super().__init__(log_dir=log_dir, **kwargs)\n            #\n            #     def on_epoch_end(self, epoch, logs=None):\n            #         logs.update({'lr': K.eval(self.model.optimizer.lr)})\n            #         super().on_epoch_end(epoch, logs)\n\n            callbacks.append(TensorBoard(log_dir=log_dir, write_grads=False, write_images=False))\n            self.logger.info(f\"To start tensorboard: python -m tensorboard.main --logdir {tensorboard_dir} --samples_per_plugin images=10\")\n            # We use samples_per_plugin to avoid a rare issue between matplotlib and tensorboard\n            # https://stackoverflow.com/questions/27147300/matplotlib-tcl-asyncdelete-async-handler-deleted-by-the-wrong-thread\n\n        return callbacks\n\n    def _get_second_learning_rate_scheduler(self) -&gt; Union[Callable, None]:\n        '''Fonction to define a Learning Rate Scheduler - second fit\n           -&gt; if it returns None, no scheduler will be used. (def.)\n           -&gt; This function will be save directly in the model configuration file\n           -&gt; This can be overridden at runing time\n\n        Returns:\n            (Callable | None): A learning rate Scheduler\n        '''\n        # e.g.\n        # def scheduler(epoch):\n        #     lim_epoch = 75\n        #     if epoch &lt; lim_epoch:\n        #         return 0.01\n        #     else:\n        #         return max(0.001, 0.01 * math.exp(0.01 * (lim_epoch - epoch)))\n        scheduler = None\n        return scheduler\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save configuration JSON\n        if json_data is None:\n            json_data = {}\n\n        json_data['with_fine_tune'] = self.with_fine_tune\n        json_data['second_epochs'] = self.second_epochs\n        json_data['second_lr'] = self.second_lr\n        json_data['second_patience'] = self.second_patience\n        # Add _compile_model code if not in json_data\n        if '_compile_model' not in json_data.keys():\n            json_data['_compile_model'] = pickle.source.getsourcelines(self._compile_model)[0]\n        # Add _get_second_learning_rate_scheduler code if not in json_data\n        if '_get_second_learning_rate_scheduler' not in json_data.keys():\n            json_data['_get_second_learning_rate_scheduler'] = pickle.source.getsourcelines(self._get_second_learning_rate_scheduler)[0]\n\n        # Save\n        super().save(json_data=json_data)\n\n    def reload_from_standalone(self, **kwargs) -&gt; None:\n        '''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            hdf5_path (str): Path to hdf5 file\n            preprocess_input_path (str): Path to preprocess input file\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If hdf5_path is None\n            ValueError: If preprocess_input_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object hdf5_path is not an existing file\n            FileNotFoundError: If the object preprocess_input_path is not an existing file\n        '''\n        # Retrieve args\n        configuration_path = kwargs.get('configuration_path', None)\n        hdf5_path = kwargs.get('hdf5_path', None)\n        preprocess_input_path = kwargs.get('preprocess_input_path', None)\n\n        # Checks\n        if configuration_path is None:\n            raise ValueError(\"The argument configuration_path can't be None\")\n        if hdf5_path is None:\n            raise ValueError(\"The argument hdf5_path can't be None\")\n        if preprocess_input_path is None:\n            raise ValueError(\"The argument preprocess_input_path can't be None\")\n        if not os.path.exists(configuration_path):\n            raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n        if not os.path.exists(hdf5_path):\n            raise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\n        if not os.path.exists(preprocess_input_path):\n            raise FileNotFoundError(f\"The file {preprocess_input_path} does not exist\")\n\n        # Load confs\n        with open(configuration_path, 'r', encoding='utf-8') as f:\n            configs = json.load(f)\n        # Can't set int as keys in json, so need to cast it after reloading\n        # dict_classes keys are always ints\n        if 'dict_classes' in configs.keys():\n            configs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n        elif 'list_classes' in configs.keys():\n            configs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n\n        # Set class vars\n        # self.model_name = # Keep the created name\n        # self.model_dir = # Keep the created folder\n        self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n        self.trained = configs.get('trained', True)  # Consider trained by default\n\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['model_type', 'list_classes', 'dict_classes', 'level_save',\n                          'batch_size', 'epochs', 'validation_split', 'patience',\n                          'width', 'height', 'depth', 'color_mode', 'in_memory',\n                          'data_augmentation_params', 'nb_train_generator_images_to_save',\n                          'with_fine_tune', 'second_epochs', 'second_lr', 'second_patience',\n                          'keras_params']:\n            setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n        # Reload model\n        self.model = load_model_keras(hdf5_path, custom_objects=self.custom_objects)\n\n        # Reload preprocess_input\n        with open(preprocess_input_path, 'rb') as f:\n            self.preprocess_input = pickle.load(f)\n\n        # Save best hdf5 in new folder\n        new_hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\n        shutil.copyfile(hdf5_path, new_hdf5_path)\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_transfer_learning_classifier/#template_vision.models_training.classifiers.model_transfer_learning_classifier.ModelTransferLearningClassifier.__init__","title":"<code>__init__(with_fine_tune=True, second_epochs=10, second_lr=1e-05, second_patience=5, **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass, ModelKeras &amp; ModelClassifierMixin for more arguments)</p> Kwargs <p>with_fine_tune (bool): If a fine-tuning step should be performed after first training second_epochs (int): Number of epochs for the fine-tuning step second_lr (float): Learning rate for the fine-tuning step second_patience (int): Patience for the fine-tuning step</p> Source code in <code>template_vision/models_training/classifiers/model_transfer_learning_classifier.py</code> <pre><code>def __init__(self, with_fine_tune: bool = True, second_epochs: int = 10, second_lr: float = 1e-5, second_patience: int = 5, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelClass, ModelKeras &amp; ModelClassifierMixin for more arguments)\n\n    Kwargs:\n        with_fine_tune (bool): If a fine-tuning step should be performed after first training\n        second_epochs (int): Number of epochs for the fine-tuning step\n        second_lr (float): Learning rate for the fine-tuning step\n        second_patience (int): Patience for the fine-tuning step\n    '''\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Params\n    self.with_fine_tune = with_fine_tune\n    self.second_epochs = second_epochs\n    self.second_lr = second_lr\n    self.second_patience = second_patience\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_transfer_learning_classifier/#template_vision.models_training.classifiers.model_transfer_learning_classifier.ModelTransferLearningClassifier.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file hdf5_path (str): Path to hdf5 file preprocess_input_path (str): Path to preprocess input file</p> <p>Raises:     ValueError: If configuration_path is None     ValueError: If hdf5_path is None     ValueError: If preprocess_input_path is None     FileNotFoundError: If the object configuration_path is not an existing file     FileNotFoundError: If the object hdf5_path is not an existing file     FileNotFoundError: If the object preprocess_input_path is not an existing file</p> Source code in <code>template_vision/models_training/classifiers/model_transfer_learning_classifier.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n    '''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        hdf5_path (str): Path to hdf5 file\n        preprocess_input_path (str): Path to preprocess input file\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If hdf5_path is None\n        ValueError: If preprocess_input_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object hdf5_path is not an existing file\n        FileNotFoundError: If the object preprocess_input_path is not an existing file\n    '''\n    # Retrieve args\n    configuration_path = kwargs.get('configuration_path', None)\n    hdf5_path = kwargs.get('hdf5_path', None)\n    preprocess_input_path = kwargs.get('preprocess_input_path', None)\n\n    # Checks\n    if configuration_path is None:\n        raise ValueError(\"The argument configuration_path can't be None\")\n    if hdf5_path is None:\n        raise ValueError(\"The argument hdf5_path can't be None\")\n    if preprocess_input_path is None:\n        raise ValueError(\"The argument preprocess_input_path can't be None\")\n    if not os.path.exists(configuration_path):\n        raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n    if not os.path.exists(hdf5_path):\n        raise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\n    if not os.path.exists(preprocess_input_path):\n        raise FileNotFoundError(f\"The file {preprocess_input_path} does not exist\")\n\n    # Load confs\n    with open(configuration_path, 'r', encoding='utf-8') as f:\n        configs = json.load(f)\n    # Can't set int as keys in json, so need to cast it after reloading\n    # dict_classes keys are always ints\n    if 'dict_classes' in configs.keys():\n        configs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n    elif 'list_classes' in configs.keys():\n        configs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n\n    # Set class vars\n    # self.model_name = # Keep the created name\n    # self.model_dir = # Keep the created folder\n    self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n    self.trained = configs.get('trained', True)  # Consider trained by default\n\n    # Try to read the following attributes from configs and, if absent, keep the current one\n    for attribute in ['model_type', 'list_classes', 'dict_classes', 'level_save',\n                      'batch_size', 'epochs', 'validation_split', 'patience',\n                      'width', 'height', 'depth', 'color_mode', 'in_memory',\n                      'data_augmentation_params', 'nb_train_generator_images_to_save',\n                      'with_fine_tune', 'second_epochs', 'second_lr', 'second_patience',\n                      'keras_params']:\n        setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n    # Reload model\n    self.model = load_model_keras(hdf5_path, custom_objects=self.custom_objects)\n\n    # Reload preprocess_input\n    with open(preprocess_input_path, 'rb') as f:\n        self.preprocess_input = pickle.load(f)\n\n    # Save best hdf5 in new folder\n    new_hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\n    shutil.copyfile(hdf5_path, new_hdf5_path)\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_transfer_learning_classifier/#template_vision.models_training.classifiers.model_transfer_learning_classifier.ModelTransferLearningClassifier.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_vision/models_training/classifiers/model_transfer_learning_classifier.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save configuration JSON\n    if json_data is None:\n        json_data = {}\n\n    json_data['with_fine_tune'] = self.with_fine_tune\n    json_data['second_epochs'] = self.second_epochs\n    json_data['second_lr'] = self.second_lr\n    json_data['second_patience'] = self.second_patience\n    # Add _compile_model code if not in json_data\n    if '_compile_model' not in json_data.keys():\n        json_data['_compile_model'] = pickle.source.getsourcelines(self._compile_model)[0]\n    # Add _get_second_learning_rate_scheduler code if not in json_data\n    if '_get_second_learning_rate_scheduler' not in json_data.keys():\n        json_data['_get_second_learning_rate_scheduler'] = pickle.source.getsourcelines(self._get_second_learning_rate_scheduler)[0]\n\n    # Save\n    super().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/","title":"Object detectors","text":""},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/","title":"Model detectron faster rcnn","text":""},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.LossEvalHook","title":"<code>LossEvalHook</code>","text":"<p>             Bases: <code>HookBase</code></p> <p>Hook to save the metrics and losses on the validation dataset</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>class LossEvalHook(HookBase):\n    '''Hook to save the metrics and losses on the validation dataset'''\n\n    def __init__(self, eval_period: int, model, data_loader) -&gt; None:\n        '''Initialization of the class\n\n        Args:\n            eval_period (int) : Number of iteration between two losses calculation\n            model : Considered model\n            data_loader : A dataloader containing the validation data\n        '''\n        self._model = model\n        self._period = eval_period\n        self._data_loader = data_loader\n\n    def _do_loss_eval(self) -&gt; dict:\n        '''Calculates the losses on the validation dataset. Saves them in the storage and\n        returns them.\n\n        Return:\n            the dict containing the losses\n\n        '''\n        # Name of the considered losses\n        list_losses = ['loss_cls', 'loss_box_reg', 'loss_rpn_cls', 'loss_rpn_loc', 'total_loss']\n        losses = {name_loss: [] for name_loss in list_losses}\n        # For each batch in the data_loader...\n        for inputs in self._data_loader:\n            if torch.cuda.is_available():\n                torch.cuda.synchronize()  # Waits for all kernels in all streams on a CUDA device to complete\n            # ... we calculates the losses...\n            loss_batch = self._get_loss(inputs)\n            # ... and we add them to the dictionary which save the results for each batch\n            for name_loss in list_losses:\n                losses[name_loss].append(loss_batch[name_loss])\n        # We get the mean of the losses on the batches\n        mean_losses = {name_loss: np.mean(list_losses_batch) for name_loss, list_losses_batch in losses.items()}\n        # We save the losses in the storage of the trainer\n        for name_loss, value_loss in mean_losses.items():\n            self.trainer.storage.put_scalar(f'validation_{name_loss}', value_loss)\n        comm.synchronize()\n        return losses\n\n    def _get_loss(self, data) -&gt; dict:\n        '''Calculates the losses corresponding to data\n\n        Args:\n            data: The data on which to calculate the losses\n        Returns:\n            dict: A dictionary containing all the losses\n        '''\n        # Calculates the losses\n        metrics_dict = self._model(data)\n        # Cast the losses to float and put them in a dictionary\n        metrics_dict = {\n            k: v.detach().cpu().item() if isinstance(v, torch.Tensor) else float(v)\n            for k, v in metrics_dict.items()\n        }\n        # Calculate the total loss and add it to the dictionary of losses\n        total_loss = sum(loss for loss in metrics_dict.values())\n        metrics_dict['total_loss'] = total_loss\n        return metrics_dict\n\n    def after_step(self) -&gt; None:\n        '''After the training step, check if we are at an iteration where we\n        should calculates the losses. If it is the case, calculate them and save\n        them in the storage.\n        '''\n        next_iter = self.trainer.iter + 1\n        is_final = next_iter == self.trainer.max_iter\n        if is_final or (self._period &gt; 0 and next_iter % self._period == 0):\n            self._do_loss_eval()\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.LossEvalHook.__init__","title":"<code>__init__(eval_period, model, data_loader)</code>","text":"<p>Initialization of the class</p> <p>Parameters:</p> Name Type Description Default <code>eval_period</code> <code>int) </code> <p>Number of iteration between two losses calculation</p> required <code>model</code> <p>Considered model</p> required <code>data_loader</code> <p>A dataloader containing the validation data</p> required Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def __init__(self, eval_period: int, model, data_loader) -&gt; None:\n    '''Initialization of the class\n\n    Args:\n        eval_period (int) : Number of iteration between two losses calculation\n        model : Considered model\n        data_loader : A dataloader containing the validation data\n    '''\n    self._model = model\n    self._period = eval_period\n    self._data_loader = data_loader\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.LossEvalHook.after_step","title":"<code>after_step()</code>","text":"<p>After the training step, check if we are at an iteration where we should calculates the losses. If it is the case, calculate them and save them in the storage.</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def after_step(self) -&gt; None:\n    '''After the training step, check if we are at an iteration where we\n    should calculates the losses. If it is the case, calculate them and save\n    them in the storage.\n    '''\n    next_iter = self.trainer.iter + 1\n    is_final = next_iter == self.trainer.max_iter\n    if is_final or (self._period &gt; 0 and next_iter % self._period == 0):\n        self._do_loss_eval()\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.ModelDetectronFasterRcnnObjectDetector","title":"<code>ModelDetectronFasterRcnnObjectDetector</code>","text":"<p>             Bases: <code>ModelObjectDetectorMixin</code>, <code>ModelClass</code></p> <p>Faster RCNN model (detectron2) for object detection</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>class ModelDetectronFasterRcnnObjectDetector(ModelObjectDetectorMixin, ModelClass):\n    '''Faster RCNN model (detectron2) for object detection'''\n\n    _default_name = 'model_detectron_faster_rcnn_object_detector'\n\n    def __init__(self, epochs: int = 99, batch_size: int = 1, validation_split: float = 0.2, lr: float = 0.00025,\n                 min_delta_es: float = 0., patience: int = 5, restore_best_weights: bool = True,\n                 data_augmentation_params: Union[dict, None] = None,\n                 rpn_min_overlap: float = 0.3, rpn_max_overlap: float = 0.7, rpn_restrict_num_regions: int = 128,\n                 roi_nms_overlap_threshold: float = 0.7, pred_bbox_proba_threshold: float = 0.5,\n                 pred_nms_overlap_threshold: float = 0.5, nb_log_write_per_epoch: int = 1, nb_log_display_per_epoch: int = 10,\n                 **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelClass &amp; ModelObjectDetectorMixin for more arguments)\n\n        Args:\n            epochs (float): Maximal number of epochs\n            batch_size (int): Number of images in a batch when training\n            validation_split (float): Validation split fraction\n                Only used if there is no validation dataset as input when fitting\n            lr (float): Base (because we can use a lr scheduler) learning rate to use\n            min_delta_es (float): Minimal change in losses to be considered an amelioration for early stopping\n            patience (int): Early stopping patience. Put to 0 to disable early stopping\n            restore_best_weights (bool): If True, when the training is done, save the model with the best\n                loss on the validation dataset instead of the last model (even if early stopping is disabled)\n            data_augmentation_params (dict): Set of allowed data augmentation\n            rpn_min_overlap (float): Under this threshold a region is classified as background (RPN model)\n            rpn_max_overlap (float): Above this threshold a region is classified as object (RPN model)\n            rpn_restrict_num_regions (int): Maximal number of regions to keep as target for the RPN\n            roi_nms_overlap_threshold (float): The NMS deletes overlapping ROIs whose IOU is above this threshold\n            pred_bbox_proba_threshold (float): Above this threshold (for probabilities), a ROI is considered to be a match\n            pred_nms_overlap_threshold (float): When predicting, the NMS deletes overlapping predictions whose IOU is above this threshold\n            nb_log_write_per_epoch (int): Number of metrics logs written during one epoch (losses for the train and the valid)\n            nb_log_display_per_epoch (int): Number of metrics logs displayed during one epoch (losses for the train only)\n        Raises:\n            ValueError: If rpn_min_overlap is not in [0, 1]\n            ValueError: If rpn_max_overlap is not in [0, 1]\n            ValueError: If rpn_min_overlap &gt; rpn_max_overlap\n            ValueError: If rpn_restrict_num_regions is not positive\n            ValueError: If roi_nms_overlap_threshold is not in [0, 1]\n            ValueError: If pred_bbox_proba_threshold is not in [0, 1]\n            ValueError: If pred_nms_overlap_threshold is not in [0, 1]\n        '''\n        # Check errors\n        if not 0 &lt;= rpn_min_overlap &lt;= 1:\n            raise ValueError(f\"The argument rpn_min_overlap ({rpn_min_overlap}) must be between 0 and 1, included\")\n        if not 0 &lt;= rpn_max_overlap &lt;= 1:\n            raise ValueError(f\"The argument rpn_max_overlap ({rpn_max_overlap}) must be between 0 and 1, included\")\n        if rpn_min_overlap &gt; rpn_max_overlap:\n            raise ValueError(f\"The argument rpn_min_overlap ({rpn_min_overlap}) can't be bigger than rpn_max_overlap ({rpn_max_overlap})\")\n        if rpn_restrict_num_regions &lt; 1:\n            raise ValueError(f\"The argument rpn_restrict_num_regions ({rpn_restrict_num_regions}) must be positive\")\n        if not 0 &lt;= roi_nms_overlap_threshold &lt;= 1:\n            raise ValueError(f\"The argument roi_nms_overlap_threshold ({roi_nms_overlap_threshold}) must be between 0 and 1, included\")\n        if not 0 &lt;= pred_bbox_proba_threshold &lt;= 1:\n            raise ValueError(f\"The argument pred_bbox_proba_threshold ({pred_bbox_proba_threshold}) must be between 0 and 1, included\")\n        if not 0 &lt;= pred_nms_overlap_threshold &lt;= 1:\n            raise ValueError(f\"The argument pred_nms_overlap_threshold ({pred_nms_overlap_threshold}) must be between 0 and 1, included\")\n\n        # Init.\n        super().__init__(**kwargs)\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Attributes\n        self.validation_split = validation_split\n\n        # Early stopping parameters\n        self.min_delta_es = min_delta_es\n        self.patience = patience\n        self.restore_best_weights = restore_best_weights\n\n        # Data augmentation\n        if data_augmentation_params is None:\n            data_augmentation_params = {'horizontal_flip': True, 'vertical_flip': True, 'rot_90': True}\n        self.data_augmentation_params = data_augmentation_params\n\n        # Parameters to \"convert\" iterations to epochs\n        # Detectron works with a number of iterations (ie. number of batch to use during training)\n        # In order to be more uniform with other models, we will use \"epochs\" rather than iterations\n        self.nb_log_write_per_epoch = nb_log_write_per_epoch\n        self.epochs = epochs\n        self.nb_log_display_per_epoch = nb_log_display_per_epoch\n\n        # Load config &amp; pre-trained model\n        self.detectron_config_base_filename = 'Base-RCNN-FPN.yaml'\n        self.detectron_config_filename = 'faster_rcnn_R_50_FPN_3x.yaml'\n        self.detectron_model_filename = 'model_final_280758.pkl'\n        detectron_config_base_path = os.path.join(utils.get_data_path(), 'detectron2_conf_files', self.detectron_config_base_filename)\n        detectron_config_path = os.path.join(utils.get_data_path(), 'detectron2_conf_files', self.detectron_config_filename)\n        detectron_model_path = os.path.join(utils.get_data_path(), 'detectron2_conf_files', self.detectron_model_filename)\n        # Backup URLs if the files do not exist\n        detectron_config_base_backup_urls = [\n            'https://raw.githubusercontent.com/facebookresearch/detectron2/main/configs/Base-RCNN-FPN.yaml',\n        ]\n        detectron_config_backup_urls = [\n            'https://raw.githubusercontent.com/facebookresearch/detectron2/main/configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml',\n        ]\n        detectron_model_backup_urls = [\n            'https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl',\n        ]\n        # Check files availability\n        files_available = True\n        # For each file, we try to download it if does not exists in the projet\n        if not os.path.exists(detectron_config_base_path):\n            try:\n                self.logger.warning(\"The base configuration file of the faster RCNN of detectron2 is not present in your data folder.\")\n                self.logger.warning(\"Trying to download the file.\")\n                utils.download_url(detectron_config_base_backup_urls, detectron_config_base_path)\n            except ConnectionError:\n                self.logger.warning(\"Can't download the file. You can try to get it manually.\")\n                self.logger.warning(\"You can find it here https://github.com/facebookresearch/detectron2/blob/main/configs/Base-RCNN-FPN.yaml\")\n                self.logger.warning(\"The model won't work, except if it is 'reloaded'\")\n                self.cfg = None\n                files_available = False\n        # /!\\ WARNING, the key _BASE_ of the configuration file of the RCNN must point to the base configuration file /!\\\n        # /!\\ It won't work if it is not the case !!! /!\\\n        if not os.path.exists(detectron_config_path):\n            try:\n                self.logger.warning(\"The configuration file of the faster RCNN of detectron2 is not present in your data folder.\")\n                self.logger.warning(\"Trying to download the file.\")\n                utils.download_url(detectron_config_backup_urls, detectron_config_path)\n\n                # Configure _BASE_ keyword from detectron_config_path\n                with open(detectron_config_path, \"r\") as f:\n                    detectron_config = yaml.load(f, yaml.CLoader)\n\n                detectron_config[\"_BASE_\"] = self.detectron_config_base_filename\n\n                with open(detectron_config_path, \"w\") as f:\n                    yaml.dump(detectron_config, f)\n\n            except ConnectionError:\n                self.logger.warning(\"Can't download the file. You can try to get it manually.\")\n                self.logger.warning(\"You can find it here https://github.com/facebookresearch/detectron2/blob/main/configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n                self.logger.warning(\"You will have to modify the key _BASE_ to point to the base configuration file\")\n                self.logger.warning(\"The model won't work, except if it is 'reloaded'\")\n                self.cfg = None\n                files_available = False\n        if not os.path.exists(detectron_model_path):\n            try:\n                self.logger.warning(\"The weights file of the faster RCNN of detectron2 is not present in your data folder.\")\n                self.logger.warning(\"Trying to download the file.\")\n                utils.download_url(detectron_model_backup_urls, detectron_model_path)\n            except ConnectionError:\n                self.logger.warning(\"Can't download the file. You can try to get it manually.\")\n                self.logger.warning(\"You can download the weights here : https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl\")\n                self.logger.warning(\"The model won't work, except if it is 'reloaded'\")\n                self.cfg = None\n                files_available = False\n        # Load if ok\n        if files_available:\n            cfg = get_cfg()  # Get base config\n            try:\n                cfg.merge_from_file(detectron_config_path)  # Merge faster RCNN config\n            except Exception:\n                self.logger.error(\"Error when reading model configurations\")\n                self.logger.error(\"A common issue is that the key _BASE_ of the configuration file of the RCNN must point to the base configuration file\")\n                self.logger.error(\"Check your file 'faster_rcnn_R_50_FPN_3x.yaml'\")\n                raise\n            self.cfg = cfg\n            # Weights\n            self.cfg.MODEL.WEIGHTS = detectron_model_path\n            # Training parameters\n            self.cfg.DATALOADER.NUM_WORKERS = 2\n            self.cfg.SOLVER.IMS_PER_BATCH = batch_size\n            self.cfg.SOLVER.BASE_LR = lr\n            self.cfg.MODEL.RPN.IOU_THRESHOLDS = [rpn_min_overlap, rpn_max_overlap]\n            self.cfg.MODEL.RPN.BATCH_SIZE_PER_IMAGE = rpn_restrict_num_regions\n            self.cfg.MODEL.RPN.NMS_THRESH = roi_nms_overlap_threshold\n            self.cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = pred_bbox_proba_threshold\n            self.cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = pred_nms_overlap_threshold\n            # We put outputs in the folder of the model\n            self.cfg.OUTPUT_DIR = self.model_dir\n            # Check that the GPU is available. Otherwise, CPU\n            if not torch.cuda.is_available():\n                self.logger.warning(\"Warning, no GPU detected, the model will use CPU\")\n                self.cfg.MODEL.DEVICE = \"cpu\"\n\n    #####################\n    # Register datasets\n    #####################\n\n    def _register_dataset(self, df: pd.DataFrame, data_type: str) -&gt; None:\n        '''Registers a dataset in the global variables used by detectron2\n\n        Args:\n            df (pd.DataFrame): Dataset to use\n                Must contain the column 'file_path' with the path to an image\n                Must contain the column 'bboxes' containing the list of bboxes of the image\n            data_type (str): Data type, 'train' or 'valid'\n        Raises:\n            ValueError: If data_type not in ['train', 'valid']\n            ValueError: If the dataframe has no 'file_path' column\n            ValueError: If the dataframe has no 'bboxes' column\n        '''\n        if data_type not in ['train', 'valid']:\n            raise ValueError(f\"The value {data_type} is not a suitable value for the argument data_type.\")\n        if 'file_path' not in df.columns:\n            raise ValueError(\"The column 'file_path' is mandatory in the input dataframe\")\n        if 'bboxes' not in df.columns:\n            raise ValueError(\"The column 'bboxes' is mandatory in the input dataframe\")\n\n        # Name of the dataset\n        name_dataset = f\"dataset_{data_type}\"\n        # Deletes the dataset in the catalog if already present\n        if name_dataset in DatasetCatalog:\n            DatasetCatalog.pop(name_dataset)\n        if name_dataset in MetadataCatalog:\n            MetadataCatalog.pop(name_dataset)\n        # Register the dataset in the catalogues\n        inv_dict_classes = {value: key for key, value in self.dict_classes.items()}\n        DatasetCatalog.register(name_dataset, lambda: self._prepare_dataset_format(df, inv_dict_classes))  # register format : (str, func)\n        MetadataCatalog.get(name_dataset).set(thing_classes=self.list_classes)\n\n    def _prepare_dataset_format(self, df: pd.DataFrame, inv_dict_classes: dict) -&gt; list:\n        '''Puts the dataframe containing the file paths and the bboxes in the suitable format for detectron2\n\n        Args:\n            df (pd.DataFrame): Dataset to use\n                Must contain the column 'file_path' with the path to an image\n                Must contain the column 'bboxes' containing the list of bboxes of the image\n        Returns:\n            A list of dictionaris each corresponding to an image (and the associated bboxes)\n        '''\n        # Dictionary mapping of the classes\n        inv_dict_classes = {value: key for key, value in self.dict_classes.items()}\n        # Get info from the dataset\n        path_list = list(df['file_path'])\n        bboxes_list = list(df['bboxes'])\n        dataset_dicts = []\n        # For each image, we will create a dictionary with the elements expected by detectron2\n        for idx, (path, bboxes) in enumerate(zip(path_list, bboxes_list)):\n            # Get the height and width of the image\n            try:\n                height, width = cv2.imread(path).shape[:2]\n            except Exception:\n                self.logger.warning(f\"Can't read image {path}. We will skip it when training.\")\n                continue\n            record = {}\n            record[\"file_name\"] = path\n            record[\"image_id\"] = idx\n            record[\"height\"] = height\n            record[\"width\"] = width\n\n            # Creation of the list of the associated bboxes (annotations)\n            objs = []\n            for bbox in bboxes:\n                bbox_coordinates = [bbox[coord] for coord in ['x1', 'y1', 'x2', 'y2']]\n                # We put it in the format expected by detectron2\n                obj = {\n                    \"bbox\": bbox_coordinates,\n                    \"bbox_mode\": BoxMode.XYXY_ABS,\n                    \"category_id\": inv_dict_classes[bbox[\"class\"]],\n                    \"iscrowd\": 0\n                }\n                objs.append(obj)\n\n            # We register all the bboxes in \"annotations\" ...\n            record[\"annotations\"] = objs\n            # ... and we append to the dictionary of the dataset\n            dataset_dicts.append(record)\n        return dataset_dicts\n\n    #####################\n    # Fit\n    #####################\n\n    def fit(self, df_train: pd.DataFrame, df_valid: Union[pd.DataFrame, None] = None, with_shuffle: bool = True) -&gt; None:\n        '''Trains the model\n\n        Args:\n            df_train (pd.DataFrame): Training dataset with columns file_path &amp; bboxes\n        Kwargs:\n            df_valid (pd.DataFrame): Validation dataset with columns file_path &amp; bboxes\n            with_shuffle (boolean): If data must be shuffled before fitting\n                This should be used if the target is not shuffled as the split_validation takes the lines in order.\n                Thus, the validation set might get classes which are not in the train set ...\n        Raises:\n            ValueError: If the same classes are not present when comparing an already trained model\n                and a new dataset\n        '''\n\n        ##############################################\n        # Manage retrain\n        ##############################################\n\n        # If a model has already been fitted, we make a new folder in order not to overwrite the existing one !\n        # And we save the old conf\n        if self.trained:\n            # Get src files to save\n            src_files = [os.path.join(self.model_dir, \"configurations.json\")]\n            if self.nb_fit &gt; 1:\n                for i in range(1, self.nb_fit):\n                    src_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n            # Change model dir\n            self.model_dir = self._get_new_model_dir()\n            self.cfg.OUTPUT_DIR = self.model_dir\n            # Get dst files\n            dst_files = [os.path.join(self.model_dir, f\"configurations_fit_{self.nb_fit}.json\")]\n            if self.nb_fit &gt; 1:\n                for i in range(1, self.nb_fit):\n                    dst_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n            # Copies\n            for src, dst in zip(src_files, dst_files):\n                try:\n                    shutil.copyfile(src, dst)\n                except Exception as e:\n                    self.logger.error(f\"Impossible to copy {src} to {dst}\")\n                    self.logger.error(\"We still continue ...\")\n                    self.logger.error(repr(e))\n\n        ##############################################\n        # Prepare dataset\n        # Also extract list of classes\n        ##############################################\n\n        # Extract list of classes from df_train\n        set_classes = set()\n        for bboxes in df_train['bboxes'].to_dict().values():\n            set_classes = set_classes.union({bbox['class'] for bbox in bboxes})\n        list_classes = sorted(list(set_classes))\n        # Also set dict_classes\n        dict_classes = {i: col for i, col in enumerate(list_classes)}\n\n        # We make sure that we have str for all classes\n        # We do not raise an error, detectron2 will do it\n        classes_not_string = {cl for cl in set_classes if not isinstance(cl, str)}\n        if len(classes_not_string):\n            self.logger.warning(f\"Warning, the following classes are not strings : {classes_not_string}. Detectron2 requires that all classes are strings.\")\n\n        # Validate classes if already trained, else set them\n        if self.trained:\n            if self.list_classes != list_classes:\n                raise ValueError(\"Error: the new dataset does not match with the already fitted model\")\n            if self.dict_classes != dict_classes:\n                raise ValueError(\"Error: the new dataset does not match with the already fitted model\")\n        else:\n            self.list_classes = list_classes\n            self.dict_classes = dict_classes\n\n        # Shuffle training dataset if wanted\n        # If not, if no validation is provided, the train_test_split could stay in order\n        # Hence, for classification task, we might have classes in the validation data that we never met in the training data\n        if with_shuffle:\n            df_train = df_train.sample(frac=1.).reset_index(drop=True)\n\n        # Manage the absence of a validation dataset\n        if df_valid is None:\n            self.logger.warning(f\"Attention, pas de jeu de validation. On va donc split le jeu de training (fraction valid = {self.validation_split})\")\n            df_train, df_valid = train_test_split(df_train, test_size=self.validation_split)\n\n        # We register the train and validation datasets\n        self._register_dataset(df=df_train, data_type='train')\n        self._register_dataset(df=df_valid, data_type='valid')\n\n        # We give the number of classes to the model\n        self.cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(self.list_classes)\n\n        # We give the datasets to use to the model (that we have previously registered in the catalogues)\n        self.cfg.DATASETS.TRAIN = (\"dataset_train\", )\n        self.cfg.DATASETS.TEST = (\"dataset_valid\", )\n\n        # \"Translate\" the number of iterations to \"epoch\"\n        nb_iter_per_epoch = max(int(len(df_train) / self.cfg.SOLVER.IMS_PER_BATCH), 1)\n        # Number of iterations between two log writes\n        nb_iter_log_write = max(int(nb_iter_per_epoch / self.nb_log_write_per_epoch), 1)\n        # Number of iterations between two log displays\n        nb_iter_log_display = max(int(nb_iter_per_epoch / self.nb_log_display_per_epoch), 1)\n        # Maximal number of iterations\n        nb_max_iter = self.epochs * nb_iter_per_epoch - 1\n        self.cfg.SOLVER.MAX_ITER = nb_max_iter\n\n        # We have to change the class attribute because it is used in a class method BEFORE instanciation\n        TrainerRCNN.data_augmentation_params.update(self.data_augmentation_params)\n\n        # We train\n        trainer = TrainerRCNN(self.cfg,\n                              length_epoch=len(df_train),\n                              nb_iter_per_epoch=nb_iter_per_epoch,\n                              nb_iter_log_write=nb_iter_log_write,\n                              nb_iter_log_display=nb_iter_log_display,\n                              nb_log_write_per_epoch=self.nb_log_write_per_epoch,\n                              min_delta_es=self.min_delta_es,\n                              patience=self.patience,\n                              restore_best_weights=self.restore_best_weights)\n        # Resume to False because we automatically save the best weights in a file,\n        # and then we point self.cfg.MODEL.WEIGHTS to this file\n        trainer.resume_or_load(resume=False)\n        trainer.train()\n\n        # Update train status\n        self.trained = True\n        self.nb_fit += 1\n\n        # We change the weights path to the post-training weights\n        self.cfg.MODEL.WEIGHTS = os.path.join(self.cfg.OUTPUT_DIR, 'best.pth')\n\n        # Plots losses &amp; metrics\n        if self.level_save in ['MEDIUM', 'HIGH']:\n            self._plot_metrics_and_loss()\n\n    #####################\n    # Predict\n    #####################\n\n    @utils.trained_needed\n    def predict(self, df_test: pd.DataFrame, write_images: bool = False,\n                output_dir_image: Union[str, None] = None, **kwargs) -&gt; List[List[dict]]:\n        '''Predictions on test set - batch size must be equal to 1\n\n        Args:\n            df_test (pd.DataFrame): Data to predict, with a column 'file_path'\n            write_images (bool): If True, we write images with the predicted bboxes\n            output_dir_image (str): Path to which we want to write the predicted images (if write_images is True)\n        Returns:\n            (list&lt;list&lt;dict&gt;&gt;): list (one entry per image) of list of bboxes\n        '''\n        # First we take care of the case where we want to write images\n        if write_images:\n            # Metadata used by the Visualizer to draw bboxes\n            metadata = Metadata(name='metadata_for_predict')\n            metadata.set(thing_classes=self.list_classes)\n            # Prepare the folders to write the images\n            # Manage case where output_dir_image is None\n            if output_dir_image is None:\n                output_dir_image = os.path.join(self.cfg.OUTPUT_DIR, 'inference', 'images')\n            # Create folder if it does not exist\n            if not os.path.exists(output_dir_image):\n                os.makedirs(output_dir_image)\n\n        # We define a predictor\n        predictor = DefaultPredictor(self.cfg)\n        # For each image...\n        list_bbox = []\n        for file_path in df_test['file_path']:\n            list_bboxes_img = []\n            # We open the image\n            im = cv2.imread(file_path)\n            if im is not None:\n                # We predict\n                outputs = predictor(im)\n                if write_images:\n                    # We draw bboxes and we write the image\n                    filename = os.path.split(file_path)[-1]\n                    v = Visualizer(im[:, :, ::-1], metadata, scale=1.0)\n                    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n                    cv2.imwrite(os.path.join(output_dir_image, filename), v.get_image()[:, :, ::-1])\n                # We get the bboxes, the scores and the classes\n                boxes = np.array(outputs['instances'].get('pred_boxes').tensor.cpu())\n                scores = np.array(outputs['instances'].get('scores').cpu())\n                classes = np.array(outputs['instances'].get('pred_classes').cpu())\n                # For each bbox predicted\n                for idx in range(len(boxes)):\n                    # We put it in bbox format\n                    coordinates = boxes[idx]\n                    bbox = {'x1': coordinates[0], 'y1': coordinates[1], 'x2': coordinates[2],\n                            'y2': coordinates[3], 'proba': scores[idx],\n                            'class': self.dict_classes[classes[idx]]}\n                    # An we append it\n                    list_bboxes_img.append(bbox.copy())\n            list_bbox.append(list_bboxes_img.copy())\n        return list_bbox\n\n    #####################\n    # Misc.\n    #####################\n\n    def _plot_metrics_and_loss(self, **kwargs) -&gt; None:\n        '''Plots interesting metrics from training and saves them in files'''\n        # Get metrics from detectron2 info\n        path_json_metrics = os.path.join(self.cfg.OUTPUT_DIR, 'metrics.json')\n        metrics = self._load_metrics_from_json(path_json_metrics)\n        # Manage plots\n        plots_path = os.path.join(self.cfg.OUTPUT_DIR, 'plots')\n        if not os.path.exists(plots_path):\n            os.makedirs(plots_path)\n        dict_plots = {'total_loss': {'title': 'Total loss', 'output_filename': 'total_loss'},\n                      'loss_cls': {'title': 'Classifier classification loss', 'output_filename': 'loss_cls_classifier'},\n                      'loss_box_reg': {'title': 'Classifier regression loss', 'output_filename': 'loss_reg_classifier'},\n                      'loss_rpn_cls': {'title': 'RPN classification loss', 'output_filename': 'loss_cls_rpn'},\n                      'loss_rpn_loc': {'title': 'RPN regression loss', 'output_filename': 'loss_reg_rpn'}}\n        # Plot each metric one by one\n        for name_metric, char_metric in dict_plots.items():\n            self._plot_one_metric(metrics=metrics,\n                                  name_metric=name_metric,\n                                  title=char_metric['title'],\n                                  output_filename=char_metric['output_filename'],\n                                  plots_path=plots_path)\n\n    def _load_metrics_from_json(self, json_path: str) -&gt; pd.DataFrame:\n        '''Reads the .json written by the training and puts it in a dataframe\n\n        Args:\n            json_path (str) : Path to the .json file\n        Returns:\n            pd.DataFrame: A dataframe containing all the metrics saved during the training\n        '''\n        lines = []\n        with open(json_path, 'r') as f:\n            for line in f:\n                lines.append(json.loads(line))\n        # We get rid of the lines which do not contain validation_total_loss\n        lines = [line for line in lines if 'validation_total_loss' in line]\n        metrics = pd.DataFrame(lines)\n        metrics = metrics.drop_duplicates()\n        return metrics\n\n    def _plot_one_metric(self, metrics: pd.DataFrame, name_metric: str, title: str,\n                         output_filename: str, plots_path: str) -&gt; None:\n        '''Plots the figure of a metric for the train and validation datasets and saves it.\n\n        Args:\n            metrics (pd.DataFrame): The dataframe containing the metrics\n            name_metric (str): The name of the metric we want to plot\n            title (str): The name we want to give to the plot\n            output_filename (str): The name of the file we want to save (without the extension)\n            plots_path (str): The path to the plot folder\n        '''\n        # Get the lists of metrics for the train and validation datasets\n        list_train = list(metrics[name_metric])\n        list_valid = list(metrics[f'validation_{name_metric}'])\n        list_iteration = list(metrics['iteration'])\n        # Plot\n        plt.figure(figsize=(10, 8))\n        plt.plot(list_iteration, list_train)\n        plt.plot(list_iteration, list_valid)\n        plt.title(title)\n        plt.ylabel(title)\n        plt.xlabel(\"Number of iterations\")\n        plt.legend(['Train', 'Validation'], loc='upper left')\n        # Save\n        filename = f\"{output_filename}.jpeg\"\n        plt.savefig(os.path.join(plots_path, filename))\n        plt.close('all')\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save configuration JSON\n        if json_data is None:\n            json_data = {}\n        # Save attributes &amp; cfg (contains all params)\n        json_data['librairie'] = 'detectron2'\n        json_data['validation_split'] = self.validation_split\n        json_data['min_delta_es'] = self.min_delta_es\n        json_data['patience'] = self.patience\n        json_data['restore_best_weights'] = self.restore_best_weights\n        json_data['data_augmentation_params'] = self.data_augmentation_params\n        json_data['nb_log_write_per_epoch'] = self.nb_log_write_per_epoch\n        json_data['epochs'] = self.epochs\n        json_data['nb_log_display_per_epoch'] = self.nb_log_display_per_epoch\n        json_data['detectron_config_base_filename'] = self.detectron_config_base_filename\n        json_data['detectron_config_filename'] = self.detectron_config_filename\n        json_data['detectron_model_filename'] = self.detectron_model_filename\n        json_data['cfg'] = self.cfg\n\n        # We save le model with CPU so that there is no problem later\n        # when we use the model (with streamlit for example)\n        device = self.cfg.MODEL.DEVICE\n        self.cfg.MODEL.DEVICE = \"cpu\"\n        super().save(json_data=json_data)\n        # We undo what we just did\n        self.cfg.MODEL.DEVICE = device\n\n    def reload_from_standalone(self, **kwargs) -&gt; None:\n        '''Loads a model from its configuration and the weights of the network\n        - /!\\\\ Experimental /!\\\\ -\n\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            pth_path (str): Path to pth file\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If pth_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object pth_path is not an existing file\n        '''\n        # Retrieve args\n        configuration_path = kwargs.get('configuration_path', None)\n        pth_path = kwargs.get('pth_path', None)\n\n        # Checks\n        if configuration_path is None:\n            raise ValueError(\"The argument configuration_path can't be None\")\n        if pth_path is None:\n            raise ValueError(\"The argument pth_path can't be None\")\n        if not os.path.exists(configuration_path):\n            raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n        if not os.path.exists(pth_path):\n            raise FileNotFoundError(f\"The file {pth_path} does not exist\")\n\n        # Load confs\n        with open(configuration_path, 'r', encoding='utf-8') as f:\n            configs = json.load(f)\n        # Can't set int as keys in json, so need to cast it after reloading\n        # dict_classes keys are always ints\n        if 'dict_classes' in configs.keys():\n            configs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n        elif 'list_classes' in configs.keys():\n            configs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n\n        # Set class vars\n        # self.model_name = # Keep the created name\n        # self.model_dir = # Keep the created folder\n        self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n        self.trained = configs.get('trained', True)  # Consider trained by default\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['model_type', 'validation_split', 'min_delta_es', 'patience', 'restore_best_weights',\n                          'list_classes', 'dict_classes', 'data_augmentation_params', 'level_save',\n                          'nb_log_write_per_epoch', 'epochs', 'nb_log_display_per_epoch', 'detectron_config_base_filename',\n                          'detectron_config_filename', 'detectron_model_filename', 'cfg']:\n            setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n        # Transform cfg into CfgNode\n        self.cfg = CfgNode(init_dict=self.cfg)\n\n        # Save best pth in new folder\n        new_pth_path = os.path.join(self.model_dir, 'best.pth')\n        shutil.copyfile(pth_path, new_pth_path)\n\n        # Reload model\n        self.cfg.MODEL.WEIGHTS = new_pth_path\n\n        # Change output path\n        self.cfg.OUTPUT_DIR = self.model_dir\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.ModelDetectronFasterRcnnObjectDetector.__init__","title":"<code>__init__(epochs=99, batch_size=1, validation_split=0.2, lr=0.00025, min_delta_es=0.0, patience=5, restore_best_weights=True, data_augmentation_params=None, rpn_min_overlap=0.3, rpn_max_overlap=0.7, rpn_restrict_num_regions=128, roi_nms_overlap_threshold=0.7, pred_bbox_proba_threshold=0.5, pred_nms_overlap_threshold=0.5, nb_log_write_per_epoch=1, nb_log_display_per_epoch=10, **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass &amp; ModelObjectDetectorMixin for more arguments)</p> <p>Parameters:</p> Name Type Description Default <code>epochs</code> <code>float</code> <p>Maximal number of epochs</p> <code>99</code> <code>batch_size</code> <code>int</code> <p>Number of images in a batch when training</p> <code>1</code> <code>validation_split</code> <code>float</code> <p>Validation split fraction Only used if there is no validation dataset as input when fitting</p> <code>0.2</code> <code>lr</code> <code>float</code> <p>Base (because we can use a lr scheduler) learning rate to use</p> <code>0.00025</code> <code>min_delta_es</code> <code>float</code> <p>Minimal change in losses to be considered an amelioration for early stopping</p> <code>0.0</code> <code>patience</code> <code>int</code> <p>Early stopping patience. Put to 0 to disable early stopping</p> <code>5</code> <code>restore_best_weights</code> <code>bool</code> <p>If True, when the training is done, save the model with the best loss on the validation dataset instead of the last model (even if early stopping is disabled)</p> <code>True</code> <code>data_augmentation_params</code> <code>dict</code> <p>Set of allowed data augmentation</p> <code>None</code> <code>rpn_min_overlap</code> <code>float</code> <p>Under this threshold a region is classified as background (RPN model)</p> <code>0.3</code> <code>rpn_max_overlap</code> <code>float</code> <p>Above this threshold a region is classified as object (RPN model)</p> <code>0.7</code> <code>rpn_restrict_num_regions</code> <code>int</code> <p>Maximal number of regions to keep as target for the RPN</p> <code>128</code> <code>roi_nms_overlap_threshold</code> <code>float</code> <p>The NMS deletes overlapping ROIs whose IOU is above this threshold</p> <code>0.7</code> <code>pred_bbox_proba_threshold</code> <code>float</code> <p>Above this threshold (for probabilities), a ROI is considered to be a match</p> <code>0.5</code> <code>pred_nms_overlap_threshold</code> <code>float</code> <p>When predicting, the NMS deletes overlapping predictions whose IOU is above this threshold</p> <code>0.5</code> <code>nb_log_write_per_epoch</code> <code>int</code> <p>Number of metrics logs written during one epoch (losses for the train and the valid)</p> <code>1</code> <code>nb_log_display_per_epoch</code> <code>int</code> <p>Number of metrics logs displayed during one epoch (losses for the train only)</p> <code>10</code> <p>Raises:     ValueError: If rpn_min_overlap is not in [0, 1]     ValueError: If rpn_max_overlap is not in [0, 1]     ValueError: If rpn_min_overlap &gt; rpn_max_overlap     ValueError: If rpn_restrict_num_regions is not positive     ValueError: If roi_nms_overlap_threshold is not in [0, 1]     ValueError: If pred_bbox_proba_threshold is not in [0, 1]     ValueError: If pred_nms_overlap_threshold is not in [0, 1]</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def __init__(self, epochs: int = 99, batch_size: int = 1, validation_split: float = 0.2, lr: float = 0.00025,\n             min_delta_es: float = 0., patience: int = 5, restore_best_weights: bool = True,\n             data_augmentation_params: Union[dict, None] = None,\n             rpn_min_overlap: float = 0.3, rpn_max_overlap: float = 0.7, rpn_restrict_num_regions: int = 128,\n             roi_nms_overlap_threshold: float = 0.7, pred_bbox_proba_threshold: float = 0.5,\n             pred_nms_overlap_threshold: float = 0.5, nb_log_write_per_epoch: int = 1, nb_log_display_per_epoch: int = 10,\n             **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelClass &amp; ModelObjectDetectorMixin for more arguments)\n\n    Args:\n        epochs (float): Maximal number of epochs\n        batch_size (int): Number of images in a batch when training\n        validation_split (float): Validation split fraction\n            Only used if there is no validation dataset as input when fitting\n        lr (float): Base (because we can use a lr scheduler) learning rate to use\n        min_delta_es (float): Minimal change in losses to be considered an amelioration for early stopping\n        patience (int): Early stopping patience. Put to 0 to disable early stopping\n        restore_best_weights (bool): If True, when the training is done, save the model with the best\n            loss on the validation dataset instead of the last model (even if early stopping is disabled)\n        data_augmentation_params (dict): Set of allowed data augmentation\n        rpn_min_overlap (float): Under this threshold a region is classified as background (RPN model)\n        rpn_max_overlap (float): Above this threshold a region is classified as object (RPN model)\n        rpn_restrict_num_regions (int): Maximal number of regions to keep as target for the RPN\n        roi_nms_overlap_threshold (float): The NMS deletes overlapping ROIs whose IOU is above this threshold\n        pred_bbox_proba_threshold (float): Above this threshold (for probabilities), a ROI is considered to be a match\n        pred_nms_overlap_threshold (float): When predicting, the NMS deletes overlapping predictions whose IOU is above this threshold\n        nb_log_write_per_epoch (int): Number of metrics logs written during one epoch (losses for the train and the valid)\n        nb_log_display_per_epoch (int): Number of metrics logs displayed during one epoch (losses for the train only)\n    Raises:\n        ValueError: If rpn_min_overlap is not in [0, 1]\n        ValueError: If rpn_max_overlap is not in [0, 1]\n        ValueError: If rpn_min_overlap &gt; rpn_max_overlap\n        ValueError: If rpn_restrict_num_regions is not positive\n        ValueError: If roi_nms_overlap_threshold is not in [0, 1]\n        ValueError: If pred_bbox_proba_threshold is not in [0, 1]\n        ValueError: If pred_nms_overlap_threshold is not in [0, 1]\n    '''\n    # Check errors\n    if not 0 &lt;= rpn_min_overlap &lt;= 1:\n        raise ValueError(f\"The argument rpn_min_overlap ({rpn_min_overlap}) must be between 0 and 1, included\")\n    if not 0 &lt;= rpn_max_overlap &lt;= 1:\n        raise ValueError(f\"The argument rpn_max_overlap ({rpn_max_overlap}) must be between 0 and 1, included\")\n    if rpn_min_overlap &gt; rpn_max_overlap:\n        raise ValueError(f\"The argument rpn_min_overlap ({rpn_min_overlap}) can't be bigger than rpn_max_overlap ({rpn_max_overlap})\")\n    if rpn_restrict_num_regions &lt; 1:\n        raise ValueError(f\"The argument rpn_restrict_num_regions ({rpn_restrict_num_regions}) must be positive\")\n    if not 0 &lt;= roi_nms_overlap_threshold &lt;= 1:\n        raise ValueError(f\"The argument roi_nms_overlap_threshold ({roi_nms_overlap_threshold}) must be between 0 and 1, included\")\n    if not 0 &lt;= pred_bbox_proba_threshold &lt;= 1:\n        raise ValueError(f\"The argument pred_bbox_proba_threshold ({pred_bbox_proba_threshold}) must be between 0 and 1, included\")\n    if not 0 &lt;= pred_nms_overlap_threshold &lt;= 1:\n        raise ValueError(f\"The argument pred_nms_overlap_threshold ({pred_nms_overlap_threshold}) must be between 0 and 1, included\")\n\n    # Init.\n    super().__init__(**kwargs)\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Attributes\n    self.validation_split = validation_split\n\n    # Early stopping parameters\n    self.min_delta_es = min_delta_es\n    self.patience = patience\n    self.restore_best_weights = restore_best_weights\n\n    # Data augmentation\n    if data_augmentation_params is None:\n        data_augmentation_params = {'horizontal_flip': True, 'vertical_flip': True, 'rot_90': True}\n    self.data_augmentation_params = data_augmentation_params\n\n    # Parameters to \"convert\" iterations to epochs\n    # Detectron works with a number of iterations (ie. number of batch to use during training)\n    # In order to be more uniform with other models, we will use \"epochs\" rather than iterations\n    self.nb_log_write_per_epoch = nb_log_write_per_epoch\n    self.epochs = epochs\n    self.nb_log_display_per_epoch = nb_log_display_per_epoch\n\n    # Load config &amp; pre-trained model\n    self.detectron_config_base_filename = 'Base-RCNN-FPN.yaml'\n    self.detectron_config_filename = 'faster_rcnn_R_50_FPN_3x.yaml'\n    self.detectron_model_filename = 'model_final_280758.pkl'\n    detectron_config_base_path = os.path.join(utils.get_data_path(), 'detectron2_conf_files', self.detectron_config_base_filename)\n    detectron_config_path = os.path.join(utils.get_data_path(), 'detectron2_conf_files', self.detectron_config_filename)\n    detectron_model_path = os.path.join(utils.get_data_path(), 'detectron2_conf_files', self.detectron_model_filename)\n    # Backup URLs if the files do not exist\n    detectron_config_base_backup_urls = [\n        'https://raw.githubusercontent.com/facebookresearch/detectron2/main/configs/Base-RCNN-FPN.yaml',\n    ]\n    detectron_config_backup_urls = [\n        'https://raw.githubusercontent.com/facebookresearch/detectron2/main/configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml',\n    ]\n    detectron_model_backup_urls = [\n        'https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl',\n    ]\n    # Check files availability\n    files_available = True\n    # For each file, we try to download it if does not exists in the projet\n    if not os.path.exists(detectron_config_base_path):\n        try:\n            self.logger.warning(\"The base configuration file of the faster RCNN of detectron2 is not present in your data folder.\")\n            self.logger.warning(\"Trying to download the file.\")\n            utils.download_url(detectron_config_base_backup_urls, detectron_config_base_path)\n        except ConnectionError:\n            self.logger.warning(\"Can't download the file. You can try to get it manually.\")\n            self.logger.warning(\"You can find it here https://github.com/facebookresearch/detectron2/blob/main/configs/Base-RCNN-FPN.yaml\")\n            self.logger.warning(\"The model won't work, except if it is 'reloaded'\")\n            self.cfg = None\n            files_available = False\n    # /!\\ WARNING, the key _BASE_ of the configuration file of the RCNN must point to the base configuration file /!\\\n    # /!\\ It won't work if it is not the case !!! /!\\\n    if not os.path.exists(detectron_config_path):\n        try:\n            self.logger.warning(\"The configuration file of the faster RCNN of detectron2 is not present in your data folder.\")\n            self.logger.warning(\"Trying to download the file.\")\n            utils.download_url(detectron_config_backup_urls, detectron_config_path)\n\n            # Configure _BASE_ keyword from detectron_config_path\n            with open(detectron_config_path, \"r\") as f:\n                detectron_config = yaml.load(f, yaml.CLoader)\n\n            detectron_config[\"_BASE_\"] = self.detectron_config_base_filename\n\n            with open(detectron_config_path, \"w\") as f:\n                yaml.dump(detectron_config, f)\n\n        except ConnectionError:\n            self.logger.warning(\"Can't download the file. You can try to get it manually.\")\n            self.logger.warning(\"You can find it here https://github.com/facebookresearch/detectron2/blob/main/configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n            self.logger.warning(\"You will have to modify the key _BASE_ to point to the base configuration file\")\n            self.logger.warning(\"The model won't work, except if it is 'reloaded'\")\n            self.cfg = None\n            files_available = False\n    if not os.path.exists(detectron_model_path):\n        try:\n            self.logger.warning(\"The weights file of the faster RCNN of detectron2 is not present in your data folder.\")\n            self.logger.warning(\"Trying to download the file.\")\n            utils.download_url(detectron_model_backup_urls, detectron_model_path)\n        except ConnectionError:\n            self.logger.warning(\"Can't download the file. You can try to get it manually.\")\n            self.logger.warning(\"You can download the weights here : https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl\")\n            self.logger.warning(\"The model won't work, except if it is 'reloaded'\")\n            self.cfg = None\n            files_available = False\n    # Load if ok\n    if files_available:\n        cfg = get_cfg()  # Get base config\n        try:\n            cfg.merge_from_file(detectron_config_path)  # Merge faster RCNN config\n        except Exception:\n            self.logger.error(\"Error when reading model configurations\")\n            self.logger.error(\"A common issue is that the key _BASE_ of the configuration file of the RCNN must point to the base configuration file\")\n            self.logger.error(\"Check your file 'faster_rcnn_R_50_FPN_3x.yaml'\")\n            raise\n        self.cfg = cfg\n        # Weights\n        self.cfg.MODEL.WEIGHTS = detectron_model_path\n        # Training parameters\n        self.cfg.DATALOADER.NUM_WORKERS = 2\n        self.cfg.SOLVER.IMS_PER_BATCH = batch_size\n        self.cfg.SOLVER.BASE_LR = lr\n        self.cfg.MODEL.RPN.IOU_THRESHOLDS = [rpn_min_overlap, rpn_max_overlap]\n        self.cfg.MODEL.RPN.BATCH_SIZE_PER_IMAGE = rpn_restrict_num_regions\n        self.cfg.MODEL.RPN.NMS_THRESH = roi_nms_overlap_threshold\n        self.cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = pred_bbox_proba_threshold\n        self.cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = pred_nms_overlap_threshold\n        # We put outputs in the folder of the model\n        self.cfg.OUTPUT_DIR = self.model_dir\n        # Check that the GPU is available. Otherwise, CPU\n        if not torch.cuda.is_available():\n            self.logger.warning(\"Warning, no GPU detected, the model will use CPU\")\n            self.cfg.MODEL.DEVICE = \"cpu\"\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.ModelDetectronFasterRcnnObjectDetector.fit","title":"<code>fit(df_train, df_valid=None, with_shuffle=True)</code>","text":"<p>Trains the model</p> <p>Parameters:</p> Name Type Description Default <code>df_train</code> <code>DataFrame</code> <p>Training dataset with columns file_path &amp; bboxes</p> required <p>Kwargs:     df_valid (pd.DataFrame): Validation dataset with columns file_path &amp; bboxes     with_shuffle (boolean): If data must be shuffled before fitting         This should be used if the target is not shuffled as the split_validation takes the lines in order.         Thus, the validation set might get classes which are not in the train set ... Raises:     ValueError: If the same classes are not present when comparing an already trained model         and a new dataset</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def fit(self, df_train: pd.DataFrame, df_valid: Union[pd.DataFrame, None] = None, with_shuffle: bool = True) -&gt; None:\n    '''Trains the model\n\n    Args:\n        df_train (pd.DataFrame): Training dataset with columns file_path &amp; bboxes\n    Kwargs:\n        df_valid (pd.DataFrame): Validation dataset with columns file_path &amp; bboxes\n        with_shuffle (boolean): If data must be shuffled before fitting\n            This should be used if the target is not shuffled as the split_validation takes the lines in order.\n            Thus, the validation set might get classes which are not in the train set ...\n    Raises:\n        ValueError: If the same classes are not present when comparing an already trained model\n            and a new dataset\n    '''\n\n    ##############################################\n    # Manage retrain\n    ##############################################\n\n    # If a model has already been fitted, we make a new folder in order not to overwrite the existing one !\n    # And we save the old conf\n    if self.trained:\n        # Get src files to save\n        src_files = [os.path.join(self.model_dir, \"configurations.json\")]\n        if self.nb_fit &gt; 1:\n            for i in range(1, self.nb_fit):\n                src_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n        # Change model dir\n        self.model_dir = self._get_new_model_dir()\n        self.cfg.OUTPUT_DIR = self.model_dir\n        # Get dst files\n        dst_files = [os.path.join(self.model_dir, f\"configurations_fit_{self.nb_fit}.json\")]\n        if self.nb_fit &gt; 1:\n            for i in range(1, self.nb_fit):\n                dst_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n        # Copies\n        for src, dst in zip(src_files, dst_files):\n            try:\n                shutil.copyfile(src, dst)\n            except Exception as e:\n                self.logger.error(f\"Impossible to copy {src} to {dst}\")\n                self.logger.error(\"We still continue ...\")\n                self.logger.error(repr(e))\n\n    ##############################################\n    # Prepare dataset\n    # Also extract list of classes\n    ##############################################\n\n    # Extract list of classes from df_train\n    set_classes = set()\n    for bboxes in df_train['bboxes'].to_dict().values():\n        set_classes = set_classes.union({bbox['class'] for bbox in bboxes})\n    list_classes = sorted(list(set_classes))\n    # Also set dict_classes\n    dict_classes = {i: col for i, col in enumerate(list_classes)}\n\n    # We make sure that we have str for all classes\n    # We do not raise an error, detectron2 will do it\n    classes_not_string = {cl for cl in set_classes if not isinstance(cl, str)}\n    if len(classes_not_string):\n        self.logger.warning(f\"Warning, the following classes are not strings : {classes_not_string}. Detectron2 requires that all classes are strings.\")\n\n    # Validate classes if already trained, else set them\n    if self.trained:\n        if self.list_classes != list_classes:\n            raise ValueError(\"Error: the new dataset does not match with the already fitted model\")\n        if self.dict_classes != dict_classes:\n            raise ValueError(\"Error: the new dataset does not match with the already fitted model\")\n    else:\n        self.list_classes = list_classes\n        self.dict_classes = dict_classes\n\n    # Shuffle training dataset if wanted\n    # If not, if no validation is provided, the train_test_split could stay in order\n    # Hence, for classification task, we might have classes in the validation data that we never met in the training data\n    if with_shuffle:\n        df_train = df_train.sample(frac=1.).reset_index(drop=True)\n\n    # Manage the absence of a validation dataset\n    if df_valid is None:\n        self.logger.warning(f\"Attention, pas de jeu de validation. On va donc split le jeu de training (fraction valid = {self.validation_split})\")\n        df_train, df_valid = train_test_split(df_train, test_size=self.validation_split)\n\n    # We register the train and validation datasets\n    self._register_dataset(df=df_train, data_type='train')\n    self._register_dataset(df=df_valid, data_type='valid')\n\n    # We give the number of classes to the model\n    self.cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(self.list_classes)\n\n    # We give the datasets to use to the model (that we have previously registered in the catalogues)\n    self.cfg.DATASETS.TRAIN = (\"dataset_train\", )\n    self.cfg.DATASETS.TEST = (\"dataset_valid\", )\n\n    # \"Translate\" the number of iterations to \"epoch\"\n    nb_iter_per_epoch = max(int(len(df_train) / self.cfg.SOLVER.IMS_PER_BATCH), 1)\n    # Number of iterations between two log writes\n    nb_iter_log_write = max(int(nb_iter_per_epoch / self.nb_log_write_per_epoch), 1)\n    # Number of iterations between two log displays\n    nb_iter_log_display = max(int(nb_iter_per_epoch / self.nb_log_display_per_epoch), 1)\n    # Maximal number of iterations\n    nb_max_iter = self.epochs * nb_iter_per_epoch - 1\n    self.cfg.SOLVER.MAX_ITER = nb_max_iter\n\n    # We have to change the class attribute because it is used in a class method BEFORE instanciation\n    TrainerRCNN.data_augmentation_params.update(self.data_augmentation_params)\n\n    # We train\n    trainer = TrainerRCNN(self.cfg,\n                          length_epoch=len(df_train),\n                          nb_iter_per_epoch=nb_iter_per_epoch,\n                          nb_iter_log_write=nb_iter_log_write,\n                          nb_iter_log_display=nb_iter_log_display,\n                          nb_log_write_per_epoch=self.nb_log_write_per_epoch,\n                          min_delta_es=self.min_delta_es,\n                          patience=self.patience,\n                          restore_best_weights=self.restore_best_weights)\n    # Resume to False because we automatically save the best weights in a file,\n    # and then we point self.cfg.MODEL.WEIGHTS to this file\n    trainer.resume_or_load(resume=False)\n    trainer.train()\n\n    # Update train status\n    self.trained = True\n    self.nb_fit += 1\n\n    # We change the weights path to the post-training weights\n    self.cfg.MODEL.WEIGHTS = os.path.join(self.cfg.OUTPUT_DIR, 'best.pth')\n\n    # Plots losses &amp; metrics\n    if self.level_save in ['MEDIUM', 'HIGH']:\n        self._plot_metrics_and_loss()\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.ModelDetectronFasterRcnnObjectDetector.predict","title":"<code>predict(df_test, write_images=False, output_dir_image=None, **kwargs)</code>","text":"<p>Predictions on test set - batch size must be equal to 1</p> <p>Parameters:</p> Name Type Description Default <code>df_test</code> <code>DataFrame</code> <p>Data to predict, with a column 'file_path'</p> required <code>write_images</code> <code>bool</code> <p>If True, we write images with the predicted bboxes</p> <code>False</code> <code>output_dir_image</code> <code>str</code> <p>Path to which we want to write the predicted images (if write_images is True)</p> <code>None</code> <p>Returns:     (list&gt;): list (one entry per image) of list of bboxes Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>@utils.trained_needed\ndef predict(self, df_test: pd.DataFrame, write_images: bool = False,\n            output_dir_image: Union[str, None] = None, **kwargs) -&gt; List[List[dict]]:\n    '''Predictions on test set - batch size must be equal to 1\n\n    Args:\n        df_test (pd.DataFrame): Data to predict, with a column 'file_path'\n        write_images (bool): If True, we write images with the predicted bboxes\n        output_dir_image (str): Path to which we want to write the predicted images (if write_images is True)\n    Returns:\n        (list&lt;list&lt;dict&gt;&gt;): list (one entry per image) of list of bboxes\n    '''\n    # First we take care of the case where we want to write images\n    if write_images:\n        # Metadata used by the Visualizer to draw bboxes\n        metadata = Metadata(name='metadata_for_predict')\n        metadata.set(thing_classes=self.list_classes)\n        # Prepare the folders to write the images\n        # Manage case where output_dir_image is None\n        if output_dir_image is None:\n            output_dir_image = os.path.join(self.cfg.OUTPUT_DIR, 'inference', 'images')\n        # Create folder if it does not exist\n        if not os.path.exists(output_dir_image):\n            os.makedirs(output_dir_image)\n\n    # We define a predictor\n    predictor = DefaultPredictor(self.cfg)\n    # For each image...\n    list_bbox = []\n    for file_path in df_test['file_path']:\n        list_bboxes_img = []\n        # We open the image\n        im = cv2.imread(file_path)\n        if im is not None:\n            # We predict\n            outputs = predictor(im)\n            if write_images:\n                # We draw bboxes and we write the image\n                filename = os.path.split(file_path)[-1]\n                v = Visualizer(im[:, :, ::-1], metadata, scale=1.0)\n                v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n                cv2.imwrite(os.path.join(output_dir_image, filename), v.get_image()[:, :, ::-1])\n            # We get the bboxes, the scores and the classes\n            boxes = np.array(outputs['instances'].get('pred_boxes').tensor.cpu())\n            scores = np.array(outputs['instances'].get('scores').cpu())\n            classes = np.array(outputs['instances'].get('pred_classes').cpu())\n            # For each bbox predicted\n            for idx in range(len(boxes)):\n                # We put it in bbox format\n                coordinates = boxes[idx]\n                bbox = {'x1': coordinates[0], 'y1': coordinates[1], 'x2': coordinates[2],\n                        'y2': coordinates[3], 'proba': scores[idx],\n                        'class': self.dict_classes[classes[idx]]}\n                # An we append it\n                list_bboxes_img.append(bbox.copy())\n        list_bbox.append(list_bboxes_img.copy())\n    return list_bbox\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.ModelDetectronFasterRcnnObjectDetector.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Loads a model from its configuration and the weights of the network - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file pth_path (str): Path to pth file</p> <p>Raises:     ValueError: If configuration_path is None     ValueError: If pth_path is None     FileNotFoundError: If the object configuration_path is not an existing file     FileNotFoundError: If the object pth_path is not an existing file</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n    '''Loads a model from its configuration and the weights of the network\n    - /!\\\\ Experimental /!\\\\ -\n\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        pth_path (str): Path to pth file\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If pth_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object pth_path is not an existing file\n    '''\n    # Retrieve args\n    configuration_path = kwargs.get('configuration_path', None)\n    pth_path = kwargs.get('pth_path', None)\n\n    # Checks\n    if configuration_path is None:\n        raise ValueError(\"The argument configuration_path can't be None\")\n    if pth_path is None:\n        raise ValueError(\"The argument pth_path can't be None\")\n    if not os.path.exists(configuration_path):\n        raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n    if not os.path.exists(pth_path):\n        raise FileNotFoundError(f\"The file {pth_path} does not exist\")\n\n    # Load confs\n    with open(configuration_path, 'r', encoding='utf-8') as f:\n        configs = json.load(f)\n    # Can't set int as keys in json, so need to cast it after reloading\n    # dict_classes keys are always ints\n    if 'dict_classes' in configs.keys():\n        configs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n    elif 'list_classes' in configs.keys():\n        configs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n\n    # Set class vars\n    # self.model_name = # Keep the created name\n    # self.model_dir = # Keep the created folder\n    self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n    self.trained = configs.get('trained', True)  # Consider trained by default\n    # Try to read the following attributes from configs and, if absent, keep the current one\n    for attribute in ['model_type', 'validation_split', 'min_delta_es', 'patience', 'restore_best_weights',\n                      'list_classes', 'dict_classes', 'data_augmentation_params', 'level_save',\n                      'nb_log_write_per_epoch', 'epochs', 'nb_log_display_per_epoch', 'detectron_config_base_filename',\n                      'detectron_config_filename', 'detectron_model_filename', 'cfg']:\n        setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n\n    # Transform cfg into CfgNode\n    self.cfg = CfgNode(init_dict=self.cfg)\n\n    # Save best pth in new folder\n    new_pth_path = os.path.join(self.model_dir, 'best.pth')\n    shutil.copyfile(pth_path, new_pth_path)\n\n    # Reload model\n    self.cfg.MODEL.WEIGHTS = new_pth_path\n\n    # Change output path\n    self.cfg.OUTPUT_DIR = self.model_dir\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.ModelDetectronFasterRcnnObjectDetector.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save configuration JSON\n    if json_data is None:\n        json_data = {}\n    # Save attributes &amp; cfg (contains all params)\n    json_data['librairie'] = 'detectron2'\n    json_data['validation_split'] = self.validation_split\n    json_data['min_delta_es'] = self.min_delta_es\n    json_data['patience'] = self.patience\n    json_data['restore_best_weights'] = self.restore_best_weights\n    json_data['data_augmentation_params'] = self.data_augmentation_params\n    json_data['nb_log_write_per_epoch'] = self.nb_log_write_per_epoch\n    json_data['epochs'] = self.epochs\n    json_data['nb_log_display_per_epoch'] = self.nb_log_display_per_epoch\n    json_data['detectron_config_base_filename'] = self.detectron_config_base_filename\n    json_data['detectron_config_filename'] = self.detectron_config_filename\n    json_data['detectron_model_filename'] = self.detectron_model_filename\n    json_data['cfg'] = self.cfg\n\n    # We save le model with CPU so that there is no problem later\n    # when we use the model (with streamlit for example)\n    device = self.cfg.MODEL.DEVICE\n    self.cfg.MODEL.DEVICE = \"cpu\"\n    super().save(json_data=json_data)\n    # We undo what we just did\n    self.cfg.MODEL.DEVICE = device\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.TrainValJSONWriter","title":"<code>TrainValJSONWriter</code>","text":"<p>             Bases: <code>EventWriter</code></p> <p>Write scalars to a json file. It saves scalars as one json per line (instead of a big json) for easy parsing.</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>class TrainValJSONWriter(EventWriter):\n    '''Write scalars to a json file.\n    It saves scalars as one json per line (instead of a big json) for easy parsing.\n    '''\n\n    def __init__(self, json_file: str, length_epoch: int, nb_iter_per_epoch: int, nb_iter_log: int = 20) -&gt; None:\n        '''Initialization of the class\n\n        Args:\n            json_file (str): File where we save the results\n            length_epoch (int): Number of images in an \"epoch\"\n            nb_iter_per_epoch (int): Number of iterations in an \"epoch\"\n        Kwargs:\n            nb_iter_log (int): Number of iteration between two writes\n        '''\n        self.json_file = json_file\n        self.length_epoch = length_epoch\n        self.nb_iter_per_epoch = nb_iter_per_epoch\n        self.nb_iter_log = nb_iter_log\n        self.open()\n        self.close()\n\n    def open(self) -&gt; None:\n        self._file_handle = PathManager.open(self.json_file, \"a\")\n\n    def write(self) -&gt; None:\n        '''Saves the results'''\n        self.open()\n        storage = get_event_storage()\n        to_save = defaultdict(dict)\n        iteration = storage.iter\n        if (iteration + 1) % self.nb_iter_log == 0:\n            losses = [loss for loss in storage.histories() if 'loss' in loss]\n            losses_valid = [loss for loss in losses if 'validation' in loss]\n            losses_train = [loss for loss in losses if loss not in losses_valid]\n\n            for key in losses_train:\n                to_save[iteration][key] = storage.histories()[key].median(self.nb_iter_per_epoch)\n            for key in losses_valid:\n                to_save[iteration][key] = storage.histories()[key].latest()\n\n            for itr, scalars_per_iter in to_save.items():\n                scalars_per_iter[\"iteration\"] = itr\n                self._file_handle.write(json.dumps(scalars_per_iter, sort_keys=True) + \"\\n\")\n            self._file_handle.flush()\n            try:\n                os.fsync(self._file_handle.fileno())\n            except AttributeError:\n                pass\n        self.close()\n\n    def close(self):\n        '''Close the open file'''\n        self._file_handle.close()\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.TrainValJSONWriter.__init__","title":"<code>__init__(json_file, length_epoch, nb_iter_per_epoch, nb_iter_log=20)</code>","text":"<p>Initialization of the class</p> <p>Parameters:</p> Name Type Description Default <code>json_file</code> <code>str</code> <p>File where we save the results</p> required <code>length_epoch</code> <code>int</code> <p>Number of images in an \"epoch\"</p> required <code>nb_iter_per_epoch</code> <code>int</code> <p>Number of iterations in an \"epoch\"</p> required <p>Kwargs:     nb_iter_log (int): Number of iteration between two writes</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def __init__(self, json_file: str, length_epoch: int, nb_iter_per_epoch: int, nb_iter_log: int = 20) -&gt; None:\n    '''Initialization of the class\n\n    Args:\n        json_file (str): File where we save the results\n        length_epoch (int): Number of images in an \"epoch\"\n        nb_iter_per_epoch (int): Number of iterations in an \"epoch\"\n    Kwargs:\n        nb_iter_log (int): Number of iteration between two writes\n    '''\n    self.json_file = json_file\n    self.length_epoch = length_epoch\n    self.nb_iter_per_epoch = nb_iter_per_epoch\n    self.nb_iter_log = nb_iter_log\n    self.open()\n    self.close()\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.TrainValJSONWriter.close","title":"<code>close()</code>","text":"<p>Close the open file</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def close(self):\n    '''Close the open file'''\n    self._file_handle.close()\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.TrainValJSONWriter.write","title":"<code>write()</code>","text":"<p>Saves the results</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def write(self) -&gt; None:\n    '''Saves the results'''\n    self.open()\n    storage = get_event_storage()\n    to_save = defaultdict(dict)\n    iteration = storage.iter\n    if (iteration + 1) % self.nb_iter_log == 0:\n        losses = [loss for loss in storage.histories() if 'loss' in loss]\n        losses_valid = [loss for loss in losses if 'validation' in loss]\n        losses_train = [loss for loss in losses if loss not in losses_valid]\n\n        for key in losses_train:\n            to_save[iteration][key] = storage.histories()[key].median(self.nb_iter_per_epoch)\n        for key in losses_valid:\n            to_save[iteration][key] = storage.histories()[key].latest()\n\n        for itr, scalars_per_iter in to_save.items():\n            scalars_per_iter[\"iteration\"] = itr\n            self._file_handle.write(json.dumps(scalars_per_iter, sort_keys=True) + \"\\n\")\n        self._file_handle.flush()\n        try:\n            os.fsync(self._file_handle.fileno())\n        except AttributeError:\n            pass\n    self.close()\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.TrainValMetricPrinter","title":"<code>TrainValMetricPrinter</code>","text":"<p>             Bases: <code>EventWriter</code></p> <p>Takes care of displaying the metrics on the train (and also on the val)</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>class TrainValMetricPrinter(EventWriter):\n    '''Takes care of displaying the metrics on the train (and also on the val)'''\n\n    def __init__(self, cfg, with_valid: bool, length_epoch: int, nb_iter_per_epoch: int, nb_iter_log: int = 20) -&gt; None:\n        '''Initialize the class.\n\n        Args:\n            cfg: Model configuration\n            with valid (bool): If true, also displays the results on the validation dataset\n            length_epoch (int): Number of images in an \"epoch\"\n            nb_iter_per_epoch (int): Number of iterations in an \"epoch\"\n        Kwargs:\n            nb_iter_log (int): Number of iteration between two displays\n        '''\n        self.logger = logging.getLogger(__name__)\n        self.with_valid = with_valid\n        self.nb_iter_log = nb_iter_log\n        self.length_epoch = length_epoch\n        self.nb_iter_per_epoch = nb_iter_per_epoch\n        self.cfg = cfg\n\n    def write(self):\n        '''Prints the wanted info'''\n        storage = get_event_storage()\n        iteration = storage.iter\n        # Calculates a number of \"epoch\" (not necessarily an integer)\n        nb_epoch = ((iteration * self.cfg.SOLVER.IMS_PER_BATCH) + 1) / self.length_epoch\n        if (iteration + 1) % self.nb_iter_log == 0:\n            try:\n                lr = \"{:.5g}\".format(storage.history(\"lr\").latest())\n            except KeyError:\n                lr = \"N/A\"\n            if torch.cuda.is_available():\n                max_mem_mb = torch.cuda.max_memory_allocated() / 1024.0 / 1024.0\n            else:\n                max_mem_mb = None\n            # NOTE: max_mem is parsed by grep in \"dev/parse_results.sh\"\n            losses = [loss for loss in storage.histories() if 'loss' in loss]\n            losses_valid = [loss for loss in losses if 'validation' in loss]\n            losses_train = [loss for loss in losses if loss not in losses_valid]\n            # Logs train results\n            self.logger.info(\n                \" iter: {iter}, epoch: {nb_epoch}  {losses}  lr: {lr}  {memory}\".format(\n                    iter=iteration,\n                    losses=\"  \".join([\"{}: {:.4g}\".format(k, storage.histories()[k].median(self.nb_iter_per_epoch)) for k in losses_train]),\n                    lr=lr,\n                    memory=\"max_mem: {:.0f}M\".format(max_mem_mb) if max_mem_mb is not None else \"\",\n                    nb_epoch=nb_epoch\n                )\n            )\n            if self.with_valid:\n                # Logs val results\n                self.logger.info(\n                    \"VALIDATION iter: {iter}, epoch: {nb_epoch}  {losses}\".format(\n                        iter=iteration,\n                        losses=\"  \".join([\"{}: {:.4g}\".format(k, storage.histories()[k].latest()) for k in losses_valid]),\n                        nb_epoch=nb_epoch\n                    )\n                )\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.TrainValMetricPrinter.__init__","title":"<code>__init__(cfg, with_valid, length_epoch, nb_iter_per_epoch, nb_iter_log=20)</code>","text":"<p>Initialize the class.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <p>Model configuration</p> required <code>with</code> <code>valid (bool</code> <p>If true, also displays the results on the validation dataset</p> required <code>length_epoch</code> <code>int</code> <p>Number of images in an \"epoch\"</p> required <code>nb_iter_per_epoch</code> <code>int</code> <p>Number of iterations in an \"epoch\"</p> required <p>Kwargs:     nb_iter_log (int): Number of iteration between two displays</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def __init__(self, cfg, with_valid: bool, length_epoch: int, nb_iter_per_epoch: int, nb_iter_log: int = 20) -&gt; None:\n    '''Initialize the class.\n\n    Args:\n        cfg: Model configuration\n        with valid (bool): If true, also displays the results on the validation dataset\n        length_epoch (int): Number of images in an \"epoch\"\n        nb_iter_per_epoch (int): Number of iterations in an \"epoch\"\n    Kwargs:\n        nb_iter_log (int): Number of iteration between two displays\n    '''\n    self.logger = logging.getLogger(__name__)\n    self.with_valid = with_valid\n    self.nb_iter_log = nb_iter_log\n    self.length_epoch = length_epoch\n    self.nb_iter_per_epoch = nb_iter_per_epoch\n    self.cfg = cfg\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.TrainValMetricPrinter.write","title":"<code>write()</code>","text":"<p>Prints the wanted info</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def write(self):\n    '''Prints the wanted info'''\n    storage = get_event_storage()\n    iteration = storage.iter\n    # Calculates a number of \"epoch\" (not necessarily an integer)\n    nb_epoch = ((iteration * self.cfg.SOLVER.IMS_PER_BATCH) + 1) / self.length_epoch\n    if (iteration + 1) % self.nb_iter_log == 0:\n        try:\n            lr = \"{:.5g}\".format(storage.history(\"lr\").latest())\n        except KeyError:\n            lr = \"N/A\"\n        if torch.cuda.is_available():\n            max_mem_mb = torch.cuda.max_memory_allocated() / 1024.0 / 1024.0\n        else:\n            max_mem_mb = None\n        # NOTE: max_mem is parsed by grep in \"dev/parse_results.sh\"\n        losses = [loss for loss in storage.histories() if 'loss' in loss]\n        losses_valid = [loss for loss in losses if 'validation' in loss]\n        losses_train = [loss for loss in losses if loss not in losses_valid]\n        # Logs train results\n        self.logger.info(\n            \" iter: {iter}, epoch: {nb_epoch}  {losses}  lr: {lr}  {memory}\".format(\n                iter=iteration,\n                losses=\"  \".join([\"{}: {:.4g}\".format(k, storage.histories()[k].median(self.nb_iter_per_epoch)) for k in losses_train]),\n                lr=lr,\n                memory=\"max_mem: {:.0f}M\".format(max_mem_mb) if max_mem_mb is not None else \"\",\n                nb_epoch=nb_epoch\n            )\n        )\n        if self.with_valid:\n            # Logs val results\n            self.logger.info(\n                \"VALIDATION iter: {iter}, epoch: {nb_epoch}  {losses}\".format(\n                    iter=iteration,\n                    losses=\"  \".join([\"{}: {:.4g}\".format(k, storage.histories()[k].latest()) for k in losses_valid]),\n                    nb_epoch=nb_epoch\n                )\n            )\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.TrainerRCNN","title":"<code>TrainerRCNN</code>","text":"<p>             Bases: <code>DefaultTrainer</code></p> <p>We overload the class DefaultTraine in order to: - change when we save the metrics - use the COCOevaluator - do data augmentation - save the validation metrics when training - do early stopping</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>class TrainerRCNN(DefaultTrainer):\n    '''We overload the class DefaultTraine in order to:\n        - change when we save the metrics\n        - use the COCOevaluator\n        - do data augmentation\n        - save the validation metrics when training\n        - do early stopping\n\n    '''\n    # We define a class attribute because it is used by a class method\n    data_augmentation_params = {}\n\n    def __init__(self, cfg, length_epoch: int, nb_iter_per_epoch: int,\n                 nb_iter_log_write: int = 20, nb_iter_log_display: int = 20, nb_log_write_per_epoch: int = 1,\n                 min_delta_es: float = 0., patience: int = 0, restore_best_weights: bool = False) -&gt; None:\n        '''Initialize the Trainer\n\n        Args:\n            cfg: Configuration to use\n            length_epoch (int): Number of image in an epoch\n            nb_iter_per_epoch (int): Number of iterations in an epoch\n        Kwargs:\n            nb_iter_log_write (int): Number of iterations between two log writes (losses\n                on train and validation datasets)\n            nb_iter_log_display (int): Number of iterations between two log displays (losses\n                on train dataset only)\n            nb_log_write_per_epoch (int): Number of metrics logs written during\n                one epoch (losses for the train and the valid)\n            min_delta_es (float): Minimal change in losses to be considered an amelioration for early stopping\n            patience (int): Early stopping patience. Put to 0 to disable early stopping\n            restore_best_weights (bool): If True, when the training is done, save the model with the best\n                loss on the validation dataset instead of the last model (even if early stopping is disabled)\n        '''\n        # We must add the definition of some attributes before the super() because we redefine the method\n        # build_hooks which is called by super().__init__ and uses them\n        self.nb_iter_log_write = nb_iter_log_write\n        self.nb_iter_log_display = nb_iter_log_display\n        self.length_epoch = length_epoch\n        self.output_dir = cfg.OUTPUT_DIR\n        self.nb_iter_per_epoch = nb_iter_per_epoch\n        super().__init__(cfg)\n\n        # Params early stopping\n        self.min_delta_es = min_delta_es\n        self.patience = patience\n        self.restore_best_weights = restore_best_weights\n        self.best_loss = np.inf\n        self.best_epoch = 0\n\n        # Misc.\n        self.nb_log_write_per_epoch = nb_log_write_per_epoch\n\n    @classmethod\n    def build_evaluator(self, cfg, dataset_name: str) -&gt; COCOEvaluator:\n        '''We redefine the method in order to use the COCOevaluator\n\n        Args:\n            cfg: Training configuration\n            dataset_name (str): Name of the dataset\n        Returns:\n            Evaluator to use\n        '''\n        output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n        return COCOEvaluator(dataset_name, cfg, True, output_folder)\n\n    @classmethod\n    def build_train_loader(self, cfg):\n        '''We redefine the method in order to use our own data augmentation\n\n        Args:\n            cfg: Training configuration\n        Returns:\n            train loader to use, with our own data augmentation\n        '''\n        horizontal_flip = self.data_augmentation_params.get('horizontal_flip', False)\n        vertical_flip = self.data_augmentation_params.get('vertical_flip', False)\n        rot_90 = self.data_augmentation_params.get('rot_90', False)\n        mapper = partial(data_augmentation_mapper, horizontal_flip=horizontal_flip, vertical_flip=vertical_flip, rot_90=rot_90)\n        return build_detection_train_loader(cfg, mapper=mapper)\n\n    def train(self):\n        '''Run training.\n\n        Raises:\n            AttributeError : No evaluation results obtained during training!\n        Returns:\n            OrderedDict of results, if evaluation is enabled. Otherwise None.\n        '''\n        logger = logging.getLogger(__name__)\n        logger.info(\"Starting training from iteration {}\".format(self.start_iter))\n        self.iter = self.start_iter\n\n        # From https://detectron2.readthedocs.io/en/latest/_modules/detectron2/engine/train_loop.html#TrainerBase\n        # The difference is that we get the result of the early stopping and we stop if triggered\n        # We also save the final model if restore_best_weights is set to False\n        with EventStorage(self.start_iter) as self.storage:\n            try:\n                self.before_train()\n                while self.iter &lt; self.max_iter:  # We substitute the for by a while in order to break easily with the early stopping\n                    self.before_step()\n                    self.run_step()\n                    test_early_stopping = self.after_step()\n                    if test_early_stopping:\n                        logger.info(\"Early stopping\")\n                        # We change the number of maximum iteration if we want to stop early\n                        # Warning, some hooks are defined with anoter self.max_iter\n                        # since build_hooks is called by the __init__ of the trainer\n                        self.max_iter = self.iter\n                    else:\n                        self.iter += 1\n                # self.iter == self.max_iter can be used by `after_train` to\n                # tell whether the training successfully finished or failed\n                # due to exceptions.\n\n            except Exception:\n                logger.exception(\"Exception during training:\")\n                raise\n            finally:\n                self.after_train()\n                self.write_model_final()  # * NEW *\n\n        # From https://detectron2.readthedocs.io/en/latest/_modules/detectron2/engine/defaults.html#DefaultTrainer\n        if len(self.cfg.TEST.EXPECTED_RESULTS) and comm.is_main_process():\n            if not hasattr(self, \"_last_eval_results\"):\n                raise AttributeError(\"No evaluation results obtained during training!\")\n            verify_results(self.cfg, self._last_eval_results)\n            return self._last_eval_results\n\n    def write_model_final(self) -&gt; None:\n        '''If self.restore_best_weights == False, no model is saved during\n        training. Thus we save the final model with the name best.pth\n        '''\n\n        path_model_best = os.path.join(self.cfg.OUTPUT_DIR, 'best.pth')\n        if not os.path.exists(path_model_best):\n            self.checkpointer.save(\"best\")\n\n    def after_step(self) -&gt; bool:\n        '''Function triggered after each step\n\n        Returns:\n            bool: If early stopping has been triggered\n        '''\n        # We trigger the hooks\n        for h in self._hooks:\n            h.after_step()\n        # We add the early stopping to the hooks\n        test_early_stopping = self.early_stopping()\n        return test_early_stopping\n\n    def early_stopping(self) -&gt; bool:\n        '''Triggers if the condition for early stopping are met. We think in term of epoch. Thus\n        the patience is indeed the number of epochs without amelioration\n\n        Returns:\n            bool: If early stopping has been triggered\n        '''\n        # We get all the validation losses\n        val_loss_values = self.storage.histories()['validation_total_loss'].values()\n        # We only keep those that are at the end of an epoch\n        val_loss_values_epoch = val_loss_values[self.nb_log_write_per_epoch-1::self.nb_log_write_per_epoch]\n        val_loss_values_epoch = [x[0] for x in val_loss_values_epoch]\n        if len(val_loss_values_epoch):\n            # We get the minimal loss and the corresponding epoch\n            min_loss = np.min(val_loss_values_epoch)\n            min_epoch = np.argmin(val_loss_values_epoch)\n            # If the loss is better, we save the new minimal loss and the corresponding epoch\n            if min_loss &lt; self.best_loss - self.min_delta_es:\n                self.best_loss = min_loss\n                self.best_epoch = min_epoch\n                # We save the model with the best weights\n                if self.restore_best_weights:\n                    self.checkpointer.save(\"best\")\n            # If the patience is up, we trigger early stopping\n            if self.best_epoch + self.patience + 1 &lt;= len(val_loss_values_epoch) and self.patience &gt; 0:\n                return True\n        # Otherwise we do not trigger it\n        return False\n\n    def build_hooks(self) -&gt; list:\n        '''Build a list of default hooks, including timing, evaluation,\n        checkpointing, lr scheduling, precise BN, writing events.\n\n        We rewrite this methos (instead of overloading it) so that we can change\n        when we save metrics\n        From : https://detectron2.readthedocs.io/en/latest/_modules/detectron2/engine/defaults.html#DefaultTrainer\n\n        Warning, we deleted the hook on the checkpoint, the early stopping hook takes care of it now !\n\n        Returns:\n            list[HookBase]:\n        '''\n        cfg = self.cfg.clone()\n        cfg.defrost()\n        cfg.DATALOADER.NUM_WORKERS = 0  # save some memory and time for PreciseBN\n\n        ret = [\n            module_hooks.IterationTimer(),\n            module_hooks.LRScheduler()\n        ]\n\n        # We add our custom hook in order to add validation losses\n        ret.append(LossEvalHook(\n            self.nb_iter_log_write,\n            self.model,\n            build_detection_test_loader(  # Use of our decorator of build_detection_test_loader\n                self.cfg,\n                self.cfg.DATASETS.TEST[0],\n                DatasetMapper(self.cfg, True)  # We keep is_train to true to keep the bboxes (needed for losses calculations)\n            )\n        ))\n\n        if comm.is_main_process():\n            # Here we take care of the writers and the printers\n            # There is a printer which displays the results of train and val when we reach\n            # a particular number of iteration (self.nb_iter_log_write)\n            # There is a writer which writes the results of train and val when we reach\n            # a particular number of iteration (self.nb_iter_log_write)\n            ret.append(module_hooks.PeriodicWriter([TrainValMetricPrinter(cfg=self.cfg,\n                                                                          with_valid=True,\n                                                                          length_epoch=self.length_epoch,\n                                                                          nb_iter_per_epoch=self.nb_iter_per_epoch,\n                                                                          nb_iter_log=self.nb_iter_log_write),\n                                                    TrainValJSONWriter(os.path.join(self.output_dir, \"metrics.json\"),\n                                                                       self.length_epoch,\n                                                                       self.nb_iter_per_epoch,\n                                                                       self.nb_iter_log_write)\n                                                    ], period=self.nb_iter_log_write))\n            # There is a printer which displays the results of train when we reach\n            # a particular number of iteration (self.nb_iter_log_display)\n            ret.append(module_hooks.PeriodicWriter([TrainValMetricPrinter(cfg=self.cfg,\n                                                                          with_valid=False,\n                                                                          length_epoch=self.length_epoch,\n                                                                          nb_iter_per_epoch=self.nb_iter_per_epoch,\n                                                                          nb_iter_log=self.nb_iter_log_display)], period=1))\n        return ret\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.TrainerRCNN.__init__","title":"<code>__init__(cfg, length_epoch, nb_iter_per_epoch, nb_iter_log_write=20, nb_iter_log_display=20, nb_log_write_per_epoch=1, min_delta_es=0.0, patience=0, restore_best_weights=False)</code>","text":"<p>Initialize the Trainer</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <p>Configuration to use</p> required <code>length_epoch</code> <code>int</code> <p>Number of image in an epoch</p> required <code>nb_iter_per_epoch</code> <code>int</code> <p>Number of iterations in an epoch</p> required <p>Kwargs:     nb_iter_log_write (int): Number of iterations between two log writes (losses         on train and validation datasets)     nb_iter_log_display (int): Number of iterations between two log displays (losses         on train dataset only)     nb_log_write_per_epoch (int): Number of metrics logs written during         one epoch (losses for the train and the valid)     min_delta_es (float): Minimal change in losses to be considered an amelioration for early stopping     patience (int): Early stopping patience. Put to 0 to disable early stopping     restore_best_weights (bool): If True, when the training is done, save the model with the best         loss on the validation dataset instead of the last model (even if early stopping is disabled)</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def __init__(self, cfg, length_epoch: int, nb_iter_per_epoch: int,\n             nb_iter_log_write: int = 20, nb_iter_log_display: int = 20, nb_log_write_per_epoch: int = 1,\n             min_delta_es: float = 0., patience: int = 0, restore_best_weights: bool = False) -&gt; None:\n    '''Initialize the Trainer\n\n    Args:\n        cfg: Configuration to use\n        length_epoch (int): Number of image in an epoch\n        nb_iter_per_epoch (int): Number of iterations in an epoch\n    Kwargs:\n        nb_iter_log_write (int): Number of iterations between two log writes (losses\n            on train and validation datasets)\n        nb_iter_log_display (int): Number of iterations between two log displays (losses\n            on train dataset only)\n        nb_log_write_per_epoch (int): Number of metrics logs written during\n            one epoch (losses for the train and the valid)\n        min_delta_es (float): Minimal change in losses to be considered an amelioration for early stopping\n        patience (int): Early stopping patience. Put to 0 to disable early stopping\n        restore_best_weights (bool): If True, when the training is done, save the model with the best\n            loss on the validation dataset instead of the last model (even if early stopping is disabled)\n    '''\n    # We must add the definition of some attributes before the super() because we redefine the method\n    # build_hooks which is called by super().__init__ and uses them\n    self.nb_iter_log_write = nb_iter_log_write\n    self.nb_iter_log_display = nb_iter_log_display\n    self.length_epoch = length_epoch\n    self.output_dir = cfg.OUTPUT_DIR\n    self.nb_iter_per_epoch = nb_iter_per_epoch\n    super().__init__(cfg)\n\n    # Params early stopping\n    self.min_delta_es = min_delta_es\n    self.patience = patience\n    self.restore_best_weights = restore_best_weights\n    self.best_loss = np.inf\n    self.best_epoch = 0\n\n    # Misc.\n    self.nb_log_write_per_epoch = nb_log_write_per_epoch\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.TrainerRCNN.after_step","title":"<code>after_step()</code>","text":"<p>Function triggered after each step</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>If early stopping has been triggered</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def after_step(self) -&gt; bool:\n    '''Function triggered after each step\n\n    Returns:\n        bool: If early stopping has been triggered\n    '''\n    # We trigger the hooks\n    for h in self._hooks:\n        h.after_step()\n    # We add the early stopping to the hooks\n    test_early_stopping = self.early_stopping()\n    return test_early_stopping\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.TrainerRCNN.build_evaluator","title":"<code>build_evaluator(cfg, dataset_name)</code>  <code>classmethod</code>","text":"<p>We redefine the method in order to use the COCOevaluator</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <p>Training configuration</p> required <code>dataset_name</code> <code>str</code> <p>Name of the dataset</p> required <p>Returns:     Evaluator to use</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>@classmethod\ndef build_evaluator(self, cfg, dataset_name: str) -&gt; COCOEvaluator:\n    '''We redefine the method in order to use the COCOevaluator\n\n    Args:\n        cfg: Training configuration\n        dataset_name (str): Name of the dataset\n    Returns:\n        Evaluator to use\n    '''\n    output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n    return COCOEvaluator(dataset_name, cfg, True, output_folder)\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.TrainerRCNN.build_hooks","title":"<code>build_hooks()</code>","text":"<p>Build a list of default hooks, including timing, evaluation, checkpointing, lr scheduling, precise BN, writing events.</p> <p>We rewrite this methos (instead of overloading it) so that we can change when we save metrics From : https://detectron2.readthedocs.io/en/latest/_modules/detectron2/engine/defaults.html#DefaultTrainer</p> <p>Warning, we deleted the hook on the checkpoint, the early stopping hook takes care of it now !</p> <p>Returns:</p> Type Description <code>list</code> <p>list[HookBase]:</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def build_hooks(self) -&gt; list:\n    '''Build a list of default hooks, including timing, evaluation,\n    checkpointing, lr scheduling, precise BN, writing events.\n\n    We rewrite this methos (instead of overloading it) so that we can change\n    when we save metrics\n    From : https://detectron2.readthedocs.io/en/latest/_modules/detectron2/engine/defaults.html#DefaultTrainer\n\n    Warning, we deleted the hook on the checkpoint, the early stopping hook takes care of it now !\n\n    Returns:\n        list[HookBase]:\n    '''\n    cfg = self.cfg.clone()\n    cfg.defrost()\n    cfg.DATALOADER.NUM_WORKERS = 0  # save some memory and time for PreciseBN\n\n    ret = [\n        module_hooks.IterationTimer(),\n        module_hooks.LRScheduler()\n    ]\n\n    # We add our custom hook in order to add validation losses\n    ret.append(LossEvalHook(\n        self.nb_iter_log_write,\n        self.model,\n        build_detection_test_loader(  # Use of our decorator of build_detection_test_loader\n            self.cfg,\n            self.cfg.DATASETS.TEST[0],\n            DatasetMapper(self.cfg, True)  # We keep is_train to true to keep the bboxes (needed for losses calculations)\n        )\n    ))\n\n    if comm.is_main_process():\n        # Here we take care of the writers and the printers\n        # There is a printer which displays the results of train and val when we reach\n        # a particular number of iteration (self.nb_iter_log_write)\n        # There is a writer which writes the results of train and val when we reach\n        # a particular number of iteration (self.nb_iter_log_write)\n        ret.append(module_hooks.PeriodicWriter([TrainValMetricPrinter(cfg=self.cfg,\n                                                                      with_valid=True,\n                                                                      length_epoch=self.length_epoch,\n                                                                      nb_iter_per_epoch=self.nb_iter_per_epoch,\n                                                                      nb_iter_log=self.nb_iter_log_write),\n                                                TrainValJSONWriter(os.path.join(self.output_dir, \"metrics.json\"),\n                                                                   self.length_epoch,\n                                                                   self.nb_iter_per_epoch,\n                                                                   self.nb_iter_log_write)\n                                                ], period=self.nb_iter_log_write))\n        # There is a printer which displays the results of train when we reach\n        # a particular number of iteration (self.nb_iter_log_display)\n        ret.append(module_hooks.PeriodicWriter([TrainValMetricPrinter(cfg=self.cfg,\n                                                                      with_valid=False,\n                                                                      length_epoch=self.length_epoch,\n                                                                      nb_iter_per_epoch=self.nb_iter_per_epoch,\n                                                                      nb_iter_log=self.nb_iter_log_display)], period=1))\n    return ret\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.TrainerRCNN.build_train_loader","title":"<code>build_train_loader(cfg)</code>  <code>classmethod</code>","text":"<p>We redefine the method in order to use our own data augmentation</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <p>Training configuration</p> required <p>Returns:     train loader to use, with our own data augmentation</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>@classmethod\ndef build_train_loader(self, cfg):\n    '''We redefine the method in order to use our own data augmentation\n\n    Args:\n        cfg: Training configuration\n    Returns:\n        train loader to use, with our own data augmentation\n    '''\n    horizontal_flip = self.data_augmentation_params.get('horizontal_flip', False)\n    vertical_flip = self.data_augmentation_params.get('vertical_flip', False)\n    rot_90 = self.data_augmentation_params.get('rot_90', False)\n    mapper = partial(data_augmentation_mapper, horizontal_flip=horizontal_flip, vertical_flip=vertical_flip, rot_90=rot_90)\n    return build_detection_train_loader(cfg, mapper=mapper)\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.TrainerRCNN.early_stopping","title":"<code>early_stopping()</code>","text":"<p>Triggers if the condition for early stopping are met. We think in term of epoch. Thus the patience is indeed the number of epochs without amelioration</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>If early stopping has been triggered</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def early_stopping(self) -&gt; bool:\n    '''Triggers if the condition for early stopping are met. We think in term of epoch. Thus\n    the patience is indeed the number of epochs without amelioration\n\n    Returns:\n        bool: If early stopping has been triggered\n    '''\n    # We get all the validation losses\n    val_loss_values = self.storage.histories()['validation_total_loss'].values()\n    # We only keep those that are at the end of an epoch\n    val_loss_values_epoch = val_loss_values[self.nb_log_write_per_epoch-1::self.nb_log_write_per_epoch]\n    val_loss_values_epoch = [x[0] for x in val_loss_values_epoch]\n    if len(val_loss_values_epoch):\n        # We get the minimal loss and the corresponding epoch\n        min_loss = np.min(val_loss_values_epoch)\n        min_epoch = np.argmin(val_loss_values_epoch)\n        # If the loss is better, we save the new minimal loss and the corresponding epoch\n        if min_loss &lt; self.best_loss - self.min_delta_es:\n            self.best_loss = min_loss\n            self.best_epoch = min_epoch\n            # We save the model with the best weights\n            if self.restore_best_weights:\n                self.checkpointer.save(\"best\")\n        # If the patience is up, we trigger early stopping\n        if self.best_epoch + self.patience + 1 &lt;= len(val_loss_values_epoch) and self.patience &gt; 0:\n            return True\n    # Otherwise we do not trigger it\n    return False\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.TrainerRCNN.train","title":"<code>train()</code>","text":"<p>Run training.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>No evaluation results obtained during training!</p> <p>Returns:     OrderedDict of results, if evaluation is enabled. Otherwise None.</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def train(self):\n    '''Run training.\n\n    Raises:\n        AttributeError : No evaluation results obtained during training!\n    Returns:\n        OrderedDict of results, if evaluation is enabled. Otherwise None.\n    '''\n    logger = logging.getLogger(__name__)\n    logger.info(\"Starting training from iteration {}\".format(self.start_iter))\n    self.iter = self.start_iter\n\n    # From https://detectron2.readthedocs.io/en/latest/_modules/detectron2/engine/train_loop.html#TrainerBase\n    # The difference is that we get the result of the early stopping and we stop if triggered\n    # We also save the final model if restore_best_weights is set to False\n    with EventStorage(self.start_iter) as self.storage:\n        try:\n            self.before_train()\n            while self.iter &lt; self.max_iter:  # We substitute the for by a while in order to break easily with the early stopping\n                self.before_step()\n                self.run_step()\n                test_early_stopping = self.after_step()\n                if test_early_stopping:\n                    logger.info(\"Early stopping\")\n                    # We change the number of maximum iteration if we want to stop early\n                    # Warning, some hooks are defined with anoter self.max_iter\n                    # since build_hooks is called by the __init__ of the trainer\n                    self.max_iter = self.iter\n                else:\n                    self.iter += 1\n            # self.iter == self.max_iter can be used by `after_train` to\n            # tell whether the training successfully finished or failed\n            # due to exceptions.\n\n        except Exception:\n            logger.exception(\"Exception during training:\")\n            raise\n        finally:\n            self.after_train()\n            self.write_model_final()  # * NEW *\n\n    # From https://detectron2.readthedocs.io/en/latest/_modules/detectron2/engine/defaults.html#DefaultTrainer\n    if len(self.cfg.TEST.EXPECTED_RESULTS) and comm.is_main_process():\n        if not hasattr(self, \"_last_eval_results\"):\n            raise AttributeError(\"No evaluation results obtained during training!\")\n        verify_results(self.cfg, self._last_eval_results)\n        return self._last_eval_results\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.TrainerRCNN.write_model_final","title":"<code>write_model_final()</code>","text":"<p>If self.restore_best_weights == False, no model is saved during training. Thus we save the final model with the name best.pth</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def write_model_final(self) -&gt; None:\n    '''If self.restore_best_weights == False, no model is saved during\n    training. Thus we save the final model with the name best.pth\n    '''\n\n    path_model_best = os.path.join(self.cfg.OUTPUT_DIR, 'best.pth')\n    if not os.path.exists(path_model_best):\n        self.checkpointer.save(\"best\")\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.data_augmentation_mapper","title":"<code>data_augmentation_mapper(dataset_dict, horizontal_flip=False, vertical_flip=False, rot_90=False)</code>","text":"<p>Applies the data augmentation on data</p> <p>Parameters:</p> Name Type Description Default <code>dataset_dict</code> <code>dict) </code> <p>Data dictionary containing the images on which to do data augmentation</p> required <code>horizontal_flip</code> <code>bool) </code> <p>If True, can do horizontal flip (with 0.5 proba)</p> <code>False</code> <code>vertical_flip</code> <code>bool) </code> <p>If True, can do vertical flip (with 0.5 proba)</p> <code>False</code> <code>rot_90</code> <code>bool) </code> <p>If True, can do a rotation of 0, 90, 180 or 270 degrees (0.25 proba for each)</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>The dictionary after data augmentation</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def data_augmentation_mapper(dataset_dict: dict, horizontal_flip: bool = False,\n                             vertical_flip: bool = False, rot_90: bool = False) -&gt; dict:\n    '''Applies the data augmentation on data\n\n    Args:\n        dataset_dict (dict) : Data dictionary containing the images on which to do data augmentation\n        horizontal_flip (bool) : If True, can do horizontal flip (with 0.5 proba)\n        vertical_flip (bool) : If True, can do vertical flip (with 0.5 proba)\n        rot_90 (bool) : If True, can do a rotation of 0, 90, 180 or 270 degrees (0.25 proba for each)\n\n    Returns:\n        The dictionary after data augmentation\n    '''\n    dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n    image = detection_utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")  # Reads the image\n\n    # Add transformations\n    transform_list = []\n    if rot_90:\n        angle = np.random.choice([0, 90, 180, 270], 1)[0]\n        transform_list.append(T.RandomRotation(angle=[angle, angle]))\n    if horizontal_flip:\n        transform_list.append(T.RandomFlip(prob=0.5, horizontal=True, vertical=False))\n    if vertical_flip:\n        transform_list.append(T.RandomFlip(prob=0.5, horizontal=False, vertical=True))\n\n    # Apply transformations to the image\n    image, transforms = T.apply_transform_gens(transform_list, image)\n    dataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1).astype(\"float32\"))\n    dataset_dict['height'], dataset_dict['width'] = dataset_dict['image'].shape[1:]\n\n    # Update of the bboxes\n    annos = [\n        detection_utils.transform_instance_annotations(obj, transforms, image.shape[:2])\n        for obj in dataset_dict.pop(\"annotations\")\n        if obj.get(\"iscrowd\", 0) == 0\n    ]\n    # Transform to \"instance\" (suitable format for detectron2)\n    instances = detection_utils.annotations_to_instances(annos, image.shape[:2])\n    dataset_dict[\"instances\"] = detection_utils.filter_empty_instances(instances)\n    # Return\n    return dataset_dict\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_keras_faster_rcnn/","title":"Model keras faster rcnn","text":""},{"location":"reference/template_vision/models_training/object_detectors/model_keras_faster_rcnn/#template_vision.models_training.object_detectors.model_keras_faster_rcnn.CustomGeneratorClassifier","title":"<code>CustomGeneratorClassifier</code>","text":"<p>             Bases: <code>Iterator</code></p> <p>Classifier generator</p> Source code in <code>template_vision/models_training/object_detectors/model_keras_faster_rcnn.py</code> <pre><code>class CustomGeneratorClassifier(Iterator):\n    '''Classifier generator'''\n\n    def __init__(self, img_data_list: List[dict], batch_size: int, shuffle: bool, seed: Union[int, None],\n                 model, shared_model_trainable: bool = False, horizontal_flip: bool = False, vertical_flip: bool = False,\n                 rot_90: bool = False, data_type: str = 'train', with_img_data: bool = False, **kwargs) -&gt; None:\n        '''Initialization of the generator for the classifier\n\n        Args:\n            img_data_list (list&lt;dict&gt;): Data list (the dictionaries containing file path and bboxes)\n            batch_size (int): Size of the batches to generate\n            shuffle (bool): If the data should be shuffled\n            seed (int): Random seed to use\n            model: Link to the model (nested structure) to access to the other methods of this script\n            shared_model_trainable (bool): If the shared model is set to trainable, we must clone the RPN\n                in order not to worsen the prediction quality of the ROIs (input of the classifier)\n            data_type (str): Data type 'train', 'valid' or 'test'\n            with_img_data (bool): If True, also gives img_data as output\n        Kwargs:\n            horizontal_flip (bool) : If True, can do horizontal flip (with 0.5 proba)\n            vertical_flip (bool) : If True, can do vertical flip (with 0.5 proba)\n            rot_90 (bool) : If True, can do a rotation of 0, 90, 180 or 270 degrees (0.25 proba for each)\n        '''\n        # Set is_test\n        if data_type == 'test':\n            self.is_test = True\n        else:\n            self.is_test = False\n\n        # Si test, on force batch size \u00e0 1\n        if self.is_test:\n            batch_size = 1\n\n        # Super init.\n        super().__init__(n=len(img_data_list), batch_size=batch_size, shuffle=shuffle, seed=seed)\n\n        # Set params\n        self.img_data_list = img_data_list\n        self.model = model\n        self.horizontal_flip = horizontal_flip\n        self.vertical_flip = vertical_flip\n        self.rot_90 = rot_90\n        self.with_img_data = with_img_data\n\n        # Manage shared_model_trainable\n        self.shared_model_trainable = shared_model_trainable\n        if self.shared_model_trainable:\n            self.rpn_clone = clone_model(self.model.model_rpn)\n            self.rpn_clone.set_weights(self.model.model_rpn.get_weights())\n        else:\n            self.rpn_clone = None\n\n        # Manage data augmentation &amp; test\n        if self.is_test and any([param for param in [self.horizontal_flip, self.vertical_flip, self.rot_90]]):\n            model.logger.warning(\"Warning, Data Augmentation detected on the test set! This is certainly not desired!\")\n\n    def _get_batches_of_transformed_samples(self, index_array: np.ndarray):\n        '''Gets a batch of inputs for the classifier model\n        Warning, in order to be sure to have the same shape between the various images, we pad them with black pixels\n\n        Args:\n            index_array (np.ndarray): List of indices to include in a batch\n        Returns:\n            np.ndarray: Data batch to be used by the classifier model (x)\n                # Shape (bach_size, max_resized_height, max_resized_width, 3)\n            dict&lt;np.ndarray&gt;: Data batch to be used by the classifier model (y)\n                dense_class: Classifier - target of the classifier\n                    # Shape (bach_size, feature_map_height, feature_map_width, 2 * nb_anchors)\n                rpn_class: Classifier - target of the regressor\n                    # Shape (bach_size, feature_map_height, feature_map_width, 2 * 4 * nb_anchors)\n            if self.with_img_data : the metadata of the images\n        '''\n        # For each selected index, get image &amp; preprocess it, and retrieve max resized_width &amp; resized_height\n        batch_prepared_img_data = []\n        max_resized_width = 0\n        max_resized_height = 0\n        for ind in index_array:\n            img_data = copy.deepcopy(self.img_data_list[ind])\n            prepared_img_data = self.model._generate_images_with_bboxes(img_data, horizontal_flip=self.horizontal_flip,\n                                                                        vertical_flip=self.vertical_flip, rot_90=self.rot_90)\n            batch_prepared_img_data.append(prepared_img_data)\n            max_resized_width = max(max_resized_width, prepared_img_data['resized_width'])\n            max_resized_height = max(max_resized_height, prepared_img_data['resized_height'])\n\n        # Set format X images\n        batch_img_shape = (len(index_array), max_resized_height, max_resized_width, 3)\n        batch_x_img = np.zeros(batch_img_shape)  # Def. pixels noirs\n\n        # Get input images\n        for ind, prepared_img_data in enumerate(batch_prepared_img_data):\n            prepared_img_data['batch_width'] = max_resized_width\n            prepared_img_data['batch_height'] = max_resized_height\n            img = prepared_img_data['img']\n            batch_x_img[ind, :img.shape[0], :img.shape[1], :] = img\n\n        # Get the input ROIs - need to be formatted\n        if self.shared_model_trainable:\n            rpn_predictions_cls, rpn_predictions_regr = self.rpn_clone.predict(batch_x_img)\n        else:\n            rpn_predictions_cls, rpn_predictions_regr = self.model.model_rpn.predict(batch_x_img)\n        rois_coordinates = utils_object_detectors.get_roi_from_rpn_predictions(self.model, batch_prepared_img_data,\n                                                                               rpn_predictions_cls, rpn_predictions_regr)\n\n        # If test, we return the images and the associated ROIs\n        if self.is_test:\n            # Get the ROIs with the right format\n            batch_x_rois = utils_object_detectors.get_classifier_test_inputs(rois_coordinates)\n            if self.with_img_data:\n                return {'input_img': batch_x_img, 'input_rois': batch_x_rois}, batch_prepared_img_data\n            else:  # Usually, this case never happens\n                return {'input_img': batch_x_img, 'input_rois': batch_x_rois}\n        # Otherwise, we also manage the targets y\n        else:\n            # Get the inputs and the targets of the classifier\n            batch_x_rois, Y1_classifier, Y2_classifier = utils_object_detectors.get_classifier_train_inputs_and_targets(self.model, batch_prepared_img_data, rois_coordinates)\n            if batch_x_rois is None:\n                self.model.logger.warning(\"We have an image batch without ROI !!!\")\n                return next(self)  # We try another batch ...\n            if self.with_img_data:\n                return {'input_img': batch_x_img, 'input_rois': batch_x_rois}, {'dense_class': Y1_classifier, 'dense_regr': Y2_classifier}, batch_prepared_img_data\n            else:\n                return {'input_img': batch_x_img, 'input_rois': batch_x_rois}, {'dense_class': Y1_classifier, 'dense_regr': Y2_classifier}\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_keras_faster_rcnn/#template_vision.models_training.object_detectors.model_keras_faster_rcnn.CustomGeneratorClassifier.__init__","title":"<code>__init__(img_data_list, batch_size, shuffle, seed, model, shared_model_trainable=False, horizontal_flip=False, vertical_flip=False, rot_90=False, data_type='train', with_img_data=False, **kwargs)</code>","text":"<p>Initialization of the generator for the classifier</p> <p>Parameters:</p> Name Type Description Default <code>img_data_list</code> <code>list&lt;dict&gt;</code> <p>Data list (the dictionaries containing file path and bboxes)</p> required <code>batch_size</code> <code>int</code> <p>Size of the batches to generate</p> required <code>shuffle</code> <code>bool</code> <p>If the data should be shuffled</p> required <code>seed</code> <code>int</code> <p>Random seed to use</p> required <code>model</code> <p>Link to the model (nested structure) to access to the other methods of this script</p> required <code>shared_model_trainable</code> <code>bool</code> <p>If the shared model is set to trainable, we must clone the RPN in order not to worsen the prediction quality of the ROIs (input of the classifier)</p> <code>False</code> <code>data_type</code> <code>str</code> <p>Data type 'train', 'valid' or 'test'</p> <code>'train'</code> <code>with_img_data</code> <code>bool</code> <p>If True, also gives img_data as output</p> <code>False</code> <p>Kwargs:     horizontal_flip (bool) : If True, can do horizontal flip (with 0.5 proba)     vertical_flip (bool) : If True, can do vertical flip (with 0.5 proba)     rot_90 (bool) : If True, can do a rotation of 0, 90, 180 or 270 degrees (0.25 proba for each)</p> Source code in <code>template_vision/models_training/object_detectors/model_keras_faster_rcnn.py</code> <pre><code>def __init__(self, img_data_list: List[dict], batch_size: int, shuffle: bool, seed: Union[int, None],\n             model, shared_model_trainable: bool = False, horizontal_flip: bool = False, vertical_flip: bool = False,\n             rot_90: bool = False, data_type: str = 'train', with_img_data: bool = False, **kwargs) -&gt; None:\n    '''Initialization of the generator for the classifier\n\n    Args:\n        img_data_list (list&lt;dict&gt;): Data list (the dictionaries containing file path and bboxes)\n        batch_size (int): Size of the batches to generate\n        shuffle (bool): If the data should be shuffled\n        seed (int): Random seed to use\n        model: Link to the model (nested structure) to access to the other methods of this script\n        shared_model_trainable (bool): If the shared model is set to trainable, we must clone the RPN\n            in order not to worsen the prediction quality of the ROIs (input of the classifier)\n        data_type (str): Data type 'train', 'valid' or 'test'\n        with_img_data (bool): If True, also gives img_data as output\n    Kwargs:\n        horizontal_flip (bool) : If True, can do horizontal flip (with 0.5 proba)\n        vertical_flip (bool) : If True, can do vertical flip (with 0.5 proba)\n        rot_90 (bool) : If True, can do a rotation of 0, 90, 180 or 270 degrees (0.25 proba for each)\n    '''\n    # Set is_test\n    if data_type == 'test':\n        self.is_test = True\n    else:\n        self.is_test = False\n\n    # Si test, on force batch size \u00e0 1\n    if self.is_test:\n        batch_size = 1\n\n    # Super init.\n    super().__init__(n=len(img_data_list), batch_size=batch_size, shuffle=shuffle, seed=seed)\n\n    # Set params\n    self.img_data_list = img_data_list\n    self.model = model\n    self.horizontal_flip = horizontal_flip\n    self.vertical_flip = vertical_flip\n    self.rot_90 = rot_90\n    self.with_img_data = with_img_data\n\n    # Manage shared_model_trainable\n    self.shared_model_trainable = shared_model_trainable\n    if self.shared_model_trainable:\n        self.rpn_clone = clone_model(self.model.model_rpn)\n        self.rpn_clone.set_weights(self.model.model_rpn.get_weights())\n    else:\n        self.rpn_clone = None\n\n    # Manage data augmentation &amp; test\n    if self.is_test and any([param for param in [self.horizontal_flip, self.vertical_flip, self.rot_90]]):\n        model.logger.warning(\"Warning, Data Augmentation detected on the test set! This is certainly not desired!\")\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_keras_faster_rcnn/#template_vision.models_training.object_detectors.model_keras_faster_rcnn.CustomGeneratorRpn","title":"<code>CustomGeneratorRpn</code>","text":"<p>             Bases: <code>Iterator</code></p> <p>RPN generator</p> Source code in <code>template_vision/models_training/object_detectors/model_keras_faster_rcnn.py</code> <pre><code>class CustomGeneratorRpn(Iterator):\n    '''RPN generator'''\n\n    def __init__(self, img_data_list: List[dict], batch_size: int, shuffle: bool, seed: Union[int, None],\n                 model, horizontal_flip: bool = False, vertical_flip: bool = False,\n                 rot_90: bool = False, data_type: str = 'train', with_img_data: bool = False, **kwargs) -&gt; None:\n        '''Initialization of the RPN generator\n\n        Args:\n            img_data_list (list&lt;dict&gt;): Data list (the dictionaries containing file path and bboxes)\n            batch_size (int): Size of the batches to generate\n            shuffle (bool): If the data should be shuffled\n            seed (int): Random seed to use\n            model: Link to the model (nested structure) to access to the other methods of this script\n            data_type (str): Data type 'train', 'valid' or 'test'\n            with_img_data (bool): If True, also gives img_data as output\n        Kwargs:\n            horizontal_flip (bool) : If True, can do horizontal flip (with 0.5 proba)\n            vertical_flip (bool) : If True, can do vertical flip (with 0.5 proba)\n            rot_90 (bool) : If True, can do a rotation of 0, 90, 180 or 270 degrees (0.25 proba for each)\n        '''\n        # Set is_test\n        if data_type == 'test':\n            self.is_test = True\n        else:\n            self.is_test = False\n\n        # If test, the batch size is set to 1\n        if self.is_test:\n            batch_size = 1\n\n        # Super init.\n        super().__init__(n=len(img_data_list), batch_size=batch_size, shuffle=shuffle, seed=seed)\n\n        # Set params\n        self.img_data_list = img_data_list\n        self.model = model\n        self.horizontal_flip = horizontal_flip\n        self.vertical_flip = vertical_flip\n        self.rot_90 = rot_90\n        self.with_img_data = with_img_data\n\n        # Manage data augmentation &amp; test\n        if self.is_test and any([param for param in [self.horizontal_flip, self.vertical_flip, self.rot_90]]):\n            model.logger.warning(\"Warning, data augmentation on the test dataset ! It is most certainly a mistake !\")\n\n    def _get_batches_of_transformed_samples(self, index_array: np.ndarray) -&gt; tuple:\n        '''Gets a batch of inputs for the RPN model\n        Warning, in order to be sure to have the same shape between the various images, we pad them with black pixels\n\n        Args:\n            index_array (np.ndarray): List of indices to include in a batch\n        Returns:\n            np.ndarray: Data batch to be used by the RPN model (x)\n                # Shape (bach_size, max_resized_height, max_resized_width, 3)\n            dict&lt;np.ndarray&gt;: Data batch to be used by the RPN model (y)\n                rpn_class: RPN - target of the classifier\n                    # Shape (bach_size, feature_map_height, feature_map_width, 2 * nb_anchors)\n                rpn_class: RPN - target of the regressor\n                    # Shape (bach_size, feature_map_height, feature_map_width, 2 * 4 * nb_anchors)\n            if self.with_img_data : the metadata of the images\n        '''\n        # For each selected index, get image &amp; preprocess it, and retrieve max resized_width &amp; resized_height\n        batch_prepared_img_data = []\n        max_resized_width = 0\n        max_resized_height = 0\n        for ind in index_array:\n            img_data = copy.deepcopy(self.img_data_list[ind])\n            prepared_img_data = self.model._generate_images_with_bboxes(img_data, horizontal_flip=self.horizontal_flip,\n                                                                        vertical_flip=self.vertical_flip, rot_90=self.rot_90)\n            batch_prepared_img_data.append(prepared_img_data)\n            max_resized_width = max(max_resized_width, prepared_img_data['resized_width'])\n            max_resized_height = max(max_resized_height, prepared_img_data['resized_height'])\n\n        # Set format X\n        batch_shape = (len(index_array), max_resized_height, max_resized_width, 3)\n        batch_x = np.zeros(batch_shape)  # Def. black pixels\n\n        # Get input images\n        for ind, prepared_img_data in enumerate(batch_prepared_img_data):\n            prepared_img_data['batch_width'] = max_resized_width\n            prepared_img_data['batch_height'] = max_resized_height\n            img = prepared_img_data['img']\n            batch_x[ind, :img.shape[0], :img.shape[1], :] = img\n\n        # If test, we return the images\n        if self.is_test:\n            if self.with_img_data:\n                return {'input_img': batch_x}, batch_prepared_img_data\n            else:\n                return {'input_img': batch_x}\n        # Otherwise, we also manage the targets y\n        else:\n            batch_y_cls, batch_y_regr = utils_object_detectors.get_rpn_targets(self.model, batch_prepared_img_data)\n            # Return\n            if self.with_img_data:\n                return {'input_img': batch_x}, {'rpn_class': batch_y_cls, 'rpn_regr': batch_y_regr}, batch_prepared_img_data\n            else:\n                return {'input_img': batch_x}, {'rpn_class': batch_y_cls, 'rpn_regr': batch_y_regr}\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_keras_faster_rcnn/#template_vision.models_training.object_detectors.model_keras_faster_rcnn.CustomGeneratorRpn.__init__","title":"<code>__init__(img_data_list, batch_size, shuffle, seed, model, horizontal_flip=False, vertical_flip=False, rot_90=False, data_type='train', with_img_data=False, **kwargs)</code>","text":"<p>Initialization of the RPN generator</p> <p>Parameters:</p> Name Type Description Default <code>img_data_list</code> <code>list&lt;dict&gt;</code> <p>Data list (the dictionaries containing file path and bboxes)</p> required <code>batch_size</code> <code>int</code> <p>Size of the batches to generate</p> required <code>shuffle</code> <code>bool</code> <p>If the data should be shuffled</p> required <code>seed</code> <code>int</code> <p>Random seed to use</p> required <code>model</code> <p>Link to the model (nested structure) to access to the other methods of this script</p> required <code>data_type</code> <code>str</code> <p>Data type 'train', 'valid' or 'test'</p> <code>'train'</code> <code>with_img_data</code> <code>bool</code> <p>If True, also gives img_data as output</p> <code>False</code> <p>Kwargs:     horizontal_flip (bool) : If True, can do horizontal flip (with 0.5 proba)     vertical_flip (bool) : If True, can do vertical flip (with 0.5 proba)     rot_90 (bool) : If True, can do a rotation of 0, 90, 180 or 270 degrees (0.25 proba for each)</p> Source code in <code>template_vision/models_training/object_detectors/model_keras_faster_rcnn.py</code> <pre><code>def __init__(self, img_data_list: List[dict], batch_size: int, shuffle: bool, seed: Union[int, None],\n             model, horizontal_flip: bool = False, vertical_flip: bool = False,\n             rot_90: bool = False, data_type: str = 'train', with_img_data: bool = False, **kwargs) -&gt; None:\n    '''Initialization of the RPN generator\n\n    Args:\n        img_data_list (list&lt;dict&gt;): Data list (the dictionaries containing file path and bboxes)\n        batch_size (int): Size of the batches to generate\n        shuffle (bool): If the data should be shuffled\n        seed (int): Random seed to use\n        model: Link to the model (nested structure) to access to the other methods of this script\n        data_type (str): Data type 'train', 'valid' or 'test'\n        with_img_data (bool): If True, also gives img_data as output\n    Kwargs:\n        horizontal_flip (bool) : If True, can do horizontal flip (with 0.5 proba)\n        vertical_flip (bool) : If True, can do vertical flip (with 0.5 proba)\n        rot_90 (bool) : If True, can do a rotation of 0, 90, 180 or 270 degrees (0.25 proba for each)\n    '''\n    # Set is_test\n    if data_type == 'test':\n        self.is_test = True\n    else:\n        self.is_test = False\n\n    # If test, the batch size is set to 1\n    if self.is_test:\n        batch_size = 1\n\n    # Super init.\n    super().__init__(n=len(img_data_list), batch_size=batch_size, shuffle=shuffle, seed=seed)\n\n    # Set params\n    self.img_data_list = img_data_list\n    self.model = model\n    self.horizontal_flip = horizontal_flip\n    self.vertical_flip = vertical_flip\n    self.rot_90 = rot_90\n    self.with_img_data = with_img_data\n\n    # Manage data augmentation &amp; test\n    if self.is_test and any([param for param in [self.horizontal_flip, self.vertical_flip, self.rot_90]]):\n        model.logger.warning(\"Warning, data augmentation on the test dataset ! It is most certainly a mistake !\")\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_keras_faster_rcnn/#template_vision.models_training.object_detectors.model_keras_faster_rcnn.ModelCheckpointAll","title":"<code>ModelCheckpointAll</code>","text":"<p>             Bases: <code>ModelCheckpoint</code></p> <p>A Callback to save the whole model and not only the model currently being fitted. In order to do so, we overload the class ModelCheckpoint by redefining its method _save_model</p> Source code in <code>template_vision/models_training/object_detectors/model_keras_faster_rcnn.py</code> <pre><code>class ModelCheckpointAll(ModelCheckpoint):\n    '''A Callback to save the whole model and not only the model currently being fitted.\n    In order to do so, we overload the class ModelCheckpoint by redefining its method _save_model\n    '''\n\n    def __init__(self, model_all, **kwargs) -&gt; None:\n        '''Initialization of the class\n\n        Args:\n            model_all (Model): Whole model (RPN &amp; Classifier) of the Faster RCNN\n        '''\n        super().__init__(**kwargs)\n        self.model_all = model_all\n\n    def _save_model(self, epoch: int, batch, logs: dict) -&gt; None:\n        \"\"\"Saves the model.\n\n        Small trick : we temporarily set the model to model_all, save it, and then reload model_main\n\n        Args:\n            epoch: the epoch this iteration is in.\n            batch: the batch this iteration is in. `None` if the `save_freq`\n            is set to `epoch`.\n            logs: the `logs` dict passed in to `on_batch_end` or `on_epoch_end`.\n        \"\"\"\n        # Small trick on the models\n        model_main = self.model\n        self.model = self.model_all\n        # Save\n        super()._save_model(epoch, batch, logs)\n        # Fix trick\n        self.model = model_main\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_keras_faster_rcnn/#template_vision.models_training.object_detectors.model_keras_faster_rcnn.ModelCheckpointAll.__init__","title":"<code>__init__(model_all, **kwargs)</code>","text":"<p>Initialization of the class</p> <p>Parameters:</p> Name Type Description Default <code>model_all</code> <code>Model</code> <p>Whole model (RPN &amp; Classifier) of the Faster RCNN</p> required Source code in <code>template_vision/models_training/object_detectors/model_keras_faster_rcnn.py</code> <pre><code>def __init__(self, model_all, **kwargs) -&gt; None:\n    '''Initialization of the class\n\n    Args:\n        model_all (Model): Whole model (RPN &amp; Classifier) of the Faster RCNN\n    '''\n    super().__init__(**kwargs)\n    self.model_all = model_all\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_keras_faster_rcnn/#template_vision.models_training.object_detectors.model_keras_faster_rcnn.ModelKerasFasterRcnnObjectDetector","title":"<code>ModelKerasFasterRcnnObjectDetector</code>","text":"<p>             Bases: <code>ModelObjectDetectorMixin</code>, <code>ModelKeras</code></p> <p>Faster RCNN model (Keras) for object detection</p> Source code in <code>template_vision/models_training/object_detectors/model_keras_faster_rcnn.py</code> <pre><code>class ModelKerasFasterRcnnObjectDetector(ModelObjectDetectorMixin, ModelKeras):\n    '''Faster RCNN model (Keras) for object detection'''\n\n    _default_name = 'model_keras_faster_rcnn_object_detector'\n\n    def __init__(self, img_min_side_size: int = 300, rpn_min_overlap: float = 0.3, rpn_max_overlap: float = 0.7, rpn_restrict_num_regions: int = 256,\n                 pool_resize_classifier: int = 7, nb_rois_classifier: int = 4, roi_nms_overlap_threshold: float = 0.7, nms_max_boxes: int = 300,\n                 classifier_min_overlap: float = 0.1, classifier_max_overlap: float = 0.5,\n                 pred_bbox_proba_threshold: float = 0.6, pred_nms_overlap_threshold: float = 0.2,\n                 data_augmentation_params: Union[dict, None] = None,\n                 batch_size_rpn_trainable_true: Union[int, None] = None, batch_size_classifier_trainable_true: Union[int, None] = None,\n                 batch_size_rpn_trainable_false: Union[int, None] = None, batch_size_classifier_trainable_false: Union[int, None] = None,\n                 epochs_rpn_trainable_true: Union[int, None] = None, epochs_classifier_trainable_true: Union[int, None] = None,\n                 epochs_rpn_trainable_false: Union[int, None] = None, epochs_classifier_trainable_false: Union[int, None] = None,\n                 patience_rpn_trainable_true: Union[int, None] = None, patience_classifier_trainable_true: Union[int, None] = None,\n                 patience_rpn_trainable_false: Union[int, None] = None, patience_classifier_trainable_false: Union[int, None] = None,\n                 lr_rpn_trainable_true: float = 1e-5, lr_classifier_trainable_true: float = 1e-5, lr_rpn_trainable_false: float = 1e-5,\n                 lr_classifier_trainable_false: float = 1e-5, **kwargs) -&gt; None:\n        '''Initialization of the class (see ModelClass, ModelKeras &amp; ModelObjectDetectorMixin for more arguments)\n\n        Kwargs:\n            img_min_side_size (int): Size to give to the smaller dimension as input of the model\n            rpn_min_overlap (float): Under this threshold a region is classified as background (RPN model)\n            rpn_max_overlap (float): Above this threshold a region is classified as object (RPN model)\n            rpn_restrict_num_regions (int): Maximal number of regions to keep as target for the RPN\n            pool_resize_classifier (int): Size to give to the crops done before the classifier (via ROI)\n            nb_rois_classifier (int): Maximal number of ROIs per image during classifier training (per image of a batch)\n            roi_nms_overlap_threshold (float): The NMS deletes overlapping ROIs whose IOU is above this threshold\n            nms_max_boxes (int): Maximal number of ROIs to be returned by the NMS\n            classifier_min_overlap (float): Above this threshold a ROI is considered to be a target of the classifier (but can be 'bg')\n            classifier_max_overlap (float): Above this threshold a ROI is considered to be matching a bbox (so the target is a class, not 'bg')\n            pred_bbox_proba_threshold (float): Above this threshold (for probabilities), a ROI is considered to be a match\n            pred_nms_overlap_threshold (float): When predicting, the NMS deletes overlapping predictions whose IOU is above this threshold\n            data_augmentation_params (dict): Set of allowed data augmentation\n            batch_size_rpn_trainable_true (int): Batch size for the RPN with for first run with trainable set to True\n            batch_size_classifier_trainable_true (int): Batch size for the classifier for the first run with trainable set to True\n            batch_size_rpn_trainable_false (int): Batch size for the RPN for the second run with trainable set to False\n            batch_size_classifier_trainable_false (int): Batch size for the classifier for the second run with trainable set to False\n            epochs_rpn_trainable_true (int): Number of epochs for the RPN for the first run with trainable set to True\n            epochs_classifier_trainable_true (int): Number of epochs for the classifier for the first run with trainable set to True\n            epochs_rpn_trainable_false (int): Number of epochs for the RPN for the second run with trainable set to False\n            epochs_classifier_trainable_false (int): lNumber of epochs for the classifier for the second run with trainable set to False\n            patience_rpn_trainable_true (int): Patience for the RPN for the first run with trainable set to True\n            patience_classifier_trainable_true (int): Patience for the classifier for the first run with trainable set to True\n            patience_rpn_trainable_false (int): Patience for the RPN for the second run with trainable set to False\n            patience_classifier_trainable_false (int): Patience for the classifier for the second run with trainable set to False\n            lr_rpn_trainable_true (float): Learning rate for the RPN for the first run with trainable set to True\n            lr_classifier_trainable_true (float): Learning rate for the classifier for the first run with trainable set to True\n            lr_rpn_trainable_false (float): Learning rate for the RPN for the second run with trainable set to False\n            lr_classifier_trainable_false (float): Learning rate for the classifier for the second run with trainable set to False\n        Raises:\n            ValueError: If img_min_side_size is not positive\n            ValueError: If rpn_min_overlap is not in [0, 1]\n            ValueError: If rpn_max_overlap is not in [0, 1]\n            ValueError: If rpn_min_overlap &gt; rpn_max_overlap\n            ValueError: If rpn_restrict_num_regions is not positive\n            ValueError: If pool_resize_classifier is not positive\n            ValueError: If nb_rois_classifier is not positive\n            ValueError: If roi_nms_overlap_threshold is not in [0, 1]\n            ValueError: If nms_max_boxes is not positive\n            ValueError: If classifier_min_overlap is not in [0, 1]\n            ValueError: If classifier_max_overlap is not in [0, 1]\n            ValueError: If classifier_min_overlap &gt; classifier_max_overlap\n            ValueError: If pred_bbox_proba_threshold is not in [0, 1]\n            ValueError: If pred_nms_overlap_threshold is not in [0, 1]\n            ValueError: If color_mode is not 'rgb'\n            ValueError: If the minimum size of the image is inferior to twice the subsampling ratio\n        '''\n        # Manage errors\n        if img_min_side_size &lt; 1:\n            raise ValueError(f\"The argument img_min_side_size ({img_min_side_size}) must be positive\")\n        if not 0 &lt;= rpn_min_overlap &lt;= 1:\n            raise ValueError(f\"The argument rpn_min_overlap ({rpn_min_overlap}) must be between 0 and 1, included\")\n        if not 0 &lt;= rpn_max_overlap &lt;= 1:\n            raise ValueError(f\"The argument rpn_max_overlap ({rpn_max_overlap}) must be between 0 and 1, included\")\n        if rpn_min_overlap &gt; rpn_max_overlap:\n            raise ValueError(f\"The argument rpn_min_overlap ({rpn_min_overlap}) can't be superior to rpn_max_overlap ({rpn_max_overlap})\")\n        if rpn_restrict_num_regions &lt; 1:\n            raise ValueError(f\"The argument rpn_restrict_num_regions ({rpn_restrict_num_regions}) must be positive\")\n        if pool_resize_classifier &lt; 1:\n            raise ValueError(f\"The argument pool_resize_classifier ({pool_resize_classifier}) must be positive\")\n        if nb_rois_classifier &lt; 1:\n            raise ValueError(f\"The argument nb_rois_classifier ({nb_rois_classifier}) must be positive\")\n        if not 0 &lt;= roi_nms_overlap_threshold &lt;= 1:\n            raise ValueError(f\"The argument roi_nms_overlap_threshold ({roi_nms_overlap_threshold}) must be between 0 and 1, included\")\n        if nms_max_boxes &lt; 1:\n            raise ValueError(f\"The argument nms_max_boxes ({nms_max_boxes}) must be positive\")\n        if not 0 &lt;= classifier_min_overlap &lt;= 1:\n            raise ValueError(f\"The argument classifier_min_overlap ({classifier_min_overlap}) must be between 0 and 1, included\")\n        if not 0 &lt;= classifier_max_overlap &lt;= 1:\n            raise ValueError(f\"The argument classifier_max_overlap ({classifier_max_overlap}) must be between 0 and 1, included\")\n        if classifier_min_overlap &gt; classifier_max_overlap:\n            raise ValueError(f\"The argument classifier_min_overlap ({classifier_min_overlap}) can't be superior to classifier_max_overlap ({classifier_max_overlap})\")\n        if not 0 &lt;= pred_bbox_proba_threshold &lt;= 1:\n            raise ValueError(f\"The argument pred_bbox_proba_threshold ({pred_bbox_proba_threshold}) must be between 0 and 1, included\")\n        if not 0 &lt;= pred_nms_overlap_threshold &lt;= 1:\n            raise ValueError(f\"The argument pred_nms_overlap_threshold ({pred_nms_overlap_threshold}) must be between 0 and 1, included\")\n\n        # Size of the input images (must be defined before the super init because it is used in the method _get_preprocess_input)\n        self.img_min_side_size = img_min_side_size  # Default 300, in the paper 600\n\n        # Init. (by default we have some data augmentation)\n        if data_augmentation_params is None:\n            data_augmentation_params = {'horizontal_flip': True, 'vertical_flip': True, 'rot_90': True}\n        super().__init__(data_augmentation_params=data_augmentation_params, **kwargs)\n        if self.color_mode != 'rgb':\n            raise ValueError(\"Faster RCNN model only accept color_mode equal to 'rgb' (compatibility VGG16).\")\n\n        # Put to None some parameters of model_keras not used by this model\n        self.width = None\n        self.height = None\n        self.depth = None\n        self.in_memory = None\n        self.nb_train_generator_images_to_save = None\n\n        # Get logger (must be done after super init)\n        self.logger = logging.getLogger(__name__)\n\n        # Models, set on fit\n        self.model: Any = None\n        self.shared_model = None\n        self.model_rpn = None\n        self.model_classifier = None\n\n        # Weights\n        self.vgg_filename = 'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n        self.vgg_path = os.path.join(utils.get_data_path(), 'transfer_learning_weights', self.vgg_filename)\n        vgg16_weights_backup_urls = [\n            'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n        ]\n        if not os.path.exists(self.vgg_path):\n            try:\n                self.logger.warning(\"The weights file for VGG16 is not present in your data folder.\")\n                self.logger.warning(\"Trying to download the file.\")\n                utils.download_url(vgg16_weights_backup_urls, self.vgg_path)\n            except ConnectionError:\n                self.logger.warning(\"Can't download. You can try to download it manually and save it on DVC.\")\n                self.logger.warning(\"Building this model will return an error.\")\n                self.logger.warning(\"You can download the weights here : https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\")\n                # We don't raise an error because we may reload a trained model\n\n        ### Model configuration\n\n        # Configurations related to the base model\n        self.shared_model_subsampling = 16  # VGG 16\n        # Error if img_min_side_size &lt; 2 * subsampling rate\n        if self.img_min_side_size &lt; 2 * self.shared_model_subsampling:\n            raise ValueError(\"Can't have a minimum size of an image inferior to twice the subsampling ratio\")\n\n        # Anchors boxes\n        self.anchor_box_sizes = [64, 128, 256]  # In the paper : [128, 256, 512]\n        self.anchor_box_ratios = [[1, 1], [1. / math.sqrt(2), 2. / math.sqrt(2)], [2. / math.sqrt(2), 1. / math.sqrt(2)]]  # In the paper : [1, 1], [1, 2], [2, 1]]\n        self.nb_anchors = len(self.anchor_box_sizes) * len(self.anchor_box_ratios)\n        self.list_anchors = [[anchor_size * anchor_ratio[0], anchor_size * anchor_ratio[1]]\n                             for anchor_size in self.anchor_box_sizes for anchor_ratio in self.anchor_box_ratios]\n\n        # Sizes\n        self.pool_resize_classifier = pool_resize_classifier  # Def 7\n\n        # Scaling (we could probably do without scaling)\n        self.rpn_regr_scaling = 4.0\n        self.classifier_regr_scaling = [8.0, 8.0, 4.0, 4.0]\n\n        # Thresholds for the RPN to find positive and negative anchor boxes\n        self.rpn_min_overlap = rpn_min_overlap  # Def 0.3\n        self.rpn_max_overlap = rpn_max_overlap  # Def 0.7\n        # Maximum number of regions targets of the RPN\n        self.rpn_restrict_num_regions = rpn_restrict_num_regions\n\n        # Classifier configuration\n        self.nb_rois_classifier = nb_rois_classifier  # Def 4\n        self.roi_nms_overlap_threshold = roi_nms_overlap_threshold  # Def 0.7\n        self.nms_max_boxes = nms_max_boxes  # Def 300\n        self.classifier_min_overlap = classifier_min_overlap  # Def 0.1\n        self.classifier_max_overlap = classifier_max_overlap  # Def 0.5\n\n        # Prediction Thresholds\n        self.pred_bbox_proba_threshold = pred_bbox_proba_threshold  # Def 0.6\n        self.pred_nms_overlap_threshold = pred_nms_overlap_threshold  # Def 0.2\n\n        ### Misc.\n\n        # We add the custom objects only when fitting because we need the number of classes\n\n        # Manage batch_size, epochs &amp; patience (back up on global values if not specified)\n        self.batch_size_rpn_trainable_true = batch_size_rpn_trainable_true if batch_size_rpn_trainable_true is not None else self.batch_size\n        self.batch_size_classifier_trainable_true = batch_size_classifier_trainable_true if batch_size_classifier_trainable_true is not None else self.batch_size\n        self.batch_size_rpn_trainable_false = batch_size_rpn_trainable_false if batch_size_rpn_trainable_false is not None else self.batch_size\n        self.batch_size_classifier_trainable_false = batch_size_classifier_trainable_false if batch_size_classifier_trainable_false is not None else self.batch_size\n        self.epochs_rpn_trainable_true = epochs_rpn_trainable_true if epochs_rpn_trainable_true is not None else self.epochs\n        self.epochs_classifier_trainable_true = epochs_classifier_trainable_true if epochs_classifier_trainable_true is not None else self.epochs\n        self.epochs_rpn_trainable_false = epochs_rpn_trainable_false if epochs_rpn_trainable_false is not None else self.epochs\n        self.epochs_classifier_trainable_false = epochs_classifier_trainable_false if epochs_classifier_trainable_false is not None else self.epochs\n        self.patience_rpn_trainable_true = patience_rpn_trainable_true if patience_rpn_trainable_true is not None else self.patience\n        self.patience_classifier_trainable_true = patience_classifier_trainable_true if patience_classifier_trainable_true is not None else self.patience\n        self.patience_rpn_trainable_false = patience_rpn_trainable_false if patience_rpn_trainable_false is not None else self.patience\n        self.patience_classifier_trainable_false = patience_classifier_trainable_false if patience_classifier_trainable_false is not None else self.patience\n\n        # Save learning rates in params_keras\n        self.keras_params['lr_rpn_trainable_true'] = lr_rpn_trainable_true\n        self.keras_params['lr_classifier_trainable_true'] = lr_classifier_trainable_true\n        self.keras_params['lr_rpn_trainable_false'] = lr_rpn_trainable_false\n        self.keras_params['lr_classifier_trainable_false'] = lr_classifier_trainable_false\n\n    #####################\n    # Modelisation\n    #####################\n\n    def _get_model(self) -&gt; Any:\n        '''Gets a model structure - returns the instance model instead if already defined\n\n        Returns:\n            (?): Shared layers of the VGG 16 (not compiled)\n            (?): RPN model\n            (?): Classifier model\n            (?): Global model (for load/save only)\n        '''\n        # Return models if already set\n        if (self.shared_model is not None) and (self.model_rpn is not None) and (self.model_classifier is not None) and (self.model is not None):\n            return self.shared_model, self.model_rpn, self.model_classifier, self.model\n\n        # First, we define the inputs\n        input_img = Input(shape=(None, None, 3), name='input_img')\n        input_rois = Input(shape=(None, 4), name='input_rois')\n\n        # Then we get the various parts of the network\n        shared_model_layers = self._get_shared_model_structure(input_img)  # List (class &amp; regr)\n        rpn_layers = self._add_rpn_layers(shared_model_layers)  # List (class &amp; regr)\n        classifier_layers = self._add_classifier_layers(shared_model_layers, input_rois)\n\n        # Base model (shared layers)\n        shared_model = Model(input_img, shared_model_layers)\n        # We instanciate our models\n        model_rpn = Model(input_img, rpn_layers)\n        model_classifier = Model([input_img, input_rois], classifier_layers)\n        # Concatenation of the two models, used to load / save the weights\n        model_all = Model([input_img, input_rois], rpn_layers + classifier_layers)\n\n        # We load the pre-trained weights\n        shared_model.load_weights(self.vgg_path, by_name=True)\n\n        # Compile models\n        self._compile_model_rpn(model_rpn, lr=self.keras_params['lr_rpn_trainable_true'])\n        self._compile_model_classifier(model_classifier, lr=self.keras_params['lr_classifier_trainable_true'])\n        # We also compile model_all\n        model_all.compile(optimizer='sgd', loss='mae')\n\n        # Display summaries\n        if self.logger.getEffectiveLevel() &lt; logging.ERROR:\n            model_all.summary()\n\n        # Try to save models as png if level_save &gt; 'LOW'\n        if self.level_save in ['MEDIUM', 'HIGH']:\n            self._save_model_png(model_all)\n\n        # Return models\n        return shared_model, model_rpn, model_classifier, model_all\n\n    def _compile_model_rpn(self, model_rpn, lr: float) -&gt; None:\n        '''Compiles the RPN model using the specified learning rate\n\n        Args:\n            model_rpn : RPN model to compile\n            lr (float): Learning rate we want to use\n        '''\n        # Set optimizer\n        decay = self.keras_params.get('decay_rpn', 0.0)\n        self.logger.info(f\"Learning rate used - RPN : {lr}\")\n        self.logger.info(f\"Decay used - RPN : {decay}\")\n        optimizer_rpn = Adam(lr=lr, decay=decay)\n\n        # Set loss &amp; metrics\n        losses_rpn = {'rpn_class': self.custom_objects['rpn_loss_cls'], 'rpn_regr': self.custom_objects['rpn_loss_regr']}\n        metrics_rpn = {'rpn_class': 'accuracy'}\n\n        # Compile model\n        model_rpn.compile(optimizer=optimizer_rpn, loss=losses_rpn, metrics=metrics_rpn)\n\n    def _compile_model_classifier(self, model_classifier, lr: float) -&gt; None:\n        '''Compiles the classifier model using the specified learning rate\n\n        Args:\n            model_classifier : Classifier to compule\n            lr (float): Learning rate we want to use\n        '''\n        # Set optimizer\n        decay = self.keras_params.get('decay_classifier', 0.0)\n        self.logger.info(f\"Learning rate used - classifier : {lr}\")\n        self.logger.info(f\"Decay used - classifier : {decay}\")\n        optimizer_classifier = Adam(lr=lr, decay=decay)\n\n        # Set loss &amp; metrics\n        losses_classifier = {'dense_class': self.custom_objects['class_loss_cls'], 'dense_regr': self.custom_objects['class_loss_regr']}\n        metrics_classifier = {'dense_class': 'accuracy'}\n\n        # Compile model\n        model_classifier.compile(optimizer=optimizer_classifier, loss=losses_classifier, metrics=metrics_classifier)\n\n    def _get_shared_model_structure(self, input_img):\n        '''We give the VGG 16 structure\n\n        Args:\n            input_img (?): Input layer for the images\n        Returns:\n            ?: VGG16 structure (without the weights)\n        '''\n\n        # Block 1\n        x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(input_img)\n        x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n        x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n\n        # Block 2\n        x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n        x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n        x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n\n        # Block 3\n        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n        x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n\n        # Block 4\n        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n        x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n\n        # Block 5\n        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n\n        return x\n\n    def _add_rpn_layers(self, base_layers) -&gt; List[Conv2D]:\n        '''Adds the RPN layers to a base model\n\n        Args:\n            base_layers: Base model - VGG16\n        Returns:\n            ?: RPN layers\n        '''\n        # We add a convolution layer\n        x = Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv1')(base_layers)\n        # Fully convolutional layer for our targets\n        # - nb_anchors, feature_map_width, feature_map_height  ---&gt;  Classification\n        # - nb_anchors * nb_coordinates, feature_map_width, feature_map_height  ---&gt;  Regression\n        x_class = Conv2D(self.nb_anchors, (1, 1), activation='sigmoid', kernel_initializer='uniform', name='rpn_class')(x)\n        x_regr = Conv2D(self.nb_anchors * 4, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_regr')(x)\n        # We return the results of the two parts (the format will be managed by the losses)\n        return [x_class, x_regr]\n\n    def _add_classifier_layers(self, base_layers, input_rois) -&gt; List[TimeDistributed]:\n        '''Adds layers for classification to a base model and a ROIs tensor\n\n        Args:\n            base_layers: Base model - VGG16\n            input_rois: Tensor with some ROIs\n                # Shape (1, num_rois, 4), with coordinates (x, y, w, h)\n        Returns:\n            list: List with the classification and regression outputs\n        '''\n        # We get the crops on the features map from the ROIs\n        out_roi_pool = utils_faster_rcnn.RoiPoolingLayer(self.pool_resize_classifier, name='roi_pool')([base_layers, input_rois])\n\n        # Add the Dense part (we use TimeDistributed to take care of the ROIs dimension)\n        out = TimeDistributed(Flatten(name='flatten'), name='distributed_flatten')(out_roi_pool)\n        out = TimeDistributed(Dense(4096, activation='relu', name='fc1'), name='distributed_fc1')(out)\n        out = TimeDistributed(Dropout(0.5), name='distributed_dropout_1')(out)\n        out = TimeDistributed(Dense(4096, activation='relu', name='fc2'), name='distributed_fc2')(out)\n        out = TimeDistributed(Dropout(0.5), name='distributed_dropout_2')(out)\n\n        # We output two parts, classifier and regressor\n        # Classifier : ROI class\n        # Regressor : ROI coordinates correction\n        nb_classes = len(self.list_classes)\n        out_class = TimeDistributed(Dense(nb_classes + 1, activation='softmax', kernel_initializer='zero'), name=\"dense_class\")(out)\n        # TODO: Do we really need to a regression for each class ?!\n        # TODO: Couldn't we simply have a single regression ? The corresponding loss would be on matching an object (whatever the class).\n        # TODO: Couldn't we event make without the regression part here? After all, it is already done by the RPN.\n        out_regr = TimeDistributed(Dense(4 * nb_classes, activation='linear', kernel_initializer='zero'), name=\"dense_regr\")(out)\n\n        return [out_class, out_regr]\n\n    #####################\n    # Images generation\n    #####################\n\n    def _get_generator(self, df: pd.DataFrame, data_type: str, batch_size: int, generator_type: str,\n                       shared_model_trainable: bool = False, with_img_data: bool = False):\n        '''Gets image generator from a list of files - object detector version\n\n        Args:\n            df (pd.DataFrame): Dataset to use must contain :\n                - a column 'file_path' with a path to an image\n                - a column 'bboxes', the list of the bboxes of the image (if train or val)\n            data_type (str): Type of data : 'train', 'valid' or 'test'\n            batch_size (int): Batch size to use\n            generator_type (str): The generator to use, 'rpn' or 'classifier'\n            shared_model_trainable (bool): Classifier &amp; train only - if the shared model is trainable,\n                we must clone the RPN in order not to worsen the quality of the ROIs prediction (which\n                are an input of the classifier)\n            with_img_data (bool): If True, the generator also gives img_data as output\n        Raises:\n            ValueError: If the type of the model is not object_detector\n            ValueError: If data_type is not in ['train', 'valid', 'test']\n            ValueError: If the dataframe has no 'file_path' column\n            ValueError: If 'train' or 'valid' and the dataframe has no 'bboxes' column\n        '''\n        # Manage errors\n        if self.model_type != 'object_detector':\n            raise ValueError(f\"Models of type {self.model_type} do not implement the method _get_generator\")\n        if data_type not in ['train', 'valid', 'test']:\n            raise ValueError(f\"The value {data_type} is not a suitable value for the argument data_type ['train', 'valid', 'test'].\")\n        if generator_type not in ['rpn', 'classifier']:\n            raise ValueError(f\"The value {generator_type} is not a suitable value for the generator_type ['rpn', 'classifier'].\")\n        if 'file_path' not in df.columns:\n            raise ValueError(\"The column 'file_path' is mandatory in the input dataframe\")\n        if data_type in ['train', 'valid'] and 'bboxes' not in df.columns:\n            raise ValueError(f\"The column 'bboxes' is mandatory in the input dataframe when data_type equal to '{data_type}'\")\n\n        # Copy\n        df = df.copy(deep=True)\n        # Extract info\n        img_data_list = []\n        for i, row in df.iterrows():\n            filepath = row['file_path']\n            if 'bboxes' in df.columns:\n                bboxes = row['bboxes']\n                img_data_list.append({'file_path': filepath, 'bboxes': bboxes})\n            else:\n                img_data_list.append({'file_path': filepath})\n\n        # TODO : Manage incorrect bboxes ?\n\n        # Get the suitable generator class\n        custom_generator = CustomGeneratorRpn if generator_type == 'rpn' else CustomGeneratorClassifier\n\n        # Set data_gen (no augmentation nor shuffle if validation/test)\n        if data_type == 'train':\n            generator = custom_generator(img_data_list=img_data_list, batch_size=batch_size, shuffle=True,\n                                         seed=None, model=self, shared_model_trainable=shared_model_trainable,\n                                         data_type=data_type, **self.data_augmentation_params, with_img_data=with_img_data)\n        else:\n            generator = custom_generator(img_data_list=img_data_list, batch_size=batch_size, shuffle=False,\n                                         seed=None, model=self, shared_model_trainable=shared_model_trainable,\n                                         data_type=data_type, with_img_data=with_img_data)\n        return generator\n\n    def _generate_images_with_bboxes(self, img_data: dict, horizontal_flip: bool = False,\n                                     vertical_flip: bool = False, rot_90: bool = False) -&gt; dict:\n        '''Generates an image and its bboxes from its info (path, etc.)\n\n        Can do data augmentation but with a limited choice because some transformations are not\n        compatible with the bboxes (eg. 20 degrees angle)\n\n        Also preprocesses the image\n\n        Args:\n            img_data (dict): Data on the image and its bboxes\n                Must contain : 'file_path' &amp; 'bboxes'\n        Kwargs:\n            horizontal_flip (bool): If True, can do horizontal flip (with 0.5 proba)\n            vertical_flip (bool): If True, can do vertical flip (with 0.5 proba)\n            rot_90 (bool): If True, can do a rotation of 0, 90, 180 or 270 degrees (0.25 proba for each)\n\n                By default the augmentations are not applied (set to False)\n        '''\n        # Read the image, as TensorFlow does\n        with open(img_data['file_path'], 'rb') as f:\n            img = Image.open(io.BytesIO(f.read()))\n            if img.mode != 'RGB':\n                img = img.convert('RGB')\n            # Convert to array\n            img = np.asarray(img)\n        # Get bboxes &amp; image size\n        bboxes = copy.deepcopy(img_data.get('bboxes', []))  # Empty if test\n        h, w = img.shape[:2]\n\n        #####################\n        ### Augmentations ###\n        #####################\n        # Horizontal flip\n        if horizontal_flip and np.random.randint(0, 2) == 0:\n            img = cv2.flip(img, 1)\n            for bbox in bboxes:\n                x1, x2 = bbox['x1'], bbox['x2']\n                bbox['x2'] = w - x1\n                bbox['x1'] = w - x2\n\n        # Vertical flip\n        if vertical_flip and np.random.randint(0, 2) == 0:\n            img = cv2.flip(img, 0)\n            for bbox in bboxes:\n                y1, y2 = bbox['y1'], bbox['y2']\n                bbox['y2'] = h - y1\n                bbox['y1'] = h - y2\n\n        # Rotation 0\u00b0, 90\u00b0, 180\u00b0 or 270\u00b0\n        if rot_90:\n            angle = np.random.choice([0, 90, 180, 270], 1)[0]\n            if angle == 270:\n                img = cv2.flip(np.transpose(img, (1, 0, 2)), 0)\n            elif angle == 180:\n                img = cv2.flip(img, -1)\n            elif angle == 90:\n                img = cv2.flip(np.transpose(img, (1, 0, 2)), 1)\n\n            for bbox in bboxes:\n                x1, x2, y1, y2 = bbox['x1'], bbox['x2'], bbox['y1'], bbox['y2']\n                if angle == 270:\n                    bbox['x1'] = y1\n                    bbox['x2'] = y2\n                    bbox['y1'] = w - x2\n                    bbox['y2'] = w - x1\n                elif angle == 180:\n                    bbox['x2'] = w - x1\n                    bbox['x1'] = w - x2\n                    bbox['y2'] = h - y1\n                    bbox['y1'] = h - y2\n                elif angle == 90:\n                    bbox['x1'] = h - y2\n                    bbox['x2'] = h - y1\n                    bbox['y1'] = x1\n                    bbox['y2'] = x2\n\n        #####################\n        ### Preprocessing ###\n        #####################\n\n        # Keep original sizes\n        original_height, original_width = img.shape[0], img.shape[1]\n\n        # Preprocess\n        img = self.preprocess_input(img)\n\n        # Get new sizes\n        resized_height, resized_width = img.shape[0], img.shape[1]\n\n        # Resize the bboxes following the preprocessing\n        # We could get floats but it is not important at this point\n        for bbox in bboxes:\n            bbox['x1'] = bbox['x1'] * (resized_width / original_width)\n            bbox['x2'] = bbox['x2'] * (resized_width / original_width)\n            bbox['y1'] = bbox['y1'] * (resized_height / original_height)\n            bbox['y2'] = bbox['y2'] * (resized_height / original_height)\n\n        #####################\n        ###  Format Data  ###\n        #####################\n\n        prepared_data = {\n            'img': img,  # Preprocessed image\n            'bboxes': bboxes,  # Bboxes after data augmentation &amp; resizing\n            'original_height': original_height,\n            'original_width': original_width,\n            'resized_height': resized_height,\n            'resized_width': resized_width,\n        }\n        return prepared_data\n\n    def _get_preprocess_input(self) -&gt; Union[Callable, None]:\n        '''Gets the preprocessing to be used before feeding images to the NN\n\n        Returns:\n            (Callable | None): Preprocessing function\n        '''\n        # Get preprocessing function (resize + vgg16)\n        img_min_side_size = self.img_min_side_size  # We take care not to have references to self in the function\n        def preprocess_input(x_img: np.ndarray, **kwargs) -&gt; np.ndarray:\n            '''Preprocessing of a numpy image\n\n            Resizes the image + classic VGG 16 preprocessing\n\n            Args:\n                x_img (np.ndarray): Image to process\n            Returns:\n                np.ndarray: Result of the preprocessing\n            '''\n            # Resize\n            height, width = x_img.shape[0], x_img.shape[1]\n            resized_height, resized_width = utils_object_detectors.get_new_img_size_from_min_side_size(height, width, img_min_side_size)\n            x_img = cv2.resize(x_img, (resized_width, resized_height), interpolation=cv2.INTER_CUBIC)  # Format dimension width, height ...\n            return preprocess_input_vgg16(x_img)\n        # Returns it\n        return preprocess_input\n\n    #####################\n    # Fit\n    #####################\n\n    def _fit_object_detector(self, df_train: pd.DataFrame, df_valid: Union[pd.DataFrame, None] = None,\n                             with_shuffle: bool = True, **kwargs) -&gt; None:\n        '''Training of the model\n\n        Args:\n            df_train (pd.DataFrame): Training data with file_path &amp; bboxes columns\n            df_valid (pd.DataFrame): Validation data with file_path &amp; bboxes columns\n        Kwargs:\n            with_shuffle (boolean): If data must be shuffled before fitting\n                This should be used if the target is not shuffled as the split_validation takes the lines in order.\n                Thus, the validation set might get classes which are not in the train set ...\n        Raises:\n            ValueError: If the type of the model is not object_detector\n            ValueError: If the class 'bg' is present in the input data\n            ValueError: If the same classes are not present when comparing an already trained model\n                and a new dataset\n        '''\n        if self.model_type != 'object_detector':\n            raise ValueError(f\"The models of type {self.model_type} do not implement the method _fit_object_detector\")\n\n        ##############################################\n        # Manage retrain\n        ##############################################\n\n        # If a model has already been fitted, we make a new folder in order not to overwrite the existing one !\n        # And we save the old conf\n        if self.trained:\n            # Get src files to save\n            src_files = [os.path.join(self.model_dir, \"configurations.json\")]\n            if self.nb_fit &gt; 1:\n                for i in range(1, self.nb_fit):\n                    src_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n            # Change model dir\n            self.model_dir = self._get_new_model_dir()\n            # Get dst files\n            dst_files = [os.path.join(self.model_dir, f\"configurations_fit_{self.nb_fit}.json\")]\n            if self.nb_fit &gt; 1:\n                for i in range(1, self.nb_fit):\n                    dst_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n            # Copies\n            for src, dst in zip(src_files, dst_files):\n                try:\n                    shutil.copyfile(src, dst)\n                except Exception as e:\n                    self.logger.error(f\"Impossible to copy {src} to {dst}\")\n                    self.logger.error(\"We still continue ...\")\n                    self.logger.error(repr(e))\n\n        ##############################################\n        # Prepare dataset\n        # Also extract list of classes\n        ##############################################\n\n        # Extract list of classes from df_train\n        set_classes = set()\n        for bboxes in df_train['bboxes'].to_dict().values():\n            set_classes = set_classes.union({bbox['class'] for bbox in bboxes})\n        if 'bg' in set_classes:\n            raise ValueError(\"The 'bg' class must not be present in the bounding boxes classes\")\n\n        list_classes = sorted(list(set_classes))\n        # Also set dict_classes\n        dict_classes = {i: col for i, col in enumerate(list_classes)}\n\n        # Validate classes if already trained, else set them\n        if self.trained:\n            if self.list_classes != list_classes:\n                raise ValueError(\"Error: the new dataset does not match with the already fitted model\")\n            if self.dict_classes != dict_classes:\n                raise ValueError(\"Error: the new dataset does not match with the already fitted model\")\n        else:\n            self.list_classes = list_classes\n            self.dict_classes = dict_classes\n\n        # Now that we have the list of the classes, we can define the custom_objects\n        self.custom_objects = {**utils_faster_rcnn.get_custom_objects_faster_rcnn(self.nb_anchors, len(self.list_classes)), **self.custom_objects}\n\n        # Shuffle training dataset if wanted\n        # It is advised as validation_split from keras does not shufle the data\n        # Hence, for classification task, we might have classes in the validation data that we never met in the training data\n        if with_shuffle:\n            df_train = df_train.sample(frac=1.).reset_index(drop=True)\n\n        # Manage absence of validation datasets\n        if df_valid is None:\n            self.logger.warning(f\"Warning, no validation set. The training set will be splitted (validation fraction = {self.validation_split})\")\n            df_train, df_valid = train_test_split(df_train, test_size=self.validation_split)\n\n        ##############################################\n        # Trainings\n        # 4 steps :\n        #   - Train RPN - sharable model trainable\n        #   - Train classifier - sharable model trainable\n        #   - Train RPN - sharable model NOT trainable\n        #   - Train classifier - sharable model NOT trainable\n        ##############################################\n\n        # Get model (if already fitted, _get_model returns instance models)\n        self.shared_model, self.model_rpn, self.model_classifier, self.model = self._get_model()\n\n        ### Train RPN - sharable model trainable\n        self.logger.info(\"RPN training - trainable set to True\")\n        self._fit_object_detector_RPN(df_train, df_valid, shared_trainable=True)\n\n        ### Train classifier - sharable model trainable\n        self.logger.info(\"Classifier training - trainable set to True\")\n        self._fit_object_detector_classifier(df_train, df_valid, shared_trainable=True)\n\n        ### Train RPN - sharable model NOT trainable\n        self.logger.info(\"RPN training - trainable set to False\")\n        self._fit_object_detector_RPN(df_train, df_valid, shared_trainable=False)\n\n        ### Train classifier - sharable model NOT trainable\n        self.logger.info(\"Classifier training - trainable set to False\")\n        self._fit_object_detector_classifier(df_train, df_valid, shared_trainable=False)\n\n        # We update trained &amp; nb_fit\n        self.trained = True\n        self.nb_fit += 1\n\n    def _fit_object_detector_RPN(self, df_train: pd.DataFrame, df_valid: Union[pd.DataFrame, None] = None,\n                                 shared_trainable: bool = True, **kwargs) -&gt; None:\n        '''RPN training\n\n        Args:\n            df_train (pd.DataFrame): Training data with file_path &amp; bboxes columns\n            df_valid (pd.DataFrame): Validation data with file_path &amp; bboxes columns\n            shared_trainable (bool): If the shared model is trainable\n        '''\n        # Manage trainable\n        for layer in self.shared_model.layers:\n            layer.trainable = shared_trainable\n        # We adapt the learning rate\n        new_lr = self.keras_params['lr_rpn_trainable_true'] if shared_trainable else self.keras_params['lr_rpn_trainable_false']\n        # /!\\ Recompile, otherwise the unfreeze is not taken into account ! /!\\\n        # Cf. https://keras.io/guides/transfer_learning/#fine-tuning\n        self._compile_model_rpn(self.model_rpn, lr=new_lr)\n        # We adapt the batch_size, the number of epochs and the patience\n        batch_size = self.batch_size_rpn_trainable_true if shared_trainable else self.batch_size_rpn_trainable_false\n        epochs = self.epochs_rpn_trainable_true if shared_trainable else self.epochs_rpn_trainable_false\n        patience = self.patience_rpn_trainable_true if shared_trainable else self.patience_rpn_trainable_false\n\n        # If the number of epoch is 0, we skip the training\n        if epochs == 0:\n            self.logger.info(f\"Number of epochs for RPN training - trainable set to {shared_trainable} is 0. We skip it.\")\n        # Entrainement\n        else:\n            # Create generators for the RPN\n            self.logger.info(\"Get a RPN generator for training data.\")\n            batch_size_train = min(batch_size, len(df_train))\n            generator_rpn_train = self._get_generator(df=df_train, data_type='train', batch_size=batch_size_train, generator_type='rpn')\n            self.logger.info(\"Get a RPN generator for validation data.\")\n            batch_size_valid = min(batch_size, len(df_valid))\n            generator_rpn_valid = self._get_generator(df=df_valid, data_type='valid', batch_size=batch_size_valid, generator_type='rpn')\n\n            # Get callbacks (early stopping &amp; checkpoint)\n            callbacks = self._get_callbacks(patience=patience)\n\n            # Fit !\n            fit_history = self.model_rpn.fit(  # type: ignore\n                x=generator_rpn_train,\n                epochs=epochs,\n                validation_data=generator_rpn_valid,\n                callbacks=callbacks,\n                verbose=1,\n                workers=8,  # TODO : Check if this is ok if there are less CPUs\n            )\n\n            # Plots losses &amp; metrics\n            if self.level_save in ['MEDIUM', 'HIGH']:\n                self._plot_metrics_and_loss(fit_history, model_type='rpn', trainable=shared_trainable)\n\n    def _fit_object_detector_classifier(self, df_train: pd.DataFrame, df_valid: Union[pd.DataFrame, None] = None,\n                                        shared_trainable: bool = True, **kwargs) -&gt; None:\n        '''Training of the classifier\n\n        Args:\n            df_train (pd.DataFrame): Training data with file_path &amp; bboxes columns\n            df_valid (pd.DataFrame): Validation data with file_path &amp; bboxes columns\n            shared_trainable (bool): If the shared model is trainable\n        '''\n        # Manage trainable\n        for layer in self.shared_model.layers:\n            layer.trainable = shared_trainable\n        # We adapt the learning rate\n        new_lr = self.keras_params['lr_classifier_trainable_true'] if shared_trainable else self.keras_params['lr_classifier_trainable_false']\n        # /!\\ Recompile, otherwise the unfreeze is not taken into account ! /!\\\n        # Cf. https://keras.io/guides/transfer_learning/#fine-tuning\n        self._compile_model_classifier(self.model_classifier, lr=new_lr)\n        # We adapt the batch_size, the number of epochs and the patience\n        batch_size = self.batch_size_classifier_trainable_true if shared_trainable else self.batch_size_classifier_trainable_false\n        epochs = self.epochs_classifier_trainable_true if shared_trainable else self.epochs_classifier_trainable_false\n        patience = self.patience_classifier_trainable_true if shared_trainable else self.patience_classifier_trainable_false\n\n        # If the number of epoch is 0, we skip the training\n        if epochs == 0:\n            self.logger.info(f\"Number of epochs for classifier training - trainable set to {shared_trainable} is 0. We skip it.\")\n        # Training\n        else:\n            # Create generators for the classifier\n            self.logger.info(\"Get a classifier generator for training data.\")\n            batch_size_train = min(batch_size, len(df_train))\n            generator_classifier_train = self._get_generator(df=df_train, data_type='train', batch_size=batch_size_train,\n                                                             generator_type='classifier', shared_model_trainable=shared_trainable)\n            self.logger.info(\"Get a classifier generator for validation data.\")\n            batch_size_valid = min(batch_size, len(df_valid))\n            generator_classifier_valid = self._get_generator(df=df_valid, data_type='valid', batch_size=batch_size_valid,\n                                                             generator_type='classifier', shared_model_trainable=shared_trainable)\n\n            # Get callbacks (early stopping &amp; checkpoint)\n            callbacks = self._get_callbacks(patience=patience)\n\n            # Fit !\n            fit_history = self.model_classifier.fit(  # type: ignore\n                x=generator_classifier_train,\n                epochs=epochs,\n                validation_data=generator_classifier_valid,\n                callbacks=callbacks,\n                verbose=1,\n                workers=8,  # TODO : Check if this is ok if there are less CPUs\n            )\n\n            # Plots losses &amp; metrics\n            if self.level_save in ['MEDIUM', 'HIGH']:\n                self._plot_metrics_and_loss(fit_history, model_type='classifier', trainable=shared_trainable)\n\n    def _get_callbacks(self, patience: int) -&gt; list:\n        '''Gets model callbacks\n\n        Args:\n            patience (int): Early stopping patience\n        Returns:\n            list: List of callbacks\n        '''\n        # Get classic callbacks\n        callbacks = [EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)]\n        if self.level_save in ['MEDIUM', 'HIGH']:\n            callbacks.append(\n                ModelCheckpointAll(\n                    filepath=os.path.join(self.model_dir, 'best.hdf5'), monitor='val_loss',\n                    save_best_only=True, mode='auto', model_all=self.model\n                )\n            )\n        callbacks.append(CSVLogger(filename=os.path.join(self.model_dir, 'logger.csv'), separator=';', append=False))\n        callbacks.append(TerminateOnNaN())\n\n        # Get LearningRateScheduler\n        # FOR NOW, WE DO NOT TAKE INTO ACCOUNT LEARNING RATE SCHEDULERS\n        # scheduler = self._get_learning_rate_scheduler()\n        # if scheduler is not None:\n        #     callbacks.append(LearningRateScheduler(scheduler))\n\n        # Manage tensorboard\n        if self.level_save in ['HIGH']:\n            # Get log directory\n            models_path = utils.get_models_path()\n            tensorboard_dir = os.path.join(models_path, 'tensorboard_logs')\n            # We add a prefix so that the function load_model works correctly (it looks for a sub-folder with model name)\n            log_dir = os.path.join(tensorboard_dir, f\"tensorboard_{ntpath.basename(self.model_dir)}\")\n            if not os.path.exists(log_dir):\n                os.makedirs(log_dir)\n\n            # TODO: Check if this class slows the process\n            # -&gt; For now: comment\n            # Create custom class to monitor LR changes\n            # https://stackoverflow.com/questions/49127214/keras-how-to-output-learning-rate-onto-tensorboard\n            # class LRTensorBoard(TensorBoard):\n            #     def __init__(self, log_dir, **kwargs) -&gt; None:  # add other arguments to __init__ if you need\n            #         super().__init__(log_dir=log_dir, **kwargs)\n            #\n            #     def on_epoch_end(self, epoch, logs=None):\n            #         logs.update({'lr': K.eval(self.model.optimizer.lr)})\n            #         super().on_epoch_end(epoch, logs)\n\n            callbacks.append(TensorBoard(log_dir=log_dir, write_grads=False, write_images=False))\n            self.logger.info(f\"To start tensorboard: python -m tensorboard.main --logdir {tensorboard_dir} --samples_per_plugin images=10\")\n            # We use the option samples_per_plugin to avoid a rare problem between matplotlib and tensorboard\n            # https://stackoverflow.com/questions/27147300/matplotlib-tcl-asyncdelete-async-handler-deleted-by-the-wrong-thread\n\n        return callbacks\n\n    #####################\n    # Predict\n    #####################\n\n    @utils.trained_needed\n    def _predict_object_detector(self, df_test: pd.DataFrame, **kwargs) -&gt; List[List[dict]]:\n        '''Predictions on test set - batch size must is equal to 1\n\n        Args:\n            df_test (pd.DataFrame): Data to predict, with a column 'file_path'\n        Raises:\n            ValueError: If the model is not of type object_detector\n        Returns:\n            (list&lt;list&lt;dict&gt;&gt;): list (one entry per image) of list of bboxes\n        '''\n        if self.model_type != 'object_detector':\n            raise ValueError(f\"The models of type {self.model_type} do not implement the method _predict_object_detector\")\n\n        # Instanciate the generator for predictions (batch size must be equal to 1)\n        test_generator = self._get_generator(df=df_test, data_type='test', batch_size=1, generator_type='classifier', with_img_data=True)\n        final_predictions = []\n        # For each image in df_test\n        for index_img in range(len(df_test)):\n            # We get the image after preprocessing and some metadata\n            input_data, batch_prepared_img_data = test_generator.next()\n            input_img = input_data['input_img'][0]  # Batch size of 1\n            input_rois = input_data['input_rois'][0]  # Batch size of 1\n            img_data = batch_prepared_img_data[0]  # Batch size of 1\n            # We predict thanks to the classifier\n            predictions = self.model_classifier.predict(input_data, verbose=0)\n            probas = predictions[0][0]  # Probas match, at the level of the features map\n            regr_coordinates = predictions[1][0]  # regressions,  at the level of the features map\n            # We only keep the boxes which have a class different from the background and with\n            # a high enough probability. At the same time, we get the class and the proba\n            fm_boxes_candidates = utils_object_detectors.get_valid_fm_boxes_from_proba(probas, self.pred_bbox_proba_threshold, len(self.list_classes))\n            # We apply the regression, then we get back to the level input bbox and we only keep the valid boxes\n            boxes_candidates = utils_object_detectors.get_valid_boxes_from_coordinates(input_img, input_rois, fm_boxes_candidates, regr_coordinates,\n                                                                                       self.classifier_regr_scaling, self.shared_model_subsampling,\n                                                                                       self.dict_classes)\n            # We apply the NMS algorithm in order to avoid overlaps\n            if len(boxes_candidates):\n                final_boxes = utils_object_detectors.non_max_suppression_fast_on_preds(boxes_candidates, self.pred_nms_overlap_threshold)\n                # Finally we resize the boxes depending on the original size of the imaeg and put it in the desired format\n                predicted_bboxes = utils_object_detectors.get_final_bboxes(final_boxes, img_data)\n            else:\n                predicted_bboxes = []\n            # We add the list of bboxes to the total list\n            final_predictions.append(copy.deepcopy(predicted_bboxes))\n        # Return\n        return final_predictions\n\n    #####################\n    # Misc.\n    #####################\n\n    def _plot_metrics_and_loss(self, fit_history, model_type: str = 'rpn', trainable: bool = True, **kwargs) -&gt; None:\n        '''Plots some metrics &amp; losses\n\n        Args:\n            fit_history (?) : Fit history\n        Kwargs:\n            model_type (str): Type of the model (rpn' or 'classifier') used\n            shared_trainable (bool): If the shared model is trainable\n        '''\n        # Manage dir\n        plots_path = os.path.join(self.model_dir, 'plots')\n        if not os.path.exists(plots_path):\n            os.makedirs(plots_path)\n\n        # Get a dictionnary of possible metrics/loss plots for both rpn &amp; classifier\n        metrics_dir_rpn = {\n            'loss': [f'RPN loss with trainable set to {trainable}', f'loss_{model_type}_trainable_{trainable}'],\n            'rpn_class_loss': [f'RPN classification loss with trainable set to {trainable}', f'loss_class_{model_type}_trainable_{trainable}'],\n            'rpn_regr_loss': [f'RPN regression loss with trainable set to {trainable}', f'loss_regr_{model_type}_trainable_{trainable}'],\n            'rpn_class_accuracy': [f'RPN classification accuracy with trainable set to {trainable}', f'accuracy_class_{model_type}_trainable_{trainable}']\n        }\n        metrics_dir_classifier = {\n            'loss': [f'Classifier loss with trainable set to {trainable}', f'loss_{model_type}_trainable_{trainable}'],\n            'dense_class_loss': [f'Classifier classification loss with trainable set to {trainable}', f'loss_class_{model_type}_trainable_{trainable}'],\n            'dense_regr_loss': [f'Classifier regression loss with trainable set to {trainable}', f'loss_regr_{model_type}_trainable_{trainable}'],\n            'dense_class_accuracy': [f'Classifier classification accuracy with trainable set to {trainable}', f'accuracy_class_{model_type}_trainable_{trainable}']\n        }\n\n        # Get correct metrics dir\n        if model_type == 'rpn':\n            metrics_dir = copy.deepcopy(metrics_dir_rpn)\n        else:\n            metrics_dir = copy.deepcopy(metrics_dir_classifier)\n\n        # Plots each available metrics &amp; losses\n        for metric in fit_history.history.keys():\n            if metric in metrics_dir.keys():\n                title = metrics_dir[metric][0]\n                filename = metrics_dir[metric][1]\n                plt.figure(figsize=(10, 8))\n                plt.plot(fit_history.history[metric])\n                plt.plot(fit_history.history[f'val_{metric}'])\n                plt.title(f\"Model {title}\")\n                plt.ylabel(title)\n                plt.xlabel('Epoch')\n                plt.legend(['Train', 'Validation'], loc='upper left')\n                # Save\n                filename = f\"{filename}.jpeg\"\n                plt.savefig(os.path.join(plots_path, filename))\n\n                # Close figures\n                plt.close('all')\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save configuration JSON\n        if json_data is None:\n            json_data = {}\n\n        json_data['vgg_filename'] = self.vgg_filename\n        json_data['shared_model_subsampling'] = self.shared_model_subsampling\n        json_data['anchor_box_sizes'] = self.anchor_box_sizes\n        json_data['anchor_box_ratios'] = self.anchor_box_ratios\n        json_data['nb_anchors'] = self.nb_anchors\n        json_data['list_anchors'] = self.list_anchors\n        json_data['img_min_side_size'] = self.img_min_side_size\n        json_data['pool_resize_classifier'] = self.pool_resize_classifier\n        json_data['rpn_regr_scaling'] = self.rpn_regr_scaling\n        json_data['classifier_regr_scaling'] = self.classifier_regr_scaling\n        json_data['rpn_min_overlap'] = self.rpn_min_overlap\n        json_data['rpn_max_overlap'] = self.rpn_max_overlap\n        json_data['rpn_restrict_num_regions'] = self.rpn_restrict_num_regions\n        json_data['nb_rois_classifier'] = self.nb_rois_classifier\n        json_data['roi_nms_overlap_threshold'] = self.roi_nms_overlap_threshold\n        json_data['nms_max_boxes'] = self.nms_max_boxes\n        json_data['classifier_min_overlap'] = self.classifier_min_overlap\n        json_data['classifier_max_overlap'] = self.classifier_max_overlap\n        json_data['pred_bbox_proba_threshold'] = self.pred_bbox_proba_threshold\n        json_data['pred_nms_overlap_threshold'] = self.pred_nms_overlap_threshold\n        json_data['batch_size_rpn_trainable_true'] = self.batch_size_rpn_trainable_true\n        json_data['batch_size_classifier_trainable_true'] = self.batch_size_classifier_trainable_true\n        json_data['batch_size_rpn_trainable_false'] = self.batch_size_rpn_trainable_false\n        json_data['batch_size_classifier_trainable_false'] = self.batch_size_classifier_trainable_false\n        json_data['epochs_rpn_trainable_true'] = self.epochs_rpn_trainable_true\n        json_data['epochs_classifier_trainable_true'] = self.epochs_classifier_trainable_true\n        json_data['epochs_rpn_trainable_false'] = self.epochs_rpn_trainable_false\n        json_data['epochs_classifier_trainable_false'] = self.epochs_classifier_trainable_false\n        json_data['patience_rpn_trainable_true'] = self.patience_rpn_trainable_true\n        json_data['patience_classifier_trainable_true'] = self.patience_classifier_trainable_true\n        json_data['patience_rpn_trainable_false'] = self.patience_rpn_trainable_false\n        json_data['patience_classifier_trainable_false'] = self.patience_classifier_trainable_false\n\n        # Add some code if not in json_data:\n        for layer_or_compile in ['_add_rpn_layers', '_add_classifier_layers', '_compile_model_rpn', '_compile_model_classifier']:\n            if layer_or_compile not in json_data:\n                json_data[layer_or_compile] = pickle.source.getsourcelines(getattr(self, layer_or_compile))[0]\n\n        # Save\n        # Save strategy :\n        # - best.hdf5 already saved in fit() &amp; contains all models\n        # - as we can't pickle models, we drop them, save, and reload them\n        shared_model = self.shared_model\n        model_rpn = self.model_rpn\n        model_classifier = self.model_classifier\n        self.shared_model = None\n        self.model_rpn = None\n        self.model_classifier = None\n        super().save(json_data=json_data)\n        self.shared_model = shared_model\n        self.model_rpn = model_rpn\n        self.model_classifier = model_classifier\n\n    def reload_models_from_hdf5(self, hdf5_path: str) -&gt; None:\n        '''Reloads all models from a unique hdf5 file. This method is specific to the faster RCNN model.\n\n        Args:\n            hdf5_path (str): Path to the .hdf5 file with the weightds of model_all\n        Raises:\n            FileNotFoundError: If the object hdf5_path is not an existing file\n        '''\n        # Check path exists\n        if not os.path.exists(hdf5_path):\n            raise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\n\n        # Reload model (based on get_models)\n        # Set layers\n        input_img = Input(shape=(None, None, 3), name='input_img')  # Warning, 3 channels !\n        input_rois = Input(shape=(None, 4), name='input_rois')\n        shared_model_layers = self._get_shared_model_structure(input_img)\n        rpn_layers = self._add_rpn_layers(shared_model_layers)\n        classifier_layers = self._add_classifier_layers(shared_model_layers, input_rois)\n        # Init. models\n        self.shared_model = Model(input_img, shared_model_layers)\n        self.model_rpn = Model(input_img, rpn_layers)\n        self.model_classifier = Model([input_img, input_rois], classifier_layers)\n        self.model = Model([input_img, input_rois], rpn_layers + classifier_layers)\n        # Load the weights (loading the weights of model all will also load the weights of the other models since they are linked)\n        self.model.load_weights(hdf5_path)\n\n    def reload_from_standalone(self, **kwargs) -&gt; None:\n        '''Reloads a model from its configuration, the network used and its preprocessing\n        - /!\\\\ Experimental /!\\\\ -\n\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            hdf5_path (str): Path to hdf5 file\n            preprocess_input_path (str): Path to preprocess input file\n        Raises:\n            ValueError : If configuration_path is None\n            ValueError : If hdf5_path is None\n            ValueError : If preprocess_input_path is None\n            FileNotFoundError : If the object configuration_path is not an existing file\n            FileNotFoundError : If the object hdf5_path is not an existing file\n            FileNotFoundError : If the object preprocess_input_path is not an existing file\n        '''\n        # Retrieve args\n        configuration_path = kwargs.get('configuration_path', None)\n        hdf5_path = kwargs.get('hdf5_path', None)\n        preprocess_input_path = kwargs.get('preprocess_input_path', None)\n\n        # Checks\n        if configuration_path is None:\n            raise ValueError(\"The argument configuration_path can't be None\")\n        if hdf5_path is None:\n            raise ValueError(\"The argument hdf5_path can't be None\")\n        if preprocess_input_path is None:\n            raise ValueError(\"The argument preprocess_input_path can't be None\")\n        if not os.path.exists(configuration_path):\n            raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n        if not os.path.exists(hdf5_path):\n            raise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\n        if not os.path.exists(preprocess_input_path):\n            raise FileNotFoundError(f\"The file {preprocess_input_path} does not exist\")\n\n        # Load confs\n        with open(configuration_path, 'r', encoding='utf-8') as f:\n            configs = json.load(f)\n        # Can't set int as keys in json, so need to cast it after reloading\n        # dict_classes keys are always ints\n        if 'dict_classes' in configs.keys():\n            configs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n        elif 'list_classes' in configs.keys():\n            configs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n\n        # Set class vars\n        # self.model_name = # Keep the created name\n        # self.model_dir = # Keep the created folder\n        self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n        self.trained = configs.get('trained', True)  # Consider trained by default\n        # Try to read the following attributes from configs and, if absent, keep the current one\n        for attribute in ['model_type', 'list_classes', 'dict_classes', 'level_save', 'batch_size',\n                          'epochs', 'validation_split', 'patience', 'color_mode', 'data_augmentation_params',\n                          'vgg_filename', 'shared_model_subsampling', 'anchor_box_sizes', 'anchor_box_ratios',\n                          'nb_anchors', 'list_anchors', 'img_min_side_size', 'pool_resize_classifier',\n                          'rpn_regr_scaling', 'classifier_regr_scaling', 'rpn_min_overlap', 'rpn_max_overlap',\n                          'rpn_restrict_num_regions', 'nb_rois_classifier', 'roi_nms_overlap_threshold',\n                          'nms_max_boxes', 'classifier_min_overlap', 'classifier_max_overlap',\n                          'pred_bbox_proba_threshold', 'pred_nms_overlap_threshold',\n                          'batch_size_rpn_trainable_true', 'batch_size_classifier_trainable_true',\n                          'batch_size_rpn_trainable_false', 'batch_size_classifier_trainable_false',\n                          'epochs_rpn_trainable_true', 'epochs_classifier_trainable_true',\n                          'epochs_rpn_trainable_false', 'epochs_classifier_trainable_false',\n                          'patience_rpn_trainable_true', 'patience_classifier_trainable_true',\n                          'patience_rpn_trainable_false', 'patience_classifier_trainable_false',\n                          'keras_params']:\n            setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n        for attribute in ['width', 'height', 'depth']:\n            setattr(self, attribute, configs.get(attribute, None))\n        self.in_memory = None\n        self.nb_train_generator_images_to_save = None\n        self.vgg_path = os.path.join(utils.get_data_path(), 'transfer_learning_weights', self.vgg_filename)  # Try to reload from usual path. Not really important if it fails.\n\n        # Set custom objects\n        self.custom_objects = {**utils_faster_rcnn.get_custom_objects_faster_rcnn(self.nb_anchors, len(self.list_classes)), **self.custom_objects}\n\n        # Reload model\n        self.reload_models_from_hdf5(hdf5_path)\n\n        # Reload preprocess_input\n        with open(preprocess_input_path, 'rb') as f:\n            self.preprocess_input = pickle.load(f)\n\n        # Save best hdf5 in new folder\n        new_hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\n        shutil.copyfile(hdf5_path, new_hdf5_path)\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_keras_faster_rcnn/#template_vision.models_training.object_detectors.model_keras_faster_rcnn.ModelKerasFasterRcnnObjectDetector.__init__","title":"<code>__init__(img_min_side_size=300, rpn_min_overlap=0.3, rpn_max_overlap=0.7, rpn_restrict_num_regions=256, pool_resize_classifier=7, nb_rois_classifier=4, roi_nms_overlap_threshold=0.7, nms_max_boxes=300, classifier_min_overlap=0.1, classifier_max_overlap=0.5, pred_bbox_proba_threshold=0.6, pred_nms_overlap_threshold=0.2, data_augmentation_params=None, batch_size_rpn_trainable_true=None, batch_size_classifier_trainable_true=None, batch_size_rpn_trainable_false=None, batch_size_classifier_trainable_false=None, epochs_rpn_trainable_true=None, epochs_classifier_trainable_true=None, epochs_rpn_trainable_false=None, epochs_classifier_trainable_false=None, patience_rpn_trainable_true=None, patience_classifier_trainable_true=None, patience_rpn_trainable_false=None, patience_classifier_trainable_false=None, lr_rpn_trainable_true=1e-05, lr_classifier_trainable_true=1e-05, lr_rpn_trainable_false=1e-05, lr_classifier_trainable_false=1e-05, **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass, ModelKeras &amp; ModelObjectDetectorMixin for more arguments)</p> Kwargs <p>img_min_side_size (int): Size to give to the smaller dimension as input of the model rpn_min_overlap (float): Under this threshold a region is classified as background (RPN model) rpn_max_overlap (float): Above this threshold a region is classified as object (RPN model) rpn_restrict_num_regions (int): Maximal number of regions to keep as target for the RPN pool_resize_classifier (int): Size to give to the crops done before the classifier (via ROI) nb_rois_classifier (int): Maximal number of ROIs per image during classifier training (per image of a batch) roi_nms_overlap_threshold (float): The NMS deletes overlapping ROIs whose IOU is above this threshold nms_max_boxes (int): Maximal number of ROIs to be returned by the NMS classifier_min_overlap (float): Above this threshold a ROI is considered to be a target of the classifier (but can be 'bg') classifier_max_overlap (float): Above this threshold a ROI is considered to be matching a bbox (so the target is a class, not 'bg') pred_bbox_proba_threshold (float): Above this threshold (for probabilities), a ROI is considered to be a match pred_nms_overlap_threshold (float): When predicting, the NMS deletes overlapping predictions whose IOU is above this threshold data_augmentation_params (dict): Set of allowed data augmentation batch_size_rpn_trainable_true (int): Batch size for the RPN with for first run with trainable set to True batch_size_classifier_trainable_true (int): Batch size for the classifier for the first run with trainable set to True batch_size_rpn_trainable_false (int): Batch size for the RPN for the second run with trainable set to False batch_size_classifier_trainable_false (int): Batch size for the classifier for the second run with trainable set to False epochs_rpn_trainable_true (int): Number of epochs for the RPN for the first run with trainable set to True epochs_classifier_trainable_true (int): Number of epochs for the classifier for the first run with trainable set to True epochs_rpn_trainable_false (int): Number of epochs for the RPN for the second run with trainable set to False epochs_classifier_trainable_false (int): lNumber of epochs for the classifier for the second run with trainable set to False patience_rpn_trainable_true (int): Patience for the RPN for the first run with trainable set to True patience_classifier_trainable_true (int): Patience for the classifier for the first run with trainable set to True patience_rpn_trainable_false (int): Patience for the RPN for the second run with trainable set to False patience_classifier_trainable_false (int): Patience for the classifier for the second run with trainable set to False lr_rpn_trainable_true (float): Learning rate for the RPN for the first run with trainable set to True lr_classifier_trainable_true (float): Learning rate for the classifier for the first run with trainable set to True lr_rpn_trainable_false (float): Learning rate for the RPN for the second run with trainable set to False lr_classifier_trainable_false (float): Learning rate for the classifier for the second run with trainable set to False</p> <p>Raises:     ValueError: If img_min_side_size is not positive     ValueError: If rpn_min_overlap is not in [0, 1]     ValueError: If rpn_max_overlap is not in [0, 1]     ValueError: If rpn_min_overlap &gt; rpn_max_overlap     ValueError: If rpn_restrict_num_regions is not positive     ValueError: If pool_resize_classifier is not positive     ValueError: If nb_rois_classifier is not positive     ValueError: If roi_nms_overlap_threshold is not in [0, 1]     ValueError: If nms_max_boxes is not positive     ValueError: If classifier_min_overlap is not in [0, 1]     ValueError: If classifier_max_overlap is not in [0, 1]     ValueError: If classifier_min_overlap &gt; classifier_max_overlap     ValueError: If pred_bbox_proba_threshold is not in [0, 1]     ValueError: If pred_nms_overlap_threshold is not in [0, 1]     ValueError: If color_mode is not 'rgb'     ValueError: If the minimum size of the image is inferior to twice the subsampling ratio</p> Source code in <code>template_vision/models_training/object_detectors/model_keras_faster_rcnn.py</code> <pre><code>def __init__(self, img_min_side_size: int = 300, rpn_min_overlap: float = 0.3, rpn_max_overlap: float = 0.7, rpn_restrict_num_regions: int = 256,\n             pool_resize_classifier: int = 7, nb_rois_classifier: int = 4, roi_nms_overlap_threshold: float = 0.7, nms_max_boxes: int = 300,\n             classifier_min_overlap: float = 0.1, classifier_max_overlap: float = 0.5,\n             pred_bbox_proba_threshold: float = 0.6, pred_nms_overlap_threshold: float = 0.2,\n             data_augmentation_params: Union[dict, None] = None,\n             batch_size_rpn_trainable_true: Union[int, None] = None, batch_size_classifier_trainable_true: Union[int, None] = None,\n             batch_size_rpn_trainable_false: Union[int, None] = None, batch_size_classifier_trainable_false: Union[int, None] = None,\n             epochs_rpn_trainable_true: Union[int, None] = None, epochs_classifier_trainable_true: Union[int, None] = None,\n             epochs_rpn_trainable_false: Union[int, None] = None, epochs_classifier_trainable_false: Union[int, None] = None,\n             patience_rpn_trainable_true: Union[int, None] = None, patience_classifier_trainable_true: Union[int, None] = None,\n             patience_rpn_trainable_false: Union[int, None] = None, patience_classifier_trainable_false: Union[int, None] = None,\n             lr_rpn_trainable_true: float = 1e-5, lr_classifier_trainable_true: float = 1e-5, lr_rpn_trainable_false: float = 1e-5,\n             lr_classifier_trainable_false: float = 1e-5, **kwargs) -&gt; None:\n    '''Initialization of the class (see ModelClass, ModelKeras &amp; ModelObjectDetectorMixin for more arguments)\n\n    Kwargs:\n        img_min_side_size (int): Size to give to the smaller dimension as input of the model\n        rpn_min_overlap (float): Under this threshold a region is classified as background (RPN model)\n        rpn_max_overlap (float): Above this threshold a region is classified as object (RPN model)\n        rpn_restrict_num_regions (int): Maximal number of regions to keep as target for the RPN\n        pool_resize_classifier (int): Size to give to the crops done before the classifier (via ROI)\n        nb_rois_classifier (int): Maximal number of ROIs per image during classifier training (per image of a batch)\n        roi_nms_overlap_threshold (float): The NMS deletes overlapping ROIs whose IOU is above this threshold\n        nms_max_boxes (int): Maximal number of ROIs to be returned by the NMS\n        classifier_min_overlap (float): Above this threshold a ROI is considered to be a target of the classifier (but can be 'bg')\n        classifier_max_overlap (float): Above this threshold a ROI is considered to be matching a bbox (so the target is a class, not 'bg')\n        pred_bbox_proba_threshold (float): Above this threshold (for probabilities), a ROI is considered to be a match\n        pred_nms_overlap_threshold (float): When predicting, the NMS deletes overlapping predictions whose IOU is above this threshold\n        data_augmentation_params (dict): Set of allowed data augmentation\n        batch_size_rpn_trainable_true (int): Batch size for the RPN with for first run with trainable set to True\n        batch_size_classifier_trainable_true (int): Batch size for the classifier for the first run with trainable set to True\n        batch_size_rpn_trainable_false (int): Batch size for the RPN for the second run with trainable set to False\n        batch_size_classifier_trainable_false (int): Batch size for the classifier for the second run with trainable set to False\n        epochs_rpn_trainable_true (int): Number of epochs for the RPN for the first run with trainable set to True\n        epochs_classifier_trainable_true (int): Number of epochs for the classifier for the first run with trainable set to True\n        epochs_rpn_trainable_false (int): Number of epochs for the RPN for the second run with trainable set to False\n        epochs_classifier_trainable_false (int): lNumber of epochs for the classifier for the second run with trainable set to False\n        patience_rpn_trainable_true (int): Patience for the RPN for the first run with trainable set to True\n        patience_classifier_trainable_true (int): Patience for the classifier for the first run with trainable set to True\n        patience_rpn_trainable_false (int): Patience for the RPN for the second run with trainable set to False\n        patience_classifier_trainable_false (int): Patience for the classifier for the second run with trainable set to False\n        lr_rpn_trainable_true (float): Learning rate for the RPN for the first run with trainable set to True\n        lr_classifier_trainable_true (float): Learning rate for the classifier for the first run with trainable set to True\n        lr_rpn_trainable_false (float): Learning rate for the RPN for the second run with trainable set to False\n        lr_classifier_trainable_false (float): Learning rate for the classifier for the second run with trainable set to False\n    Raises:\n        ValueError: If img_min_side_size is not positive\n        ValueError: If rpn_min_overlap is not in [0, 1]\n        ValueError: If rpn_max_overlap is not in [0, 1]\n        ValueError: If rpn_min_overlap &gt; rpn_max_overlap\n        ValueError: If rpn_restrict_num_regions is not positive\n        ValueError: If pool_resize_classifier is not positive\n        ValueError: If nb_rois_classifier is not positive\n        ValueError: If roi_nms_overlap_threshold is not in [0, 1]\n        ValueError: If nms_max_boxes is not positive\n        ValueError: If classifier_min_overlap is not in [0, 1]\n        ValueError: If classifier_max_overlap is not in [0, 1]\n        ValueError: If classifier_min_overlap &gt; classifier_max_overlap\n        ValueError: If pred_bbox_proba_threshold is not in [0, 1]\n        ValueError: If pred_nms_overlap_threshold is not in [0, 1]\n        ValueError: If color_mode is not 'rgb'\n        ValueError: If the minimum size of the image is inferior to twice the subsampling ratio\n    '''\n    # Manage errors\n    if img_min_side_size &lt; 1:\n        raise ValueError(f\"The argument img_min_side_size ({img_min_side_size}) must be positive\")\n    if not 0 &lt;= rpn_min_overlap &lt;= 1:\n        raise ValueError(f\"The argument rpn_min_overlap ({rpn_min_overlap}) must be between 0 and 1, included\")\n    if not 0 &lt;= rpn_max_overlap &lt;= 1:\n        raise ValueError(f\"The argument rpn_max_overlap ({rpn_max_overlap}) must be between 0 and 1, included\")\n    if rpn_min_overlap &gt; rpn_max_overlap:\n        raise ValueError(f\"The argument rpn_min_overlap ({rpn_min_overlap}) can't be superior to rpn_max_overlap ({rpn_max_overlap})\")\n    if rpn_restrict_num_regions &lt; 1:\n        raise ValueError(f\"The argument rpn_restrict_num_regions ({rpn_restrict_num_regions}) must be positive\")\n    if pool_resize_classifier &lt; 1:\n        raise ValueError(f\"The argument pool_resize_classifier ({pool_resize_classifier}) must be positive\")\n    if nb_rois_classifier &lt; 1:\n        raise ValueError(f\"The argument nb_rois_classifier ({nb_rois_classifier}) must be positive\")\n    if not 0 &lt;= roi_nms_overlap_threshold &lt;= 1:\n        raise ValueError(f\"The argument roi_nms_overlap_threshold ({roi_nms_overlap_threshold}) must be between 0 and 1, included\")\n    if nms_max_boxes &lt; 1:\n        raise ValueError(f\"The argument nms_max_boxes ({nms_max_boxes}) must be positive\")\n    if not 0 &lt;= classifier_min_overlap &lt;= 1:\n        raise ValueError(f\"The argument classifier_min_overlap ({classifier_min_overlap}) must be between 0 and 1, included\")\n    if not 0 &lt;= classifier_max_overlap &lt;= 1:\n        raise ValueError(f\"The argument classifier_max_overlap ({classifier_max_overlap}) must be between 0 and 1, included\")\n    if classifier_min_overlap &gt; classifier_max_overlap:\n        raise ValueError(f\"The argument classifier_min_overlap ({classifier_min_overlap}) can't be superior to classifier_max_overlap ({classifier_max_overlap})\")\n    if not 0 &lt;= pred_bbox_proba_threshold &lt;= 1:\n        raise ValueError(f\"The argument pred_bbox_proba_threshold ({pred_bbox_proba_threshold}) must be between 0 and 1, included\")\n    if not 0 &lt;= pred_nms_overlap_threshold &lt;= 1:\n        raise ValueError(f\"The argument pred_nms_overlap_threshold ({pred_nms_overlap_threshold}) must be between 0 and 1, included\")\n\n    # Size of the input images (must be defined before the super init because it is used in the method _get_preprocess_input)\n    self.img_min_side_size = img_min_side_size  # Default 300, in the paper 600\n\n    # Init. (by default we have some data augmentation)\n    if data_augmentation_params is None:\n        data_augmentation_params = {'horizontal_flip': True, 'vertical_flip': True, 'rot_90': True}\n    super().__init__(data_augmentation_params=data_augmentation_params, **kwargs)\n    if self.color_mode != 'rgb':\n        raise ValueError(\"Faster RCNN model only accept color_mode equal to 'rgb' (compatibility VGG16).\")\n\n    # Put to None some parameters of model_keras not used by this model\n    self.width = None\n    self.height = None\n    self.depth = None\n    self.in_memory = None\n    self.nb_train_generator_images_to_save = None\n\n    # Get logger (must be done after super init)\n    self.logger = logging.getLogger(__name__)\n\n    # Models, set on fit\n    self.model: Any = None\n    self.shared_model = None\n    self.model_rpn = None\n    self.model_classifier = None\n\n    # Weights\n    self.vgg_filename = 'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n    self.vgg_path = os.path.join(utils.get_data_path(), 'transfer_learning_weights', self.vgg_filename)\n    vgg16_weights_backup_urls = [\n        'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n    ]\n    if not os.path.exists(self.vgg_path):\n        try:\n            self.logger.warning(\"The weights file for VGG16 is not present in your data folder.\")\n            self.logger.warning(\"Trying to download the file.\")\n            utils.download_url(vgg16_weights_backup_urls, self.vgg_path)\n        except ConnectionError:\n            self.logger.warning(\"Can't download. You can try to download it manually and save it on DVC.\")\n            self.logger.warning(\"Building this model will return an error.\")\n            self.logger.warning(\"You can download the weights here : https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\")\n            # We don't raise an error because we may reload a trained model\n\n    ### Model configuration\n\n    # Configurations related to the base model\n    self.shared_model_subsampling = 16  # VGG 16\n    # Error if img_min_side_size &lt; 2 * subsampling rate\n    if self.img_min_side_size &lt; 2 * self.shared_model_subsampling:\n        raise ValueError(\"Can't have a minimum size of an image inferior to twice the subsampling ratio\")\n\n    # Anchors boxes\n    self.anchor_box_sizes = [64, 128, 256]  # In the paper : [128, 256, 512]\n    self.anchor_box_ratios = [[1, 1], [1. / math.sqrt(2), 2. / math.sqrt(2)], [2. / math.sqrt(2), 1. / math.sqrt(2)]]  # In the paper : [1, 1], [1, 2], [2, 1]]\n    self.nb_anchors = len(self.anchor_box_sizes) * len(self.anchor_box_ratios)\n    self.list_anchors = [[anchor_size * anchor_ratio[0], anchor_size * anchor_ratio[1]]\n                         for anchor_size in self.anchor_box_sizes for anchor_ratio in self.anchor_box_ratios]\n\n    # Sizes\n    self.pool_resize_classifier = pool_resize_classifier  # Def 7\n\n    # Scaling (we could probably do without scaling)\n    self.rpn_regr_scaling = 4.0\n    self.classifier_regr_scaling = [8.0, 8.0, 4.0, 4.0]\n\n    # Thresholds for the RPN to find positive and negative anchor boxes\n    self.rpn_min_overlap = rpn_min_overlap  # Def 0.3\n    self.rpn_max_overlap = rpn_max_overlap  # Def 0.7\n    # Maximum number of regions targets of the RPN\n    self.rpn_restrict_num_regions = rpn_restrict_num_regions\n\n    # Classifier configuration\n    self.nb_rois_classifier = nb_rois_classifier  # Def 4\n    self.roi_nms_overlap_threshold = roi_nms_overlap_threshold  # Def 0.7\n    self.nms_max_boxes = nms_max_boxes  # Def 300\n    self.classifier_min_overlap = classifier_min_overlap  # Def 0.1\n    self.classifier_max_overlap = classifier_max_overlap  # Def 0.5\n\n    # Prediction Thresholds\n    self.pred_bbox_proba_threshold = pred_bbox_proba_threshold  # Def 0.6\n    self.pred_nms_overlap_threshold = pred_nms_overlap_threshold  # Def 0.2\n\n    ### Misc.\n\n    # We add the custom objects only when fitting because we need the number of classes\n\n    # Manage batch_size, epochs &amp; patience (back up on global values if not specified)\n    self.batch_size_rpn_trainable_true = batch_size_rpn_trainable_true if batch_size_rpn_trainable_true is not None else self.batch_size\n    self.batch_size_classifier_trainable_true = batch_size_classifier_trainable_true if batch_size_classifier_trainable_true is not None else self.batch_size\n    self.batch_size_rpn_trainable_false = batch_size_rpn_trainable_false if batch_size_rpn_trainable_false is not None else self.batch_size\n    self.batch_size_classifier_trainable_false = batch_size_classifier_trainable_false if batch_size_classifier_trainable_false is not None else self.batch_size\n    self.epochs_rpn_trainable_true = epochs_rpn_trainable_true if epochs_rpn_trainable_true is not None else self.epochs\n    self.epochs_classifier_trainable_true = epochs_classifier_trainable_true if epochs_classifier_trainable_true is not None else self.epochs\n    self.epochs_rpn_trainable_false = epochs_rpn_trainable_false if epochs_rpn_trainable_false is not None else self.epochs\n    self.epochs_classifier_trainable_false = epochs_classifier_trainable_false if epochs_classifier_trainable_false is not None else self.epochs\n    self.patience_rpn_trainable_true = patience_rpn_trainable_true if patience_rpn_trainable_true is not None else self.patience\n    self.patience_classifier_trainable_true = patience_classifier_trainable_true if patience_classifier_trainable_true is not None else self.patience\n    self.patience_rpn_trainable_false = patience_rpn_trainable_false if patience_rpn_trainable_false is not None else self.patience\n    self.patience_classifier_trainable_false = patience_classifier_trainable_false if patience_classifier_trainable_false is not None else self.patience\n\n    # Save learning rates in params_keras\n    self.keras_params['lr_rpn_trainable_true'] = lr_rpn_trainable_true\n    self.keras_params['lr_classifier_trainable_true'] = lr_classifier_trainable_true\n    self.keras_params['lr_rpn_trainable_false'] = lr_rpn_trainable_false\n    self.keras_params['lr_classifier_trainable_false'] = lr_classifier_trainable_false\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_keras_faster_rcnn/#template_vision.models_training.object_detectors.model_keras_faster_rcnn.ModelKerasFasterRcnnObjectDetector.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration, the network used and its preprocessing - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file hdf5_path (str): Path to hdf5 file preprocess_input_path (str): Path to preprocess input file</p> <p>Raises:     ValueError : If configuration_path is None     ValueError : If hdf5_path is None     ValueError : If preprocess_input_path is None     FileNotFoundError : If the object configuration_path is not an existing file     FileNotFoundError : If the object hdf5_path is not an existing file     FileNotFoundError : If the object preprocess_input_path is not an existing file</p> Source code in <code>template_vision/models_training/object_detectors/model_keras_faster_rcnn.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n    '''Reloads a model from its configuration, the network used and its preprocessing\n    - /!\\\\ Experimental /!\\\\ -\n\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        hdf5_path (str): Path to hdf5 file\n        preprocess_input_path (str): Path to preprocess input file\n    Raises:\n        ValueError : If configuration_path is None\n        ValueError : If hdf5_path is None\n        ValueError : If preprocess_input_path is None\n        FileNotFoundError : If the object configuration_path is not an existing file\n        FileNotFoundError : If the object hdf5_path is not an existing file\n        FileNotFoundError : If the object preprocess_input_path is not an existing file\n    '''\n    # Retrieve args\n    configuration_path = kwargs.get('configuration_path', None)\n    hdf5_path = kwargs.get('hdf5_path', None)\n    preprocess_input_path = kwargs.get('preprocess_input_path', None)\n\n    # Checks\n    if configuration_path is None:\n        raise ValueError(\"The argument configuration_path can't be None\")\n    if hdf5_path is None:\n        raise ValueError(\"The argument hdf5_path can't be None\")\n    if preprocess_input_path is None:\n        raise ValueError(\"The argument preprocess_input_path can't be None\")\n    if not os.path.exists(configuration_path):\n        raise FileNotFoundError(f\"The file {configuration_path} does not exist\")\n    if not os.path.exists(hdf5_path):\n        raise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\n    if not os.path.exists(preprocess_input_path):\n        raise FileNotFoundError(f\"The file {preprocess_input_path} does not exist\")\n\n    # Load confs\n    with open(configuration_path, 'r', encoding='utf-8') as f:\n        configs = json.load(f)\n    # Can't set int as keys in json, so need to cast it after reloading\n    # dict_classes keys are always ints\n    if 'dict_classes' in configs.keys():\n        configs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n    elif 'list_classes' in configs.keys():\n        configs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n\n    # Set class vars\n    # self.model_name = # Keep the created name\n    # self.model_dir = # Keep the created folder\n    self.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\n    self.trained = configs.get('trained', True)  # Consider trained by default\n    # Try to read the following attributes from configs and, if absent, keep the current one\n    for attribute in ['model_type', 'list_classes', 'dict_classes', 'level_save', 'batch_size',\n                      'epochs', 'validation_split', 'patience', 'color_mode', 'data_augmentation_params',\n                      'vgg_filename', 'shared_model_subsampling', 'anchor_box_sizes', 'anchor_box_ratios',\n                      'nb_anchors', 'list_anchors', 'img_min_side_size', 'pool_resize_classifier',\n                      'rpn_regr_scaling', 'classifier_regr_scaling', 'rpn_min_overlap', 'rpn_max_overlap',\n                      'rpn_restrict_num_regions', 'nb_rois_classifier', 'roi_nms_overlap_threshold',\n                      'nms_max_boxes', 'classifier_min_overlap', 'classifier_max_overlap',\n                      'pred_bbox_proba_threshold', 'pred_nms_overlap_threshold',\n                      'batch_size_rpn_trainable_true', 'batch_size_classifier_trainable_true',\n                      'batch_size_rpn_trainable_false', 'batch_size_classifier_trainable_false',\n                      'epochs_rpn_trainable_true', 'epochs_classifier_trainable_true',\n                      'epochs_rpn_trainable_false', 'epochs_classifier_trainable_false',\n                      'patience_rpn_trainable_true', 'patience_classifier_trainable_true',\n                      'patience_rpn_trainable_false', 'patience_classifier_trainable_false',\n                      'keras_params']:\n        setattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n    for attribute in ['width', 'height', 'depth']:\n        setattr(self, attribute, configs.get(attribute, None))\n    self.in_memory = None\n    self.nb_train_generator_images_to_save = None\n    self.vgg_path = os.path.join(utils.get_data_path(), 'transfer_learning_weights', self.vgg_filename)  # Try to reload from usual path. Not really important if it fails.\n\n    # Set custom objects\n    self.custom_objects = {**utils_faster_rcnn.get_custom_objects_faster_rcnn(self.nb_anchors, len(self.list_classes)), **self.custom_objects}\n\n    # Reload model\n    self.reload_models_from_hdf5(hdf5_path)\n\n    # Reload preprocess_input\n    with open(preprocess_input_path, 'rb') as f:\n        self.preprocess_input = pickle.load(f)\n\n    # Save best hdf5 in new folder\n    new_hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\n    shutil.copyfile(hdf5_path, new_hdf5_path)\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_keras_faster_rcnn/#template_vision.models_training.object_detectors.model_keras_faster_rcnn.ModelKerasFasterRcnnObjectDetector.reload_models_from_hdf5","title":"<code>reload_models_from_hdf5(hdf5_path)</code>","text":"<p>Reloads all models from a unique hdf5 file. This method is specific to the faster RCNN model.</p> <p>Parameters:</p> Name Type Description Default <code>hdf5_path</code> <code>str</code> <p>Path to the .hdf5 file with the weightds of model_all</p> required <p>Raises:     FileNotFoundError: If the object hdf5_path is not an existing file</p> Source code in <code>template_vision/models_training/object_detectors/model_keras_faster_rcnn.py</code> <pre><code>def reload_models_from_hdf5(self, hdf5_path: str) -&gt; None:\n    '''Reloads all models from a unique hdf5 file. This method is specific to the faster RCNN model.\n\n    Args:\n        hdf5_path (str): Path to the .hdf5 file with the weightds of model_all\n    Raises:\n        FileNotFoundError: If the object hdf5_path is not an existing file\n    '''\n    # Check path exists\n    if not os.path.exists(hdf5_path):\n        raise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\n\n    # Reload model (based on get_models)\n    # Set layers\n    input_img = Input(shape=(None, None, 3), name='input_img')  # Warning, 3 channels !\n    input_rois = Input(shape=(None, 4), name='input_rois')\n    shared_model_layers = self._get_shared_model_structure(input_img)\n    rpn_layers = self._add_rpn_layers(shared_model_layers)\n    classifier_layers = self._add_classifier_layers(shared_model_layers, input_rois)\n    # Init. models\n    self.shared_model = Model(input_img, shared_model_layers)\n    self.model_rpn = Model(input_img, rpn_layers)\n    self.model_classifier = Model([input_img, input_rois], classifier_layers)\n    self.model = Model([input_img, input_rois], rpn_layers + classifier_layers)\n    # Load the weights (loading the weights of model all will also load the weights of the other models since they are linked)\n    self.model.load_weights(hdf5_path)\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_keras_faster_rcnn/#template_vision.models_training.object_detectors.model_keras_faster_rcnn.ModelKerasFasterRcnnObjectDetector.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_vision/models_training/object_detectors/model_keras_faster_rcnn.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save configuration JSON\n    if json_data is None:\n        json_data = {}\n\n    json_data['vgg_filename'] = self.vgg_filename\n    json_data['shared_model_subsampling'] = self.shared_model_subsampling\n    json_data['anchor_box_sizes'] = self.anchor_box_sizes\n    json_data['anchor_box_ratios'] = self.anchor_box_ratios\n    json_data['nb_anchors'] = self.nb_anchors\n    json_data['list_anchors'] = self.list_anchors\n    json_data['img_min_side_size'] = self.img_min_side_size\n    json_data['pool_resize_classifier'] = self.pool_resize_classifier\n    json_data['rpn_regr_scaling'] = self.rpn_regr_scaling\n    json_data['classifier_regr_scaling'] = self.classifier_regr_scaling\n    json_data['rpn_min_overlap'] = self.rpn_min_overlap\n    json_data['rpn_max_overlap'] = self.rpn_max_overlap\n    json_data['rpn_restrict_num_regions'] = self.rpn_restrict_num_regions\n    json_data['nb_rois_classifier'] = self.nb_rois_classifier\n    json_data['roi_nms_overlap_threshold'] = self.roi_nms_overlap_threshold\n    json_data['nms_max_boxes'] = self.nms_max_boxes\n    json_data['classifier_min_overlap'] = self.classifier_min_overlap\n    json_data['classifier_max_overlap'] = self.classifier_max_overlap\n    json_data['pred_bbox_proba_threshold'] = self.pred_bbox_proba_threshold\n    json_data['pred_nms_overlap_threshold'] = self.pred_nms_overlap_threshold\n    json_data['batch_size_rpn_trainable_true'] = self.batch_size_rpn_trainable_true\n    json_data['batch_size_classifier_trainable_true'] = self.batch_size_classifier_trainable_true\n    json_data['batch_size_rpn_trainable_false'] = self.batch_size_rpn_trainable_false\n    json_data['batch_size_classifier_trainable_false'] = self.batch_size_classifier_trainable_false\n    json_data['epochs_rpn_trainable_true'] = self.epochs_rpn_trainable_true\n    json_data['epochs_classifier_trainable_true'] = self.epochs_classifier_trainable_true\n    json_data['epochs_rpn_trainable_false'] = self.epochs_rpn_trainable_false\n    json_data['epochs_classifier_trainable_false'] = self.epochs_classifier_trainable_false\n    json_data['patience_rpn_trainable_true'] = self.patience_rpn_trainable_true\n    json_data['patience_classifier_trainable_true'] = self.patience_classifier_trainable_true\n    json_data['patience_rpn_trainable_false'] = self.patience_rpn_trainable_false\n    json_data['patience_classifier_trainable_false'] = self.patience_classifier_trainable_false\n\n    # Add some code if not in json_data:\n    for layer_or_compile in ['_add_rpn_layers', '_add_classifier_layers', '_compile_model_rpn', '_compile_model_classifier']:\n        if layer_or_compile not in json_data:\n            json_data[layer_or_compile] = pickle.source.getsourcelines(getattr(self, layer_or_compile))[0]\n\n    # Save\n    # Save strategy :\n    # - best.hdf5 already saved in fit() &amp; contains all models\n    # - as we can't pickle models, we drop them, save, and reload them\n    shared_model = self.shared_model\n    model_rpn = self.model_rpn\n    model_classifier = self.model_classifier\n    self.shared_model = None\n    self.model_rpn = None\n    self.model_classifier = None\n    super().save(json_data=json_data)\n    self.shared_model = shared_model\n    self.model_rpn = model_rpn\n    self.model_classifier = model_classifier\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_object_detector/","title":"Model object detector","text":""},{"location":"reference/template_vision/models_training/object_detectors/model_object_detector/#template_vision.models_training.object_detectors.model_object_detector.ModelObjectDetectorMixin","title":"<code>ModelObjectDetectorMixin</code>","text":"<p>Parent class (Mixin) for the model of type object detector</p> Source code in <code>template_vision/models_training/object_detectors/model_object_detector.py</code> <pre><code>class ModelObjectDetectorMixin:\n    '''Parent class (Mixin) for the model of type object detector'''\n\n    # Not implemented :\n    # -&gt; predict : to be implemented by the parent class using this mixin\n\n    def __init__(self, level_save: str = 'HIGH', **kwargs) -&gt; None:\n        '''Initialization of the parent class - Object detector\n\n        Kwargs:\n            level_save (str): Level of saving\n                LOW: stats + configurations + logger keras - /!\\\\ The model can't be reused /!\\\\ -\n                MEDIUM: LOW + hdf5 + pkl + plots\n                HIGH: MEDIUM + predictions\n        Raises:\n            ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\n        '''\n        super().__init__(level_save=level_save, **kwargs)  # forwards level_save &amp; all unused arguments\n\n        if level_save not in ['LOW', 'MEDIUM', 'HIGH']:\n            raise ValueError(f\"The object level_save ({level_save}) is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\")\n\n        # Get logger\n        self.logger = logging.getLogger(__name__)\n\n        # Model type\n        self.model_type = 'object_detector'\n\n        # List of classes to consider (set on fit)\n        self.list_classes = None\n        self.dict_classes = None\n\n        # Other options\n        self.level_save = level_save\n\n    def inverse_transform(self, y) -&gt; list:\n        '''Gets a list of classes from predictions.\n        Useless here, used solely for compatibility.\n\n        Args:\n            y (?): Array-like, shape = [n_samples, n_features], arrays of 0s and 1s\n        Returns:\n            (list)\n        '''\n        return list(y) if isinstance(y, np.ndarray) else y\n\n    def get_and_save_metrics(self, y_true: list, y_pred: list, list_files_x: list = None,\n                             type_data: str = '', **kwargs) -&gt; pd.DataFrame:\n        '''Gets and saves the metrics of a model\n\n        Args:\n            y_true (list): Bboxes list, one entry corresponds to the bboxes of one file - truth\n                format bbox : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ...}\n            y_pred (list): Bboxes list, one entry corresponds to the bboxes of one file - predicted\n                format bbox : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ..., 'proba': ...}\n        Kwargs:\n            list_files_x (?): List of input files for the prediction\n            type_data (str): Type of the dataset (validation, test, ...)\n        Returns:\n            pd.DataFrame: The dataframe containing statistics\n        '''\n        # Manage errors\n        if len(y_true) != len(y_pred):\n            raise ValueError(f\"The size of the two lists (y_true et y_pred) must be equal ({len(y_true)} != {len(y_pred)})\")\n        if list_files_x is not None and len(y_true) != len(list_files_x):\n            raise ValueError(f\"The size of the two lists (y_true et list_files_x) must be equal ({len(y_true)} != {len(list_files_x)})\")\n\n        # Construction dataframe\n        if list_files_x is None:\n            df = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred})\n        else:\n            df = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred, 'file_path': list_files_x})\n\n        # Save a prediction file if wanted\n        if self.level_save == 'HIGH':\n            file_path = os.path.join(self.model_dir, f\"predictions{'_' + type_data if len(type_data) &gt; 0 else ''}.csv\")\n            if 'file_path' in df.columns:\n                df = df.sort_values('file_path')\n            df.to_csv(file_path, sep=';', index=None, encoding='utf-8')\n\n        # Print info on missing classes and the impact on metrics\n        gt_classes = set([bbox['class'] for bboxes in y_true for bbox in bboxes])\n        gt_classes_not_in_model = gt_classes.difference(set(self.list_classes))\n        model_classes_not_in_gt = set(self.list_classes).difference(gt_classes)\n        # Prints\n        if len(gt_classes_not_in_model):\n            self.logger.info(f\"Classes {gt_classes_not_in_model} are not predicted by the model.\")\n            self.logger.info(\"We won't take them into account in the calculation of the metrics.\")\n        if len(model_classes_not_in_gt):\n            self.logger.info(f\"Classes {model_classes_not_in_gt} are not present in the dataset used to calculate the metrics.\")\n            self.logger.info(\"Metrics on these classes won't be accurate.\")\n\n        # Get the classes support\n        total_bbox = sum([1 for image in y_true for bbox in image if bbox['class'] in self.list_classes])\n        classes_support = {}\n        if total_bbox == 0:\n            total_bbox = 1\n        for cl in self.list_classes:\n            classes_support[cl] = sum([bbox['class'] == cl for image in y_true for bbox in image]) / total_bbox\n\n        # Get metrics\n        # We use the COCO method to get the Average Precision (AP)\n        dict_ap_coco = self._get_coco_ap(y_true, y_pred)\n\n        # Calculate the mean Average Precision (mAP) (weighted or not)\n        coco_map = np.mean([value for value in list(dict_ap_coco.values()) if not np.isnan(value)])\n        coco_wap = sum([dict_ap_coco[cl] * classes_support[cl] for cl in self.list_classes if classes_support[cl] &gt; 0])\n\n        # Global statistics\n        self.logger.info('-- * * * * * * * * * * * * * * --')\n        self.logger.info(f\"Statistics mAP{' ' + type_data if len(type_data) &gt; 0 else ''}\")\n        self.logger.info('--------------------------------')\n        self.logger.info(f\"mean Average Precision (mAP) - COCO method : {round(coco_map, 4)}\")\n        self.logger.info('--------------------------------')\n        self.logger.info(f\"weighted Average Precision (wAP) - COCO method : {round(coco_wap, 4)}\")\n        self.logger.info('--------------------------------')\n\n        # Statistics per classes\n        for cl in self.list_classes:\n            self.logger.info(f\"Class {cl}: AP COCO = {round(dict_ap_coco[cl], 4)} /// Support = {round(classes_support[cl], 4)}\")\n        self.logger.info('--------------------------------')\n\n        # Construction df_stats\n        dict_df_stats = {}\n        dict_df_stats[0] = {'Label': 'All', 'AP COCO': coco_map, 'Support': 1.0}\n        for i, cl in enumerate(self.list_classes):\n            dict_df_stats[i+1] = {'Label': cl, 'AP COCO': dict_ap_coco[cl], 'Support': classes_support[cl]}\n\n        df_stats = pd.DataFrame.from_dict(dict_df_stats, orient='index')\n        # Save csv\n        file_path = os.path.join(self.model_dir, f\"map_coco{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(coco_map, 4)}.csv\")\n        df_stats.to_csv(file_path, sep=';', index=False, encoding='utf-8')\n\n        # Return df_stats\n        return df_stats\n\n    def _get_coco_ap(self, y_true: list, y_pred: list) -&gt; dict:\n        '''Calculate COCO's AP for each of the class and gives the result in a dictionary\n        where the keys are the classes and the valeus, the corresponding AP value\n\n         Args:\n            y_true (list): Bboxes list, one entry corresponds to the bboxes of one file - truth\n                format bbox : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ...}\n            y_pred (list): Bboxes list, one entry corresponds to the bboxes of one file - predicted\n                format bbox : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ..., 'proba': ...}\n        Returns:\n            The dictionary containing AP for each class\n        '''\n        inv_dict_classes = {value: key for key, value in self.dict_classes.items()}\n        # Put the bboxes in COCO format\n        coco_true = self._put_bboxes_in_coco_format(y_true, inv_dict_classes)\n        coco_pred = self._put_bboxes_in_coco_format(y_pred, inv_dict_classes)\n        images = [{'id': i + 1} for i in range(len(y_true))]\n        categories = [{'id': class_id, 'name': class_name, 'supercategory': 'none'}\n                      for class_id, class_name in self.dict_classes.items()]\n        dataset_coco_true = {'type': 'instances',\n                             'images': images.copy(),\n                             'categories': categories.copy(),\n                             'annotations': coco_true}\n        dataset_coco_pred = {'images': images.copy(),\n                             'categories': categories.copy(),\n                             'annotations': coco_pred}\n        # Call pycocotools API to calculate the AP\n        coco_eval = self._get_coco_evaluations(dataset_coco_true, dataset_coco_pred)\n        dict_ap = self._get_ap_for_classes(coco_eval)\n        return dict_ap\n\n    @classmethod\n    def _put_bboxes_in_coco_format(self, bboxes: List[List[dict]], inv_dict_classes: dict) -&gt; List[dict]:\n        '''Puts a list of list of bboxes (for example from a prediction) in the right format for pycocotools API.\n\n        Args:\n            bboxes (list&lt;list&lt;dict&gt;&gt;) : A list of list of bboxes. The first level of list corresponds to the images and the second level to the\n            bboxes of this image.\n            inv_dict_classes (dict) : The dictionary of classes in the format {class_name: class_id}\n        Returns:\n            A list of bboxes\n        '''\n        annotations = []\n        idx_bbox = 1  # WARNING: index begins at 1\n        for idx_img, list_bboxes in enumerate(bboxes):\n            for bbox in list_bboxes:\n                dict_bbox = {'id': idx_bbox,\n                             'image_id': idx_img + 1,  # WARNING : index begins at 1\n                             'category_id': inv_dict_classes[bbox['class']],\n                             'bbox': np.array([bbox['x1'], bbox['y1'], bbox['x2'] - bbox['x1'], bbox['y2'] - bbox['y1']]),\n                             'area': (bbox['y2'] - bbox['y1']) * (bbox['x2'] - bbox['x1']),\n                             'iscrowd': 0,\n                             'score': bbox.get('proba', 1)}\n                idx_bbox += 1\n                annotations.append(dict_bbox.copy())\n        return annotations\n\n    @classmethod\n    def _get_coco_evaluations(self, dataset_coco_true: dict, dataset_coco_pred: dict) -&gt; COCOeval:\n        '''Calculates the AP from true and predicted datasets in the COCO format, the returns COCOeval,\n        the pycocotools API containing all the results.\n\n        Args:\n            dataset_coco_true (dict) : Ground truth bboxes in COCO format\n            dataset_coco_pred (dict) : Predicted bboxes in COCO format\n        Returns:\n            A COCOeval (pycocotools API) containing the AP\n        '''\n        # Everything on mute ! pycocotools library prints too much logs and there are no level settings\n        with utils.HiddenPrints():\n            # Put the ground truth bboxes in the pycocotools API\n            coco_ds = COCO()\n            coco_ds.dataset = dataset_coco_true.copy()\n            coco_ds.createIndex()\n\n            # Put the predicted bboxes in the pycocotools API\n            coco_dt = COCO()\n            coco_dt.dataset = dataset_coco_pred.copy()\n            coco_dt.createIndex()\n\n            # Get image IDs\n            imgIds = sorted(coco_ds.getImgIds())\n\n            # Set evaluator\n            cocoEval = COCOeval(coco_ds, coco_dt, 'bbox')\n            cocoEval.params.imgIds = imgIds\n            cocoEval.params.useCats = True\n            cocoEval.params.iouType = \"bbox\"\n\n            # Evaluate\n            cocoEval.evaluate()\n            cocoEval.accumulate()\n\n        # Return evaluator\n        return cocoEval\n\n    def _get_ap_for_classes(self, coco_eval: COCOeval) -&gt; dict:\n        '''Gets the AP per class from cocoEval, the pycocotools API.\n\n        Args:\n            coco_eval (COCOeval) : A pycocotools COCOeval which calculated the AP.\n                In this function, we just get them, we do not calculate them\n        Raises:\n            ValueError : The precision has not the right shape (iou, recall, cls, area range, max dets)\n        Returns:\n            The dictionary containing the AP for each class\n        '''\n        # Compute per-category AP\n        # from https://detectron2.readthedocs.io/en/latest/_modules/detectron2/evaluation/coco_evaluation.html\n        precisions = coco_eval.eval[\"precision\"]\n        # precision has dims (iou, recall, cls, area range, max dets)\n        if len(self.dict_classes) != precisions.shape[2]:\n            raise ValueError(f\"The precision has not the right shape (iou, recall, cls, area range, max dets): {precisions.shape}\")\n\n        # Retrieve APs\n        dict_ap = {}\n        for idx, name in self.dict_classes.items():\n            # area range index 0: all area ranges\n            # max dets index -1: typically 100 per image\n            precision = precisions[:, :, idx, 0, -1]\n            precision = precision[precision &gt; -1]\n            ap = np.mean(precision) if precision.size else float(\"nan\")\n            dict_ap[name] = ap\n        return dict_ap\n\n    def save(self, json_data: Union[dict, None] = None) -&gt; None:\n        '''Saves the model\n\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n        # Save model\n        if json_data is None:\n            json_data = {}\n\n        json_data['list_classes'] = self.list_classes\n        json_data['dict_classes'] = self.dict_classes\n\n        # Save\n        super().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_object_detector/#template_vision.models_training.object_detectors.model_object_detector.ModelObjectDetectorMixin.__init__","title":"<code>__init__(level_save='HIGH', **kwargs)</code>","text":"<p>Initialization of the parent class - Object detector</p> Kwargs <p>level_save (str): Level of saving     LOW: stats + configurations + logger keras - /! The model can't be reused /! -     MEDIUM: LOW + hdf5 + pkl + plots     HIGH: MEDIUM + predictions</p> <p>Raises:     ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])</p> Source code in <code>template_vision/models_training/object_detectors/model_object_detector.py</code> <pre><code>def __init__(self, level_save: str = 'HIGH', **kwargs) -&gt; None:\n    '''Initialization of the parent class - Object detector\n\n    Kwargs:\n        level_save (str): Level of saving\n            LOW: stats + configurations + logger keras - /!\\\\ The model can't be reused /!\\\\ -\n            MEDIUM: LOW + hdf5 + pkl + plots\n            HIGH: MEDIUM + predictions\n    Raises:\n        ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\n    '''\n    super().__init__(level_save=level_save, **kwargs)  # forwards level_save &amp; all unused arguments\n\n    if level_save not in ['LOW', 'MEDIUM', 'HIGH']:\n        raise ValueError(f\"The object level_save ({level_save}) is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\")\n\n    # Get logger\n    self.logger = logging.getLogger(__name__)\n\n    # Model type\n    self.model_type = 'object_detector'\n\n    # List of classes to consider (set on fit)\n    self.list_classes = None\n    self.dict_classes = None\n\n    # Other options\n    self.level_save = level_save\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_object_detector/#template_vision.models_training.object_detectors.model_object_detector.ModelObjectDetectorMixin.get_and_save_metrics","title":"<code>get_and_save_metrics(y_true, y_pred, list_files_x=None, type_data='', **kwargs)</code>","text":"<p>Gets and saves the metrics of a model</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>list</code> <p>Bboxes list, one entry corresponds to the bboxes of one file - truth format bbox : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ...}</p> required <code>y_pred</code> <code>list</code> <p>Bboxes list, one entry corresponds to the bboxes of one file - predicted format bbox : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ..., 'proba': ...}</p> required <p>Kwargs:     list_files_x (?): List of input files for the prediction     type_data (str): Type of the dataset (validation, test, ...) Returns:     pd.DataFrame: The dataframe containing statistics</p> Source code in <code>template_vision/models_training/object_detectors/model_object_detector.py</code> <pre><code>def get_and_save_metrics(self, y_true: list, y_pred: list, list_files_x: list = None,\n                         type_data: str = '', **kwargs) -&gt; pd.DataFrame:\n    '''Gets and saves the metrics of a model\n\n    Args:\n        y_true (list): Bboxes list, one entry corresponds to the bboxes of one file - truth\n            format bbox : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ...}\n        y_pred (list): Bboxes list, one entry corresponds to the bboxes of one file - predicted\n            format bbox : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ..., 'proba': ...}\n    Kwargs:\n        list_files_x (?): List of input files for the prediction\n        type_data (str): Type of the dataset (validation, test, ...)\n    Returns:\n        pd.DataFrame: The dataframe containing statistics\n    '''\n    # Manage errors\n    if len(y_true) != len(y_pred):\n        raise ValueError(f\"The size of the two lists (y_true et y_pred) must be equal ({len(y_true)} != {len(y_pred)})\")\n    if list_files_x is not None and len(y_true) != len(list_files_x):\n        raise ValueError(f\"The size of the two lists (y_true et list_files_x) must be equal ({len(y_true)} != {len(list_files_x)})\")\n\n    # Construction dataframe\n    if list_files_x is None:\n        df = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred})\n    else:\n        df = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred, 'file_path': list_files_x})\n\n    # Save a prediction file if wanted\n    if self.level_save == 'HIGH':\n        file_path = os.path.join(self.model_dir, f\"predictions{'_' + type_data if len(type_data) &gt; 0 else ''}.csv\")\n        if 'file_path' in df.columns:\n            df = df.sort_values('file_path')\n        df.to_csv(file_path, sep=';', index=None, encoding='utf-8')\n\n    # Print info on missing classes and the impact on metrics\n    gt_classes = set([bbox['class'] for bboxes in y_true for bbox in bboxes])\n    gt_classes_not_in_model = gt_classes.difference(set(self.list_classes))\n    model_classes_not_in_gt = set(self.list_classes).difference(gt_classes)\n    # Prints\n    if len(gt_classes_not_in_model):\n        self.logger.info(f\"Classes {gt_classes_not_in_model} are not predicted by the model.\")\n        self.logger.info(\"We won't take them into account in the calculation of the metrics.\")\n    if len(model_classes_not_in_gt):\n        self.logger.info(f\"Classes {model_classes_not_in_gt} are not present in the dataset used to calculate the metrics.\")\n        self.logger.info(\"Metrics on these classes won't be accurate.\")\n\n    # Get the classes support\n    total_bbox = sum([1 for image in y_true for bbox in image if bbox['class'] in self.list_classes])\n    classes_support = {}\n    if total_bbox == 0:\n        total_bbox = 1\n    for cl in self.list_classes:\n        classes_support[cl] = sum([bbox['class'] == cl for image in y_true for bbox in image]) / total_bbox\n\n    # Get metrics\n    # We use the COCO method to get the Average Precision (AP)\n    dict_ap_coco = self._get_coco_ap(y_true, y_pred)\n\n    # Calculate the mean Average Precision (mAP) (weighted or not)\n    coco_map = np.mean([value for value in list(dict_ap_coco.values()) if not np.isnan(value)])\n    coco_wap = sum([dict_ap_coco[cl] * classes_support[cl] for cl in self.list_classes if classes_support[cl] &gt; 0])\n\n    # Global statistics\n    self.logger.info('-- * * * * * * * * * * * * * * --')\n    self.logger.info(f\"Statistics mAP{' ' + type_data if len(type_data) &gt; 0 else ''}\")\n    self.logger.info('--------------------------------')\n    self.logger.info(f\"mean Average Precision (mAP) - COCO method : {round(coco_map, 4)}\")\n    self.logger.info('--------------------------------')\n    self.logger.info(f\"weighted Average Precision (wAP) - COCO method : {round(coco_wap, 4)}\")\n    self.logger.info('--------------------------------')\n\n    # Statistics per classes\n    for cl in self.list_classes:\n        self.logger.info(f\"Class {cl}: AP COCO = {round(dict_ap_coco[cl], 4)} /// Support = {round(classes_support[cl], 4)}\")\n    self.logger.info('--------------------------------')\n\n    # Construction df_stats\n    dict_df_stats = {}\n    dict_df_stats[0] = {'Label': 'All', 'AP COCO': coco_map, 'Support': 1.0}\n    for i, cl in enumerate(self.list_classes):\n        dict_df_stats[i+1] = {'Label': cl, 'AP COCO': dict_ap_coco[cl], 'Support': classes_support[cl]}\n\n    df_stats = pd.DataFrame.from_dict(dict_df_stats, orient='index')\n    # Save csv\n    file_path = os.path.join(self.model_dir, f\"map_coco{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(coco_map, 4)}.csv\")\n    df_stats.to_csv(file_path, sep=';', index=False, encoding='utf-8')\n\n    # Return df_stats\n    return df_stats\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_object_detector/#template_vision.models_training.object_detectors.model_object_detector.ModelObjectDetectorMixin.inverse_transform","title":"<code>inverse_transform(y)</code>","text":"<p>Gets a list of classes from predictions. Useless here, used solely for compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features], arrays of 0s and 1s</p> required <p>Returns:     (list)</p> Source code in <code>template_vision/models_training/object_detectors/model_object_detector.py</code> <pre><code>def inverse_transform(self, y) -&gt; list:\n    '''Gets a list of classes from predictions.\n    Useless here, used solely for compatibility.\n\n    Args:\n        y (?): Array-like, shape = [n_samples, n_features], arrays of 0s and 1s\n    Returns:\n        (list)\n    '''\n    return list(y) if isinstance(y, np.ndarray) else y\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_object_detector/#template_vision.models_training.object_detectors.model_object_detector.ModelObjectDetectorMixin.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_vision/models_training/object_detectors/model_object_detector.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n    '''Saves the model\n\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n    # Save model\n    if json_data is None:\n        json_data = {}\n\n    json_data['list_classes'] = self.list_classes\n    json_data['dict_classes'] = self.dict_classes\n\n    # Save\n    super().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_faster_rcnn/","title":"Utils faster rcnn","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_faster_rcnn/#template_vision.models_training.object_detectors.utils_faster_rcnn.RoiPoolingLayer","title":"<code>RoiPoolingLayer</code>","text":"<p>             Bases: <code>Layer</code></p> <p>Layer selecting a zone from a ROI in a features map and resize it</p>"},{"location":"reference/template_vision/models_training/object_detectors/utils_faster_rcnn/#template_vision.models_training.object_detectors.utils_faster_rcnn.RoiPoolingLayer--input-shape","title":"Input shape","text":"<pre><code>List of two 4D tensors [X_img, X_roi] with shape:\nX_img : list of images\n    (batch_size, cols, rows, channels)\nX_roi : list of ROI with 4 coordinates (x, y, w, h)\n    (batch_size, nb_rois, 4)\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_faster_rcnn/#template_vision.models_training.object_detectors.utils_faster_rcnn.RoiPoolingLayer--output-shape","title":"Output shape","text":"<pre><code>5D tensor with shape:\n    (batch_size, nb_rois, pool_size, pool_size, channels)\n    pool_size is a parameter of resizing of the features map\n</code></pre> Source code in <code>template_vision/models_training/object_detectors/utils_faster_rcnn.py</code> <pre><code>class RoiPoolingLayer(Layer):\n    '''Layer selecting a zone from a ROI in a features map and resize it\n\n    # Input shape\n        List of two 4D tensors [X_img, X_roi] with shape:\n        X_img : list of images\n            (batch_size, cols, rows, channels)\n        X_roi : list of ROI with 4 coordinates (x, y, w, h)\n            (batch_size, nb_rois, 4)\n    # Output shape\n        5D tensor with shape:\n            (batch_size, nb_rois, pool_size, pool_size, channels)\n            pool_size is a parameter of resizing of the features map\n    '''\n    def __init__(self, pool_size: int, **kwargs) -&gt; None:\n        '''Initialization of the layer\n\n        Args:\n            pool_size (int): Output size of the layer\n        '''\n        self.pool_size = pool_size\n        super().__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.nb_channels = input_shape[0][3]\n\n    def compute_output_shape(self, input_shape):\n        return None, None, self.pool_size, self.pool_size, self.nb_channels\n\n    def cut_feature_map(self, feature_map, roi):\n        '''Cuts a features map with a ROI\n\n        Args:\n            feature_map : input features map\n                # Shape : (cols, rows, channels)\n            roi : input ROI\n                # Shape : (4,)\n        '''\n        x, y, h, w = roi[0], roi[1], roi[2], roi[3]\n        return tf.image.resize(feature_map[y: y + h, x: x + w, :], (self.pool_size, self.pool_size))\n\n    def call(self, x: list, mask=None):\n        '''Call to the layer\n\n        Args:\n            x (list): List of two tensors\n                0 -&gt; features maps # Shape (batch_size, cols, rows, channels)\n                1 -&gt; rois # Shape (batch_size, nb_rois, 4)\n        Returns:\n            tensor: images (features maps) cut with the ROIs\n                # Shape (batch_size, nb_rois, cols, rows, nb_channel)\n        '''\n        # Get the tensors\n        feature_maps = x[0]  # Shape (batch_size, cols, rows, channels)\n        rois = K.cast(x[1], 'int32')  # Shape (batch_size, nb_rois, 4)\n\n        # We loop on each batch, and then, we loop on each ROI\n        # We crop each image with the ROIs of the batch\n        # We also resize (cf. pool_size)\n        # We format the results and return it\n        # TODO: WARNING, WE MUST HAVE THE SAME NUMBER OF ROIS PER IMAGE :'(\n        output = tf.map_fn(lambda batch:\n            # IN : fmap (h, w, nb_channel)\n            # IN : rois (nb_rois, 4)\n            tf.map_fn(\n                # IN : fmap (h, w, nb_channel)\n                # IN : roi (4,)\n                lambda roi: self.cut_feature_map(batch[0], roi)\n                , batch[1], fn_output_signature=tf.float32\n                # OUT (pool_size, pool_size, nb_channel)\n            )\n            # OUT : (nb_rois, pool_size, pool_size, nb_channel)\n        , (feature_maps, rois), fn_output_signature=tf.float32)\n        # OUT : (batch_size, nb_rois, pool_size, pool_size, nb_channel)\n        return output\n\n    def get_config(self) -&gt; dict:\n        config = {'pool_size': self.pool_size}\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_faster_rcnn/#template_vision.models_training.object_detectors.utils_faster_rcnn.RoiPoolingLayer.__init__","title":"<code>__init__(pool_size, **kwargs)</code>","text":"<p>Initialization of the layer</p> <p>Parameters:</p> Name Type Description Default <code>pool_size</code> <code>int</code> <p>Output size of the layer</p> required Source code in <code>template_vision/models_training/object_detectors/utils_faster_rcnn.py</code> <pre><code>def __init__(self, pool_size: int, **kwargs) -&gt; None:\n    '''Initialization of the layer\n\n    Args:\n        pool_size (int): Output size of the layer\n    '''\n    self.pool_size = pool_size\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_faster_rcnn/#template_vision.models_training.object_detectors.utils_faster_rcnn.RoiPoolingLayer.call","title":"<code>call(x, mask=None)</code>","text":"<p>Call to the layer</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>list</code> <p>List of two tensors 0 -&gt; features maps # Shape (batch_size, cols, rows, channels) 1 -&gt; rois # Shape (batch_size, nb_rois, 4)</p> required <p>Returns:     tensor: images (features maps) cut with the ROIs         # Shape (batch_size, nb_rois, cols, rows, nb_channel)</p> Source code in <code>template_vision/models_training/object_detectors/utils_faster_rcnn.py</code> <pre><code>def call(self, x: list, mask=None):\n    '''Call to the layer\n\n    Args:\n        x (list): List of two tensors\n            0 -&gt; features maps # Shape (batch_size, cols, rows, channels)\n            1 -&gt; rois # Shape (batch_size, nb_rois, 4)\n    Returns:\n        tensor: images (features maps) cut with the ROIs\n            # Shape (batch_size, nb_rois, cols, rows, nb_channel)\n    '''\n    # Get the tensors\n    feature_maps = x[0]  # Shape (batch_size, cols, rows, channels)\n    rois = K.cast(x[1], 'int32')  # Shape (batch_size, nb_rois, 4)\n\n    # We loop on each batch, and then, we loop on each ROI\n    # We crop each image with the ROIs of the batch\n    # We also resize (cf. pool_size)\n    # We format the results and return it\n    # TODO: WARNING, WE MUST HAVE THE SAME NUMBER OF ROIS PER IMAGE :'(\n    output = tf.map_fn(lambda batch:\n        # IN : fmap (h, w, nb_channel)\n        # IN : rois (nb_rois, 4)\n        tf.map_fn(\n            # IN : fmap (h, w, nb_channel)\n            # IN : roi (4,)\n            lambda roi: self.cut_feature_map(batch[0], roi)\n            , batch[1], fn_output_signature=tf.float32\n            # OUT (pool_size, pool_size, nb_channel)\n        )\n        # OUT : (nb_rois, pool_size, pool_size, nb_channel)\n    , (feature_maps, rois), fn_output_signature=tf.float32)\n    # OUT : (batch_size, nb_rois, pool_size, pool_size, nb_channel)\n    return output\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_faster_rcnn/#template_vision.models_training.object_detectors.utils_faster_rcnn.RoiPoolingLayer.cut_feature_map","title":"<code>cut_feature_map(feature_map, roi)</code>","text":"<p>Cuts a features map with a ROI</p> <p>Parameters:</p> Name Type Description Default <code>feature_map</code> <p>input features map</p> required <code>roi</code> <p>input ROI</p> required Source code in <code>template_vision/models_training/object_detectors/utils_faster_rcnn.py</code> <pre><code>def cut_feature_map(self, feature_map, roi):\n    '''Cuts a features map with a ROI\n\n    Args:\n        feature_map : input features map\n            # Shape : (cols, rows, channels)\n        roi : input ROI\n            # Shape : (4,)\n    '''\n    x, y, h, w = roi[0], roi[1], roi[2], roi[3]\n    return tf.image.resize(feature_map[y: y + h, x: x + w, :], (self.pool_size, self.pool_size))\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_faster_rcnn/#template_vision.models_training.object_detectors.utils_faster_rcnn.RoiPoolingLayer.cut_feature_map--shape-cols-rows-channels","title":"Shape : (cols, rows, channels)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_faster_rcnn/#template_vision.models_training.object_detectors.utils_faster_rcnn.RoiPoolingLayer.cut_feature_map--shape-4","title":"Shape : (4,)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_faster_rcnn/#template_vision.models_training.object_detectors.utils_faster_rcnn.class_loss_cls","title":"<code>class_loss_cls(y_true, y_pred)</code>","text":"<p>Calculates the classifier classification loss (Cross entropy)</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Model's target. Shape (batch_size, nb_bboxes, nb_classes)</p> required <code>y_pred</code> <p>Outputs of the model. Shape (batch_size, nb_bboxes, nb_classes)</p> required <p>Returns:     float: Calculated loss</p> Source code in <code>template_vision/models_training/object_detectors/utils_faster_rcnn.py</code> <pre><code>def class_loss_cls(y_true, y_pred):\n    '''Calculates the classifier classification loss (Cross entropy)\n\n    Args:\n        y_true: Model's target. Shape (batch_size, nb_bboxes, nb_classes)\n        y_pred: Outputs of the model. Shape (batch_size, nb_bboxes, nb_classes)\n    Returns:\n        float: Calculated loss\n    '''\n    return K.mean(categorical_crossentropy(y_true, y_pred))\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_faster_rcnn/#template_vision.models_training.object_detectors.utils_faster_rcnn.get_class_loss_regr","title":"<code>get_class_loss_regr(nb_classes)</code>","text":"<p>Gets the classifier regression loss depending on the number of classes of the model</p> <p>Parameters:</p> Name Type Description Default <code>nb_classes</code> <code>int</code> <p>Number of classes of the model</p> required <p>Returns:     Callable: Classifier regression loss</p> Source code in <code>template_vision/models_training/object_detectors/utils_faster_rcnn.py</code> <pre><code>def get_class_loss_regr(nb_classes: int) -&gt; Callable:\n    '''Gets the classifier regression loss depending on the number of classes of the model\n\n    Args:\n        nb_classes (int): Number of classes of the model\n    Returns:\n        Callable: Classifier regression loss\n    '''\n    def class_loss_regr(y_true, y_pred):\n        '''Calculates the classifier regression loss (Huber loss)\n               0.5*x\u00b2 (if x_abs &lt; 1)\n               x_abs - 0.5 (otherwise)\n\n        Args:\n            y_true: Model's targets. Shape (batch_size, nb_bboxes, 2*4*num_classes)\n                first part : ROI class\n                                  Example with two classes (without taking 'bg' into account)\n                                    -&gt; [0, 0, 0, 0, 0, 0, 0, 0] : background\n                                    -&gt; [0, 0, 0, 0, 1, 1, 1, 1] : classe n\u00b01\n                                    etc...\n                                    the loss is calculated only on ROIs associated to an object\n                second part : regression\n            y_pred: Outputs of the model. Shape (batch_size, nb_bboxes, 4*num_classes)\n        Returns:\n            float: Calculated loss\n        '''\n        y_true = tf.cast(y_true, tf.float32)\n        y_pred = tf.cast(y_pred, tf.float32)\n\n        # Evaluate difference\n        ind_sep = 4 * nb_classes  # Separation index of the two parts of y_true\n        x = y_true[:, :, ind_sep:] - y_pred\n        x_abs = K.abs(x)\n\n        # If x_abs &lt;= 1.0, x_bool = 1\n        x_bool = K.cast(K.less_equal(x_abs, 1.0), tf.float32)\n\n        return K.sum(y_true[:, :, :ind_sep] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(1e-4 + y_true[:, :, :ind_sep])\n    # Return loss\n    return class_loss_regr\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_faster_rcnn/#template_vision.models_training.object_detectors.utils_faster_rcnn.get_custom_objects_faster_rcnn","title":"<code>get_custom_objects_faster_rcnn(nb_anchors, nb_classes)</code>","text":"<p>Gets the keras custom_objects depending of the number of anchors and of classes of a model</p> <p>Parameters:</p> Name Type Description Default <code>nb_anchors</code> <code>int</code> <p>Number of anchors of the model</p> required <code>nb_classes</code> <code>int</code> <p>Number of classes of the model</p> required <p>Returns:     dict: Set of customs objects</p> Source code in <code>template_vision/models_training/object_detectors/utils_faster_rcnn.py</code> <pre><code>def get_custom_objects_faster_rcnn(nb_anchors: int, nb_classes: int) -&gt; dict:\n    '''Gets the keras custom_objects depending of the number of anchors and of classes of a model\n\n    Args:\n        nb_anchors (int): Number of anchors of the model\n        nb_classes (int): Number of classes of the model\n    Returns:\n        dict: Set of customs objects\n    '''\n\n    # /!\\ Important -&gt; This dictionary defines the \"custom\" objets used in our Faster RCNN models\n    # /!\\ Important -&gt; They are mandatory in order to serialize and save the model\n    # /!\\ Important -&gt; All customs objects must be added to it\n    custom_objects_faster_rcnn = {\n        'RoiPoolingLayer': RoiPoolingLayer,\n        'rpn_loss_regr': get_rpn_loss_regr(nb_anchors),\n        'rpn_loss_cls': get_rpn_loss_cls(nb_anchors),\n        'class_loss_cls': class_loss_cls,\n        'class_loss_regr': get_class_loss_regr(nb_classes),\n    }\n    return custom_objects_faster_rcnn\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_faster_rcnn/#template_vision.models_training.object_detectors.utils_faster_rcnn.get_rpn_loss_cls","title":"<code>get_rpn_loss_cls(nb_anchors)</code>","text":"<p>Gets the RPN classification loss depending on the number of anchor of the model</p> <p>Parameters:</p> Name Type Description Default <code>nb_anchors</code> <code>int</code> <p>Number of anchors of the model</p> required <p>Returns:     Callable: RPN classification loss</p> Source code in <code>template_vision/models_training/object_detectors/utils_faster_rcnn.py</code> <pre><code>def get_rpn_loss_cls(nb_anchors: int) -&gt; Callable:\n    '''Gets the RPN classification loss depending on the number of anchor of the model\n\n    Args:\n        nb_anchors (int): Number of anchors of the model\n    Returns:\n        Callable: RPN classification loss\n    '''\n    def rpn_loss_cls(y_true, y_pred) -&gt; float:\n        '''Calculates the RPN classification loss (Cross entropy)\n\n        Args:\n            y_true: Model's targets. Shape (batch_size, height, width, 2*nb_anchors)\n                first part : validity of the anchor box. Valid if positive (object match) or negative (background match),\n                                                         Not valid if neutral (in between object and background)\n                             the loss is calculated only on valid anchor boxes\n                second part : classe of the anchor box\n                                 0  --&gt; background\n                                 1  --&gt; object\n            y_pred: Outputs of the model. Shape (batch_size, height, width, nb_anchors)\n        Returns:\n            float: Calculated loss\n        '''\n        ind_sep = nb_anchors  # Separation index of the two parts of y_true\n        return K.sum(y_true[:, :, :, :ind_sep] * K.binary_crossentropy(y_pred[:, :, :, :], y_true[:, :, :, ind_sep:])) / K.sum(1e-4 + y_true[:, :, :, :ind_sep])\n    # Return loss\n    return rpn_loss_cls\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_faster_rcnn/#template_vision.models_training.object_detectors.utils_faster_rcnn.get_rpn_loss_regr","title":"<code>get_rpn_loss_regr(nb_anchors)</code>","text":"<p>Gets the RPN regression loss depending on the number of anchor of the model</p> <p>Parameters:</p> Name Type Description Default <code>nb_anchors</code> <code>int</code> <p>Number of anchors of the model</p> required <p>Returns:     Callable: RPN regression loss</p> Source code in <code>template_vision/models_training/object_detectors/utils_faster_rcnn.py</code> <pre><code>def get_rpn_loss_regr(nb_anchors: int) -&gt; Callable:\n    '''Gets the RPN regression loss depending on the number of anchor of the model\n\n    Args:\n        nb_anchors (int): Number of anchors of the model\n    Returns:\n        Callable: RPN regression loss\n    '''\n    def rpn_loss_regr(y_true, y_pred) -&gt; float:\n        '''Calculates the RPN regression loss (Huber loss)\n               0.5*x\u00b2 (if x_abs &lt; 1)\n               x_abs - 0.5 (otherwise)\n\n        Args:\n            y_true: Model's targets. Shape (batch_size, height, width, 2*4*nb_anchors)\n                first part : class of the anchor boxes -&gt; object or background\n                                the loss is calculated only on anchor boxes associated to an object\n                second part : regression\n            y_pred: Outputs of the model. Shape (batch_size, height, width, 4*nb_anchors)\n        Returns:\n            float: Calculated loss\n        '''\n        y_true = tf.cast(y_true, tf.float32)\n        y_pred = tf.cast(y_pred, tf.float32)\n\n        # Evaluate difference\n        ind_sep = 4 * nb_anchors  # Separation index of the two parts of y_true\n        x = y_true[:, :, :, ind_sep:] - y_pred\n        x_abs = K.abs(x)\n\n        # If x_abs &lt;= 1.0, x_bool = 1\n        x_bool = K.cast(K.less_equal(x_abs, 1.0), tf.float32)\n\n        # robust loss function (smooth L1)\n        return K.sum(y_true[:, :, :, :ind_sep] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(1e-4 + y_true[:, :, :, :ind_sep])\n    # Return loss\n    return rpn_loss_regr\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/","title":"Utils object detectors","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.add_regression_target_to_pos_valid","title":"<code>add_regression_target_to_pos_valid(anchor_boxes_dict)</code>","text":"<p>Add the regression target for positive and valid anchors Otherwise, keep (0, 0, 0, 0) and won't be used by the loss</p> <p>Parameters:</p> Name Type Description Default <code>anchor_boxes_dict</code> <code>dict</code> <p>Anchor boxes dictionary - 'anchor_img_coordinates': Coordinates of the anchor box (input format ie. image space) - 'bboxes': bboxes with their coordinates xyxy (in image space) and iou - 'anchor_type': anchor type (pos, neg or neutral) - 'anchor_validity': anchor validity - 'best_bbox_index': bbox associated to this anchor</p> required <p>Returns:     dict: Updated anchor boxes withe the regression targets</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def add_regression_target_to_pos_valid(anchor_boxes_dict: dict) -&gt; dict:\n    '''Add the regression target for positive and valid anchors\n    Otherwise, keep (0, 0, 0, 0) and won't be used by the loss\n\n    Args:\n        anchor_boxes_dict (dict): Anchor boxes dictionary\n            - 'anchor_img_coordinates': Coordinates of the anchor box (input format ie. image space)\n            - 'bboxes': bboxes with their coordinates xyxy (in image space) and iou\n            - 'anchor_type': anchor type (pos, neg or neutral)\n            - 'anchor_validity': anchor validity\n            - 'best_bbox_index': bbox associated to this anchor\n    Returns:\n        dict: Updated anchor boxes withe the regression targets\n    '''\n    # For each anchor ...\n    for anchor_idx, anchor in anchor_boxes_dict.items():\n        # ... and if the anchor is positive and valid ...\n        if anchor['anchor_type'] == 'pos' and anchor['anchor_validity'] == 1:\n            # ... we get the regression target between this anchor and the closest bbox (best iou)\n            coordinates_anchor = anchor['anchor_img_coordinates']\n            best_bbox_index = anchor['best_bbox_index']\n            coordinates_bbox = anchor['bboxes'][best_bbox_index]['bbox_img_coordinates']\n            anchor['regression_target'] = calc_regr(coordinates_bbox, coordinates_anchor)\n        # Otherwise, default to 0\n        else:\n            anchor['regression_target'] = (0, 0, 0, 0)\n        anchor_boxes_dict[anchor_idx] = anchor\n    # Return updated dict\n    return anchor_boxes_dict\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.apply_regression","title":"<code>apply_regression(coordinates_and_regression)</code>","text":"<p>Applies the result of a regression on an anchor box (or a ROI) given in xyhw format.</p> <p>Parameters:</p> Name Type Description Default <code>coordinates_and_regression</code> <code>ndarray</code> <p>An array composed of 8 objects x_anc, y_anc, h_anc, w_anc, tx, ty, th, tw. (x_anc, y_anc, h_anc, w_anc) are the coordinates of the anchor (or of a ROI) (tx, ty, th, tw) are the predictions of a regression</p> required <p>Returns:     float: coordinates after regression applied on the anchor box (or on the ROI) - x coordinate of the upper left corner     float: coordinates after regression applied on the anchor box (or on the ROI) - y coordinate of the upper left corner     float: coordinates after regression applied on the anchor box (or on the ROI) - height     float: coordinates after regression applied on the anchor box (or on the ROI) - width</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def apply_regression(coordinates_and_regression: np.ndarray) -&gt; Tuple[float, float, float, float]:\n    '''Applies the result of a regression on an anchor box (or a ROI) given in xyhw format.\n\n    Args:\n        coordinates_and_regression (np.ndarray): An array composed of 8 objects x_anc, y_anc, h_anc, w_anc, tx, ty, th, tw.\n            (x_anc, y_anc, h_anc, w_anc) are the coordinates of the anchor (or of a ROI)\n            (tx, ty, th, tw) are the predictions of a regression\n            # Shape (8,)\n    Returns:\n        float: coordinates after regression applied on the anchor box (or on the ROI) - x coordinate of the upper left corner\n        float: coordinates after regression applied on the anchor box (or on the ROI) - y coordinate of the upper left corner\n        float: coordinates after regression applied on the anchor box (or on the ROI) - height\n        float: coordinates after regression applied on the anchor box (or on the ROI) - width\n    '''\n    x_anc, y_anc, h_anc, w_anc, tx, ty, th, tw = coordinates_and_regression\n    w_roi = np.exp(tw) * w_anc  # Take the inverse of the log and get rid of the normalization\n    h_roi = np.exp(th) * h_anc  # Take the inverse of the log and get rid of the normalization\n    x_roi = (tx * w_anc + (x_anc + w_anc / 2)) - w_roi / 2  # Get rid of the normalization, then add the center of the anchor = center of ROI, then remove half the width to get x1\n    y_roi = (ty * h_anc + (y_anc + h_anc / 2)) - h_roi / 2  # Get rid of the normalization, then add the center of the anchor = center of ROI, then remove half the height to get y1\n    return x_roi, y_roi, h_roi, w_roi\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.apply_regression--shape-8","title":"Shape (8,)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.calc_regr","title":"<code>calc_regr(coordinates_bbox, coordinates_anchor)</code>","text":"<p>Gives the target of a regression given the coordinates of a bbox and of an anchor (or a ROI)</p> <p>Parameters:</p> Name Type Description Default <code>coordinates_bbox</code> <code>tuple</code> <p>The coordinates of a bbox in opposite points format</p> required <code>coordinates_anchor</code> <code>tuple</code> <p>The coordinates of an anchor (or a ROI) in opposite points format</p> required <p>Returns:     float: Gap between the centers (x coordinate) normalized by the width of the anchor     float: Gap between the centers (y coordinate) normalized by the height of the anchor     float: Height ratio : bbox / anchor (log version)     float: Width ratio : bbox / anchor (log version)</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def calc_regr(coordinates_bbox: Tuple[float, float, float, float],\n              coordinates_anchor: Tuple[float, float, float, float]) -&gt; Tuple[float, float, float, float]:\n    '''Gives the target of a regression given the coordinates of a bbox and of an anchor (or a ROI)\n\n    Args:\n        coordinates_bbox (tuple): The coordinates of a bbox in opposite points format\n        coordinates_anchor (tuple): The coordinates of an anchor (or a ROI) in opposite points format\n    Returns:\n        float: Gap between the centers (x coordinate) normalized by the width of the anchor\n        float: Gap between the centers (y coordinate) normalized by the height of the anchor\n        float: Height ratio : bbox / anchor (log version)\n        float: Width ratio : bbox / anchor (log version)\n    '''\n    cx_bbox, cy_bbox, height_bbox, width_bbox = xyxy_to_cxcyhw(*coordinates_bbox)\n    cx_anchor, cy_anchor, height_anchor, width_anchor = xyxy_to_cxcyhw(*coordinates_anchor)\n    tx = (cx_bbox - cx_anchor) / width_anchor\n    ty = (cy_bbox - cy_anchor) / height_anchor\n    th = np.log(height_bbox / height_anchor)\n    tw = np.log(width_bbox / width_anchor)\n    return tx, ty, th, tw\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.check_coordinates_validity","title":"<code>check_coordinates_validity(function)</code>","text":"<p>Decorator to make sure that the coordinates are valid</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>Callable</code> <p>Function to decorate</p> required <p>Raises:     ValueError: If a set of coordinates is impossible Returns:     function: The decorated function</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def check_coordinates_validity(function: Callable) -&gt; Callable:\n    '''Decorator to make sure that the coordinates are valid\n\n    Args:\n        function (Callable): Function to decorate\n    Raises:\n        ValueError: If a set of coordinates is impossible\n    Returns:\n        function: The decorated function\n    '''\n    # Get wrapper\n    def wrapper(*args, **kwargs):\n        '''Wrapper'''\n        # Get vars.\n        f_args = inspect.getfullargspec(function).args\n        x1 = kwargs['x1'] if 'x1' in kwargs.keys() else (args[f_args.index('x1')] if 'x1' in f_args else None)\n        y1 = kwargs['y1'] if 'y1' in kwargs.keys() else (args[f_args.index('y1')] if 'y1' in f_args else None)\n        x2 = kwargs['x2'] if 'x2' in kwargs.keys() else (args[f_args.index('x2')] if 'x2' in f_args else None)\n        y2 = kwargs['y2'] if 'y2' in kwargs.keys() else (args[f_args.index('y2')] if 'y2' in f_args else None)\n        x = kwargs['x'] if 'x' in kwargs.keys() else (args[f_args.index('x')] if 'x' in f_args else None)\n        y = kwargs['y'] if 'y' in kwargs.keys() else (args[f_args.index('y')] if 'y' in f_args else None)\n        w = kwargs['w'] if 'w' in kwargs.keys() else (args[f_args.index('w')] if 'w' in f_args else None)\n        h = kwargs['h'] if 'h' in kwargs.keys() else (args[f_args.index('h')] if 'h' in f_args else None)\n        # Apply checks\n        if x1 is not None and x1 &lt;0:\n            raise ValueError('x1 must be non negative')\n        if x2 is not None and x2 &lt;0:\n            raise ValueError('x2 must be non negative')\n        if y1 is not None and y1 &lt;0:\n            raise ValueError('y1 must be non negative')\n        if y2 is not None and y2 &lt;0:\n            raise ValueError('y2 must be non negative')\n        if x is not None and x &lt;0:\n            raise ValueError('x must be non negative')\n        if y is not None and y &lt;0:\n            raise ValueError('y must be non negative')\n        if w is not None and w &lt;= 0:\n            raise ValueError('w must be positive')\n        if h is not None and h &lt;= 0:\n            raise ValueError('h must be positive')\n\n        if x1 is not None and x2 is not None and x1 &gt;= x2:\n            raise ValueError('x2 must be bigger than x1')\n        if y1 is not None and y2 is not None and y1 &gt;= y2:\n            raise ValueError('y2 must be bigger than y1')\n        # Return\n        return function(*args, **kwargs)\n    return wrapper\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.complete_at_least_one_anchor_per_bbox","title":"<code>complete_at_least_one_anchor_per_bbox(anchor_boxes_dict, bboxes_index_with_no_positive)</code>","text":"<p>Completes the dictionary of anchor to have at least one positive anchor per bbox if it is not already the case. If a bbox is not associated to an anchor, we associate it to the anchor with which it has the biggest iou (if this anchor is not already associated with another bbox) Args:     anchor_boxes_dict (dict): Anchor boxes dictionary         - 'anchor_img_coordinates': Coordinates of the anchor box (input format ie. image space)         - 'bboxes': bboxes with their coordinates xyxy (in image space) and iou         - 'anchor_type': anchor type (pos, neg or neutral)         - 'anchor_validity': anchor validity         - 'best_bbox_index': bbox associated to this anchor     bboxes_index_with_no_positive (list): List of bboxes with no positive anchor associated Returns:     dict: Updated anchor boxes dictionary</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def complete_at_least_one_anchor_per_bbox(anchor_boxes_dict: dict, bboxes_index_with_no_positive: List[dict]) -&gt; dict:\n    '''Completes the dictionary of anchor to have at least one positive anchor per bbox if it is not\n    already the case.\n    If a bbox is not associated to an anchor, we associate it to the anchor with which it has\n    the biggest iou (if this anchor is not already associated with another bbox)\n    Args:\n        anchor_boxes_dict (dict): Anchor boxes dictionary\n            - 'anchor_img_coordinates': Coordinates of the anchor box (input format ie. image space)\n            - 'bboxes': bboxes with their coordinates xyxy (in image space) and iou\n            - 'anchor_type': anchor type (pos, neg or neutral)\n            - 'anchor_validity': anchor validity\n            - 'best_bbox_index': bbox associated to this anchor\n        bboxes_index_with_no_positive (list): List of bboxes with no positive anchor associated\n    Returns:\n        dict: Updated anchor boxes dictionary\n    '''\n    # For each missing bbox ...\n    for index_bbox in bboxes_index_with_no_positive:\n        # ... we look for the anchor box with the best iou ...\n        best_iou = -1  # We could set it to 0 but there exists rare case where all the anchor boxes have a 0 iou.\n        best_anchor_idx = -1\n        for anchor_idx, anchor in anchor_boxes_dict.items():\n            iou = anchor['bboxes'][index_bbox]['iou']\n            if iou &gt; best_iou:\n                best_iou = iou\n                best_anchor_idx = anchor_idx\n        # ... and we update this anchor if it is not already positive\n        anchor = anchor_boxes_dict[best_anchor_idx]\n        if anchor['anchor_type'] != 'pos' and best_iou &gt; 0:\n            anchor['anchor_type'] = 'pos'\n            anchor['anchor_validity'] = 1\n            anchor['best_bbox_index'] = index_bbox\n            anchor_boxes_dict[best_anchor_idx] = anchor  # Update\n    # Return\n    return anchor_boxes_dict\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.create_fake_dict_rois_targets","title":"<code>create_fake_dict_rois_targets(img_data, subsampling_ratio, nb_rois_per_img)</code>","text":"<p>Creates fake dict_rois_targets in the rare cases where the function limit_rois_targets gives an empty object (None).</p> <pre><code>Process : we return ROIs on the entire image, considered as background\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>img_data</code> <code>dict</code> <p>Metadata of the image after the preprocessing. In particular, the size of the image</p> required <code>subsampling_ratio</code> <code>int</code> <p>Subsampling of the base model (shared layers)</p> required <code>nb_rois_per_img</code> <code>int</code> <p>Number of fake ROIs to return</p> required <p>Returns:     dict: The dictionary of fake \"selected\" ROIs</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def create_fake_dict_rois_targets(img_data: dict, subsampling_ratio: int, nb_rois_per_img: int) -&gt; dict:\n    '''Creates fake dict_rois_targets in the rare cases where the function limit_rois_targets gives an empty object (None).\n\n        Process : we return ROIs on the entire image, considered as background\n\n    Args:\n        img_data (dict): Metadata of the image after the preprocessing. In particular, the size of the image\n        subsampling_ratio (int): Subsampling of the base model (shared layers)\n        nb_rois_per_img (int): Number of fake ROIs to return\n    Returns:\n        dict: The dictionary of fake \"selected\" ROIs\n    '''\n    # Get the size of the image in the features map\n    height_img_in_feature_map, width_img_in_feature_map = get_feature_map_size(img_data['resized_height'], img_data['resized_width'], subsampling_ratio)\n    # Create a dictionary with an unique entry : a ROI on the entire image, considered as background\n    dict_rois_targets = {\n        0: {\n            'coordinates': {'x1': 0, 'y1': 0, 'x2': width_img_in_feature_map, 'y2': height_img_in_feature_map,\n                            'h': height_img_in_feature_map, 'w': width_img_in_feature_map},\n            'classifier_regression_target': (0, 0, 0, 0),\n            'classifier_class_target': 'bg',\n        }\n    }\n    # We clone this ROI to have as many as wanted\n    dict_rois_targets = {i: copy.deepcopy(dict_rois_targets[0]) for i in range(nb_rois_per_img)}\n    # Return\n    return dict_rois_targets\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.draw_bboxes","title":"<code>draw_bboxes(input_img, output_path=None, gt_bboxes=None, predicted_bboxes=None)</code>","text":"<p>Adds bboxes to an image (np.ndarray)</p> <p>Parameters:</p> Name Type Description Default <code>input_img</code> <code>ndarray</code> <p>Input image</p> required <p>Kwargs:     output_path (str): Path to the output file. If None, the result is not saved     gt_bboxes (list): List of \"ground truth\" bboxes to display         Each entry must be a dictionary with keys x1, y1, x2, y2 and (optional) class     predicted_bboxes (list): List of bboxes coming from a prediction (same format as gt_bboxes) Raises:     FileExistsError: If the output file already exists Returns:     (np.ndarray) : The image with the boxes</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def draw_bboxes(input_img: np.ndarray, output_path: Union[str, None] = None, gt_bboxes: Union[List[dict], None] = None,\n                predicted_bboxes: Union[List[dict], None] = None) -&gt; np.ndarray:\n    '''Adds bboxes to an image (np.ndarray)\n\n    Args:\n        input_img (np.ndarray): Input image\n    Kwargs:\n        output_path (str): Path to the output file. If None, the result is not saved\n        gt_bboxes (list): List of \"ground truth\" bboxes to display\n            Each entry must be a dictionary with keys x1, y1, x2, y2 and (optional) class\n        predicted_bboxes (list): List of bboxes coming from a prediction (same format as gt_bboxes)\n    Raises:\n        FileExistsError: If the output file already exists\n    Returns:\n        (np.ndarray) : The image with the boxes\n    '''\n    if output_path is not None and os.path.exists(output_path):\n        raise FileExistsError(f\"The file {output_path} already exists\")\n\n    if gt_bboxes is None:\n        gt_bboxes = []\n    if predicted_bboxes is None:\n        predicted_bboxes = []\n    # Define colors\n    green = (0, 255, 0, 255)\n    red = (255, 0, 0, 255)\n    # Create green rectangles for each bbox\n    for bbox in gt_bboxes:\n        draw_rectangle_from_bbox(input_img, bbox, color=green, thickness=5)\n    # Create red rectangles for each predicted bbox\n    for bbox in predicted_bboxes:\n        draw_rectangle_from_bbox(input_img, bbox, color=red, thickness=5)\n    if output_path is not None:\n        io.imsave(output_path, input_img)\n        logger.info(f\"Image saved here : {output_path}\")\n    return input_img\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.draw_bboxes_from_file","title":"<code>draw_bboxes_from_file(input_path, output_path=None, gt_bboxes=None, predicted_bboxes=None)</code>","text":"<p>Adds bboxes to an image from a file</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to the input image</p> required <p>Kwargs:     output_path (str): Path to the output file. If None, the result is not saved     gt_bboxes (list): List of \"ground truth\" bboxes to display         Each entry must be a dictionary with keys x1, y1, x2, y2 and (optional) class     predicted_bboxes (list): List of bboxes coming from a prediction (same format as gt_bboxes) Raises:     FileNotFoundError: If the input file does not exist Returns:     (np.ndarray) : The image with the boxes</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def draw_bboxes_from_file(input_path: str, output_path: Union[str, None] = None, gt_bboxes: Union[List[dict], None] = None,\n                          predicted_bboxes: Union[List[dict], None] = None) -&gt; np.ndarray:\n    '''Adds bboxes to an image from a file\n\n    Args:\n        input_path (str): Path to the input image\n    Kwargs:\n        output_path (str): Path to the output file. If None, the result is not saved\n        gt_bboxes (list): List of \"ground truth\" bboxes to display\n            Each entry must be a dictionary with keys x1, y1, x2, y2 and (optional) class\n        predicted_bboxes (list): List of bboxes coming from a prediction (same format as gt_bboxes)\n    Raises:\n        FileNotFoundError: If the input file does not exist\n    Returns:\n        (np.ndarray) : The image with the boxes\n    '''\n    if not os.path.exists(input_path):\n        raise FileNotFoundError(f\"The file {input_path} does not exist\")\n    # Load image\n    input_img = io.imread(input_path)\n    return draw_bboxes(input_img, output_path, gt_bboxes, predicted_bboxes)\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.draw_rectangle_from_bbox","title":"<code>draw_rectangle_from_bbox(img, bbox, color=None, thickness=None, with_center=False)</code>","text":"<p>Draws a rectangle in the image and adds a text (optional)</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray</code> <p>The considered image</p> required <code>bbox</code> <code>dict</code> <p>The dictionary containing the coordinates and the text</p> required <code>color</code> <code>tuple</code> <p>A RGB tuple giving the color of the rectangle</p> <code>None</code> <code>thickness</code> <code>int</code> <p>The thickness of the rectangle</p> <code>None</code> <code>with_center</code> <code>bool</code> <p>If True, also draws the center of the rectangle</p> <code>False</code> <p>Raises:     ValueError: If one of the keys 'x1', 'y1', 'x2', 'y2' is missing</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def draw_rectangle_from_bbox(img: np.array, bbox: dict, color: Union[tuple, None] = None,\n                             thickness: Union[int, None] = None, with_center: bool = False):\n    '''Draws a rectangle in the image and adds a text (optional)\n\n    Args:\n        img (np.ndarray): The considered image\n        bbox (dict): The dictionary containing the coordinates and the text\n        color (tuple): A RGB tuple giving the color of the rectangle\n        thickness (int): The thickness of the rectangle\n        with_center (bool): If True, also draws the center of the rectangle\n    Raises:\n        ValueError: If one of the keys 'x1', 'y1', 'x2', 'y2' is missing\n    '''\n    # Check mandatory keys\n    if any([key not in bbox.keys() for key in ['x1', 'y1', 'x2', 'y2']]):\n        raise ValueError(\"One of the mandatory keys ('x1', 'y1', 'x2', 'y2') is missing in the object bbox.\")\n    # Process\n    x1, y1, x2, y2 = bbox['x1'], bbox['y1'], bbox['x2'], bbox['y2']\n    class_name = bbox.get('class', None)\n    cv2.rectangle(img, (x1, y1), (x2, y2), color, thickness)\n    if class_name is not None:\n        font = cv2.FONT_HERSHEY_SIMPLEX\n        if 'proba' in bbox:\n            proba = format(bbox['proba'], \".2f\")\n            class_name = class_name + f\" ({proba})\"\n        cv2.putText(img, class_name, (x1 + 5, y1 + 30), font, 1, color, 2)\n    if with_center:\n        center_x = int((x1 + x2) / 2)\n        center_y = int((y1 + y2) / 2)\n        cv2.circle(img, (center_x, center_y), 3, color, -1)\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.format_classifier_inputs_and_targets","title":"<code>format_classifier_inputs_and_targets(dict_rois_targets, dict_classes, classifier_regr_scaling)</code>","text":"<p>Transforms a dictionary of target ROIs into a suitable format for the classifier model</p> <p>Parameters:</p> Name Type Description Default <code>dict_rois</code> <code>dict</code> <p>Dictionary containing the possible inputs / targets of the classifier</p> required <code>dict_classes</code> <code>dict</code> <p>Mapping of the classes of the model (must not contain 'bg'), format :  {idx: label}</p> required <code>classifier_regr_scaling</code> <code>list&lt;float&gt;</code> <p>Regression coefficient to apply to coordinates</p> required <p>Returns:     np.ndarray: coordinates of each selected ROIs         # Shape : (1, nb_rois, 4), format x, y, h, w     np.ndarray: Classification target of the classifier (with the background)         # Shape (1, nb_rois, (nb_classes + 1))     np.ndarray: Two parts array:         # Shape (1, nb_rois, 2 * nb_classes * 4)         -&gt; First half : identification class ground truth to calculate the regression loss for the classifier             # Shape (1, nb_rois, nb_classes * 4)         -&gt; Second hald : regression target for the classifier (one regression per class)             # Shape (1, nb_rois, nb_classes * 4)</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def format_classifier_inputs_and_targets(dict_rois_targets: dict, dict_classes: dict,\n                                         classifier_regr_scaling: List[float]) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    '''Transforms a dictionary of target ROIs into a suitable format for the classifier model\n\n    Args:\n        dict_rois (dict): Dictionary containing the possible inputs / targets of the classifier\n        dict_classes (dict): Mapping of the classes of the model (must not contain 'bg'), format :  {idx: label}\n        classifier_regr_scaling (list&lt;float&gt;): Regression coefficient to apply to coordinates\n    Returns:\n        np.ndarray: coordinates of each selected ROIs\n            # Shape : (1, nb_rois, 4), format x, y, h, w\n        np.ndarray: Classification target of the classifier (with the background)\n            # Shape (1, nb_rois, (nb_classes + 1))\n        np.ndarray: Two parts array:\n            # Shape (1, nb_rois, 2 * nb_classes * 4)\n            -&gt; First half : identification class ground truth to calculate the regression loss for the classifier\n                # Shape (1, nb_rois, nb_classes * 4)\n            -&gt; Second hald : regression target for the classifier (one regression per class)\n                # Shape (1, nb_rois, nb_classes * 4)\n    '''\n\n    # Get the number of selected ROIs (the same for each image) and info on classes\n    nb_rois = len(dict_rois_targets)\n    nb_classes = len(dict_classes)\n    class_mapping = {name_class: index for index, name_class in dict_classes.items()}\n    class_mapping['bg'] = len(class_mapping)  # Add background to the mapping\n\n    # Init. of output arrays\n    X = np.zeros((nb_rois, 4))  # ROIs coordinates\n    Y1 = np.zeros((nb_rois, (nb_classes + 1)))  # + 1 for background\n    Y2_1 = np.zeros((nb_rois, 4 * nb_classes))  # OHE class (without background), but repeated 4 times (one for each coordinate), will be used by the loss\n    Y2_2 = np.zeros((nb_rois, 4 * nb_classes))  # One regression per class (# TODO verify if we can't do only one regression)\n\n    # For each ROI, we fill up the output arrays\n    for i, (roi_index, roi) in enumerate(dict_rois_targets.items()):\n        ### ROI coordinates\n        X[i, :] = (roi['coordinates']['x1'], roi['coordinates']['y1'], roi['coordinates']['h'], roi['coordinates']['w'])  # Format x, y, h, w\n\n        ### Targets of the classifier model\n        # ROI class\n        gt_class = roi['classifier_class_target']\n        idx_gt_class = class_mapping[gt_class]\n        ohe_target = [0] * (nb_classes + 1)\n        ohe_target[idx_gt_class] = 1  # --&gt; e.g. [0, 1, 0] / 2 classes + 'bg'\n        Y1[i, :] = ohe_target\n\n        # ROI regression - loss target\n        ohe_target_no_bg = ohe_target[:nb_classes]  # We get rid of the background, no regression here --&gt; e.g. [0, 1]\n        ohe_target_no_bg_repeated = np.repeat(ohe_target_no_bg, 4)  # We repeat the OHE targets four times 4 fois (one for each coordinate)\n        Y2_1[i, :] = ohe_target_no_bg_repeated  # e.g. [0, 0, 0, 0, 1, 1, 1, 1]\n\n        # ROI regression - regression - only if not background\n        if gt_class != 'bg':\n            target_regression = [0.] * nb_classes * 4\n            regression_values = [a * b for a, b in zip(roi['classifier_regression_target'], classifier_regr_scaling)]  # Apply a scaling\n            target_regression[idx_gt_class * 4: (idx_gt_class + 1) * 4] = regression_values  # e.g. [0, 0, 0, 0, 0.2, -0.3, 0.1, 0.9]\n            Y2_2[i, :] = target_regression\n\n    # Concatenate Y2\n    Y2 = np.concatenate([Y2_1, Y2_2], axis=1)\n    # Add batch dimension\n    X = np.expand_dims(X, axis=0)\n    Y1 = np.expand_dims(Y1, axis=0)\n    Y2 = np.expand_dims(Y2, axis=0)\n    # Returns\n    return X, Y1, Y2\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_all_viable_anchors_boxes","title":"<code>get_all_viable_anchors_boxes(base_anchors, subsampling_ratio, feature_map_height, feature_map_width, im_resized_height, im_resized_width)</code>","text":"<p>Gets a dictionary of 'viable' anchor boxes.</p> <p>From a list of \"base\" anchors, we will take each point of a features map, get its initial coordinates (input of the model) and build as many anchors as \"base\" anchors with this point as a center. Then we filter out the ones which are outside the image</p> <p>Parameters:</p> Name Type Description Default <code>base_anchors</code> <code>list</code> <p>List of base anchors</p> required <code>subsampling_ratio</code> <code>int</code> <p>Subsampling ratio of the shared model</p> required <code>feature_map_height</code> <code>int</code> <p>Height of the features map</p> required <code>feature_map_width</code> <code>int</code> <p>Width of the features map</p> required <code>im_resized_height</code> <code>int</code> <p>Height of the input image (preprocessed, without padding)</p> required <code>im_resized_width</code> <code>int</code> <p>Width of the input image (preprocessed, without padding)</p> required <p>Returns:     dict : set of 'viable' anchor boxes identified by (y, x, index_anchor)</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def get_all_viable_anchors_boxes(base_anchors: List[tuple], subsampling_ratio: int, feature_map_height: int,\n                                 feature_map_width: int, im_resized_height: int, im_resized_width: int) -&gt; dict:\n    '''Gets a dictionary of 'viable' anchor boxes.\n\n    From a list of \"base\" anchors, we will take each point of a features map, get its initial coordinates\n    (input of the model) and build as many anchors as \"base\" anchors with this point as a center.\n    Then we filter out the ones which are outside the image\n\n    Args:\n        base_anchors (list): List of base anchors\n        subsampling_ratio (int): Subsampling ratio of the shared model\n        feature_map_height (int): Height of the features map\n        feature_map_width (int): Width of the features map\n        im_resized_height (int): Height of the input image (preprocessed, without padding)\n        im_resized_width (int): Width of the input image (preprocessed, without padding)\n    Returns:\n        dict : set of 'viable' anchor boxes identified by (y, x, index_anchor)\n    '''\n    viable_anchor_boxes = {}\n    # For each anchor...\n    for index_anchor, (height_anchor, width_anchor) in enumerate(base_anchors):\n        # For each point of the features map...\n        for x_feature_map in range(feature_map_width):\n            # x coordinate of the anchor, input format (ie in image space)\n            x1_anchor = subsampling_ratio * (x_feature_map + 0.5) - width_anchor / 2  # center - width / 2\n            x2_anchor = subsampling_ratio * (x_feature_map + 0.5) + width_anchor / 2  # center + width / 2\n            # We do not consider the anchors outside the image (before padding)\n            if x1_anchor &lt; 0 or x2_anchor &gt;= im_resized_width:\n                continue\n            for y_feature_map in range(feature_map_height):\n                # y coordinate of the anchor, input format (ie in image space)\n                y1_anchor = subsampling_ratio * (y_feature_map + 0.5) - height_anchor / 2  # center - height / 2\n                y2_anchor = subsampling_ratio * (y_feature_map + 0.5) + height_anchor / 2  # center + height / 2\n                # We do not consider the anchors outside the image (before padding)\n                if y1_anchor &lt; 0 or y2_anchor &gt;= im_resized_height:\n                    continue\n                # We update the dictionary of the 'viable' anchor boxes (y, x, anchor)\n                id_key = (y_feature_map, x_feature_map, index_anchor)\n                viable_anchor_boxes[id_key] = {\n                    'anchor_img_coordinates': (x1_anchor, y1_anchor, x2_anchor, y2_anchor)\n                }\n    # Check errors\n    if len(viable_anchor_boxes) == 0:\n        logger.error(\"No viable bbox for one of the input images.\")\n        logger.error(\"The size of the preprocessed images may be too small when compared to the list of anchors of the model.\")\n        raise RuntimeError(\"No viable bbox for one of the input images.\")\n    # Return\n    return viable_anchor_boxes\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_area_from_xyxy","title":"<code>get_area_from_xyxy(x1, y1, x2, y2)</code>","text":"<p>Gives the area (absolute, not relative) of a rectangle in opposite points format</p> Args <p>x1 (float): x coordinate of the upper left point y1 (float): y coordinate of the upper left point x2 (float): x coordinate of the bottom right point y2 (float): y coordinate of the bottom right point</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The absolute area of the rectangle</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>@check_coordinates_validity\ndef get_area_from_xyxy(x1: float, y1: float, x2: float, y2: float) -&gt; float:\n    '''Gives the area (absolute, not relative) of a rectangle in opposite points format\n\n    Args :\n        x1 (float): x coordinate of the upper left point\n        y1 (float): y coordinate of the upper left point\n        x2 (float): x coordinate of the bottom right point\n        y2 (float): y coordinate of the bottom right point\n\n    Returns:\n        float : The absolute area of the rectangle\n    '''\n    return abs((x2 - x1) * (y2 - y1))\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_classifier_test_inputs","title":"<code>get_classifier_test_inputs(rois_coordinates)</code>","text":"<p>Formats the inputs for the classifier from ROIs proposed by the RPN (test case)</p> <p>Process : For each ROI, we simply get the format x, y, h, w</p> <p>Parameters:</p> Name Type Description Default <code>rois_coordinates</code> <code>list&lt;np.ndarray&gt;</code> <p>ROIs to transform rois_coordinates must be a list with only one entry : the ROIs of the current image (for prediction, the batch_size is forced to 1) The unique entry is a numpy array:     # Shape (nb_rois, 4)     # Format x1, y1, x2, y2</p> required <p>Raises:     ValueError: If the number of elements in the list is different from 1 Returns:     np.ndarray : ROIs to use as inputs of the model         # Shape : (1, nb_rois, 4), format x, y, h, w</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def get_classifier_test_inputs(rois_coordinates: List[np.ndarray]) -&gt; np.ndarray:\n    '''Formats the inputs for the classifier from ROIs proposed by the RPN (test case)\n\n    Process : For each ROI, we simply get the format x, y, h, w\n\n    Args:\n        rois_coordinates (list&lt;np.ndarray&gt;): ROIs to transform\n            rois_coordinates must be a list with only one entry : the ROIs of the current image (for prediction, the batch_size is forced to 1)\n            The unique entry is a numpy array:\n                # Shape (nb_rois, 4)\n                # Format x1, y1, x2, y2\n    Raises:\n        ValueError: If the number of elements in the list is different from 1\n    Returns:\n        np.ndarray : ROIs to use as inputs of the model\n            # Shape : (1, nb_rois, 4), format x, y, h, w\n    '''\n    if len(rois_coordinates) != 1:\n        raise ValueError(\"In prediction mode, the batch_size must be 1.\")\n    # Init. of the output array\n    nb_rois = rois_coordinates[0].shape[0]\n    output_shape = (1, nb_rois, 4)\n    X = np.zeros(output_shape)\n    # We process ROIs one at a time\n    for i, roi in enumerate(rois_coordinates[0]):\n        # Get format x, y, h, w\n        x1, y1, x2, y2 = roi[0], roi[1], roi[2], roi[3]\n        x1, y1, h, w = xyxy_to_xyhw(x1, y1, x2, y2)\n        # Update output array\n        X[0, i, :] = (x1, y1, h, w)\n    # Return result\n    return X\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_classifier_train_inputs_and_targets","title":"<code>get_classifier_train_inputs_and_targets(model, img_data_batch, rois_coordinates)</code>","text":"<p>Gives the regression and classification of the classifier from the ROIs given by the RPN</p> We got the ROIs from the RPN prediction (and transformed them via get_roi_from_rpn_predictions) <p>For each image we will : - Calculate the ious between bboxes and ROIs - Keep, for each ROI, the bbox with the biggest iou (if the iou is bigger than a threshold) - Filter the ROIs to only some of them:     - Allows to keep OOM in check     - We respect to the loss, we will, of course, only take into account the selected ROIs     - We will keep a balance between positive ROIs (match with an object) and negative ROIs (match with 'bg') - Format the inputs and targets of the model</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelKerasFasterRcnnObjectDetector</code> <p>Model used (contains all the necessary configs)</p> required <code>img_data_batch</code> <code>list&lt;dict&gt;</code> <p>List of img_data for the batch Used here to get the bboxes of the images to define the targets of the classifier</p> required <code>rois_coordinates</code> <code>list&lt;np.ndarray&gt;</code> <p>Final list of the ROIs selected for the classifier part. Each element is a numpy array containing the coordinates of the ROIs calculated for an image of the batch</p> required <p>Returns:     np.ndarray : ROIs coordinates in input of the model - Format x, y, h, w         # Shape : (batch_size, nb_rois_per_img, 4), format x, y, h, w     np.ndarray : Targets of the classifier - classification         # Shape (batch_size, nb_rois_per_img, (nb_classes + 1))     np.ndarray : Targets of the classifier - regression         # Shape (batch_size, nb_rois_per_img, 2 * nb_classes * 4)</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def get_classifier_train_inputs_and_targets(model, img_data_batch: List[dict],\n                                            rois_coordinates: List[np.ndarray]) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    '''Gives the regression and classification of the classifier from the ROIs given by the RPN\n\n    Process : We got the ROIs from the RPN prediction (and transformed them via get_roi_from_rpn_predictions)\n              For each image we will :\n              - Calculate the ious between bboxes and ROIs\n              - Keep, for each ROI, the bbox with the biggest iou (if the iou is bigger than a threshold)\n              - Filter the ROIs to only some of them:\n                  - Allows to keep OOM in check\n                  - We respect to the loss, we will, of course, only take into account the selected ROIs\n                  - We will keep a balance between positive ROIs (match with an object) and negative ROIs (match with 'bg')\n              - Format the inputs and targets of the model\n\n    Args:\n        model (ModelKerasFasterRcnnObjectDetector): Model used (contains all the necessary configs)\n        img_data_batch (list&lt;dict&gt;): List of img_data for the batch\n            Used here to get the bboxes of the images to define the targets of the classifier\n        rois_coordinates (list&lt;np.ndarray&gt;): Final list of the ROIs selected for the classifier part.\n            Each element is a numpy array containing the coordinates of the ROIs calculated for an image of the batch\n            # Format x1, y1, x2, y2 (opposite points)\n    Returns:\n        np.ndarray : ROIs coordinates in input of the model - Format x, y, h, w\n            # Shape : (batch_size, nb_rois_per_img, 4), format x, y, h, w\n        np.ndarray : Targets of the classifier - classification\n            # Shape (batch_size, nb_rois_per_img, (nb_classes + 1))\n        np.ndarray : Targets of the classifier - regression\n            # Shape (batch_size, nb_rois_per_img, 2 * nb_classes * 4)\n    '''\n\n    # Get model attributes\n    subsampling_ratio = model.shared_model_subsampling\n    classifier_min_overlap = model.classifier_min_overlap\n    classifier_max_overlap = model.classifier_max_overlap\n    nb_rois_per_img = model.nb_rois_classifier\n    classifier_regr_scaling = model.classifier_regr_scaling\n    dict_classes = model.dict_classes\n\n    # Init. of output arrays\n    X, Y1, Y2 = None, None, None\n\n    # Preprocess one image at a time\n    for img_data, rois in zip(img_data_batch, rois_coordinates):\n        # Get all the ious betwee, ROIs and bboxes\n        dict_rois = get_rois_bboxes_iou(rois, img_data, subsampling_ratio)\n        # Find the best bbox for each ROI et the corresponding regression\n        dict_rois_targets = get_rois_targets(dict_rois, classifier_min_overlap, classifier_max_overlap)\n        # Limit the number of ROI\n        dict_rois_targets = limit_rois_targets(dict_rois_targets, nb_rois_per_img)\n        # If we have no more targets (very rare !), we consider the entire image as 'bg'\n        if dict_rois_targets is None:\n            logger.warning(\"There is an image with no suitable target for the classifier. We consider the entire image as background.\")\n            dict_rois_targets = create_fake_dict_rois_targets(img_data, subsampling_ratio, nb_rois_per_img)\n        # Format the classification and regression targets\n        X_tmp, Y1_tmp, Y2_tmp = format_classifier_inputs_and_targets(dict_rois_targets, dict_classes, classifier_regr_scaling)\n        # Increment output\n        if X is None:\n            X, Y1, Y2 = X_tmp, Y1_tmp, Y2_tmp\n        else:\n            # TODO : get rid of the concatenate in the loop and concatenate a list one time at the end instead (much faster)\n            X = np.concatenate((X, X_tmp), axis=0)\n            Y1 = np.concatenate((Y1, Y1_tmp), axis=0)\n            Y2 = np.concatenate((Y2, Y2_tmp), axis=0)\n    # We return the result\n    return X, Y1, Y2\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_classifier_train_inputs_and_targets--format-x1-y1-x2-y2-opposite-points","title":"Format x1, y1, x2, y2 (opposite points)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_feature_map_size","title":"<code>get_feature_map_size(input_height, input_width, subsampling_ratio)</code>","text":"<p>Gives the size of the features map given the height and width of the image using the subsampling_ratio of the shared model. For exemple, for VGG16, the subsampling_ratio is 16</p> <p>Parameters:</p> Name Type Description Default <code>input_height</code> <code>int</code> <p>Height of the image</p> required <code>input_width</code> <code>int</code> <p>Width of the image</p> required <code>subsampling_ratio</code> <code>int</code> <p>Subsampling ratio of the shared model</p> required <p>Raises:     ValueError: If incorrect dimension of the image (&lt; 1)     ValueError: If the subsampling_ratio is incorrect (&lt; 1) Returns:     int: Height of the features map     int: Width of the features map</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def get_feature_map_size(input_height: int, input_width: int, subsampling_ratio: int) -&gt; Tuple[int, int]:\n    '''Gives the size of the features map given the height and width of the image using\n    the subsampling_ratio of the shared model. For exemple, for VGG16, the subsampling_ratio is 16\n\n    Args:\n        input_height (int): Height of the image\n        input_width (int): Width of the image\n        subsampling_ratio (int): Subsampling ratio of the shared model\n    Raises:\n        ValueError: If incorrect dimension of the image (&lt; 1)\n        ValueError: If the subsampling_ratio is incorrect (&lt; 1)\n    Returns:\n        int: Height of the features map\n        int: Width of the features map\n    '''\n    # Manage errors\n    if input_height &lt; 1 or input_width &lt; 1:\n        raise ValueError(f\"Bad image shape (H : {input_height} / W : {input_width})\")\n    if subsampling_ratio &lt; 1:\n        raise ValueError(f\"Bad subsampling ratio ({subsampling_ratio})\")\n    # Process\n    return input_height // subsampling_ratio, input_width // subsampling_ratio\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_final_bboxes","title":"<code>get_final_bboxes(final_boxes, img_data)</code>","text":"<p>Resizes the final predicted boxes to image space and formats them.</p> <p>Parameters:</p> Name Type Description Default <code>final_boxes</code> <code>list) </code> <p>list of boxes valid from a probability AND coordinates xyxy points of view and with \"no\" overlap</p> required <code>img_data</code> <code>dict) </code> <p>Metadata associated with the image (used to resize predictions)</p> required <p>Returns:     A list of bboxes corresponding to the model predictions</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def get_final_bboxes(final_boxes: List[tuple], img_data: dict) -&gt; List[dict]:\n    '''Resizes the final predicted boxes to image space and formats them.\n\n    Args:\n        final_boxes (list) : list of boxes valid from a probability AND coordinates xyxy points of view and with \"no\" overlap\n            # Format [(cl, proba, coordinates), (...), ...)\n        img_data (dict) : Metadata associated with the image (used to resize predictions)\n    Returns:\n        A list of bboxes corresponding to the model predictions\n    '''\n    list_bboxes = []\n    resized_width = img_data['resized_width']\n    original_width = img_data['original_width']\n    resized_height = img_data['resized_height']\n    original_height = img_data['original_height']\n    for cl, proba, coordinates in final_boxes:\n        bbox = {'class': cl, 'proba': proba, 'x1': coordinates[0],\n                'y1': coordinates[1], 'x2': coordinates[2], 'y2': coordinates[3]}\n        bbox['x1'] = int(bbox['x1'] * (original_width / resized_width))\n        bbox['x2'] = int(bbox['x2'] * (original_width / resized_width))\n        bbox['y1'] = int(bbox['y1'] * (original_height / resized_height))\n        bbox['y2'] = int(bbox['y2'] * (original_height / resized_height))\n        list_bboxes.append(copy.deepcopy(bbox))\n    return list_bboxes\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_final_bboxes--format-cl-proba-coordinates","title":"Format [(cl, proba, coordinates), (...), ...)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_iou","title":"<code>get_iou(coordinatesA, coordinatesB)</code>","text":"<p>Gives the intersection over union (iou) from the coordinates of two rectangles (in opposite points format)</p> <p>Parameters:</p> Name Type Description Default <code>coordinatesA</code> <code>tuple&lt;float&gt;</code> <p>The coordinates of the first rectangle in the format (x1, y1, x2, y2)</p> required <code>coordinatesB</code> <code>tuple&lt;float&gt;</code> <p>The coordinates of the second rectangle in the format (x1, y1, x2, y2)</p> required <p>Returns:     float: Intersection over union of the two rectangles</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def get_iou(coordinatesA: Tuple[float, float, float, float], coordinatesB: Tuple[float, float, float, float]) -&gt; float:\n    '''Gives the intersection over union (iou) from the coordinates of two\n    rectangles (in opposite points format)\n\n    Args:\n        coordinatesA (tuple&lt;float&gt;): The coordinates of the first rectangle in the format (x1, y1, x2, y2)\n        coordinatesB (tuple&lt;float&gt;): The coordinates of the second rectangle in the format (x1, y1, x2, y2)\n    Returns:\n        float: Intersection over union of the two rectangles\n    '''\n    # Get areas of A and B\n    areaA = get_area_from_xyxy(*coordinatesA)\n    areaB = get_area_from_xyxy(*coordinatesB)\n    # If any null, iou is equal to 0\n    if areaA == 0 or areaB == 0:\n        return 0\n\n    x1A, y1A, x2A, y2A = coordinatesA\n    x1B, y1B, x2B, y2B = coordinatesB\n    # Get coordinates of the intersection\n    x1_inter = max(x1A, x1B)\n    y1_inter = max(y1A, y1B)\n    x2_inter = min(x2A, x2B)\n    y2_inter = min(y2A, y2B)\n    # Get intersection area\n    if x2_inter &gt; x1_inter and y2_inter &gt; y1_inter:\n        area_inter = get_area_from_xyxy(x1_inter, y1_inter, x2_inter, y2_inter)\n    else:\n        area_inter = 0\n\n    # Return IOU\n    return area_inter / (areaA + areaB - area_inter)\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_iou_anchors_bboxes","title":"<code>get_iou_anchors_bboxes(anchor_boxes_dict, image_bboxes)</code>","text":"<p>Gives the iou for each anchor boxes with all the bboxes of a list (for example, all the bboxes of an image)</p> <p>Parameters:</p> Name Type Description Default <code>anchor_boxes_dict</code> <code>dict</code> <p>Anchor boxes dictionary - 'anchor_img_coordinates': xyxy coordinates of the anchor boxe (input format, ie. image space)</p> required <code>image_bboxes</code> <code>list&lt;dict&gt;</code> <p>List of bboxes</p> required <p>Returns:     dict: The input dictionary to which we added a bboxes field containing coordinates and iou</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def get_iou_anchors_bboxes(anchor_boxes_dict: dict, image_bboxes: List[dict]) -&gt; dict:\n    '''Gives the iou for each anchor boxes with all the bboxes of a list (for example, all the\n    bboxes of an image)\n\n    Args:\n        anchor_boxes_dict (dict): Anchor boxes dictionary\n            - 'anchor_img_coordinates': xyxy coordinates of the anchor boxe (input format, ie. image space)\n        image_bboxes (list&lt;dict&gt;): List of bboxes\n    Returns:\n        dict: The input dictionary to which we added a bboxes field containing coordinates and iou\n    '''\n    # For each anchor ...\n    for anchor_idx, anchor in anchor_boxes_dict.items():\n        anchor['bboxes'] = {}\n        anchor_img_coordinates = anchor['anchor_img_coordinates']\n        # ... and for each bbox in the list ...\n        for index_bbox, bbox in enumerate(image_bboxes):\n            # ... we calculate the iou and add the info to the dictionary of anchors\n            bbox_coordinates = (bbox['x1'], bbox['y1'], bbox['x2'], bbox['y2'])\n            iou = get_iou(anchor_img_coordinates, bbox_coordinates)\n            anchor['bboxes'][index_bbox] = {\n                'iou': iou,\n                'bbox_img_coordinates': bbox_coordinates,\n            }\n        anchor_boxes_dict[anchor_idx] = anchor\n    # Return\n    return anchor_boxes_dict\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_new_img_size_from_min_side_size","title":"<code>get_new_img_size_from_min_side_size(height, width, img_min_side_size=300)</code>","text":"<p>Gets the new dimensions of an image so that the smaller dimension is equal to img_min_side_size but keeping the ratio.</p> <p>Parameters:</p> Name Type Description Default <code>height</code> <code>int</code> <p>Height of the base image</p> required <code>width</code> <code>int</code> <p>Width of the base image</p> required <p>Kwargs:     img_min_side_size (int): Final size of the smaller dimension Raises:     ValueError: If incorrect dimension of the image (&lt; 1)     ValueError: If img_min_side_size is incorrect (&lt; 1) Returns:     int: Resized height     int: Resized width</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def get_new_img_size_from_min_side_size(height: int, width: int, img_min_side_size: int = 300) -&gt; Tuple[int, int]:\n    '''Gets the new dimensions of an image so that the smaller dimension is equal to img_min_side_size\n    but keeping the ratio.\n\n    Args:\n        height (int): Height of the base image\n        width (int): Width of the base image\n    Kwargs:\n        img_min_side_size (int): Final size of the smaller dimension\n    Raises:\n        ValueError: If incorrect dimension of the image (&lt; 1)\n        ValueError: If img_min_side_size is incorrect (&lt; 1)\n    Returns:\n        int: Resized height\n        int: Resized width\n    '''\n    # Manage errors\n    if height &lt; 1 or width &lt; 1:\n        raise ValueError(f\"Incorrect dimension of the image (H : {height} / W : {width})\")\n    if img_min_side_size &lt; 1:\n        raise ValueError(f\"Minimal size wanted incorrect ({img_min_side_size})\")\n    # Width smaller than height, we calculates the new height and set the width to img_min_side_size\n    if width &lt;= height:\n        f = float(img_min_side_size) / width\n        resized_height = int(f * height)\n        resized_width = img_min_side_size\n    # Height smaller than width, we calculates the new width and set the height to img_min_side_size\n    else:\n        f = float(img_min_side_size) / height\n        resized_width = int(f * width)\n        resized_height = img_min_side_size\n    # Return\n    return resized_height, resized_width\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_roi_from_rpn_predictions","title":"<code>get_roi_from_rpn_predictions(model, img_data_batch, rpn_predictions_cls, rpn_predictions_regr)</code>","text":"<p>Converts the output layers of the RPN (classification and regression) in ROIs</p> We get the prediction results of the RPN and we want to select regions of interest (ROIs) for the <p>classifier part. For each point and each base anchor, we apply the results of the regression. Then we crop the resulting ROIs in order to stay in the limit of the image. Then we delete the unsuitable ROIs (ie. invalid) and finally we apply a Non Max Suppression (NMS) algorithm to remove the ROIs which overlap too much.</p> <p>Note : We work with float coordinates. It is no big deal, we will recast them to int to display them.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelKerasFasterRcnnObjectDetector</code> <p>Model used (contains all the necessary configs)</p> required <code>img_data_batch</code> <code>list&lt;dict&gt;</code> <p>List of img_data of the batch Here, it is used to get the (preprocessed) size of the images in order to remove the ROIs which are outside the image. Each entry must contain 'resized_height' &amp; 'resized_width'</p> required <code>rpn_predictions_cls</code> <code>ndarray</code> <p>Classification prediction (output RPN)</p> required <code>rpn_predictions_regr</code> <code>ndarray</code> <p>Regression prediction (output RPN)</p> required <p>Returns:     list : Final ROIs list selected for the classifier part (coordinates in features map space)         Each element is a numpy array of the ROIs coordinates calculated for an image of the batch (variable number).         The coordinates are returned as int (whereas they were float as output of the RPN)         # Format x1, y1, x2, y2         Note : We can't return a numpy array because there are not the same number of ROIs for each image,                thus, we return a list Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def get_roi_from_rpn_predictions(model, img_data_batch: List[dict], rpn_predictions_cls: np.ndarray,\n                                 rpn_predictions_regr: np.ndarray) -&gt; List[np.ndarray]:\n    '''Converts the output layers of the RPN (classification and regression) in ROIs\n\n\n    Process : We get the prediction results of the RPN and we want to select regions of interest (ROIs) for the\n              classifier part. For each point and each base anchor, we apply the results of the regression. Then\n              we crop the resulting ROIs in order to stay in the limit of the image. Then we delete the unsuitable\n              ROIs (ie. invalid) and finally we apply a Non Max Suppression (NMS) algorithm to remove the ROIs\n              which overlap too much.\n\n    Note : We work with float coordinates. It is no big deal, we will recast them to int to display them.\n\n    Args:\n        model (ModelKerasFasterRcnnObjectDetector): Model used (contains all the necessary configs)\n        img_data_batch (list&lt;dict&gt;): List of img_data of the batch\n            Here, it is used to get the (preprocessed) size of the images in order to remove the ROIs which\n            are outside the image.\n            Each entry must contain 'resized_height' &amp; 'resized_width'\n        rpn_predictions_cls (np.ndarray): Classification prediction (output RPN)\n            # shape: (batch_size, height_feature_map, width_feature_map, nb_anchor)\n        rpn_predictions_regr (np.ndarray): Regression prediction (output RPN)\n            # shape: (batch_size, height_feature_map, width_feature_map, 4 * nb_anchor)\n    Returns:\n        list&lt;np.ndarray&gt; : Final ROIs list selected for the classifier part (coordinates in features map space)\n            Each element is a numpy array of the ROIs coordinates calculated for an image of the batch (variable number).\n            The coordinates are returned as int (whereas they were float as output of the RPN)\n            # Format x1, y1, x2, y2\n            Note : We can't return a numpy array because there are not the same number of ROIs for each image,\n                   thus, we return a list\n    '''\n\n    # Get model attributes and info from the input shapes\n    rpn_regr_scaling = model.rpn_regr_scaling\n    subsampling_ratio = model.shared_model_subsampling\n    base_anchors = model.list_anchors\n    nb_anchors = model.nb_anchors\n    roi_nms_overlap_threshold = model.roi_nms_overlap_threshold\n    nms_max_boxes = model.nms_max_boxes\n    batch_size, height_feature_map, width_feature_map, _ = rpn_predictions_cls.shape\n\n    # First we unscale the regression prediction (we scaled the target of the RPN)\n    rpn_predictions_regr = rpn_predictions_regr / rpn_regr_scaling\n    # We get the base anchor base in features map space\n    base_anchors_feature_maps = [(size[0] / subsampling_ratio, size[1] / subsampling_ratio) for size in base_anchors]\n\n    # First we get all the possible anchor boxes on the features map\n    # ie., for each point and each base anchor, we get the coordinates of the anchor box centered on the point\n    # TODO : check if the + 0.5 are necessary\n    anchor_on_feature_maps = np.array([\n        [\n            [\n                [(x + 0.5 - width_anchor / 2, y + 0.5 - height_anchor / 2, height_anchor, width_anchor) for height_anchor, width_anchor in base_anchors_feature_maps]\n                for x in range(width_feature_map)\n            ]\n            for y in range(height_feature_map)\n        ]\n        for i in range(batch_size)\n    ])  # Format (batch_size, height, width, nb_anchors, nb_coords (format xyhw -&gt; 4))\n    # Then we apply the regression result to these anchors\n    # First we put together the coordinates of the anchor box and the regression next to each other for each point /anchor box /image\n    # Format (batch_size, height, width, nb_anchors, 8 (x_anc, y_anc, h_anc, w_anc, tx, ty, th, tw))\n    # Note : first we reshape the regression where the results of each anchors were concatenated\n    rpn_predictions_regr = rpn_predictions_regr.reshape((batch_size, height_feature_map, width_feature_map, nb_anchors, 4))\n    concatenation_anchor_regr = np.concatenate([anchor_on_feature_maps, rpn_predictions_regr], axis=4)\n    # Then we apply the regression for each entry and obtain the candidate ROIs\n    # Format (batch_size, height, width, nb_anchors, 4 (x_roi, y_roi, h_roi, w_roi))\n    rois_on_feature_maps = np.apply_along_axis(func1d=apply_regression, axis=4, arr=concatenation_anchor_regr)  # Format x, y, h, w\n    # Then we crop the ROIs to stay inside the image\n    # Problem : in a batch, we padded the images so that they all have the same size,\n    #           and we want to crop the ROIs with respect to the initial size (unpadded)\n    # Solution : We apply same trick as before, ie. we put the limit size of each image after the coordinates of the associated ROIs\n    feature_map_sizes = np.array([get_feature_map_size(img_data['resized_height'], img_data['resized_width'], subsampling_ratio)\n                                  for img_data in img_data_batch])\n    array_img_size = np.broadcast_to(feature_map_sizes, (height_feature_map, width_feature_map, nb_anchors, batch_size, 2))\n    array_img_size = np.transpose(array_img_size, (3, 0, 1, 2, 4))  # Format (batch_size, height, width, nb_anchors, 2)\n    rois_on_feature_maps = np.concatenate([rois_on_feature_maps, array_img_size], axis=4)  # We add the sizes to the coordinates\n    # We do some work on ROI coordinates\n    # Format (batch_size, height, width, nb_anchors, 4 (x_roi, y_roi, h_roi, w_roi))\n    rois_on_feature_maps = np.apply_along_axis(func1d=restrict_and_convert_roi_boxes, axis=4, arr=rois_on_feature_maps)   # Format x1, y1, x2, y2\n\n    # Reshape the ROIs in order to have (batch_size, nb_rois, 4), nb_rois = height_feature_map * width_feature_map * nb_anchors\n    rois_on_feature_maps = np.reshape(rois_on_feature_maps.transpose((0, 4, 1, 2, 3)), (batch_size, 4, -1)).transpose((0, 2, 1))\n    # Same thing with RPN probabilities, ie. shape (batch_size, nb_rois)\n    rois_probas = rpn_predictions_cls.reshape((batch_size, -1))\n\n    # Finally we select the final ROIs by deleting the invalid ones and by limiting the overlaps\n    # We cast the coordinateds to int in order to use them when cutting the ROIs\n    # TODO : Could we try to always get 300? --&gt; Shape consistente\n    rois_on_feature_maps = select_final_rois(rois_on_feature_maps, rois_probas, roi_nms_overlap_threshold,\n                                             nms_max_boxes, feature_map_sizes)\n    return rois_on_feature_maps\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_roi_from_rpn_predictions--shape-batch_size-height_feature_map-width_feature_map-nb_anchor","title":"shape: (batch_size, height_feature_map, width_feature_map, nb_anchor)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_roi_from_rpn_predictions--shape-batch_size-height_feature_map-width_feature_map-4-nb_anchor","title":"shape: (batch_size, height_feature_map, width_feature_map, 4 * nb_anchor)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_rois_bboxes_iou","title":"<code>get_rois_bboxes_iou(rois, img_data, subsampling_ratio)</code>","text":"<p>Gives the ious between the ROIs (in rois) and the bboxes (in img_data).</p> <p>Parameters:</p> Name Type Description Default <code>rois</code> <code>ndarray</code> <p>ROIs given by the RPN (ie. by the function get_roi_from_rpn_predictions())</p> required <code>img_data</code> <code>dict</code> <p>Metadata of the image after the preprocessing. In particular, the bboxes have been resized and rotated if the image has been resized and rotated. We only use the 'bboxes' field</p> required <code>subsampling_ratio</code> <code>int</code> <p>Subsampling of the base model (shared layers) - to apply to bboxes (which are in image space)</p> required <p>Returns:     dict: Dictionary containing all the IOUs betwee, ROIs and bboxes of the image         Keys : (index_roi) -&gt; 'coordinates' -&gt; 'x1', 'y1', 'x2', 'y2', 'h', 'w'                            -&gt; (index_bbox)  -&gt; 'coordinates': 'x1', 'y1', 'x2', 'y2'                                             -&gt; 'iou'                                             -&gt; 'class'</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def get_rois_bboxes_iou(rois: np.ndarray, img_data: dict, subsampling_ratio: int) -&gt; dict:\n    '''Gives the ious between the ROIs (in rois) and the bboxes (in img_data).\n\n    Args:\n        rois (np.ndarray): ROIs given by the RPN (ie. by the function get_roi_from_rpn_predictions())\n            # Shape (N, 4), N corresponds to the number of given ROIs (in general max 300, cf. model.nms_max_boxes)\n        img_data (dict): Metadata of the image after the preprocessing. In particular, the bboxes have been resized\n            and rotated if the image has been resized and rotated. We only use the 'bboxes' field\n        subsampling_ratio (int): Subsampling of the base model (shared layers) - to apply to bboxes (which are in image space)\n    Returns:\n        dict: Dictionary containing all the IOUs betwee, ROIs and bboxes of the image\n            Keys : (index_roi) -&gt; 'coordinates' -&gt; 'x1', 'y1', 'x2', 'y2', 'h', 'w'\n                               -&gt; (index_bbox)  -&gt; 'coordinates': 'x1', 'y1', 'x2', 'y2'\n                                                -&gt; 'iou'\n                                                -&gt; 'class'\n    '''\n    # Init. output dictionary\n    dict_rois = {}\n    # For each ROI, we get its coordinates, and the ious with each bbox of the image\n    for index_roi in range(rois.shape[0]):\n        # Get the coordinates of each ROI\n        x1_roi, y1_roi, x2_roi, y2_roi = rois[index_roi, :]\n        _, _, h_roi, w_roi = xyxy_to_xyhw(*rois[index_roi, :])\n        dict_roi = {\n            'coordinates': {'x1': x1_roi, 'y1': y1_roi, 'x2': x2_roi, 'y2': y2_roi, 'h': h_roi, 'w': w_roi},\n            'bboxes': {}\n        }\n        # Get the iou of each bbox\n        for index_bbox, bbox in enumerate(img_data['bboxes']):\n            # bbox coordinates - input image format\n            # Bbox coordinates (input format, ie. image space)\n            bbox_coordinates = (bbox['x1'], bbox['y1'], bbox['x2'], bbox['y2'])\n            # Coordinates transformation to features map space\n            x1_bbox, y1_bbox, x2_bbox, y2_bbox = (coord / subsampling_ratio for coord in bbox_coordinates)\n            # Calculus iou\n            iou = get_iou((x1_bbox, y1_bbox, x2_bbox, y2_bbox), (x1_roi, y1_roi, x2_roi, y2_roi))\n            dict_roi['bboxes'][index_bbox] = {\n                'coordinates': {'x1': x1_bbox, 'y1': y1_bbox, 'x2': x2_bbox, 'y2': y2_bbox},\n                'iou': iou,\n                'class': bbox['class']\n            }\n        # Append results\n        dict_rois[index_roi] = dict_roi\n    # Returns\n    return dict_rois\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_rois_bboxes_iou--shape-n-4-n-corresponds-to-the-number-of-given-rois-in-general-max-300-cf-modelnms_max_boxes","title":"Shape (N, 4), N corresponds to the number of given ROIs (in general max 300, cf. model.nms_max_boxes)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_rois_targets","title":"<code>get_rois_targets(dict_rois, classifier_min_overlap, classifier_max_overlap)</code>","text":"<p>Finds the bbox with the biggest iou with an ROI and associate them. Then associates the class of this bbox to the ROI and, if the iou is sufficiently big, gives the associated regression.</p> <p>Parameters:</p> Name Type Description Default <code>dict_rois</code> <code>dict</code> <p>Dictionary containing all the ious between the ROIs and the bboxes of the image</p> required <code>classifier_min_overlap</code> <code>float</code> <p>Minimal threshold to consider a ROI as a target of the classifier (which can still be 'bg')</p> required <code>classifier_max_overlap</code> <code>float</code> <p>Minimal threshold to consider a ROI as matching with a bbox (so with a class which is not 'bg')</p> required <p>Returns:     dict: Dictionary containing the 'viable' ROIs and their classification and regression targets</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def get_rois_targets(dict_rois: dict, classifier_min_overlap: float, classifier_max_overlap: float) -&gt; dict:\n    '''Finds the bbox with the biggest iou with an ROI and associate them. Then associates the class\n    of this bbox to the ROI and, if the iou is sufficiently big, gives the associated regression.\n\n    Args:\n        dict_rois (dict): Dictionary containing all the ious between the ROIs and the bboxes of the image\n        classifier_min_overlap (float): Minimal threshold to consider a ROI as a target of the classifier (which can still be 'bg')\n        classifier_max_overlap (float): Minimal threshold to consider a ROI as matching with a bbox (so with a class which is not 'bg')\n    Returns:\n        dict: Dictionary containing the 'viable' ROIs and their classification and regression targets\n    '''\n    dict_rois_targets = {}\n    # For each ROI ...\n    for roi_index, dict_roi in dict_rois.items():\n        # ... get the coordinates ...\n        coords_roi = dict_roi['coordinates']\n        x1_roi, y1_roi, x2_roi, y2_roi = (coords_roi['x1'], coords_roi['y1'], coords_roi['x2'], coords_roi['y2'])\n        # ... get the best associated bbox (highest iou) ...\n        dict_iou = {bbox_index: dict_roi['bboxes'][bbox_index]['iou'] for bbox_index in dict_roi['bboxes']}\n        best_bbox_index = max(dict_iou, key=dict_iou.get)\n        best_iou = dict_iou[best_bbox_index]\n        # ... if best_iou lower than a threshold, we ignore this ROI ...\n        if best_iou &lt; classifier_min_overlap:\n            continue\n        # ... otherwise, we define the best bbox and we complete the targets\n        dict_roi['best_bbox_index'] = best_bbox_index\n        dict_roi['best_iou'] = best_iou\n        dict_roi['classifier_regression_target'] = (0, 0, 0, 0)\n        # ... if best_iou is above a threshold, we consider a match on a class,\n        # and get the regression target ...\n        if best_iou &gt;= classifier_max_overlap:\n            # class\n            dict_roi['classifier_class_target'] = dict_roi['bboxes'][best_bbox_index]['class']\n            # regression\n            coords_bbox = dict_roi['bboxes'][best_bbox_index]['coordinates']\n            x1_bbox, y1_bbox, x2_bbox, y2_bbox = coords_bbox['x1'], coords_bbox['y1'], coords_bbox['x2'], coords_bbox['y2']\n            dict_roi['classifier_regression_target'] = calc_regr((x1_bbox, y1_bbox, x2_bbox, y2_bbox), (x1_roi, y1_roi, x2_roi, y2_roi))\n        # ... otherwise, we consider the ROI to be background (ie. 'bg')\n        # Note : the regression target is not calculated if background\n        else:\n            dict_roi['classifier_class_target'] = 'bg'\n        # Append results\n        dict_rois_targets[roi_index] = dict_roi\n    # Returns\n    return dict_rois_targets\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_rpn_targets","title":"<code>get_rpn_targets(model, img_data_batch)</code>","text":"<p>Gives the classification and regression targets for the RPN</p> We defined a set of possible anchor boxes (def. 9). For each point of the features map, <p>we look at the possible anchor boxes. We get back to the input image space and keep only the anchor boxes which are totally included in the image. Then, for each anchor box, we check if it matches with a bbox (via iou) and we define our target : match bbox vs match background and gap between anchor box and bbox for the regression part (only if there is a match on a bbox). We use this process for each image</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelKerasFasterRcnnObjectDetector</code> <p>Model used (contains all the necessary configs)</p> required <code>img_data_batch</code> <code>list</code> <p>The list of img_data (dict) for the batch. Each entry is a dictionary with the content of an image (already preprocessed) and associated metadata:         - 'img' -&gt; image in the numpy format (h, w, c), preprocessed and ready to be used by the model         - 'bboxes' -&gt; (dict) associated bboxes (preprocessed image format)              'x1', 'x2', 'y1', 'y2'         - 'original_width' -&gt; Original width of the image         - 'original_height' -&gt; Original height of the image         - 'resized_width' -&gt; Resized width of the image (ie. smaller dim set to img_min_side_size px (def 300))         - 'resized_height' -&gt; Resized height of the image (ie. smaller dim set to img_min_side_size px (def 300))         - 'batch_width' -&gt; Width of the images in the batch (max width of the batch, we pad the smaller images with zeroes)         - 'batch_height' -&gt; Height of the images in the batch (max height of the batch, we pad the smaller images with zeroes)</p> required <p>Returns:     np.ndarray: Classification targets : [y_is_box_valid] + [y_rpn_overlap] for each image with :                 - y_is_box_valid -&gt; if a box is valid (and thus, should enter in the classification loss)                 - y_rpn_overlap -&gt; target of the classification ('pos', 'neg' or 'neutral')         # Shape (batch_size, feature_map_height, feature_map_width, nb_anchors * 2)     np.ndarray: Regression targets : [y_rpn_overlap (repeated x 4)] + [y_rpn_regr] for each image with :                 - y_rpn_overlap -&gt; if a box is an object (and thus, should enter in the regression loss)                     repeated to account for the 4 coordinates                 - y_rpn_regr -&gt; regression targets         # Shape (batch_size, feature_map_height, feature_map_width, nb_anchors * 2 * 4)</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def get_rpn_targets(model, img_data_batch: List[dict]) -&gt; Tuple[np.ndarray, np.ndarray]:\n    '''Gives the classification and regression targets for the RPN\n\n    Process : We defined a set of possible anchor boxes (def. 9). For each point of the features map,\n              we look at the possible anchor boxes. We get back to the input image space and keep only\n              the anchor boxes which are totally included in the image. Then, for each anchor box, we check\n              if it matches with a bbox (via iou) and we define our target : match bbox vs match background\n              and gap between anchor box and bbox for the regression part (only if there is a match on a bbox).\n              We use this process for each image\n\n    Args:\n        model (ModelKerasFasterRcnnObjectDetector): Model used (contains all the necessary configs)\n        img_data_batch (list): The list of img_data (dict) for the batch.\n            Each entry is a dictionary with the content of an image (already preprocessed) and associated metadata:\n                    - 'img' -&gt; image in the numpy format (h, w, c), preprocessed and ready to be used by the model\n                    - 'bboxes' -&gt; (dict) associated bboxes (preprocessed image format)\n                         'x1', 'x2', 'y1', 'y2'\n                    - 'original_width' -&gt; Original width of the image\n                    - 'original_height' -&gt; Original height of the image\n                    - 'resized_width' -&gt; Resized width of the image (ie. smaller dim set to img_min_side_size px (def 300))\n                    - 'resized_height' -&gt; Resized height of the image (ie. smaller dim set to img_min_side_size px (def 300))\n                    - 'batch_width' -&gt; Width of the images in the batch (max width of the batch, we pad the smaller images with zeroes)\n                    - 'batch_height' -&gt; Height of the images in the batch (max height of the batch, we pad the smaller images with zeroes)\n    Returns:\n        np.ndarray: Classification targets : [y_is_box_valid] + [y_rpn_overlap] for each image with :\n                    - y_is_box_valid -&gt; if a box is valid (and thus, should enter in the classification loss)\n                    - y_rpn_overlap -&gt; target of the classification ('pos', 'neg' or 'neutral')\n            # Shape (batch_size, feature_map_height, feature_map_width, nb_anchors * 2)\n        np.ndarray: Regression targets : [y_rpn_overlap (repeated x 4)] + [y_rpn_regr] for each image with :\n                    - y_rpn_overlap -&gt; if a box is an object (and thus, should enter in the regression loss)\n                        repeated to account for the 4 coordinates\n                    - y_rpn_regr -&gt; regression targets\n            # Shape (batch_size, feature_map_height, feature_map_width, nb_anchors * 2 * 4)\n    '''\n    # Extract params from model\n    base_anchors = model.list_anchors\n    nb_anchors = model.nb_anchors\n    subsampling_ratio = model.shared_model_subsampling\n    rpn_min_overlap = model.rpn_min_overlap\n    rpn_max_overlap = model.rpn_max_overlap\n    rpn_regr_scaling = model.rpn_regr_scaling\n    num_regions = model.rpn_restrict_num_regions\n\n    # Info batch size\n    batch_size = len(img_data_batch)\n    # Get size of the features map of the batch (for example by taking the first image)\n    feature_map_height, feature_map_width = get_feature_map_size(img_data_batch[0]['batch_height'], img_data_batch[0]['batch_width'], subsampling_ratio)\n\n    # Setup target arrays\n    Y1 = np.zeros((batch_size, feature_map_height, feature_map_width, nb_anchors * 2))\n    Y2 = np.zeros((batch_size, feature_map_height, feature_map_width, nb_anchors * 2 * 4))\n\n    # We process each image\n    for ind, img_data in enumerate(img_data_batch):\n\n        # Info image data\n        im_resized_height, im_resized_width = img_data['resized_height'], img_data['resized_width']\n        image_bboxes = img_data['bboxes']\n\n        # Get the \"viable\" anchor boxes : one for each couple (point features map, base anchor) except if it does not fit\n        # in the image\n        anchor_boxes_dict = get_all_viable_anchors_boxes(base_anchors, subsampling_ratio, feature_map_height,\n                                                         feature_map_width, im_resized_height, im_resized_width)\n        # Get iou for each couple (anchor box / bbox)\n        anchor_boxes_dict = get_iou_anchors_bboxes(anchor_boxes_dict, image_bboxes)\n\n        # Set anchor validity &amp; type for each anchor\n        # - pos &amp; valid if match on a bbox (ie. an object)\n        # - neg &amp; valid if match on background\n        # - neutral &amp; invalid otherwise\n        anchor_boxes_dict, bboxes_index_with_no_positive = set_anchors_type_validity(anchor_boxes_dict, image_bboxes, rpn_min_overlap, rpn_max_overlap)\n\n        # We add at least one positive anchor box for each bbox which does not have one match\n        # (in some cases, it is not possible, in that case : skip)\n        anchor_boxes_dict = complete_at_least_one_anchor_per_bbox(anchor_boxes_dict, bboxes_index_with_no_positive)\n\n        # Invalidate some anchors in order not to have too many\n        anchor_boxes_dict = restrict_valid_to_n_regions(anchor_boxes_dict, num_regions=num_regions)\n\n        # Add the regression target for the positive and valid anchors\n        anchor_boxes_dict = add_regression_target_to_pos_valid(anchor_boxes_dict)\n\n        # We have the anchors, their type and validity and the regression targets for the pos/valid anchors\n        # We format the result. Here we initialize\n        y_rpn_overlap = np.zeros((feature_map_height, feature_map_width, nb_anchors))  # Target classifier\n        y_is_box_valid = np.zeros((feature_map_height, feature_map_width, nb_anchors))  # couples (ix, anchor) which will enter the loss (ie. are not neutral)\n        y_rpn_regr = np.zeros((feature_map_height, feature_map_width, nb_anchors * 4))\n\n        # For each anchor, we add data.\n        # The deleted anchors (because not 'viable') are not in anchor_boxes_dict, BUT all their characteristics should be zero\n        # which is the case thanks to the initialization\n        for anchor_idx, anchor in anchor_boxes_dict.items():\n            y_rpn_overlap[anchor_idx[0], anchor_idx[1], anchor_idx[2]] = 1 if anchor['anchor_type'] == 'pos' else 0\n            y_is_box_valid[anchor_idx[0], anchor_idx[1], anchor_idx[2]] = anchor['anchor_validity']\n            start_regr_index = 4 * anchor_idx[2]\n            y_rpn_regr[anchor_idx[0], anchor_idx[1], start_regr_index: start_regr_index + 4] = anchor['regression_target']\n\n        # We then concat all final arrays\n        # For regression part, we add y_rpn_overlap (repeated) to y_rpn_regr in order to identify data that should be used by the regression loss\n        y_rpn_cls = np.concatenate([y_is_box_valid, y_rpn_overlap], axis=2)\n        y_rpn_regr = np.concatenate([np.repeat(y_rpn_overlap, 4, axis=2), y_rpn_regr], axis=2)\n\n        # We scale the regression target\n        y_rpn_regr[:, :, y_rpn_regr.shape[2] // 2:] *= rpn_regr_scaling\n\n        # We finally update the output arrays\n        Y1[ind, :, :, :] = y_rpn_cls\n        Y2[ind, :, :, :] = y_rpn_regr\n\n    # Return\n    return Y1, Y2\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_valid_boxes_from_coordinates","title":"<code>get_valid_boxes_from_coordinates(input_img, input_rois, fm_boxes_candidates, regr_coordinates, classifier_regr_scaling, subsampling_ratio, dict_classes)</code>","text":"<p>Calculates the coordinates (in image space) after application of the regression of the boxes (in features map space) whose probability is sufficiently high. Then restricts them to the image and keeps only the valid boxes</p> <p>Parameters:</p> Name Type Description Default <code>input_img</code> <code>ndarray</code> <p>Resized image (useful to get the dimensions)</p> required <code>input_rois</code> <code>ndarray</code> <p>ROIs given by the RPN</p> required <code>fm_boxes_candidates</code> <code>list</code> <p>The boxes (in features map space) valid with respect to their proba</p> required <code>regr_coordinates</code> <code>ndarray</code> <p>Regression prediction for the boxes</p> required <code>classifier_regr_scaling</code> <code>list</code> <p>Scaling to remove from the regression results</p> required <code>subsampling_ratio</code> <code>int</code> <p>Subsampling of the base model (shared layers) - to apply to bboxes (which are in image space)</p> required <code>dict_classes</code> <code>dict</code> <p>Dictionary of the classes of the model</p> required <p>Returns:     A list of boxes valid from a probability AND coordinates xyxy points of view         # Format [(cl, proba, coordinates), (...), ...)</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def get_valid_boxes_from_coordinates(input_img: np.ndarray, input_rois: np.ndarray, fm_boxes_candidates: List[tuple],\n                                     regr_coordinates: np.ndarray, classifier_regr_scaling: List[float], subsampling_ratio: int,\n                                     dict_classes: dict) -&gt; List[tuple]:\n    '''Calculates the coordinates (in image space) after application of the regression of the boxes (in features map space) whose\n    probability is sufficiently high. Then restricts them to the image and keeps only the valid boxes\n\n    Args:\n        input_img (np.ndarray): Resized image (useful to get the dimensions)\n        input_rois (np.ndarray): ROIs given by the RPN\n        fm_boxes_candidates (list): The boxes (in features map space) valid with respect to their proba\n        regr_coordinates (np.ndarray): Regression prediction for the boxes\n        classifier_regr_scaling (list): Scaling to remove from the regression results\n        subsampling_ratio (int): Subsampling of the base model (shared layers) - to apply to bboxes (which are in image space)\n        dict_classes (dict): Dictionary of the classes of the model\n    Returns:\n        A list of boxes valid from a probability AND coordinates xyxy points of view\n            # Format [(cl, proba, coordinates), (...), ...)\n    '''\n    boxes_candidates = []\n    # For each box (in features map space)...\n    for index_box, predicted_class, predicted_proba in fm_boxes_candidates:\n        roi_coordinates = input_rois[index_box]  # Get corresponding ROI\n        regr_predicted = regr_coordinates[index_box][predicted_class * 4: (predicted_class + 1) * 4]  # Get the regression associated to this class\n        regr_predicted = np.array([a / b for a, b in zip(regr_predicted, classifier_regr_scaling)])  # Remove the scaling\n        # Apply predicted regression\n        coordinates_after_regr = list(apply_regression(np.concatenate([roi_coordinates, regr_predicted])))\n        # Make sure that the upper left point is in the features map\n        coordinates_after_regr[0] = max(0, coordinates_after_regr[0])\n        coordinates_after_regr[1] = max(0, coordinates_after_regr[1])\n        bbox_fm_coords = xyhw_to_xyxy(*coordinates_after_regr)\n        # Get the coordinates in the input format (ie. in image space)\n        x1_bbox, y1_bbox, x2_bbox, y2_bbox = (coord * subsampling_ratio for coord in bbox_fm_coords)\n        # Make sure that the point defining the box are in the image\n        x1_bbox = max(0, x1_bbox)\n        y1_bbox = max(0, y1_bbox)\n        x2_bbox = min(input_img.shape[1], x2_bbox)\n        y2_bbox = min(input_img.shape[0], y2_bbox)\n        # If the box is valid ...\n        if x1_bbox &lt; x2_bbox and y1_bbox &lt; y2_bbox:\n            # ... we add it to the list\n            box_infos = (dict_classes[predicted_class], predicted_proba, (x1_bbox, y1_bbox, x2_bbox, y2_bbox))\n            boxes_candidates.append(box_infos)\n    return boxes_candidates\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_valid_fm_boxes_from_proba","title":"<code>get_valid_fm_boxes_from_proba(probas, proba_threshold, bg_index)</code>","text":"<p>Keeps predicted (in features map space) boxes whose probability is above a threshold. Also deletes all the boxes which matched on background</p> <p>Parameters:</p> Name Type Description Default <code>probas</code> <code>ndarray</code> <p>Probabilities of the boxes predicted by the model</p> required <code>proba_threshold</code> <code>float</code> <p>Threshold below which, boxes are eliminated</p> required <p>Returns:     A list of boxes (in features map space) valid from a probability point of view         # Format [(index, index_cl, proba), (...), ...)</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def get_valid_fm_boxes_from_proba(probas: np.ndarray, proba_threshold: float, bg_index: int) -&gt; List[tuple]:\n    '''Keeps predicted (in features map space) boxes whose probability is above a threshold. Also deletes\n    all the boxes which matched on background\n\n    Args:\n        probas (np.ndarray): Probabilities of the boxes predicted by the model\n        proba_threshold (float): Threshold below which, boxes are eliminated\n    Returns:\n        A list of boxes (in features map space) valid from a probability point of view\n            # Format [(index, index_cl, proba), (...), ...)\n    '''\n    fm_boxes_candidates = []\n    # For each box ...\n    for index_box, box_probas in enumerate(probas):\n        # ... get the class ...\n        predicted_class = np.argmax(box_probas)\n        # ... get the corresponding probability ...\n        predicted_proba = box_probas[predicted_class]\n        # ..., and, if we are above the threshold and the predicted class is not the background...\n        if predicted_proba &gt;= proba_threshold and predicted_class != bg_index:\n            # ... we add the box to the list\n            fm_boxes_candidates.append((index_box, predicted_class, predicted_proba))\n    return fm_boxes_candidates\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.limit_rois_targets","title":"<code>limit_rois_targets(dict_rois_targets, nb_rois_per_img)</code>","text":"<p>Limits the number of input / output for each image in order not to have OOM</p> <p>Parameters:</p> Name Type Description Default <code>dict_rois</code> <code>dict</code> <p>Dictionary containing the possible inputs / targets of the classifier</p> required <code>nb_rois_per_img</code> <code>int</code> <p>Maximal number of ROIs to return for each image In the rare case where there are not enough ROIs, we clone the ROIs in order to have enough If no ROI, we return None. This case is then handled by the function create_fake_dict_rois_targets</p> required <p>Returns:     dict: The dictionary containing the \"selected\" dictionary</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def limit_rois_targets(dict_rois_targets: dict, nb_rois_per_img: int) -&gt; Union[dict, None]:\n    '''Limits the number of input / output for each image in order not to have OOM\n\n    Args:\n        dict_rois (dict): Dictionary containing the possible inputs / targets of the classifier\n        nb_rois_per_img (int): Maximal number of ROIs to return for each image\n            In the rare case where there are not enough ROIs, we clone the ROIs in order to have enough\n            If no ROI, we return None. This case is then handled by the function create_fake_dict_rois_targets\n    Returns:\n        dict: The dictionary containing the \"selected\" dictionary\n    '''\n    # Get the positive and negative ROIs\n    pos_rois_indexes = [roi_index for roi_index, roi in dict_rois_targets.items() if roi['classifier_class_target'] != 'bg']\n    neg_rois_indexes = [roi_index for roi_index, roi in dict_rois_targets.items() if roi['classifier_class_target'] == 'bg']\n    # Case 1 : no ROI (very rare ?!), return None\n    if len(pos_rois_indexes) + len(neg_rois_indexes) == 0:\n        logger.warning(\"Warning, there is an image for which we do not have a target ROI for the classifier.\")\n        return None\n    # Case 2 : not enough ROIs\n    elif len(pos_rois_indexes) + len(neg_rois_indexes) &lt; nb_rois_per_img:\n        logger.warning(f\"Warning, there is an image for which we have less than {nb_rois_per_img} target ROIs. We randomly clone some ROIs\")\n        selected_indexes = pos_rois_indexes + neg_rois_indexes\n        selected_indexes = random.sample(selected_indexes, k=len(selected_indexes))\n        selected_indexes = list(np.resize(selected_indexes, nb_rois_per_img))\n    # Case 3 : enough ROIs\n    else:\n        # Case 3.1 : not enough positive ROIs\n        if len(pos_rois_indexes) &lt; nb_rois_per_img // 2:\n            selected_neg_indexes = random.sample(neg_rois_indexes, k=(nb_rois_per_img - len(pos_rois_indexes)))\n            selected_indexes = pos_rois_indexes + selected_neg_indexes\n            selected_indexes = random.sample(selected_indexes, k=len(selected_indexes))\n        # Cas 3.2 : not enough negative ROIs\n        elif len(neg_rois_indexes) &lt; nb_rois_per_img // 2:\n            selected_pos_indexes = random.sample(pos_rois_indexes, k=(nb_rois_per_img - len(neg_rois_indexes)))\n            selected_indexes = selected_pos_indexes + neg_rois_indexes\n            selected_indexes = random.sample(selected_indexes, k=len(selected_indexes))\n        # Cas 3.3 : nominal case, we have everything we need\n        else:\n            selected_pos_indexes = random.sample(pos_rois_indexes, k=nb_rois_per_img // 2)\n            selected_neg_indexes = random.sample(neg_rois_indexes, k=(nb_rois_per_img - len(selected_pos_indexes)))\n            selected_indexes = selected_pos_indexes + selected_neg_indexes\n            selected_indexes = random.sample(selected_indexes, k=len(selected_indexes))\n    # We return the ROIs whose index are in selected_indexes\n    # We are careful to manage \"duplicates\" in the list of selected indices\n    return {i: copy.deepcopy(dict_rois_targets[roi_index]) for i, roi_index in enumerate(selected_indexes)}\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.non_max_suppression_fast","title":"<code>non_max_suppression_fast(img_boxes_coordinates, img_boxes_probas, nms_overlap_threshold, nms_max_boxes, img_boxes_classes=None)</code>","text":"<p>Filters boxes in order to limit overlaps on the same object using a list of boxes (ROIs or final predictions) and the probabilities of matching with an object.</p> <p>Parameters:</p> Name Type Description Default <code>img_boxes_coordinates</code> <code>ndarray</code> <p>The coordinates of the boxes (in opposite points format)</p> required <code>img_boxes_probas</code> <code>ndarray</code> <p>The probabilities associated to the boxes</p> required <code>nms_overlap_threshold</code> <code>float</code> <p>The iou value above which we assume that two boxes overlap</p> required <code>nms_max_boxes</code> <code>int</code> <p>The maximal number of boxes that this function can return</p> required <p>Kwargs:     img_boxes_classes (np.ndarray): The classes associated with the boxes (optional)         # shape: (nb_boxes) Raises:     ValueError: If img_boxes_probas is not the same length as img_boxes_coordinates     ValueError: If nms_overlap_threshold &lt;= 0 or &gt; 1     ValueError: If nms_max_boxes &lt; 1     ValueError: If img_boxes_classes is not the same length as img_boxes_coordinates (if != None) Returns:     np.ndarray: List of kept boxes         # shape: (nb_boxes_kept, 4)     np.ndarray: Associated probabilities     np.ndarray: Associated classes (if prediction)</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def non_max_suppression_fast(img_boxes_coordinates: np.ndarray, img_boxes_probas: np.ndarray, nms_overlap_threshold: float,\n                             nms_max_boxes: int, img_boxes_classes: Union[np.ndarray, None] = None) -&gt; np.ndarray:\n    '''Filters boxes in order to limit overlaps on the same object using a list of boxes (ROIs or final predictions)\n    and the probabilities of matching with an object.\n\n    Args:\n        img_boxes_coordinates (np.ndarray): The coordinates of the boxes (in opposite points format)\n            # shape: (nb_boxes, 4)\n        img_boxes_probas (np.ndarray): The probabilities associated to the boxes\n            # shape: (nb_boxes)\n        nms_overlap_threshold (float): The iou value above which we assume that two boxes overlap\n        nms_max_boxes (int): The maximal number of boxes that this function can return\n    Kwargs:\n        img_boxes_classes (np.ndarray): The classes associated with the boxes (optional)\n            # shape: (nb_boxes)\n    Raises:\n        ValueError: If img_boxes_probas is not the same length as img_boxes_coordinates\n        ValueError: If nms_overlap_threshold &lt;= 0 or &gt; 1\n        ValueError: If nms_max_boxes &lt; 1\n        ValueError: If img_boxes_classes is not the same length as img_boxes_coordinates (if != None)\n    Returns:\n        np.ndarray: List of kept boxes\n            # shape: (nb_boxes_kept, 4)\n        np.ndarray: Associated probabilities\n        np.ndarray: Associated classes (if prediction)\n    '''\n    # code taken from here: http://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/\n    # if there are no boxes, returns an empty list\n\n    # Manage errors\n    if img_boxes_coordinates.shape[0] != img_boxes_probas.shape[0]:\n        raise ValueError(\"The arrays img_boxes_coordinates and img_boxes_probas must have the same length.\")\n    if not 0 &lt; nms_overlap_threshold &lt;= 1:\n        raise ValueError(\"The value of nms_overlap_threshold must be between 0 and 1 (0 excluded, 1 included)\")\n    if nms_max_boxes &lt; 1:\n        raise ValueError(\"The argument nms_max_boxes must be positive\")\n    if img_boxes_classes is not None and img_boxes_coordinates.shape[0] != img_boxes_classes.shape[0]:\n        raise ValueError(\"The arrays img_boxes_coordinates and img_boxes_classes must have the same length.\")\n\n    # Process explanation:\n    #   Step 1: Sort the probs list\n    #   Step 2: Find the larget prob 'Last' in the list and save it to the pick list\n    #   Step 3: Calculate the IoU with 'Last' box and other boxes in the list. If the IoU is larger than overlap_threshold, delete the box from list\n    #   Step 4: Repeat step 2 and step 3 until there is no item in the probs list\n\n    # If empty, return an empty array\n    if len(img_boxes_coordinates) == 0:\n        return np.array([]), np.array([]), np.array([])\n\n    # Grab the coordinates of the boxes &amp; calculate the areas\n    x1_box, y1_box, x2_box, y2_box = (img_boxes_coordinates[:, i] for i in range(4))\n    boxes_areas = (x2_box - x1_box) * (y2_box - y1_box)\n\n    # We now loop over each boxes, sorted by max probas\n    picked_index = []\n    idxs = np.argsort(img_boxes_probas)\n    # Keep looping while some indexes still remain in the list\n    while len(idxs) &gt; 0:\n        # If we have enough boxes, break\n        if len(picked_index) &gt;= nms_max_boxes:\n            break\n\n        # Add highest proba remaining to picked indexes\n        picked_index.append(idxs[-1])\n\n        # Find intersection area between picked box &amp; remaining candidates\n        xx1_int = np.maximum(x1_box[idxs[-1]], x1_box[idxs[:-1]])\n        yy1_int = np.maximum(y1_box[idxs[-1]], y1_box[idxs[:-1]])\n        xx2_int = np.minimum(x2_box[idxs[-1]], x2_box[idxs[:-1]])\n        yy2_int = np.minimum(y2_box[idxs[-1]], y2_box[idxs[:-1]])\n        ww_int = np.maximum(0, xx2_int - xx1_int)\n        hh_int = np.maximum(0, yy2_int - yy1_int)\n        area_int = ww_int * hh_int\n\n        # Get the union\n        area_union = boxes_areas[idxs[-1]] + boxes_areas[idxs[:-1]] - area_int\n\n        # Compute the overlap (i.e. iou)\n        overlap = area_int / (area_union + 1e-6)\n\n        # Delete last index (selected) &amp; all indexes from the index list that have an IOU higher than a given threhsold\n        idxs = np.delete(idxs, np.concatenate(([len(idxs) - 1], np.where(overlap &gt; nms_overlap_threshold)[0])))\n\n    # Return only the boxes that were picked\n    img_boxes_coordinates = img_boxes_coordinates[picked_index, :]\n    if img_boxes_classes is not None:\n        return img_boxes_coordinates, img_boxes_probas[picked_index], img_boxes_classes[picked_index]\n    else:\n        return img_boxes_coordinates, img_boxes_probas[picked_index], None\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.non_max_suppression_fast--shape-nb_boxes-4","title":"shape: (nb_boxes, 4)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.non_max_suppression_fast--shape-nb_boxes","title":"shape: (nb_boxes)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.non_max_suppression_fast_on_preds","title":"<code>non_max_suppression_fast_on_preds(boxes_candidates, nms_overlap_threshold)</code>","text":"<p>Applies the NMS algorithm on the valid predicted boxes to avoid overlaps.</p> <p>Parameters:</p> Name Type Description Default <code>boxes_candidates</code> <code>list</code> <p>Valid predicted boxes</p> required <code>nms_overlap_threshold</code> <code>float</code> <p>Above this threshold for the iou, two boxes are said to be overlapping</p> required <p>Returns:     A list of boxes valid from a probability AND coordinates xyxy points of view and with \"no\" overlap         # Format [(cl, proba, coordinates), (...), ...)</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def non_max_suppression_fast_on_preds(boxes_candidates: List[tuple], nms_overlap_threshold: float) -&gt; List[tuple]:\n    '''Applies the NMS algorithm on the valid predicted boxes to avoid overlaps.\n\n    Args:\n        boxes_candidates (list): Valid predicted boxes\n            # Format [(cl, proba, coordinates), (...), ...)\n        nms_overlap_threshold (float): Above this threshold for the iou, two boxes are said to be overlapping\n    Returns:\n        A list of boxes valid from a probability AND coordinates xyxy points of view and with \"no\" overlap\n            # Format [(cl, proba, coordinates), (...), ...)\n    '''\n    # If there are no valid boxes\n    if len(boxes_candidates) == 0:\n        return []\n    # First we format the inputs to the format for the NMS\n    img_boxes_classes = np.array([cl for cl, _, _ in boxes_candidates])\n    img_boxes_probas = np.array([proba for _, proba, _ in boxes_candidates])\n    img_boxes_coordinates = np.array([coordinates for _, _, coordinates in boxes_candidates])\n    # Apply NMS\n    nms_result = non_max_suppression_fast(img_boxes_coordinates, img_boxes_probas, nms_overlap_threshold, np.inf, img_boxes_classes=img_boxes_classes)\n    img_boxes_coordinates, img_boxes_probas, img_boxes_classes = nms_result\n    # Format final result\n    final_boxes = []\n    for i in range(img_boxes_coordinates.shape[0]):\n        final_boxes.append((img_boxes_classes[i], img_boxes_probas[i], img_boxes_coordinates[i]))\n    # Return\n    return final_boxes\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.non_max_suppression_fast_on_preds--format-cl-proba-coordinates","title":"Format [(cl, proba, coordinates), (...), ...)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.restrict_and_convert_roi_boxes","title":"<code>restrict_and_convert_roi_boxes(bbox_coordinates)</code>","text":"<p>Resizes the box to have the minimal size and crops it to stay in the features map. Finally, converts it in xyxy coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>bbox_coordinates</code> <code>ndarray</code> <p>An array composed of 6 objects : x_roi, y_roi, h_roi, w_roi, height_img_in_feature_map, width_img_in_feature_map. (x_roi, y_roi, h_roi, w_roi) are the coordinates of a ROI (height_img_in_feature_map, width_img_in_feature_map) sont les tailles avant padding de l'image correspondantes, puis downsampled au format feature map</p> required <p>Returns:     float: Coordinates of the ROI after correction - x coordinate of the upper left point     float: Coordinates of the ROI after correction - y coordinate of the upper left point     float: Coordinates of the ROI after correction - x coordinate of the bottom right point     float: Coordinates of the ROI after correction - y coordinate of the bottom right point</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def restrict_and_convert_roi_boxes(bbox_coordinates: np.ndarray) -&gt; Tuple[float, float, float, float]:\n    '''Resizes the box to have the minimal size and crops it to stay in the features map. Finally,\n    converts it in xyxy coordinates.\n\n    Args:\n        bbox_coordinates (np.ndarray): An array composed of 6 objects : x_roi, y_roi, h_roi, w_roi, height_img_in_feature_map, width_img_in_feature_map.\n            (x_roi, y_roi, h_roi, w_roi) are the coordinates of a ROI\n            (height_img_in_feature_map, width_img_in_feature_map) sont les tailles avant padding de l'image correspondantes, puis downsampled au format feature map\n    Returns:\n        float: Coordinates of the ROI after correction - x coordinate of the upper left point\n        float: Coordinates of the ROI after correction - y coordinate of the upper left point\n        float: Coordinates of the ROI after correction - x coordinate of the bottom right point\n        float: Coordinates of the ROI after correction - y coordinate of the bottom right point\n    '''\n    x, y, h, w, height_img_in_feature_map, width_img_in_feature_map = bbox_coordinates\n    # We want the box to have a size of at least 1\n    h = np.maximum(1, h)\n    w = np.maximum(1, w)\n    # We want the upper left point to be in the image (projected on the features map)\n    x = np.maximum(0, x)\n    y = np.maximum(0, y)\n    # Convert in xyxy (opposite points format)\n    x1, y1, x2, y2 = xyhw_to_xyxy(x, y, h, w)\n    # We want the bottom right point to be in the image (projected on the features map)\n    x2 = np.minimum(width_img_in_feature_map, x2)\n    y2 = np.minimum(height_img_in_feature_map, y2)\n    # Return new coordinates\n    return x1, y1, x2, y2\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.restrict_valid_to_n_regions","title":"<code>restrict_valid_to_n_regions(anchor_boxes_dict, num_regions)</code>","text":"<p>Restricts the number of valid anchor boxes. If there are more positive anchor boxes than hald of num_regions :     - we invalidate positive anchors until there are less than num_regions / 2     - Then, we invalidate positive anchors until the number of valid anchors is equal to num_regions</p> <p>Parameters:</p> Name Type Description Default <code>anchor_boxes_dict</code> <code>dict</code> <p>Anchor boxes dictionary - 'anchor_img_coordinates': Coordinates of the anchor box (input format ie. image space) - 'bboxes': bboxes with their coordinates xyxy (in image space) and iou - 'anchor_type': anchor type (pos, neg or neutral) - 'anchor_validity': anchor validity - 'best_bbox_index': bbox associated to this anchor</p> required <code>num_regions</code> <code>int</code> <p>The number of valid anchors we want to consider</p> required <p>Returns:     dict: Updated anchor boxes dictionary</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def restrict_valid_to_n_regions(anchor_boxes_dict: dict, num_regions: int) -&gt; dict:\n    '''Restricts the number of valid anchor boxes.\n    If there are more positive anchor boxes than hald of num_regions :\n        - we invalidate positive anchors until there are less than num_regions / 2\n        - Then, we invalidate positive anchors until the number of valid anchors is equal to num_regions\n\n    Args:\n        anchor_boxes_dict (dict): Anchor boxes dictionary\n            - 'anchor_img_coordinates': Coordinates of the anchor box (input format ie. image space)\n            - 'bboxes': bboxes with their coordinates xyxy (in image space) and iou\n            - 'anchor_type': anchor type (pos, neg or neutral)\n            - 'anchor_validity': anchor validity\n            - 'best_bbox_index': bbox associated to this anchor\n        num_regions (int): The number of valid anchors we want to consider\n    Returns:\n        dict: Updated anchor boxes dictionary\n    '''\n    # We look at both positive and negative anchor boxes\n    # No need to test for validity at this point, positive and negative anchor boxes are necessarily valid\n    positive_anchor_indexes = [anchor_idx for anchor_idx, anchor in anchor_boxes_dict.items() if anchor['anchor_type'] == 'pos']\n    negative_anchor_indexes = [anchor_idx for anchor_idx, anchor in anchor_boxes_dict.items() if anchor['anchor_type'] == 'neg']\n\n    # First we invalidate the surplus of positive anchors if needed ...\n    nb_pos = len(positive_anchor_indexes)\n    nb_pos_to_invalidate = max(0, nb_pos - int(num_regions / 2))\n    if nb_pos_to_invalidate &gt; 0:\n        # Random select\n        anchors_indexes_to_unvalid = random.sample(positive_anchor_indexes, nb_pos_to_invalidate)\n        for anchor_idx in anchors_indexes_to_unvalid:\n            anchor_boxes_dict[anchor_idx]['anchor_validity'] = 0\n        nb_pos = int(num_regions / 2)\n\n    # ... Then we invalidate negative regions until we have num_regions valid anchor boxes\n    nb_neg_to_invalidate = len(negative_anchor_indexes) + nb_pos - num_regions\n    if nb_neg_to_invalidate &gt; 0:\n        # Random select\n        anchors_indexes_to_unvalid = random.sample(negative_anchor_indexes, nb_neg_to_invalidate)\n        for anchor_idx in anchors_indexes_to_unvalid:\n            anchor_boxes_dict[anchor_idx]['anchor_validity'] = 0\n\n    # Return updated dict\n    return anchor_boxes_dict\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.select_final_rois","title":"<code>select_final_rois(rois_coordinates, rois_probas, roi_nms_overlap_threshold, nms_max_boxes, feature_map_sizes)</code>","text":"<p>Deletes the invalid ROIs and selects some of them to limit the overlaps</p> <p>Parameters:</p> Name Type Description Default <code>rois_coordinates</code> <code>ndarray</code> <p>The set of all selected ROIs for all the images of the batch</p> required <code>rois_probas</code> <code>ndarray</code> <p>The probabilities associated to each selected ROIsfor all the images</p> required <code>roi_nms_overlap_threshold</code> <code>float</code> <p>Above this threshold for the iou, we assume that two ROIs overlap</p> required <code>nms_max_boxes</code> <code>int</code> <p>Maximal number of ROIs that this function can return for each image</p> required <code>feature_map_sizes</code> <code>ndarray</code> <p>Theoretical heights and widths of the features maps - useful if we have no valid ROI anymore. Allows to manage the fact that, in a batch, we padded the images so that they all have the same size     # shape: (batch_size, 2)</p> required <p>Returns:     list: Final list of the ROIs selected for the classifier part Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def select_final_rois(rois_coordinates: np.ndarray, rois_probas: np.ndarray, roi_nms_overlap_threshold: float,\n                      nms_max_boxes: int, feature_map_sizes: np.ndarray) -&gt; List[np.ndarray]:\n    '''Deletes the invalid ROIs and selects some of them to limit the overlaps\n\n    Args:\n        rois_coordinates (np.ndarray): The set of all selected ROIs for all the images of the batch\n            # shape: (batch_size, nb_rois, 4)\n        rois_probas (np.ndarray): The probabilities associated to each selected ROIsfor all the images\n            # shape: (batch_size, nb_rois)\n        roi_nms_overlap_threshold (float): Above this threshold for the iou, we assume that two ROIs overlap\n        nms_max_boxes (int): Maximal number of ROIs that this function can return for each image\n        feature_map_sizes (np.ndarray): Theoretical heights and widths of the features maps - useful if we have no valid ROI anymore.\n            Allows to manage the fact that, in a batch, we padded the images so that they all have the same size\n                # shape: (batch_size, 2)\n    Returns:\n        list&lt;np.ndarray&gt;: Final list of the ROIs selected for the classifier part\n    '''\n    # We process each image of the batch separately and stocks the results in list_rois\n    # We can't return a numpy array because the number of ROIs for each image is not the same -&gt; we return a list\n    list_rois = []\n    for img_index in range(rois_coordinates.shape[0]):\n        # Get infos\n        img_rois_coordinates = rois_coordinates[img_index]\n        img_rois_probas = rois_probas[img_index]\n        x1, y1, x2, y2 = (img_rois_coordinates[:, i] for i in range(4))\n        # Eliminate invalid anchors\n        idxs = np.where((x1 - x2 &gt;= 0) | (y1 - y2 &gt;= 0))\n        img_rois_coordinates = np.delete(img_rois_coordinates, idxs, 0)\n        img_rois_probas = np.delete(img_rois_probas, idxs, 0)\n        # In the rare cases where there are no more ROIs, we create one artificially (the whole image)\n        if img_rois_coordinates.shape[0] == 0:\n            logger.warning(\"Warning, there is an image for which we can't find a valid ROI.\")\n            logger.warning(\"By default, we create an artificial ROI which cover the whole image.\")\n            height_img_in_feature_map, width_img_in_feature_map = feature_map_sizes[img_index, :]\n            img_rois_coordinates = np.array([[0, 0, width_img_in_feature_map, height_img_in_feature_map]])  # x1, y1, x2, y2\n        # Otherwise, we continue the process\n        else:\n            # We keep the ROIs which do not overlap\n            img_rois_coordinates, _, _ = non_max_suppression_fast(img_rois_coordinates, img_rois_probas, roi_nms_overlap_threshold, nms_max_boxes)\n            # Finally, we cast to int (because we will cut the features map per index)\n            # Round\n            img_rois_coordinates = np.around(img_rois_coordinates).astype(\"int\")\n            # Delete invalid ROIs again (after rounding)\n            x1, y1, x2, y2 = (img_rois_coordinates[:, i] for i in range(4))\n            idxs = np.where((x1 - x2 &gt;= 0) | (y1 - y2 &gt;= 0))\n            img_rois_coordinates = np.delete(img_rois_coordinates, idxs, 0)\n        # Append result\n        list_rois.append(img_rois_coordinates)\n    # Return ROIs\n    return list_rois\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.select_final_rois--shape-batch_size-nb_rois-4","title":"shape: (batch_size, nb_rois, 4)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.select_final_rois--shape-batch_size-nb_rois","title":"shape: (batch_size, nb_rois)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.set_anchors_type_validity","title":"<code>set_anchors_type_validity(anchor_boxes_dict, image_bboxes, rpn_min_overlap, rpn_max_overlap)</code>","text":"<p>Defines the type and the validity of each anchor     Type:         - pos -&gt; Match between the anchor and a bbox         - neg -&gt; Match between the anchor and the background         - neutral -&gt; In between the two, won't be used by the model     Validity:         - 1 -&gt; If pos or neg         - 0 -&gt; If neutral Args:     anchor_boxes_dict (dict): Anchor boxes dictionary         - 'anchor_img_coordinates': Coordinates of the anchor box (input format ie. image space)         - 'bboxes': bboxes with their coordinates xyxy (in image space) and iou     image_bboxes (list): List of bboxes of the image     rpn_min_overlap (float): Threshold below which a bbox is marked as negative     rpn_max_overlap (float): Threshold aboce which a bbox is marked as positive Returns:     dict: Dictionary of the anchors boxes (input dictionary) with type and validity added ('anchor_type', 'anchor_validity')     list: Liste of the bboxes with no positive anchor associated Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def set_anchors_type_validity(anchor_boxes_dict: dict, image_bboxes: List[dict], rpn_min_overlap: float,\n                              rpn_max_overlap: float) -&gt; Tuple[dict, list]:\n    '''Defines the type and the validity of each anchor\n        Type:\n            - pos -&gt; Match between the anchor and a bbox\n            - neg -&gt; Match between the anchor and the background\n            - neutral -&gt; In between the two, won't be used by the model\n        Validity:\n            - 1 -&gt; If pos or neg\n            - 0 -&gt; If neutral\n    Args:\n        anchor_boxes_dict (dict): Anchor boxes dictionary\n            - 'anchor_img_coordinates': Coordinates of the anchor box (input format ie. image space)\n            - 'bboxes': bboxes with their coordinates xyxy (in image space) and iou\n        image_bboxes (list&lt;dict&gt;): List of bboxes of the image\n        rpn_min_overlap (float): Threshold below which a bbox is marked as negative\n        rpn_max_overlap (float): Threshold aboce which a bbox is marked as positive\n    Returns:\n        dict: Dictionary of the anchors boxes (input dictionary) with type and validity added ('anchor_type', 'anchor_validity')\n        list: Liste of the bboxes with no positive anchor associated\n    '''\n    bboxes_index_with_positive = set()\n    # For each anchor ...\n    for anchor_idx, anchor in anchor_boxes_dict.items():\n        # Get the dictionary where the keys are the bboxes and the values, the iou\n        dict_iou = {index_bbox: dict_bbox['iou'] for index_bbox, dict_bbox in anchor['bboxes'].items()}\n        # Get max iou (if for some reason no bbox, set it to 0 (i.e 'neg'))\n        max_iou = max(dict_iou.values()) if len(dict_iou) &gt; 0 else 0\n        # If we are above threshold max, the anchor is positive and valid\n        if max_iou &gt; rpn_max_overlap:\n            anchor['anchor_type'] = 'pos'\n            anchor['anchor_validity'] = 1\n            best_bbox_index = max(dict_iou, key=dict_iou.get)\n            anchor['best_bbox_index'] = best_bbox_index\n            bboxes_index_with_positive.add(best_bbox_index)\n        # If we are below threshold min, the anchor is negative and valid\n        elif 0 &lt;= max_iou &lt; rpn_min_overlap:\n            anchor['anchor_type'] = 'neg'\n            anchor['anchor_validity'] = 1\n            anchor['best_bbox_index'] = -1\n        # Otherwise, it is invalid (and we set it to neutral)\n        else:\n            anchor['anchor_type'] = 'neutral'\n            anchor['anchor_validity'] = 0\n            anchor['best_bbox_index'] = -1\n        anchor_boxes_dict[anchor_idx] = anchor\n    # Get list of bboxes index without positive anchor\n    bboxes_index_with_no_positive = [index_bbox for index_bbox in range(len(image_bboxes))\n                                     if index_bbox not in bboxes_index_with_positive]\n    return anchor_boxes_dict, bboxes_index_with_no_positive\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.xyhw_to_xyxy","title":"<code>xyhw_to_xyxy(x, y, h, w)</code>","text":"<p>Changes a rectangle in the format xyhw (x, y, h, w) to the format xyxy (x1, y1, x2, y2)</p> Args <p>x (float): x coordinate of the upper left point y (float): y coordinate of the upper left point h (float): height of the rectangle w (float): width of the rectangle</p> <p>Returns:     float: x coordinate of the upper left point     float: y coordinate of the upper left point     float: x coordinate of the bottom right point     float: y coordinate of the bottom right point</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>@check_coordinates_validity\ndef xyhw_to_xyxy(x: float, y: float, h: float, w: float) -&gt; Tuple[float, float, float, float]:\n    '''Changes a rectangle in the format xyhw (x, y, h, w) to\n    the format xyxy (x1, y1, x2, y2)\n\n    Args :\n        x (float): x coordinate of the upper left point\n        y (float): y coordinate of the upper left point\n        h (float): height of the rectangle\n        w (float): width of the rectangle\n    Returns:\n        float: x coordinate of the upper left point\n        float: y coordinate of the upper left point\n        float: x coordinate of the bottom right point\n        float: y coordinate of the bottom right point\n    '''\n    x2 = x + w\n    y2 = y + h\n    return x, y, x2, y2\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.xyxy_to_cxcyhw","title":"<code>xyxy_to_cxcyhw(x1, y1, x2, y2)</code>","text":"<p>Changes a rectangle in the format xyxy (x1, y1, x2, y2) to the format cxcyhw (cx, cy, h, w)</p> Args <p>x1 (float): x coordinate of the upper left point y1 (float): y coordinate of the upper left point x2 (float): x coordinate of the bottom right point y2 (float): y coordinate of the bottom right point</p> <p>Returns:     float: x coordinate of the center of the rectangle     float: y coordinate of the center of the rectangle     float: height of the rectangle     float: width of the rectangle</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>@check_coordinates_validity\ndef xyxy_to_cxcyhw(x1: float, y1: float, x2: float, y2: float) -&gt; Tuple[float, float, float, float]:\n    '''Changes a rectangle in the format xyxy (x1, y1, x2, y2) to\n    the format cxcyhw (cx, cy, h, w)\n\n    Args :\n        x1 (float): x coordinate of the upper left point\n        y1 (float): y coordinate of the upper left point\n        x2 (float): x coordinate of the bottom right point\n        y2 (float): y coordinate of the bottom right point\n    Returns:\n        float: x coordinate of the center of the rectangle\n        float: y coordinate of the center of the rectangle\n        float: height of the rectangle\n        float: width of the rectangle\n    '''\n    cx = (x1 + x2) / 2.0\n    cy = (y1 + y2) / 2.0\n    width = x2 - x1\n    height = y2 - y1\n    return cx, cy, height, width\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.xyxy_to_xyhw","title":"<code>xyxy_to_xyhw(x1, y1, x2, y2)</code>","text":"<p>Changes a rectangle in the format xyxy (x1, y1, x2, y2) to the format xyhw (x, y, h, w)</p> Args <p>x1 (float): x coordinate of the upper left point y1 (float): y coordinate of the upper left point x2 (float): x coordinate of the bottom right point y2 (float): y coordinate of the bottom right point</p> <p>Returns:     float: x coordinate of the upper left point     float: y coordinate of the upper left point     float: height of the rectangle     float: width of the rectangle</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>@check_coordinates_validity\ndef xyxy_to_xyhw(x1: float, y1: float, x2: float, y2: float) -&gt; Tuple[float, float, float, float]:\n    '''Changes a rectangle in the format xyxy (x1, y1, x2, y2) to\n    the format xyhw (x, y, h, w)\n\n    Args :\n        x1 (float): x coordinate of the upper left point\n        y1 (float): y coordinate of the upper left point\n        x2 (float): x coordinate of the bottom right point\n        y2 (float): y coordinate of the bottom right point\n    Returns:\n        float: x coordinate of the upper left point\n        float: y coordinate of the upper left point\n        float: height of the rectangle\n        float: width of the rectangle\n    '''\n    h = y2 - y1\n    w = x2 - x1\n    return x1, y1, h, w\n</code></pre>"},{"location":"reference/template_vision/monitoring/","title":"Monitoring","text":""},{"location":"reference/template_vision/monitoring/mlflow_logger/","title":"Mlflow logger","text":""},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger","title":"<code>MLflowLogger</code>","text":"<p>Abstracts how MlFlow works</p> Source code in <code>template_vision/monitoring/mlflow_logger.py</code> <pre><code>class MLflowLogger:\n    '''Abstracts how MlFlow works'''\n\n    def __init__(self, experiment_name: str, tracking_uri: str = '', artifact_uri: str = '') -&gt; None:\n        '''Class initialization\n        Args:\n            experiment_name (str):  Name of the experiment to activate\n        Kwargs:\n            tracking_uri (str): URI of the tracking server\n            artifact_uri (str): URI where to store artifacts\n        '''\n        # Get logger\n        self.logger = logging.getLogger(__name__)\n\n        # Backup to local save if no uri (i.e. empty string)\n        if not tracking_uri:\n            tracking_uri = pathlib.Path(os.path.join(utils.get_data_path(), 'experiments', 'mlruns')).as_uri()\n        # Add \"file\" scheme if no scheme in the tracking_uri\n        elif not urlparse(tracking_uri).scheme:\n            tracking_uri = pathlib.Path(tracking_uri).resolve().as_uri()\n\n        # If no artifact_uri and tracking_uri scheme is \"file\", we set a default artifact_uri in experiments folder\n        # Otherwise we suppose artifact_uri is configured by the system\n        if not artifact_uri and urlparse(tracking_uri).scheme == \"file\":\n            artifact_uri = pathlib.Path(os.path.join(utils.get_data_path(), 'experiments', 'mlruns_artifacts')).as_uri()\n\n        # Set tracking URI &amp; experiment name\n        self.tracking_uri = tracking_uri\n\n        # Get the experiment if it exists and check if there is a connection error by doing it\n        try:\n            experiment = mlflow.get_experiment_by_name(experiment_name)\n        except Exception as e:\n            self.logger.error(repr(e))\n            raise ConnectionError(f\"Can't reach MLflow at {self.tracking_uri}. Please check the URI.\")\n\n        # If the experiment exists, we recover experiment id and artifact_uri (which is link to the experiment)\n        if experiment:\n            experiment_id = experiment.experiment_id\n            artifact_uri = experiment.artifact_location\n        # Otherwise we create a new experiment with the provided artifact_uri\n        else:\n            experiment_id = mlflow.create_experiment(experiment_name, artifact_location=artifact_uri)\n            experiment = mlflow.get_experiment_by_name(experiment_name)\n            artifact_uri = experiment.artifact_location\n\n        mlflow.set_experiment(experiment_id=experiment_id)\n\n        self.__experiment_id = experiment_id\n        self.__experiment_name = experiment_name\n        self.__artifact_uri = artifact_uri\n\n        self.logger.info(f'MLflow running. Metrics available @ {self.tracking_uri}. Experiment artifacts availaible @ {self.artifact_uri}')\n\n    @property\n    def tracking_uri(self) -&gt; str:\n        '''Current tracking uri'''\n        return mlflow.get_tracking_uri()\n\n    @tracking_uri.setter\n    def tracking_uri(self, uri:str) -&gt; None:\n        '''Set tracking uri'''\n        mlflow.set_tracking_uri(uri)\n\n    @property\n    def experiment_id(self) -&gt; str:\n        '''Experiment id. It can not be changed.'''\n        return self.__experiment_id\n\n    @property\n    def experiment_name(self) -&gt; str:\n        '''Experiment name. It can not be changed.'''\n        return self.__experiment_name\n\n    @property\n    def artifact_uri(self) -&gt; str:\n        '''Experiment artifact URI. It can not be changed.'''\n        return self.__artifact_uri\n\n    def end_run(self) -&gt; None:\n        '''Stops an MLflow run'''\n        try:\n            mlflow.end_run()\n        except Exception:\n            self.logger.error(\"Can't stop mlflow run\")\n\n    def log_metric(self, key: str, value, step: Union[int, None] = None) -&gt; None:\n        '''Logs a metric on mlflow\n\n        Args:\n            key (str): Name of the metric\n            value (float, ?): Value of the metric\n        Kwargs:\n            step (int): Step of the metric\n        '''\n        # Check for None\n        if value is None:\n            value = math.nan\n        # Log metric\n        mlflow.log_metric(key, value, step)\n\n    def log_metrics(self, metrics: dict, step: Union[int, None] = None) -&gt; None:\n        '''Logs a set of metrics in mlflow\n\n        Args:\n            metrics (dict): Metrics to log\n        Kwargs:\n            step (int): Step of the metric\n        '''\n        # Check for Nones\n        for k, v in metrics.items():\n            if v is None:\n                metrics[k] = math.nan\n        # Log metrics\n        mlflow.log_metrics(metrics, step)\n\n    def log_param(self, key: str, value) -&gt; None:\n        '''Logs a parameter in mlflow\n\n        Args:\n            key (str): Name of the parameter\n            value (str, ?): Value of the parameter (which will be cast to str if not already of type str)\n        '''\n        if value is None:\n            value = 'None'\n        # Log parameter\n        mlflow.log_param(key, value)\n\n    def log_params(self, params: dict) -&gt; None:\n        '''Logs a set of parameters in mlflow\n\n        Args:\n            params (dict): Name and value of each parameter\n        '''\n        # Check for Nones\n        for k, v in params.items():\n            if v is None:\n                params[k] = 'None'\n        # Log parameters\n        mlflow.log_params(params)\n\n    def set_tag(self, key: str, value) -&gt; None:\n        '''Logs a tag in mlflow\n\n        Args:\n            key (str): Name of the tag\n            value (str, ?): Value of the tag (which will be cast to str if not already of type str)\n        Raises:\n            ValueError: If the object value is None\n        '''\n        if value is None:\n            raise ValueError('value must not be None')\n        # Log tag\n        mlflow.set_tag(key, value)\n\n    def set_tags(self, tags: dict) -&gt; None:\n        '''Logs a set of tags in mlflow\n\n        Args:\n            tags (dict): Name and value of each tag\n        '''\n        # Log tags\n        mlflow.set_tags(tags)\n\n    def valid_name(self, key: str) -&gt; bool:\n        '''Validates key names\n\n        Args:\n            key (str): Key to check\n        Returns:\n            bool: If key is a valid mlflow key\n        '''\n        if mlflow.utils.validation._VALID_PARAM_AND_METRIC_NAMES.match(key):\n            return True\n        else:\n            return False\n\n    def log_df_stats(self, df_stats: pd.DataFrame, label_col: str = 'Label') -&gt; None:\n        '''Log a dataframe containing metrics from a training\n\n        Args:\n            df_stats (pd.Dataframe): Dataframe containing metrics from a training\n        Kwargs:\n            label_col (str): default labelc column name\n        '''\n        if label_col not in df_stats.columns:\n            raise ValueError(f\"The provided label column name ({label_col}) not found in df_stats' columns.\")\n\n        # Get metrics columns\n        metrics_columns = [col for col in df_stats.columns if col != label_col]\n\n        # Log labels\n        labels = df_stats[label_col].values\n        for i, label in enumerate(labels):  # type: ignore\n            self.log_param(f'Label {i}', label)\n\n        # Log metrics\n        ml_flow_metrics = {}\n        for i, row in df_stats.iterrows():\n            for j, col in enumerate(metrics_columns):\n                metric_key = f\"{row[label_col]} --- {col}\"\n                # Check that mlflow accepts the key, otherwise, replace it\n                # TODO: could be improved ...\n                if not self.valid_name(metric_key):\n                    metric_key = f\"Label {i} --- {col}\"\n                if not self.valid_name(metric_key):\n                    metric_key = f\"{row[label_col]} --- Col {j}\"\n                if not self.valid_name(metric_key):\n                    metric_key = f\"Label {i} --- Col {j}\"\n                ml_flow_metrics[metric_key] = row[col]\n\n        # Log metrics\n        self.log_metrics(ml_flow_metrics)\n\n    def log_dict(self, dictionary: dict, artifact_file: str) -&gt; None:\n        '''Logs a dictionary as an artifact in MLflow\n\n        Args:\n            dictionary (dict): A dictionary\n            artifact_file (str): The run-relative artifact file path in posixpath format to which the dictionary is saved\n        '''\n        mlflow.log_dict(dictionary=dictionary, artifact_file=artifact_file)\n\n    def log_text(self, text: str, artifact_file: str) -&gt; None:\n        '''Logs a text as an artifact in MLflow\n\n        Args:\n            text (str): A text\n            artifact_file (str): The run-relative artifact file path in posixpath format to which the dictionary is saved\n        '''\n        mlflow.log_text(text=text, artifact_file=artifact_file)\n\n    def log_figure(self, figure: Figure, artifact_file: str) -&gt; None:\n        '''Logs a text as an artifact in MLflow\n\n        Args:\n            figure (matplotlib.figure.Figure): A matplotlib figure\n            artifact_file (str): The run-relative artifact file path in posixpath format to which the figure is saved\n        '''\n        mlflow.log_figure(figure=figure, artifact_file=artifact_file)\n</code></pre>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.artifact_uri","title":"<code>artifact_uri: str</code>  <code>property</code>","text":"<p>Experiment artifact URI. It can not be changed.</p>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.experiment_id","title":"<code>experiment_id: str</code>  <code>property</code>","text":"<p>Experiment id. It can not be changed.</p>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.experiment_name","title":"<code>experiment_name: str</code>  <code>property</code>","text":"<p>Experiment name. It can not be changed.</p>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.tracking_uri","title":"<code>tracking_uri: str</code>  <code>property</code> <code>writable</code>","text":"<p>Current tracking uri</p>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.__init__","title":"<code>__init__(experiment_name, tracking_uri='', artifact_uri='')</code>","text":"<p>Class initialization Args:     experiment_name (str):  Name of the experiment to activate Kwargs:     tracking_uri (str): URI of the tracking server     artifact_uri (str): URI where to store artifacts</p> Source code in <code>template_vision/monitoring/mlflow_logger.py</code> <pre><code>def __init__(self, experiment_name: str, tracking_uri: str = '', artifact_uri: str = '') -&gt; None:\n    '''Class initialization\n    Args:\n        experiment_name (str):  Name of the experiment to activate\n    Kwargs:\n        tracking_uri (str): URI of the tracking server\n        artifact_uri (str): URI where to store artifacts\n    '''\n    # Get logger\n    self.logger = logging.getLogger(__name__)\n\n    # Backup to local save if no uri (i.e. empty string)\n    if not tracking_uri:\n        tracking_uri = pathlib.Path(os.path.join(utils.get_data_path(), 'experiments', 'mlruns')).as_uri()\n    # Add \"file\" scheme if no scheme in the tracking_uri\n    elif not urlparse(tracking_uri).scheme:\n        tracking_uri = pathlib.Path(tracking_uri).resolve().as_uri()\n\n    # If no artifact_uri and tracking_uri scheme is \"file\", we set a default artifact_uri in experiments folder\n    # Otherwise we suppose artifact_uri is configured by the system\n    if not artifact_uri and urlparse(tracking_uri).scheme == \"file\":\n        artifact_uri = pathlib.Path(os.path.join(utils.get_data_path(), 'experiments', 'mlruns_artifacts')).as_uri()\n\n    # Set tracking URI &amp; experiment name\n    self.tracking_uri = tracking_uri\n\n    # Get the experiment if it exists and check if there is a connection error by doing it\n    try:\n        experiment = mlflow.get_experiment_by_name(experiment_name)\n    except Exception as e:\n        self.logger.error(repr(e))\n        raise ConnectionError(f\"Can't reach MLflow at {self.tracking_uri}. Please check the URI.\")\n\n    # If the experiment exists, we recover experiment id and artifact_uri (which is link to the experiment)\n    if experiment:\n        experiment_id = experiment.experiment_id\n        artifact_uri = experiment.artifact_location\n    # Otherwise we create a new experiment with the provided artifact_uri\n    else:\n        experiment_id = mlflow.create_experiment(experiment_name, artifact_location=artifact_uri)\n        experiment = mlflow.get_experiment_by_name(experiment_name)\n        artifact_uri = experiment.artifact_location\n\n    mlflow.set_experiment(experiment_id=experiment_id)\n\n    self.__experiment_id = experiment_id\n    self.__experiment_name = experiment_name\n    self.__artifact_uri = artifact_uri\n\n    self.logger.info(f'MLflow running. Metrics available @ {self.tracking_uri}. Experiment artifacts availaible @ {self.artifact_uri}')\n</code></pre>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.end_run","title":"<code>end_run()</code>","text":"<p>Stops an MLflow run</p> Source code in <code>template_vision/monitoring/mlflow_logger.py</code> <pre><code>def end_run(self) -&gt; None:\n    '''Stops an MLflow run'''\n    try:\n        mlflow.end_run()\n    except Exception:\n        self.logger.error(\"Can't stop mlflow run\")\n</code></pre>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.log_df_stats","title":"<code>log_df_stats(df_stats, label_col='Label')</code>","text":"<p>Log a dataframe containing metrics from a training</p> <p>Parameters:</p> Name Type Description Default <code>df_stats</code> <code>Dataframe</code> <p>Dataframe containing metrics from a training</p> required <p>Kwargs:     label_col (str): default labelc column name</p> Source code in <code>template_vision/monitoring/mlflow_logger.py</code> <pre><code>def log_df_stats(self, df_stats: pd.DataFrame, label_col: str = 'Label') -&gt; None:\n    '''Log a dataframe containing metrics from a training\n\n    Args:\n        df_stats (pd.Dataframe): Dataframe containing metrics from a training\n    Kwargs:\n        label_col (str): default labelc column name\n    '''\n    if label_col not in df_stats.columns:\n        raise ValueError(f\"The provided label column name ({label_col}) not found in df_stats' columns.\")\n\n    # Get metrics columns\n    metrics_columns = [col for col in df_stats.columns if col != label_col]\n\n    # Log labels\n    labels = df_stats[label_col].values\n    for i, label in enumerate(labels):  # type: ignore\n        self.log_param(f'Label {i}', label)\n\n    # Log metrics\n    ml_flow_metrics = {}\n    for i, row in df_stats.iterrows():\n        for j, col in enumerate(metrics_columns):\n            metric_key = f\"{row[label_col]} --- {col}\"\n            # Check that mlflow accepts the key, otherwise, replace it\n            # TODO: could be improved ...\n            if not self.valid_name(metric_key):\n                metric_key = f\"Label {i} --- {col}\"\n            if not self.valid_name(metric_key):\n                metric_key = f\"{row[label_col]} --- Col {j}\"\n            if not self.valid_name(metric_key):\n                metric_key = f\"Label {i} --- Col {j}\"\n            ml_flow_metrics[metric_key] = row[col]\n\n    # Log metrics\n    self.log_metrics(ml_flow_metrics)\n</code></pre>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.log_dict","title":"<code>log_dict(dictionary, artifact_file)</code>","text":"<p>Logs a dictionary as an artifact in MLflow</p> <p>Parameters:</p> Name Type Description Default <code>dictionary</code> <code>dict</code> <p>A dictionary</p> required <code>artifact_file</code> <code>str</code> <p>The run-relative artifact file path in posixpath format to which the dictionary is saved</p> required Source code in <code>template_vision/monitoring/mlflow_logger.py</code> <pre><code>def log_dict(self, dictionary: dict, artifact_file: str) -&gt; None:\n    '''Logs a dictionary as an artifact in MLflow\n\n    Args:\n        dictionary (dict): A dictionary\n        artifact_file (str): The run-relative artifact file path in posixpath format to which the dictionary is saved\n    '''\n    mlflow.log_dict(dictionary=dictionary, artifact_file=artifact_file)\n</code></pre>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.log_figure","title":"<code>log_figure(figure, artifact_file)</code>","text":"<p>Logs a text as an artifact in MLflow</p> <p>Parameters:</p> Name Type Description Default <code>figure</code> <code>Figure</code> <p>A matplotlib figure</p> required <code>artifact_file</code> <code>str</code> <p>The run-relative artifact file path in posixpath format to which the figure is saved</p> required Source code in <code>template_vision/monitoring/mlflow_logger.py</code> <pre><code>def log_figure(self, figure: Figure, artifact_file: str) -&gt; None:\n    '''Logs a text as an artifact in MLflow\n\n    Args:\n        figure (matplotlib.figure.Figure): A matplotlib figure\n        artifact_file (str): The run-relative artifact file path in posixpath format to which the figure is saved\n    '''\n    mlflow.log_figure(figure=figure, artifact_file=artifact_file)\n</code></pre>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.log_metric","title":"<code>log_metric(key, value, step=None)</code>","text":"<p>Logs a metric on mlflow</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of the metric</p> required <code>value</code> <code>float, ?</code> <p>Value of the metric</p> required <p>Kwargs:     step (int): Step of the metric</p> Source code in <code>template_vision/monitoring/mlflow_logger.py</code> <pre><code>def log_metric(self, key: str, value, step: Union[int, None] = None) -&gt; None:\n    '''Logs a metric on mlflow\n\n    Args:\n        key (str): Name of the metric\n        value (float, ?): Value of the metric\n    Kwargs:\n        step (int): Step of the metric\n    '''\n    # Check for None\n    if value is None:\n        value = math.nan\n    # Log metric\n    mlflow.log_metric(key, value, step)\n</code></pre>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.log_metrics","title":"<code>log_metrics(metrics, step=None)</code>","text":"<p>Logs a set of metrics in mlflow</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>dict</code> <p>Metrics to log</p> required <p>Kwargs:     step (int): Step of the metric</p> Source code in <code>template_vision/monitoring/mlflow_logger.py</code> <pre><code>def log_metrics(self, metrics: dict, step: Union[int, None] = None) -&gt; None:\n    '''Logs a set of metrics in mlflow\n\n    Args:\n        metrics (dict): Metrics to log\n    Kwargs:\n        step (int): Step of the metric\n    '''\n    # Check for Nones\n    for k, v in metrics.items():\n        if v is None:\n            metrics[k] = math.nan\n    # Log metrics\n    mlflow.log_metrics(metrics, step)\n</code></pre>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.log_param","title":"<code>log_param(key, value)</code>","text":"<p>Logs a parameter in mlflow</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of the parameter</p> required <code>value</code> <code>str, ?</code> <p>Value of the parameter (which will be cast to str if not already of type str)</p> required Source code in <code>template_vision/monitoring/mlflow_logger.py</code> <pre><code>def log_param(self, key: str, value) -&gt; None:\n    '''Logs a parameter in mlflow\n\n    Args:\n        key (str): Name of the parameter\n        value (str, ?): Value of the parameter (which will be cast to str if not already of type str)\n    '''\n    if value is None:\n        value = 'None'\n    # Log parameter\n    mlflow.log_param(key, value)\n</code></pre>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.log_params","title":"<code>log_params(params)</code>","text":"<p>Logs a set of parameters in mlflow</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Name and value of each parameter</p> required Source code in <code>template_vision/monitoring/mlflow_logger.py</code> <pre><code>def log_params(self, params: dict) -&gt; None:\n    '''Logs a set of parameters in mlflow\n\n    Args:\n        params (dict): Name and value of each parameter\n    '''\n    # Check for Nones\n    for k, v in params.items():\n        if v is None:\n            params[k] = 'None'\n    # Log parameters\n    mlflow.log_params(params)\n</code></pre>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.log_text","title":"<code>log_text(text, artifact_file)</code>","text":"<p>Logs a text as an artifact in MLflow</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>A text</p> required <code>artifact_file</code> <code>str</code> <p>The run-relative artifact file path in posixpath format to which the dictionary is saved</p> required Source code in <code>template_vision/monitoring/mlflow_logger.py</code> <pre><code>def log_text(self, text: str, artifact_file: str) -&gt; None:\n    '''Logs a text as an artifact in MLflow\n\n    Args:\n        text (str): A text\n        artifact_file (str): The run-relative artifact file path in posixpath format to which the dictionary is saved\n    '''\n    mlflow.log_text(text=text, artifact_file=artifact_file)\n</code></pre>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.set_tag","title":"<code>set_tag(key, value)</code>","text":"<p>Logs a tag in mlflow</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of the tag</p> required <code>value</code> <code>str, ?</code> <p>Value of the tag (which will be cast to str if not already of type str)</p> required <p>Raises:     ValueError: If the object value is None</p> Source code in <code>template_vision/monitoring/mlflow_logger.py</code> <pre><code>def set_tag(self, key: str, value) -&gt; None:\n    '''Logs a tag in mlflow\n\n    Args:\n        key (str): Name of the tag\n        value (str, ?): Value of the tag (which will be cast to str if not already of type str)\n    Raises:\n        ValueError: If the object value is None\n    '''\n    if value is None:\n        raise ValueError('value must not be None')\n    # Log tag\n    mlflow.set_tag(key, value)\n</code></pre>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.set_tags","title":"<code>set_tags(tags)</code>","text":"<p>Logs a set of tags in mlflow</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>dict</code> <p>Name and value of each tag</p> required Source code in <code>template_vision/monitoring/mlflow_logger.py</code> <pre><code>def set_tags(self, tags: dict) -&gt; None:\n    '''Logs a set of tags in mlflow\n\n    Args:\n        tags (dict): Name and value of each tag\n    '''\n    # Log tags\n    mlflow.set_tags(tags)\n</code></pre>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.valid_name","title":"<code>valid_name(key)</code>","text":"<p>Validates key names</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Key to check</p> required <p>Returns:     bool: If key is a valid mlflow key</p> Source code in <code>template_vision/monitoring/mlflow_logger.py</code> <pre><code>def valid_name(self, key: str) -&gt; bool:\n    '''Validates key names\n\n    Args:\n        key (str): Key to check\n    Returns:\n        bool: If key is a valid mlflow key\n    '''\n    if mlflow.utils.validation._VALID_PARAM_AND_METRIC_NAMES.match(key):\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"reference/template_vision/monitoring/model_explainer/","title":"Model explainer","text":""},{"location":"reference/template_vision/monitoring/model_explainer/#template_vision.monitoring.model_explainer.Explainer","title":"<code>Explainer</code>","text":"<p>Parent class for the explainers</p> Source code in <code>template_vision/monitoring/model_explainer.py</code> <pre><code>class Explainer:\n    '''Parent class for the explainers'''\n\n    def __init__(self, *args, **kwargs) -&gt; None:\n        '''Initialization of the parent class'''\n        self.logger = logging.getLogger(__name__)\n\n    def explain_instance(self, content: Image.Image, **kwargs) -&gt; Any:\n        '''Explains a prediction\n\n        Args:\n            content (Image.Image): Image to be explained\n        Returns:\n            (?): An explanation object\n        '''\n        raise NotImplementedError(\"'explain_instance' needs to be overridden\")\n\n    def explain_instance_as_html(self, content: Image.Image, **kwargs) -&gt; str:\n        '''Explains a prediction - returns an HTML object\n\n        Args:\n            content (Image.Image): Image to be explained\n        Returns:\n            str: An HTML code with the explanation\n        '''\n        raise NotImplementedError(\"'explain_instance_as_html' needs to be overridden\")\n\n    def explain_instance_as_json(self, content: Image.Image, **kwargs) -&gt; Union[dict, list]:\n        '''Explains a prediction - returns an JSON serializable object\n\n        Args:\n            content (str): Text to be explained\n        Returns:\n            str: A JSON serializable object with the explanation\n        '''\n        raise NotImplementedError(\"'explain_instance_as_json' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_vision/monitoring/model_explainer/#template_vision.monitoring.model_explainer.Explainer.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialization of the parent class</p> Source code in <code>template_vision/monitoring/model_explainer.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n    '''Initialization of the parent class'''\n    self.logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/template_vision/monitoring/model_explainer/#template_vision.monitoring.model_explainer.Explainer.explain_instance","title":"<code>explain_instance(content, **kwargs)</code>","text":"<p>Explains a prediction</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>Image</code> <p>Image to be explained</p> required <p>Returns:     (?): An explanation object</p> Source code in <code>template_vision/monitoring/model_explainer.py</code> <pre><code>def explain_instance(self, content: Image.Image, **kwargs) -&gt; Any:\n    '''Explains a prediction\n\n    Args:\n        content (Image.Image): Image to be explained\n    Returns:\n        (?): An explanation object\n    '''\n    raise NotImplementedError(\"'explain_instance' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_vision/monitoring/model_explainer/#template_vision.monitoring.model_explainer.Explainer.explain_instance_as_html","title":"<code>explain_instance_as_html(content, **kwargs)</code>","text":"<p>Explains a prediction - returns an HTML object</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>Image</code> <p>Image to be explained</p> required <p>Returns:     str: An HTML code with the explanation</p> Source code in <code>template_vision/monitoring/model_explainer.py</code> <pre><code>def explain_instance_as_html(self, content: Image.Image, **kwargs) -&gt; str:\n    '''Explains a prediction - returns an HTML object\n\n    Args:\n        content (Image.Image): Image to be explained\n    Returns:\n        str: An HTML code with the explanation\n    '''\n    raise NotImplementedError(\"'explain_instance_as_html' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_vision/monitoring/model_explainer/#template_vision.monitoring.model_explainer.Explainer.explain_instance_as_json","title":"<code>explain_instance_as_json(content, **kwargs)</code>","text":"<p>Explains a prediction - returns an JSON serializable object</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Text to be explained</p> required <p>Returns:     str: A JSON serializable object with the explanation</p> Source code in <code>template_vision/monitoring/model_explainer.py</code> <pre><code>def explain_instance_as_json(self, content: Image.Image, **kwargs) -&gt; Union[dict, list]:\n    '''Explains a prediction - returns an JSON serializable object\n\n    Args:\n        content (str): Text to be explained\n    Returns:\n        str: A JSON serializable object with the explanation\n    '''\n    raise NotImplementedError(\"'explain_instance_as_json' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_vision/monitoring/model_explainer/#template_vision.monitoring.model_explainer.LimeExplainer","title":"<code>LimeExplainer</code>","text":"<p>             Bases: <code>Explainer</code></p> <p>Lime Explainer wrapper class</p> Source code in <code>template_vision/monitoring/model_explainer.py</code> <pre><code>class LimeExplainer(Explainer):\n    '''Lime Explainer wrapper class'''\n\n    def __init__(self, model: Type[ModelClassifierMixin], model_conf: dict) -&gt; None:\n        ''' Initialization\n\n        Args:\n            model: A model instance with predict &amp; predict_proba functions, and list_classes attribute\n            model_conf (dict): The model's configuration\n        Raises:\n            ValueError: If the provided model is not a classifier\n            TypeError: If the provided model does not implement a `predict_proba` function\n            TypeError: If the provided model does not have a `list_classes` attribute\n        '''\n        super().__init__()\n        pred_proba_op = getattr(model, \"predict_proba\", None)\n\n        # Check classifier\n        if not model.model_type == 'classifier':\n            raise ValueError(\"LimeExplainer only supported with classifier models\")\n        # Check needed methods\n        if pred_proba_op is None or not callable(pred_proba_op):\n            raise TypeError(\"The supplied model must implement a predict_proba() function\")\n        if getattr(model, \"list_classes\", None) is None:\n            raise TypeError(\"The supplied model must have a list_classes attribute\")\n\n        self.model = model\n        self.model_conf = model_conf\n        self.class_names = self.model.list_classes\n        # Our explainers will explain a prediction for a given class / label\n        # These atributes are set on the fly\n        self.current_class_index = 0\n        # Create the explainer\n        self.explainer = LimeImageExplainer()\n\n    def classifier_fn(self, content_arrays: np.ndarray) -&gt; np.ndarray:\n        '''Function to get probabilities from a list of (not preprocessed) images\n\n        Args:\n            content_arrays (np.ndarray): images to be considered\n        Returns:\n            np.array: probabilities\n        '''\n        # Get preprocessor\n        if 'preprocess_str' in self.model_conf.keys():\n            preprocess_str = self.model_conf['preprocess_str']\n        else:\n            preprocess_str = \"no_preprocess\"\n        preprocessor = preprocess.get_preprocessor(preprocess_str)\n        # Preprocess images\n        images = [Image.fromarray(img, 'RGB') for img in content_arrays]\n        images_preprocessed = preprocessor(images)\n        # Temporary folder\n        with tempfile.TemporaryDirectory(dir=utils.get_data_path()) as tmp_folder:\n            # Save images\n            images_path = [os.path.join(tmp_folder, f'image_{i}.png') for i in range(len(images_preprocessed))]\n            for i, img_preprocessed in enumerate(images_preprocessed):\n                img_preprocessed.save(images_path[i], format='PNG')\n            # Get predictions\n            df = pd.DataFrame({'file_path': images_path})\n            probas = self.model.predict_proba(df)\n        # Return probas\n        return probas\n\n    def explain_instance(self, content: Image.Image, class_index: Union[int, None] = None,\n                         num_samples: int = 100, batch_size: int = 100, hide_color=0, **kwargs):\n        '''Explains a prediction\n\n        This function calls the Lime module. It generates neighborhood data by randomly perturbing features from the instance.\n        Then, it learns locally weighted linear models on this neighborhood data to explain each of the classes in an interpretable way.\n\n        Args:\n            img (Image.Image): Image to be explained\n        Kwargs:\n            class_index (int): for classification only. Class or label index to be considered.\n            num_samples (int): size of the neighborhood to learn the linear model (cf. Lime documentation)\n            batch_size (int): classifier_fn will be called on batches of this size (cf. Lime documentation)\n            hide_color (?): TODO\n        Returns:\n            (?): An explanation object\n        '''\n        # Set index\n        if class_index is not None:\n            self.current_class_index = class_index\n        else:\n            self.current_class_index = 1  # Def to 1\n        # Get explanations (images must be convert into rgb, then into np array)\n        return self.explainer.explain_instance(np.array(content.convert('RGB')), self.classifier_fn,\n                                               labels=(self.current_class_index,),\n                                               num_samples=num_samples, batch_size=batch_size,\n                                               hide_color=hide_color, top_labels=None)\n\n    def explain_instance_as_html(self, content: Image.Image, **kwargs) -&gt; str:\n        '''Explains a prediction - returns an HTML object\n        ** NOT IMPLEMENTED **\n        '''\n        raise NotImplementedError(\"'explain_instance_as_html' is not defined for LimeExplainer\")\n\n    def explain_instance_as_json(self, content: Image.Image, **kwargs) -&gt; Union[dict, list]:\n        '''Explains a prediction - returns an JSON serializable object\n        ** NOT IMPLEMENTED **\n        '''\n        raise NotImplementedError(\"'explain_instance_as_json' is not defined for LimeExplainer\")\n</code></pre>"},{"location":"reference/template_vision/monitoring/model_explainer/#template_vision.monitoring.model_explainer.LimeExplainer.__init__","title":"<code>__init__(model, model_conf)</code>","text":"<p>Initialization</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Type[ModelClassifierMixin]</code> <p>A model instance with predict &amp; predict_proba functions, and list_classes attribute</p> required <code>model_conf</code> <code>dict</code> <p>The model's configuration</p> required <p>Raises:     ValueError: If the provided model is not a classifier     TypeError: If the provided model does not implement a <code>predict_proba</code> function     TypeError: If the provided model does not have a <code>list_classes</code> attribute</p> Source code in <code>template_vision/monitoring/model_explainer.py</code> <pre><code>def __init__(self, model: Type[ModelClassifierMixin], model_conf: dict) -&gt; None:\n    ''' Initialization\n\n    Args:\n        model: A model instance with predict &amp; predict_proba functions, and list_classes attribute\n        model_conf (dict): The model's configuration\n    Raises:\n        ValueError: If the provided model is not a classifier\n        TypeError: If the provided model does not implement a `predict_proba` function\n        TypeError: If the provided model does not have a `list_classes` attribute\n    '''\n    super().__init__()\n    pred_proba_op = getattr(model, \"predict_proba\", None)\n\n    # Check classifier\n    if not model.model_type == 'classifier':\n        raise ValueError(\"LimeExplainer only supported with classifier models\")\n    # Check needed methods\n    if pred_proba_op is None or not callable(pred_proba_op):\n        raise TypeError(\"The supplied model must implement a predict_proba() function\")\n    if getattr(model, \"list_classes\", None) is None:\n        raise TypeError(\"The supplied model must have a list_classes attribute\")\n\n    self.model = model\n    self.model_conf = model_conf\n    self.class_names = self.model.list_classes\n    # Our explainers will explain a prediction for a given class / label\n    # These atributes are set on the fly\n    self.current_class_index = 0\n    # Create the explainer\n    self.explainer = LimeImageExplainer()\n</code></pre>"},{"location":"reference/template_vision/monitoring/model_explainer/#template_vision.monitoring.model_explainer.LimeExplainer.classifier_fn","title":"<code>classifier_fn(content_arrays)</code>","text":"<p>Function to get probabilities from a list of (not preprocessed) images</p> <p>Parameters:</p> Name Type Description Default <code>content_arrays</code> <code>ndarray</code> <p>images to be considered</p> required <p>Returns:     np.array: probabilities</p> Source code in <code>template_vision/monitoring/model_explainer.py</code> <pre><code>def classifier_fn(self, content_arrays: np.ndarray) -&gt; np.ndarray:\n    '''Function to get probabilities from a list of (not preprocessed) images\n\n    Args:\n        content_arrays (np.ndarray): images to be considered\n    Returns:\n        np.array: probabilities\n    '''\n    # Get preprocessor\n    if 'preprocess_str' in self.model_conf.keys():\n        preprocess_str = self.model_conf['preprocess_str']\n    else:\n        preprocess_str = \"no_preprocess\"\n    preprocessor = preprocess.get_preprocessor(preprocess_str)\n    # Preprocess images\n    images = [Image.fromarray(img, 'RGB') for img in content_arrays]\n    images_preprocessed = preprocessor(images)\n    # Temporary folder\n    with tempfile.TemporaryDirectory(dir=utils.get_data_path()) as tmp_folder:\n        # Save images\n        images_path = [os.path.join(tmp_folder, f'image_{i}.png') for i in range(len(images_preprocessed))]\n        for i, img_preprocessed in enumerate(images_preprocessed):\n            img_preprocessed.save(images_path[i], format='PNG')\n        # Get predictions\n        df = pd.DataFrame({'file_path': images_path})\n        probas = self.model.predict_proba(df)\n    # Return probas\n    return probas\n</code></pre>"},{"location":"reference/template_vision/monitoring/model_explainer/#template_vision.monitoring.model_explainer.LimeExplainer.explain_instance","title":"<code>explain_instance(content, class_index=None, num_samples=100, batch_size=100, hide_color=0, **kwargs)</code>","text":"<p>Explains a prediction</p> <p>This function calls the Lime module. It generates neighborhood data by randomly perturbing features from the instance. Then, it learns locally weighted linear models on this neighborhood data to explain each of the classes in an interpretable way.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>Image</code> <p>Image to be explained</p> required <p>Kwargs:     class_index (int): for classification only. Class or label index to be considered.     num_samples (int): size of the neighborhood to learn the linear model (cf. Lime documentation)     batch_size (int): classifier_fn will be called on batches of this size (cf. Lime documentation)     hide_color (?): TODO Returns:     (?): An explanation object</p> Source code in <code>template_vision/monitoring/model_explainer.py</code> <pre><code>def explain_instance(self, content: Image.Image, class_index: Union[int, None] = None,\n                     num_samples: int = 100, batch_size: int = 100, hide_color=0, **kwargs):\n    '''Explains a prediction\n\n    This function calls the Lime module. It generates neighborhood data by randomly perturbing features from the instance.\n    Then, it learns locally weighted linear models on this neighborhood data to explain each of the classes in an interpretable way.\n\n    Args:\n        img (Image.Image): Image to be explained\n    Kwargs:\n        class_index (int): for classification only. Class or label index to be considered.\n        num_samples (int): size of the neighborhood to learn the linear model (cf. Lime documentation)\n        batch_size (int): classifier_fn will be called on batches of this size (cf. Lime documentation)\n        hide_color (?): TODO\n    Returns:\n        (?): An explanation object\n    '''\n    # Set index\n    if class_index is not None:\n        self.current_class_index = class_index\n    else:\n        self.current_class_index = 1  # Def to 1\n    # Get explanations (images must be convert into rgb, then into np array)\n    return self.explainer.explain_instance(np.array(content.convert('RGB')), self.classifier_fn,\n                                           labels=(self.current_class_index,),\n                                           num_samples=num_samples, batch_size=batch_size,\n                                           hide_color=hide_color, top_labels=None)\n</code></pre>"},{"location":"reference/template_vision/monitoring/model_explainer/#template_vision.monitoring.model_explainer.LimeExplainer.explain_instance_as_html","title":"<code>explain_instance_as_html(content, **kwargs)</code>","text":"<p>Explains a prediction - returns an HTML object ** NOT IMPLEMENTED **</p> Source code in <code>template_vision/monitoring/model_explainer.py</code> <pre><code>def explain_instance_as_html(self, content: Image.Image, **kwargs) -&gt; str:\n    '''Explains a prediction - returns an HTML object\n    ** NOT IMPLEMENTED **\n    '''\n    raise NotImplementedError(\"'explain_instance_as_html' is not defined for LimeExplainer\")\n</code></pre>"},{"location":"reference/template_vision/monitoring/model_explainer/#template_vision.monitoring.model_explainer.LimeExplainer.explain_instance_as_json","title":"<code>explain_instance_as_json(content, **kwargs)</code>","text":"<p>Explains a prediction - returns an JSON serializable object ** NOT IMPLEMENTED **</p> Source code in <code>template_vision/monitoring/model_explainer.py</code> <pre><code>def explain_instance_as_json(self, content: Image.Image, **kwargs) -&gt; Union[dict, list]:\n    '''Explains a prediction - returns an JSON serializable object\n    ** NOT IMPLEMENTED **\n    '''\n    raise NotImplementedError(\"'explain_instance_as_json' is not defined for LimeExplainer\")\n</code></pre>"},{"location":"reference/template_vision/preprocessing/","title":"Preprocessing","text":""},{"location":"reference/template_vision/preprocessing/manage_white_borders/","title":"Manage white borders","text":""},{"location":"reference/template_vision/preprocessing/manage_white_borders/#template_vision.preprocessing.manage_white_borders.fill_with_white","title":"<code>fill_with_white(im, image_ratio)</code>","text":"<p>Fills an image with white such that it respects a wanted ratio</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>Image</code> <p>Image to be processed</p> required <p>Kwargs:     image_ratio (float): Wanted image ratio Returns:     Image: Transformed image</p> Source code in <code>template_vision/preprocessing/manage_white_borders.py</code> <pre><code>def fill_with_white(im: Image, image_ratio: float) -&gt; Image:\n    '''Fills an image with white such that it respects a wanted ratio\n\n    Args:\n        im (Image): Image to be processed\n    Kwargs:\n        image_ratio (float): Wanted image ratio\n    Returns:\n        Image: Transformed image\n    '''\n    width, height = im.size\n    ratio = width / height\n    if ratio &gt; image_ratio:\n        # Increase height\n        wanted_height = round(width / image_ratio)\n        new_size = (width, wanted_height)\n        old_size = (width, height)\n        # Set new image\n        new_im = Image.new(\"RGB\", new_size, (255, 255, 255))\n        # Fill it with the old image, centered\n        # (use floor to ensure the old image fits into the new one)\n        x_pos = 0\n        y_pos = floor((new_size[1] - old_size[1]) / 2)\n        new_im.paste(im, (x_pos, y_pos))\n    elif ratio &lt; image_ratio:\n        # Increase width\n        wanted_width = round(height * image_ratio)\n        new_size = (wanted_width, height)\n        old_size = (width, height)\n        # Set new image\n        new_im = Image.new(\"RGB\", new_size, (255, 255, 255))\n        # Fill it with the old image, centered\n        # (use floor to ensure the old image fits into the new one)\n        x_pos = floor((new_size[0] - old_size[0]) / 2)\n        y_pos = 0\n        new_im.paste(im, (x_pos, y_pos))\n    else:  # Already correct ratio\n        new_im = im\n    return new_im\n</code></pre>"},{"location":"reference/template_vision/preprocessing/manage_white_borders/#template_vision.preprocessing.manage_white_borders.remove_white_borders","title":"<code>remove_white_borders(images, image_ratio_strategy=None, image_ratio=0.75, with_rotation=True)</code>","text":"<p>Removes white border Also change the image ratio and rotate (if wanted) along largest dim. (i.e. portrait mode)</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list</code> <p>Images to be processed</p> required <p>Kwargs:     image_ratio_strategy (str): wanted strategy to apply new image_ratio         - None: no change in image ratio         - 'fill': add white borders on the smallest dimension         - 'stretch': stretch the images such that they have the wanted ratio     image_ratio (float): Wanted final image ratio (unused if image_ratio_strategy is None)     with_rotation (bool): If the images must be rotated along largest dim. (i.e. portrait mode) Raises:     ValueError: If image_ratio_strategy value is not a valid option ([None, 'fill', 'stretch']) Returns:     list: Images sans les marges blanches</p> Source code in <code>template_vision/preprocessing/manage_white_borders.py</code> <pre><code>def remove_white_borders(images: list, image_ratio_strategy: Union[str, None] = None, image_ratio: float = 0.75, with_rotation: bool = True) -&gt; list:\n    '''Removes white border\n    Also change the image ratio and rotate (if wanted) along largest dim. (i.e. portrait mode)\n\n    Args:\n        images (list): Images to be processed\n    Kwargs:\n        image_ratio_strategy (str): wanted strategy to apply new image_ratio\n            - None: no change in image ratio\n            - 'fill': add white borders on the smallest dimension\n            - 'stretch': stretch the images such that they have the wanted ratio\n        image_ratio (float): Wanted final image ratio (unused if image_ratio_strategy is None)\n        with_rotation (bool): If the images must be rotated along largest dim. (i.e. portrait mode)\n    Raises:\n        ValueError: If image_ratio_strategy value is not a valid option ([None, 'fill', 'stretch'])\n    Returns:\n        list: Images sans les marges blanches\n    '''\n    if image_ratio_strategy not in [None, 'fill', 'stretch']:\n        raise ValueError(f\"image ratio strategy (image_ratio_strategy) '{image_ratio_strategy}' is not a valid option ([None, 'fill', 'stretch'])\")\n    # Get 'True' white\n    # TODO : to be improved !\n    true_white = _rgb2gray(np.array([255, 255, 255]))\n    # Process each image, one by one\n    results = []\n    for i, im in enumerate(tqdm.tqdm(images)):\n        # Remove white borders\n        pixels = _rgb2gray(np.array(im))\n        # x : horizontal\n        # y : vertical\n        first_x = _get_first_x(pixels, true_white)  # Left\n        first_y = _get_first_y(pixels, true_white)  # Upper\n        last_x = _get_last_x(pixels, true_white)  # Right\n        last_y = _get_last_y(pixels, true_white)  # Lower\n        # If first_x -1 -&gt; no 'non-white' pixel, do nothing\n        if first_x == -1:\n            continue\n        else:\n            # Crop image (left, upper, right, lower)\n            im = im.crop((first_x, first_y, last_x + 1, last_y + 1))\n            # Rotate image if wanted\n            if with_rotation:\n                im = rotate_image_largest_dim(im)\n            # Manage new ratio strategy\n            if image_ratio_strategy == 'fill':  # fill with white borders to get correct format\n                im = fill_with_white(im, image_ratio)\n            elif image_ratio_strategy == 'stretch':\n                im = stretch_image(im, image_ratio)\n            # If None, do nothign\n        # Update list\n        results.append(im)\n    # Return\n    return results\n</code></pre>"},{"location":"reference/template_vision/preprocessing/manage_white_borders/#template_vision.preprocessing.manage_white_borders.rotate_image_largest_dim","title":"<code>rotate_image_largest_dim(im)</code>","text":"<p>Rotates an image along largest dim. (i.e. portrait mode)</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>Image</code> <p>Image to be processed</p> required <p>Returns:     Image: Rotated image</p> Source code in <code>template_vision/preprocessing/manage_white_borders.py</code> <pre><code>def rotate_image_largest_dim(im: Image) -&gt; Image:\n    '''Rotates an image along largest dim. (i.e. portrait mode)\n\n    Args:\n        im (Image): Image to be processed\n    Returns:\n        Image: Rotated image\n    '''\n    orientation = _get_orientation(im)\n    if orientation != 0:\n        im = im.rotate(orientation, expand=True)\n    return im\n</code></pre>"},{"location":"reference/template_vision/preprocessing/manage_white_borders/#template_vision.preprocessing.manage_white_borders.stretch_image","title":"<code>stretch_image(im, image_ratio)</code>","text":"<p>Stretch an image such that it respects a wanted ratio</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>Image</code> <p>Image to be processed</p> required <p>Kwargs:     image_ratio (float): Wanted image ratio Returns:     Image: Transformed image</p> Source code in <code>template_vision/preprocessing/manage_white_borders.py</code> <pre><code>def stretch_image(im: Image, image_ratio: float) -&gt; Image:\n    '''Stretch an image such that it respects a wanted ratio\n\n    Args:\n        im (Image): Image to be processed\n    Kwargs:\n        image_ratio (float): Wanted image ratio\n    Returns:\n        Image: Transformed image\n    '''\n    width, height = im.size\n    ratio = width / height\n    if ratio &gt; image_ratio:\n        # Increase height\n        wanted_height = round(width / image_ratio)\n        new_size = (width, wanted_height)\n        new_im = im.resize(new_size)\n    elif ratio &lt; image_ratio:\n        # Increase width\n        wanted_width = round(height * image_ratio)\n        new_size = (wanted_width, height)\n        new_im = im.resize(new_size)\n    else:  # Already correct ratio\n        new_im = im\n    return new_im\n</code></pre>"},{"location":"reference/template_vision/preprocessing/preprocess/","title":"Preprocess","text":""},{"location":"reference/template_vision/preprocessing/preprocess/#template_vision.preprocessing.preprocess.apply_pipeline","title":"<code>apply_pipeline(images, pipeline)</code>","text":"<p>Applies a pipeline (i.e. list of transformations)</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list</code> <p>List of images to be transformed</p> required <code>pipeline</code> <code>list</code> <p>List of transformation to be applied</p> required <p>Returns:     list: Preprocessed images</p> Source code in <code>template_vision/preprocessing/preprocess.py</code> <pre><code>def apply_pipeline(images: list, pipeline: list) -&gt; list:\n    '''Applies a pipeline (i.e. list of transformations)\n\n    Args:\n        images (list): List of images to be transformed\n        pipeline (list): List of transformation to be applied\n    Returns:\n        list: Preprocessed images\n    '''\n    # Process\n    results = None\n    for transformer in pipeline:\n        if results is None:\n            results = transformer(images)\n        else:\n            results = transformer(results)\n    return results\n</code></pre>"},{"location":"reference/template_vision/preprocessing/preprocess/#template_vision.preprocessing.preprocess.convert_rgb","title":"<code>convert_rgb(images)</code>","text":"<p>Converts a list of image into RGB images</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list</code> <p>List of images to be converted</p> required <p>Returns:     list: RGB images</p> Source code in <code>template_vision/preprocessing/preprocess.py</code> <pre><code>def convert_rgb(images: list) -&gt; list:\n    '''Converts a list of image into RGB images\n\n    Args:\n        images (list): List of images to be converted\n    Returns:\n        list: RGB images\n    '''\n    return [im.convert('RGB') for im in images]\n</code></pre>"},{"location":"reference/template_vision/preprocessing/preprocess/#template_vision.preprocessing.preprocess.get_preprocessor","title":"<code>get_preprocessor(preprocess_str)</code>","text":"<p>Gets a preprocessing (function) from its name</p> <p>Parameters:</p> Name Type Description Default <code>preprocess_str</code> <code>str</code> <p>Name of the preprocess</p> required <p>Raises:     ValueError: If the name of the preprocess is not known Returns:     Callable: Function to be used for the preprocessing</p> Source code in <code>template_vision/preprocessing/preprocess.py</code> <pre><code>def get_preprocessor(preprocess_str: str) -&gt; Callable:\n    '''Gets a preprocessing (function) from its name\n\n    Args:\n        preprocess_str (str): Name of the preprocess\n    Raises:\n        ValueError: If the name of the preprocess is not known\n    Returns:\n        Callable: Function to be used for the preprocessing\n    '''\n    # Process\n    preprocessors_dict = get_preprocessors_dict()\n    if preprocess_str not in preprocessors_dict.keys():\n        raise ValueError(f\"The preprocess {preprocess_str} is not known.\")\n    # Get preprocessor\n    preprocessor = preprocessors_dict[preprocess_str]\n    # Return\n    return preprocessor\n</code></pre>"},{"location":"reference/template_vision/preprocessing/preprocess/#template_vision.preprocessing.preprocess.get_preprocessors_dict","title":"<code>get_preprocessors_dict()</code>","text":"<p>Gets a dictionary of available preprocessing</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary of preprocessing</p> Source code in <code>template_vision/preprocessing/preprocess.py</code> <pre><code>def get_preprocessors_dict() -&gt; dict:\n    '''Gets a dictionary of available preprocessing\n\n    Returns:\n        dict: Dictionary of preprocessing\n    '''\n    preprocessors_dict = {\n        'no_preprocess': lambda x: x,  # - /!\\ DO NOT DELETE -&gt; necessary for compatibility /!\\ -\n        'preprocess_convert_rgb': preprocess_convert_rgb,  # Simple RGB converter\n        'preprocess_docs': preprocess_docs,  # Example pipeline with documents (remove white borders, 3/4 ratio, resize 224x224)\n    }\n    return preprocessors_dict\n</code></pre>"},{"location":"reference/template_vision/preprocessing/preprocess/#template_vision.preprocessing.preprocess.jpeg_compression","title":"<code>jpeg_compression(images, quality=75)</code>","text":"<p>Simulates a JPEG compression Might be useful for prediction if a model is trained with JPEG compressed images.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list</code> <p>List of images to be compressed</p> required <p>Kwargs:     quality (int): Wanted quality Returns:     list: Compressed images</p> Source code in <code>template_vision/preprocessing/preprocess.py</code> <pre><code>def jpeg_compression(images: list, quality: int = 75) -&gt; list:\n    '''Simulates a JPEG compression\n    Might be useful for prediction if a model is trained with JPEG compressed images.\n\n    Args:\n        images (list): List of images to be compressed\n    Kwargs:\n        quality (int): Wanted quality\n    Returns:\n        list: Compressed images\n    '''\n    # Process images one by one\n    results = []\n    for i, im in enumerate(tqdm.tqdm(images)):\n        out = BytesIO()\n        im.save(out, format='JPEG', quality=quality)\n        out.seek(0)\n        tmp_im = Image.open(out)\n        tmp_im.load()  # Cumpulsory to avoid \"Too many open files\" issue\n        results.append(tmp_im)\n    return results\n</code></pre>"},{"location":"reference/template_vision/preprocessing/preprocess/#template_vision.preprocessing.preprocess.preprocess_convert_rgb","title":"<code>preprocess_convert_rgb(images)</code>","text":"<p>Applies a simple RGB conversion</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list</code> <p>List of images to be transformed</p> required <p>Returns:     list: Preprocessed images</p> Source code in <code>template_vision/preprocessing/preprocess.py</code> <pre><code>def preprocess_convert_rgb(images: list) -&gt; list:\n    '''Applies a simple RGB conversion\n\n    Args:\n        images (list): List of images to be transformed\n    Returns:\n        list: Preprocessed images\n    '''\n    # Process\n    pipeline = [convert_rgb]\n    return apply_pipeline(images, pipeline=pipeline)\n</code></pre>"},{"location":"reference/template_vision/preprocessing/preprocess/#template_vision.preprocessing.preprocess.preprocess_docs","title":"<code>preprocess_docs(images)</code>","text":"<p>Applies a list of usual transformations with scanned documents</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list</code> <p>List of images to be transformed</p> required <p>Returns:     list: Preprocessed images</p> Source code in <code>template_vision/preprocessing/preprocess.py</code> <pre><code>def preprocess_docs(images: list) -&gt; list:\n    '''Applies a list of usual transformations with scanned documents\n\n    Args:\n        images (list): List of images to be transformed\n    Returns:\n        list: Preprocessed images\n    '''\n    # Process\n    pipeline = [convert_rgb, functools.partial(manage_white_borders.remove_white_borders, image_ratio_strategy='fill', image_ratio=0.75, with_rotation=True), resize]\n    return apply_pipeline(images, pipeline=pipeline)\n</code></pre>"},{"location":"reference/template_vision/preprocessing/preprocess/#template_vision.preprocessing.preprocess.resize","title":"<code>resize(images, width=224, height=224)</code>","text":"<p>Resizes images</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list</code> <p>List of images to be resized</p> required <p>Kwargs:     width (int): Wanted width     height (int): Wanted height Raises:     ValueError: If width &lt; 1     ValueError: If height &lt; 1 Returns:     list: Resized images</p> Source code in <code>template_vision/preprocessing/preprocess.py</code> <pre><code>def resize(images: list, width: int = 224, height: int = 224) -&gt; list:\n    '''Resizes images\n\n    Args:\n        images (list): List of images to be resized\n    Kwargs:\n        width (int): Wanted width\n        height (int): Wanted height\n    Raises:\n        ValueError: If width &lt; 1\n        ValueError: If height &lt; 1\n    Returns:\n        list: Resized images\n    '''\n    if width &lt; 1:\n        raise ValueError('Width must be strictly positive.')\n    if height &lt; 1:\n        raise ValueError('Height must be strictly positive.')\n    # Process images one by one\n    results = []\n    for i, im in enumerate(tqdm.tqdm(images)):\n        results.append(im.resize((width, height)))\n    return results\n</code></pre>"}]}