{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ds.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to this Exploratory Data Analysis notebook for Text Data ! :)\n",
    "\n",
    "<br/>\n",
    "\n",
    "This notebook aims to facilitate the understanding of your data through various analyzes on the texts.\n",
    "\n",
    "<br/>\n",
    "\n",
    "Python cells are to be executed sequentially, and some blocks must be modified according to your project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Prerequisites\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "- **A working python 3.8 virtual environment, with your project installed** (in develop mode)\n",
    "\n",
    "\n",
    "- Your virtual env must have been added as an ipython kernel, and defined as the currently used kernel !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding your virtual env\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "**This should be done for the proper functioning of this notebook !**\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "By default, python notebooks use your main python installation.   \n",
    "In this step, we detail how to add your virtual environment as an ipython kernel:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "~ Open a command prompt\n",
    "\n",
    "~ Activate your virtual environment (e.g. `venv_awesome`)\n",
    "\n",
    "~ Install `ipykernel` -> `pip install ipykernel`\n",
    "\n",
    "~ dd your virtual environment as an ipython kernel (e.g. `python -m ipykernel install --user --name=venv_awesome`)\n",
    "\n",
    "<br>\n",
    "\n",
    "Aide :\n",
    "\n",
    "- The `jupyter kernelspec list` commande allows you to list all the installed kernels<br>\n",
    "<br>\n",
    "- The `jupyter kernelspec uninstall venv_awesome` commande allows to uninstall the `venv_awesome` kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then use this notebook with your virtual env:\n",
    "\n",
    "\n",
    "~ Change your kernel (e.g. `venv_awesome`):\n",
    "\n",
    "<img src=\"images/kernel.jpg\">\n",
    "<br>\n",
    "<img src=\"images/kernel2.jpg\">\n",
    "\n",
    "_info : you may need to restart your jupyter notebook_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important\n",
    "\n",
    "In this notebook, we \"hide\" all the functions definitions as to not spoil the reading.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "# Imports and local variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize  # Should be installed through words_n_fun\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML, Javascript, clear_output\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from {{package_name}} import utils\n",
    "\n",
    "# Increase pandas display size\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "# Set default figure size & font\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 60\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "# Center figures\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{% raw %}# From https://stackoverflow.com/questions/31517194/how-to-hide-one-specific-cell-input-or-output-in-ipython-notebook\n",
    "def hide_toggle(for_next=False, text_display='Toggle show/hide'):\n",
    "    '''Function to hide a notebook cell'''\n",
    "    this_cell = \"\"\"$('div.cell.code_cell.rendered.selected')\"\"\"\n",
    "    target_cell = this_cell  # target cell to control with toggle\n",
    "    js_f_name = 'code_toggle_{}'.format(str(random.randint(1,2**64)))\n",
    "    html = f\"\"\"\n",
    "        <script>\n",
    "            function {js_f_name}() {{\n",
    "                {target_cell}.find('div.input').toggle();\n",
    "            }}\n",
    "\n",
    "        </script>\n",
    "\n",
    "        <a href=\"javascript:{js_f_name}()\" id=\"{js_f_name}\">{text_display}</a>\n",
    "    \"\"\"\n",
    "    js = f'''\n",
    "            var output_area = this;\n",
    "            var cell_element = output_area.element.parents('.cell');\n",
    "            var cell_idx = Jupyter.notebook.get_cell_elements().index(cell_element);\n",
    "            var current_cell = Jupyter.notebook.get_cell(cell_idx);\n",
    "            $(current_cell.element[0]).find('div.input').toggle();\n",
    "            Jupyter.notebook.select(cell_idx +  1);\n",
    "            Jupyter.notebook.focus_cell();\n",
    "         '''\n",
    "    display(HTML(html))\n",
    "    display(Javascript(js))\n",
    "\n",
    "{% endraw %}\n",
    "\n",
    "hide_toggle(text_display='Toggle show/hide --- function hide_toggle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples = 20e3\n",
    "filename = 'filename.csv'  # FILE NAME TO BE MODIFIED !\n",
    "text_col = 'sentence'  # TEXT COLUMN NAME TO BE MODIFIED !\n",
    "y_cols = ['col1', 'col2']  # TARGET(S) COLUMNS NAMES TO BE MODIFIED ! - can be empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "# Data loading and first analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = utils.get_data_path()\n",
    "file_path = os.path.join(data_path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_path, sep='{{default_sep}}', encoding='{{default_encoding}}', nrows=max_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display shape\n",
    "n_rows = df.shape[0]\n",
    "n_columns = df.shape[1]\n",
    "print(f\"Number of lines : {n_rows}\")\n",
    "print(f\"Number of columns : {n_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 10 random lines\n",
    "df.sample(10).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check input & target(s) columns presence\n",
    "assert text_col in df.columns, f\"Column '{text_col}' is not present in the loaded DataFrame\"\n",
    "for col in y_cols:\n",
    "    assert col in df.columns, f\"Column '{col}' is not present in the loaded DataFrame\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values\n",
    "for col in [text_col] + y_cols:\n",
    "    nb_missing = df[col].isna().sum()\n",
    "    print(f\"Missing values for column \\033[1m{col}\\033[0m : {nb_missing} -> {round(nb_missing / n_rows * 100, 2)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicates text_col\n",
    "n_duplicates = n_rows - len(df[text_col].drop_duplicates())\n",
    "print(f\"Number of duplicates for column \\033[1m{text_col}\\033[0m : {n_duplicates} -> {round(n_duplicates / n_rows * 100, 2)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique words in the corpus\n",
    "corpus_lower = [[_.lower() for _ in word_tokenize(token)] for token in df[text_col]]\n",
    "flat_corpus_lower = [item for sublist in corpus_lower for item in sublist]\n",
    "unique_words = set([token for token in flat_corpus_lower if token.isalpha()])\n",
    "print(f\"Number (approximate) of unique words in the corpus : {len(unique_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple target(s) analysis\n",
    "for col in y_cols:\n",
    "    ax = sns.countplot(x=df[col])\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width()/2., 1.01 * p.get_height()), ha='center', color='black', size=18)\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_regex = r\"(?!_)[\\w\\s]|((?<=\\w)'(?=\\w))\"\n",
    "# [\\w\\s] -> match all \"not special\" characters, but includes '_'\n",
    "# (?!_) -> do not capture '_'\n",
    "# ((?<=\\w)'(?=\\w)) -> we also match apostrophes that match French words (e.g. \"c'est\", \"t'ai\", etc.)\n",
    "\n",
    "df['word_count'] = df[text_col].apply(lambda x : len(x.split()))\n",
    "df['char_count'] = df[text_col].apply(lambda x : len(x.replace(\" \", \"\"))) # Not 100 % exact, but whatever\n",
    "df['word_density'] = df['word_count'] / (df['char_count'] + 1)\n",
    "df['punct_count'] = df[text_col].str.replace(punct_regex, '').apply(lambda x : len(x))\n",
    "df['uppercase_ratio'] = df[text_col].apply(lambda x: len([c for c in x if c.isupper()]) / len(x))\n",
    "\n",
    "hide_toggle(text_display='Toggle show/hide --- Add columns for stats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stats & display\n",
    "df_stats = pd.DataFrame(columns=['Min', 'Quantile 0.05', 'Quantile 0.125', 'Quantile 0.25', 'Mean', 'Median', 'Quantile 0.75', 'Quantile 0.875', 'Quantile 0.95', 'Max'])\n",
    "for col in ['word_count', 'char_count', 'word_density', 'punct_count', 'uppercase_ratio']:\n",
    "    df_stats.loc[col] = [df[col].min(),\n",
    "                         df[col].quantile(0.05),\n",
    "                         df[col].quantile(0.125),\n",
    "                         df[col].quantile(0.25),\n",
    "                         df[col].mean(),\n",
    "                         df[col].median(),\n",
    "                         df[col].quantile(0.75),\n",
    "                         df[col].quantile(0.825),\n",
    "                         df[col].quantile(0.95),\n",
    "                         df[col].max()]\n",
    "\n",
    "hide_toggle(text_display='Toggle show/hide --- Add statistics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display stats\n",
    "df_stats.style.background_gradient(axis=1).set_precision(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lines are to be uncommented at the user's choice\n",
    "\n",
    "# Violin plots\n",
    "ax = sns.violinplot(x=df[\"word_count\"], inner='quartile', color=sns.color_palette(\"muted\", 8)[0])\n",
    "plt.show(block=False)\n",
    "# ax = sns.violinplot(x=df[\"char_count\"], inner='quartile', color=sns.color_palette(\"muted\", 8)[1])\n",
    "# plt.show(block=False)\n",
    "# ax = sns.violinplot(x=df[\"word_density\"], inner='quartile', color=sns.color_palette(\"muted\", 8)[2])\n",
    "# plt.show(block=False)\n",
    "# ax = sns.violinplot(x=df[\"punct_count\"], inner='quartile', color=sns.color_palette(\"muted\", 8)[3])\n",
    "# plt.show(block=False)\n",
    "# ax = sns.violinplot(x=df[\"uppercase_ratio\"], inner='quartile', color=sns.color_palette(\"muted\", 8)[4])\n",
    "# plt.show(block=False)\n",
    "\n",
    "# Example with target comparison (WARNING, must be used with a unique boolean target)\n",
    "# (small trick on y to be able to use side by side comparison)\n",
    "# ax = sns.violinplot(x=df[\"word_count\"], y=['' for _ in range(n_rows)],  hue=df[y_cols[0]], inner='quartile', palette=\"Set2\", split=True)\n",
    "# plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_outliers(col: str, text: str):\n",
    "    '''Displays outliers on a given col (min & max included)'''\n",
    "    # Outliers\n",
    "    outliers_min = df[df[col] <= df[col].quantile(0.02)][text_col].sample(5)\n",
    "    outlier_min = df[df[col] == df[col].min()][text_col].iloc[0]\n",
    "    if outlier_min not in outliers_min.values:\n",
    "        outliers_min = pd.Series(outlier_min).append(outliers_min, ignore_index=True)\n",
    "    outliers_max = df[df[col] >= df[col].quantile(0.98)][text_col].sample(5)\n",
    "    outlier_max = df[df[col] == df[col].max()][text_col].iloc[0]\n",
    "    if outlier_max not in outliers_max.values:\n",
    "        outliers_max = pd.Series(outlier_max).append(outliers_max, ignore_index=True)\n",
    "    print(text)\n",
    "    print('')\n",
    "    for i, sentence in enumerate(outliers_min):\n",
    "        print(f'---- Outlier min - text {i + 1} ----')\n",
    "        print('')\n",
    "        print('\\t', sentence)\n",
    "        print('')\n",
    "    for i, sentence in enumerate(outliers_max):\n",
    "        print(f'---- Outlier max - text {i + 1} ----')\n",
    "        print('')\n",
    "        print('\\t', sentence)\n",
    "        print('')\n",
    "    print('')\n",
    "\n",
    "hide_toggle(text_display='Toggle show/hide --- function display_outliers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lines are to be uncommented at the user's choice\n",
    "\n",
    "display_outliers('word_count', 'Outliers number of words:')\n",
    "# display_outliers('char_count', 'Outliers number of text characters:')\n",
    "# display_outliers('word_density', 'Outliers words density:')\n",
    "# display_outliers('punct_count', 'Outliers number of punctuations:')\n",
    "# display_outliers('uppercase_ratio', 'Outliers uppercase ratio:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "# Topic modelling\n",
    "\n",
    "Warning: this part requires gensim to be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim==3.8.3\n",
    "!pip install pyLDAvis==3.2.2\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "pyLDAvis.enable_notebook()\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Bag of Words corpus\n",
    "# You can easily edit the first line to add a filter on a class (for example)\n",
    "corpus_lower = [[_.lower() for _ in word_tokenize(token)] for token in df[text_col]]\n",
    "corpus_no_stops = [[t for t in doc if t not in stopwords.words('french')] for doc in corpus_lower]\n",
    "corpus_alphas = [[token for token in doc if token.isalpha()] for doc in corpus_no_stops]\n",
    "dictionary = corpora.Dictionary(corpus_alphas)\n",
    "corpus_bow = [dictionary.doc2bow(t) for t in corpus_alphas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_topics = 5  # To be modified according to your project (should not be too big, otherwise the graphic will take too long to set up)\n",
    "lda_model = LdaModel(corpus=corpus_bow, id2word=dictionary, num_topics=num_topics, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.gensim.prepare(lda_model, corpus_bow, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "# N-grams counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_count(corpus, ngram_range, n=-1):\n",
    "    '''Counts N-grams'''\n",
    "    # Using CountVectorizer to build a bag of words using the given corpus\n",
    "    vectorizer = CountVectorizer(stop_words=stopwords.words('french'), ngram_range=ngram_range).fit(corpus)\n",
    "    bag_of_words = vectorizer.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    total_list = words_freq[:n]\n",
    "    \n",
    "    # Returning a DataFrame with the ngrams count\n",
    "    count_df = pd.DataFrame(total_list, columns=['ngram', 'count'])\n",
    "    return count_df\n",
    "\n",
    "\n",
    "hide_toggle(text_display='Toggle show/hide --- function ngrams_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_spines(ax, right_border=True):\n",
    "    '''Sets up borders from an axis and personalize colors\n",
    "    Args:\n",
    "        ax: figure axis\n",
    "        right_border: flag to determine if the right border will be visible or not\n",
    "    '''\n",
    "    # Setting up colors\n",
    "    ax.spines['bottom'].set_color('#CCCCCC')\n",
    "    ax.spines['left'].set_color('#CCCCCC')\n",
    "    ax.spines['top'].set_color('#FFFFFF')\n",
    "    if right_border:\n",
    "        ax.spines['right'].set_color('#CCCCCC')\n",
    "    else:\n",
    "        ax.spines['right'].set_color('#FFFFFF')\n",
    "    ax.patch.set_facecolor('#FFFFFF')\n",
    "\n",
    "def plot_ngrams(pos_corpus, neg_corpus, col, negative_val, positive_val):\n",
    "    '''Plots ngrams counts'''\n",
    "    # Extracting the top 10 unigrams\n",
    "    unigrams_pos = ngrams_count(pos_corpus, (1, 1), 10)\n",
    "    unigrams_neg = ngrams_count(neg_corpus, (1, 1), 10)\n",
    "\n",
    "    # Extracting the top 10 bigrams\n",
    "    bigrams_pos = ngrams_count(pos_corpus, (2, 2), 10)\n",
    "    bigrams_neg = ngrams_count(neg_corpus, (2, 2), 10)\n",
    "\n",
    "    # Extracting the top 10 trigrams\n",
    "    trigrams_pos = ngrams_count(pos_corpus, (3, 3), 10)\n",
    "    trigrams_neg = ngrams_count(neg_corpus, (3, 3), 10)\n",
    "    \n",
    "    # Joining everything in a python dictionary to make the plots easier\n",
    "    ngram_dict_plot = {\n",
    "        f'Top Unigrams on {col} = {negative_val}': unigrams_neg,\n",
    "        f'Top Unigrams on {col} = {positive_val}': unigrams_pos,\n",
    "        f'Top Bigrams on {col} = {negative_val}': bigrams_neg,\n",
    "        f'Top Bigrams on {col} = {positive_val}': bigrams_pos,\n",
    "        f'Top Trigrams on {col} = {negative_val}': trigrams_neg,\n",
    "        f'Top Trigrams on {col} = {positive_val}': trigrams_pos,\n",
    "    }\n",
    "\n",
    "    # Plotting the ngrams analysis\n",
    "    fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(15, 18))\n",
    "    i, j = 0, 0\n",
    "    colors = ['Blues_d', 'Reds_d']\n",
    "    for title, ngram_data in ngram_dict_plot.items():\n",
    "        ax = axs[i, j]\n",
    "        sns.barplot(x='count', y='ngram', data=ngram_data, ax=ax, palette=colors[j])\n",
    "\n",
    "        # Customizing plots\n",
    "        format_spines(ax, right_border=False)\n",
    "        ax.set_title(title, size=14)\n",
    "        ax.set_ylabel('')\n",
    "        ax.set_xlabel('')\n",
    "\n",
    "        # Incrementing the index\n",
    "        j += 1\n",
    "        if j == 2:\n",
    "            j = 0\n",
    "            i += 1\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "hide_toggle(text_display='Toggle show/hide --- function plot_ngrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus choice\n",
    "if len(y_cols) != 0:\n",
    "    col = y_cols[0]  # CHOOSE YOUR TARGET HERE\n",
    "    negative_val = 0  # CORRESPONDING NEGATIVE VALUE - MIGHT NEED TO BE CHANGED (or to adapt if multiclass)\n",
    "    positive_val = 1  # CORRESPONDING POSITIVE VALUE - MIGHT NEED TO BE CHANGED (or to adapt if multiclass)\n",
    "    neg_corpus = df[df[col] == negative_val][text_col]\n",
    "    pos_corpus = df[df[col] == positive_val][text_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ngrams\n",
    "if len(y_cols) != 0:\n",
    "    plot_ngrams(pos_corpus, neg_corpus, col, negative_val, positive_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "# Word clouds\n",
    "\n",
    "Warning: this part requires wordcloud to be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud==1.8.1\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example positive / negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordclouds(pos_corpus, neg_corpus, col, negative_val, positive_val):\n",
    "    # Prepare format\n",
    "    neg_corpus_lower = [[_.lower() for _ in word_tokenize(token)] for token in neg_corpus]\n",
    "    pos_corpus_lower = [[_.lower() for _ in word_tokenize(token)] for token in pos_corpus]\n",
    "    neg_corpus_no_stops = [[t for t in doc if t not in stopwords.words('french')] for doc in neg_corpus_lower]\n",
    "    pos_corpus_no_stops = [[t for t in doc if t not in stopwords.words('french')] for doc in pos_corpus_lower]\n",
    "    neg_corpus_alphas = [[token for token in doc if token.isalpha()] for doc in neg_corpus_no_stops]\n",
    "    pos_corpus_alphas = [[token for token in doc if token.isalpha()] for doc in pos_corpus_no_stops]\n",
    "    negative_words = [item for sublist in neg_corpus_alphas for item in sublist]\n",
    "    positive_words = [item for sublist in pos_corpus_alphas for item in sublist]\n",
    "    \n",
    "    # Reading and preparing a mask for serving as wordcloud background\n",
    "    pos_img = Image.open(\"./images/positive.png\")\n",
    "    neg_img = Image.open(\"./images/negative.jpeg\")\n",
    "\n",
    "    # Transforming like mask\n",
    "    pos_mask = np.array(pos_img)\n",
    "\n",
    "    # Transforming bomb mask\n",
    "    neg_mask = np.array(neg_img)\n",
    "\n",
    "    # Using Counter for creating a dictionary counting\n",
    "    positive_dict = Counter(positive_words)\n",
    "    negative_dict = Counter(negative_words)\n",
    "\n",
    "    # Generating wordclouds for both positive and negative comments\n",
    "    positive_wc = WordCloud(width=1280, height=720, collocations=False, random_state=42, mask=pos_mask,\n",
    "                          colormap='Blues', background_color='white', max_words=50).generate_from_frequencies(positive_dict)\n",
    "    negative_wc = WordCloud(width=1280, height=720, collocations=False, random_state=42, mask=neg_mask,\n",
    "                          colormap='Reds', background_color='white', max_words=50).generate_from_frequencies(negative_dict)\n",
    "\n",
    "    # Visualizing the WC created\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(20, 20))\n",
    "    ax1 = axs[0]\n",
    "    ax2 = axs[1]\n",
    "\n",
    "    ax1.imshow(negative_wc)\n",
    "    ax1.axis('off')\n",
    "    ax2.set_title(f'WordCloud for values equal to {negative_val}, for target column {col}', size=18, pad=20)\n",
    "\n",
    "    ax2.imshow(positive_wc)\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title(f'WordCloud for values equal to {positive_val}, for target column {col}', size=18, pad=20)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "hide_toggle(text_display='Toggle show/hide --- function plot_wordclouds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus choice\n",
    "if len(y_cols) != 0:\n",
    "    col = y_cols[0]  # CHOOSE YOUR TARGET HERE\n",
    "    negative_val = 0  # CORRESPONDING NEGATIVE VALUE - MIGHT NEED TO BE CHANGED (or to adapt if multiclass)\n",
    "    positive_val = 1  # CORRESPONDING POSITIVE VALUE - MIGHT NEED TO BE CHANGED (or to adapt if multiclass)\n",
    "    neg_corpus = df[df[col] == negative_val][text_col]\n",
    "    pos_corpus = df[df[col] == positive_val][text_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot word clouds\n",
    "if len(y_cols) != 0:\n",
    "    plot_wordclouds(pos_corpus, neg_corpus, col, negative_val, positive_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "# Corpus comparison\n",
    "\n",
    "\n",
    "This part compares the importance of words in the studied corpus with their importance in the wikipedia FR dataset (or whatever corpus). It makes it possible to highlight abnormally over- or under-represented words.\n",
    "\n",
    "This part requires a .pkl file with a dictionnary of IDF over the corpus to be compared to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = utils.get_data_path()\n",
    "idf_name = \"wikipedia_idf.pkl\"  # TO BE CHANGED WITH YOUR IDF FILE\n",
    "idf_path = os.path.join(data_path, idf_name)\n",
    "if not os.path.exists(idf_path):\n",
    "    raise FileNotFoundError(f\"Can't find file {idf_path}\")\n",
    "idf = pickle.load(open(idf_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by normalizing the IDF dictionnary\n",
    "max_idf = np.max([idf[x] for x in idf.keys()])\n",
    "for x in idf.keys():\n",
    "    idf[x] /= max_idf\n",
    "\n",
    "# Then, we fit a TFIDF on our corpus\n",
    "# The corpus should be preprocessed the same way as the other corpus when you created the IDF file\n",
    "# Usually we apply \"lower\" and \"remove stopwords\" steps\n",
    "tfidf = TfidfVectorizer(min_df=100)\n",
    "tfidf.fit_transform(df[text_col].dropna().apply(lambda x: \" \".join([t for t in re.sub('[^A-Za-zÀ-ÖØ-öø-ÿ-+]+', ' ', x.lower()).strip().split()\n",
    "                                                                    if t not in stopwords.words('french')])))\n",
    "\n",
    "# Retrieve the IDF and normalize it\n",
    "idf_corpus = {}\n",
    "for x in tfidf.vocabulary_.keys():\n",
    "    idf_corpus[x] = tfidf.idf_[tfidf.vocabulary_[x]]\n",
    "max_idf_corpus = np.max([idf_corpus[x] for x in idf_corpus.keys()])\n",
    "for x in idf_corpus.keys():\n",
    "    idf_corpus[x] /= max_idf_corpus\n",
    "\n",
    "# Only consider the words in common between our corpus and the generic corpus\n",
    "intersect = idf.keys() & idf_corpus.keys()\n",
    "\n",
    "# Evaluate the differences between the IDF of our corpus and the IDF of the generic corpus\n",
    "data_plot = {}\n",
    "for x in intersect:\n",
    "    data_plot[x] = (idf_corpus[x] - idf[x]) / idf_corpus[x]\n",
    "\n",
    "# Get everything in a DataFrame and sort by values\n",
    "data_plot = pd.DataFrame.from_dict(data_plot, orient='index', columns=[\"value\"])\n",
    "data_plot = data_plot.sort_values(by=\"value\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10  # Number of words to be displayed for both most positives and most negatives values\n",
    "plt.barh(list(data_plot.index[0:n-1]) + list(data_plot.index[-n:]), list(data_plot[\"value\"][0:n-1]) + list(data_plot[\"value\"][-n:]))\n",
    "plt.title(\"Over-represented words (<0) and under-represented words (>0) compared to a generic corpus.\")\n",
    "plt.ylabel(\"Words\")\n",
    "plt.xlabel(\"IDF difference\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
