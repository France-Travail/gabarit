{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Disclaimer: This tutorial is in **FRENCH** only.*\n",
    "\n",
    "# Tutoriel Jules Vernes\n",
    "\n",
    "\n",
    "<img src=\"images/jules_verne_1.png\" style='float: left; width: 30%; margin:10%; margin-top:0%; margin-bottom:0%'><img src=\"images/jules_verne_2.png\" style='float: left; width: 30%; margin:10%; margin-top:0%; margin-bottom:0%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Pré-requis :**\n",
    "\n",
    "- Ce notebook doit avoir été généré avec le template NLP du projet Gabarit.\n",
    "\n",
    "\n",
    "- Télécharger le fichier `texts.csv` se trouvant ici (https://github.com/OSS-Pole-Emploi/AI_frameworks/tree/main/gabarit/template_nlp/nlp_data) et le placer dans le répertoire `{{package_name}}-data` du package que vous avez généré.\n",
    "\n",
    "\n",
    "- **Lancer ce notebook avec un kernel utilisant l'environnement virtuel de votre projet**. Pour ce faire, il suffit de faire : `python -m ipykernel install --user --name=your_venv_name` (une fois votre environnement virtuel activé). Evidemment, le projet généré doit être installé sur cet environnement virtuel. Il peut être nécessaire de rafraîchir votre navigateur pour voir le kernel apparaître.\n",
    "\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Objectifs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "- L'objectif principal de ce tutoriel est de vous introduire au projet Open Source `Gabarit`, développé par l’équipe IA de Pôle Emploi.\n",
    "\n",
    "\n",
    "- Nous allons traiter un cas d'usage \"jouet\" : la détection automatique de l'auteur d'un texte du XIXème siècle.\n",
    "\n",
    "\n",
    "- A la fin de ce tutoriel, nous espérons vous donner tous les outils nécessaires pour démarrer rapidement un nouveau projet d'IA, propre et prêt à être industrialisé.\n",
    "\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. Principes\n",
    "\n",
    "\n",
    "- Ce projet a été developpé dans une logique d'accélération des développements de nos projets IA, et dans l'objectif de fournir une base de code commune facilitant le passage en industrialisation de nos modèles.\n",
    "\n",
    "\n",
    "- Pour ce faire, nous proposons différents **templates** de projets IA :  \n",
    "</br>  \n",
    "</br>  \n",
    "    - Template NLP : classification de données textuelles  \n",
    "</br>  \n",
    "</br>  \n",
    "    \n",
    "    - Template NUM : classification et régression sur des données numériques  \n",
    "</br>  \n",
    "</br>  \n",
    "    \n",
    "    - Template VISION : classification d'images et détection d'objets  \n",
    "</br>  \n",
    "</br>  \n",
    "    \n",
    "    \n",
    "- L'idée n'est pas de créer un outil clé en main 'low-code', mais bien de fournir une base de code que chaque Data Scientist pourra adapter à son projet. Il aura ainsi le contrôle de tout ce qu'il s'y passe.\n",
    "\n",
    "\n",
    "- Les templates sont très similaires entre eux, même s'ils ont évidemment chacun leur spécificité.  \n",
    "\n",
    "\n",
    "- Les projets générés sont composés d'une partie 'package' et d'une partie 'scripts' qui permettent notamment de lancer les entrainements.  \n",
    "\n",
    "\n",
    "- Le projet est construit de manière à ce que tous les modèles soient '**agnostic**'. De cette manière, on appelera un modèle toujours de la même façon (e.g. model.predict(...)) peu importe qu'il s'agisse d'un modèle Sklearn ou TensorFlow.\n",
    "\n",
    "\n",
    "- Des démonstrateurs Streamlit sont intégrés aux projets pour pouvoir rapidement faire des présentations à votre métier !\n",
    "\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Entrainement d'un modèle de détection d'auteurs 'out of the box'\n",
    "\n",
    "On va partir de textes de différents auteurs du XIXème siècle et fabriquer un modèle permettant **d'identifier automatiquement son auteur**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1 Imports et fonctions utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "from {{package_name}} import utils\n",
    "from {{package_name}}.models_training import utils_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.2 Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Certaines fonctions contenues dans utils permettent d'obtenir les chemins vers les répertoires clés du projet\n",
    "# Ici, on récupère le chemin vers le dossier {{package_name}}-data\n",
    "data_path = utils.get_data_path()\n",
    "# On charge les textes pour regarder leur contenu\n",
    "df = pd.read_csv(os.path.join(data_path, 'texts.csv'), sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.3 Analyse rapide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage du nombre de lignes / colonnes\n",
    "n_rows = df.shape[0]\n",
    "n_columns = df.shape[1]\n",
    "print(f\"Nombre de lignes : {n_rows}\")\n",
    "print(f\"Nombre de colonnes : {n_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de 10 lignes aléatoires\n",
    "df.sample(10).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valeurs manquantes\n",
    "for col in df.columns:\n",
    "    nb_missing = df[col].isna().sum()\n",
    "    print(f\"Valeurs manquantes pour la colonne \\033[1m{col}\\033[0m : {nb_missing} -> {round(nb_missing / n_rows * 100, 2)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse de la cible\n",
    "ax = sns.countplot(x=df['author'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.4 Découpage du jeu de données en train / test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons tout d'abord **découper notre dataset en deux parties**, un ensemble d'entrainement et un ensemble de test afin de pouvoir entraîner et tester notre modèle (nous ne fabriquons pas d'ensemble de validation pour simplifier la présentation). \n",
    "\n",
    "Pour cela, nous allons utiliser les scripts contenus dans le package. Les scripts sont des aides pour effectuer les opérations courantes de Data Science. Rien ne vous empêche de créer vos propres scripts !  \n",
    "\n",
    "</br>  \n",
    "</br>  \n",
    "\n",
    "Pour couper en deux le dataset, procédez de la manière suivante :\n",
    "\n",
    "\n",
    "- Lancez un terminal et naviguez dans le répertoire de votre projet  \n",
    "\n",
    "\n",
    "- Activez votre environnement virtuel  \n",
    "\n",
    "    (e.g. sur unix : `source venv_{{package_name}}/bin/activate`)\n",
    "\n",
    "\n",
    "- Naviguez dans le répertoire `{{package_name}}-scripts/utils`\n",
    "\n",
    "    ```bash\n",
    "    cd {{package_name}}-scripts/utils\n",
    "    ```\n",
    "\n",
    "\n",
    "- Appelez le script de découpe du dataset pour réaliser une séparation \"stratified\" : \n",
    "\n",
    "    ```bash\n",
    "    python 0_split_train_valid_test.py -f texts.csv --perc_train 0.6 --perc_valid 0.0 --perc_test 0.4\n",
    "    ```\n",
    "    \n",
    "    Note : les scripts vont automatiquement chercher à charger les fichiers de données, modèles, etc. directement depuis les répertoires du projet prévus à cet effet (e.g. `{{package_name}}-data`, `{{package_name}}-models`, ...)\n",
    "\n",
    "</br>  \n",
    "</br>  \n",
    "\n",
    "Vous pouvez alors voir que deux fichiers, `texts_train.csv` et `texts_test.csv` ont été créés dans `{{package_name}}-data`. \n",
    "\n",
    "</br>  \n",
    "</br>  \n",
    "\n",
    "On peut regarder à quoi ressemble ce nouveau jeu de données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comme nous allons le voir, certains jeux de données peuvent être accompagnés d'une première ligne de metadata.\n",
    "# On conseil donc d'utiliser la fonction utils.read_csv qui va simplement vérifier la présence de cette ligne\n",
    "df_train, metadata_train = utils.read_csv(os.path.join(data_path, 'texts_train.csv'))\n",
    "df_test, metadata_test = utils.read_csv(os.path.join(data_path, 'texts_test.csv'))\n",
    "\n",
    "print('---------------------')\n",
    "print('------- TRAIN -------')\n",
    "print('---------------------')\n",
    "print(f\"Métadonnées train : {metadata_train}\")\n",
    "print('Echantillon :')\n",
    "display(df_train.sample(5).head(5))\n",
    "print('---------------------')\n",
    "print('------- TEST -------')\n",
    "print('---------------------')\n",
    "print(f\"Métadonnées test : {metadata_test}\")\n",
    "print('Echantillon :')\n",
    "display(df_test.sample(5).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si par hasard, vous avez fait une fausse manipulation et que vous devez regénérer les fichiers de train / test, vous pouvez tout simplement ajouter l'option `--overwrite` pour écraser les fichiers erronés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.5 Preprocessing sur les textes\n",
    "\n",
    "Nous allons maintenant faire une étape de préprocessing sur les textes afin de les normaliser et de simplifier l'entrainent de nos modèles. Pour cela, nous allons utiliser le script `1_preprocess_data.py` situé dans `{{package_name}}-scripts`.\n",
    "\n",
    "</br>  \n",
    "</br>  \n",
    "\n",
    "\n",
    "Pour préprocesser nos données textuelles :\n",
    "\n",
    "\n",
    "- Lancez un terminal et naviguez dans le répertoire de votre projet  \n",
    "\n",
    "\n",
    "- Activez votre environnement virtuel  \n",
    "\n",
    "    (e.g. sur unix : `source venv_{{package_name}}/bin/activate`)\n",
    "\n",
    "\n",
    "- Naviguez dans le répertoire `{{package_name}}-scripts`\n",
    "\n",
    "    ```bash\n",
    "    cd {{package_name}}-scripts\n",
    "    ```\n",
    "\n",
    "\n",
    "- Appelez le script de préprocessing : \n",
    "\n",
    "    ```bash\n",
    "    python 1_preprocess_data.py -f texts_train.csv --input_col text\n",
    "    ```\n",
    "    Notes :\n",
    "    - `--input_col` permet de préciser sur quelle colonne appliquer le preprocessing.\n",
    "    - On applique le preprocessing uniquement sur le jeu d'entrainement (et le jeu de validation si on en avait un)\n",
    "\n",
    "</br>  \n",
    "</br>  \n",
    "\n",
    "Un nouveau fichier est donc créé dans le répertoire `{{package_name}}-data` : `texts_train_preprocess_P1.csv`.  \n",
    "Ce fichier est identique au fichier de base, sauf qu'il possède une colonne supplémentaire ('preprocessed_text') qui contient le texte modifié, et une ligne de metadata qui précise quel preprocessing a été appliqué (e.g. `#preprocess_P1`).\n",
    "\n",
    "Cette métadonnée sera réutilisée par nos modèles pour savoir quel preprocessing doit être appliquée sur une nouvelle entrée en amont de la prédiction.\n",
    "\n",
    "</br>  \n",
    "</br>  \n",
    "\n",
    "On peut regarder à quoi ressemble ce nouveau jeu de données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_preprocessed, metadata_train_preprocessed = utils.read_csv(os.path.join(data_path, 'texts_train_preprocess_P1.csv'))\n",
    "print('------------------------------------')\n",
    "print('------- TRAIN - PREPROCESSED -------')\n",
    "print('------------------------------------')\n",
    "print(f\"Métadonnées train - preprocessed : {metadata_train_preprocessed}\")\n",
    "print('Echantillon :')\n",
    "display(df_train_preprocessed.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le préprocessing appliqué est celui fourni de base par le package mais il est très facile d'ajouter ses propres pipelines de preprocessing.\n",
    "\n",
    "Pour cela :\n",
    "\n",
    "- Ouvrez le fichier : `{{package_name}}/preprocessing/preprocess.py`.  \n",
    "\n",
    "\n",
    "- Dans ce module, vous voyez que l'ensemble des pipelines de preprocessing sont renseignées dans la fonction `get_preprocessors_dict`. C'est ici que vous pouvez rajouter vos pipelines personnalisées.  \n",
    "\n",
    "\n",
    "- On peut également jeter un coup d'oeil à la fonction `preprocess_sentence_P1` qui indique les différentes étapes de préprocessing (pour plus d'informations, il suffit de lire la documentation du package words_n_fun, une librairie facilitant tout le travail de manipulation de texte).\n",
    "\n",
    "- Si vous avez l'oeil, vous pouvez remarquer que certains accents n'ont pas été traités correctement par la pipeline `preprocess_sentence_P1`. On va donc corriger cela. Pour ce faire, il suffit de rajouter `'remove_accents'` en première étape dans la pipeline de la fonction `preprocess_sentence_P1`. Puis relancez :\n",
    "\n",
    "    ```bash\n",
    "    python 1_preprocess_data.py -f texts_train.csv --input_col text\n",
    "    ```\n",
    "    Note : Par défaut, ce script écrase les anciennes versions (comportement susceptible de changer dans le futur).\n",
    "\n",
    "</br>  \n",
    "</br>  \n",
    "\n",
    "\n",
    "Les accents sont maintenant correctement traités comme vous pouvez le vérifiez en utilisant la cellule ci-dessous.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_preprocessed, metadata_train_preprocessed = utils.read_csv(os.path.join(data_path, 'texts_train_preprocess_P1.csv'))\n",
    "print('------------------------------------')\n",
    "print('------- TRAIN - PREPROCESSED -------')\n",
    "print('------------------------------------')\n",
    "print(f\"Métadonnées train - preprocessed : {metadata_train_preprocessed}\")\n",
    "print('Echantillon :')\n",
    "display(df_train_preprocessed.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez que, de manière générale, il est **fortement conseillé de créer une nouvelle pipeline de préprocessing plutôt que d'en modifier une existante**.  \n",
    "En effet, si un de vos anciens modèles utilisait cette pipeline, ses performances vont changer car il utilisera maintenant la nouvelle (qui a le même nom!).  \n",
    "Comme nous sommes ici au début d'un projet et que nous n'avons pas encore entraîné de modèles, nous pouvons modifier la pipeline de préprocessing.\n",
    "\n",
    "N.B. : Ce comportement sera certainement amélioré dans les prochaines mises à jour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.6. Entrainement d'un premier modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant entraîner un premier modèle en utilisant le script `2_training.py`\n",
    "\n",
    "Pour cela :\n",
    "\n",
    "\n",
    "- Lancez un terminal et naviguez dans le répertoire de votre projet  \n",
    "\n",
    "\n",
    "- Activez votre environnement virtuel  \n",
    "\n",
    "    (e.g. sur unix : `source venv_{{package_name}}/bin/activate`)\n",
    "\n",
    "\n",
    "- Naviguez dans le répertoire `{{package_name}}-scripts`\n",
    "\n",
    "    ```bash\n",
    "    cd {{package_name}}-scripts\n",
    "    ```\n",
    "\n",
    "\n",
    "- Appelez le script d'entrainement : \n",
    "\n",
    "    ```bash\n",
    "    python 2_training.py -f texts_train_preprocess_P1.csv --x_col preprocessed_text --y_col author\n",
    "    ```\n",
    "    Note :\n",
    "    - Par défaut, le modèle entrainé sera un simple TF-IDF + SVM sur les données\n",
    "    - Plusieurs autres modèles sont proposés, il suffit d'aller commenter / décommenter les lignes correspondantes dans le fichier d'entrainement `2_training.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pourrions utiliser ensuite le script `3_predict.py` pour utiliser le modèle pour prédire sur notre ensemble de test mais nous allons procéder autrement pour avoir les résultats directement dans ce notebook. Si vous allez dans `{{package_name}}-models/model_tfidf_svm` vous allez voir un dossier du type `model_tfidf_svm_{YYYY_MM_DD-hh_mm_ss}` qui contient la sauvegarde du modèle que nous venons d'entraîner. \n",
    "\n",
    "Vous pouvez par exemple voir le f1_score sur l'ensemble d'entrainement, la matrice de confusion sur l'ensemble d'entraînement etc. Notez que nous aurions pu spécifier un dataset de validation au script `2_training.py` et nous aurions alors aussi accès à des métriques sur ce dataset. \n",
    "\n",
    "Ce dossier contient également les fichiers de sauvegarde du modèle que nous allons charger dans la cellule suivante (en remplacant bien sûr `{YYYY_MM_DD-hh_mm_ss}` par les nombres correspondant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du modèle entrainé\n",
    "model, model_conf = utils_models.load_model('model_tfidf_svm_{YYYY_MM_DD-hh_mm_ss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voilà, le modèle est chargé, prêt à être utilisé. Ce que nous allons directement faire sur l'ensemble de test chargé plus haut :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_preprocessed['prediction'] = model.predict(df_test_preprocessed['preprocessed_text'])\n",
    "print('accuracy :', len(df_test_preprocessed[df_test_preprocessed['prediction']==df_test_preprocessed['author']])/len(df_test_preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais, il est tout nul ce modèle !!!!\n",
    "\n",
    "Pour remédier à ça, nous pourrions essayer d'autres modèles qui sont déjà packagés dans le projet. \n",
    "\n",
    "Mais nous allons utiliser une autre manière de faire qui illustre toute la puissance de ces frameworks. Nous allons créer notre propre modèle qui incorporera tous les à-côtés que nous avons vu (sauvegarde automatique des modèles, chargement des modèles, calcul des métriques...). \n",
    "\n",
    "La logique que nous allons appliquer est simple. Au lieu d'appliquer un TF-IDF + SVM à des livres entiers, nous allons découper les livres en 'phrases' et faire apprendre le modèle sur ces 'phrases'. Pour prédire l'auteur d'un livre, nous allons donc découper en 'phrases', prédire pour chaque 'phrase' un auteur et prendre pour prédiction finale, l'auteur qui a le plus de 'phrases' lui correspondant.\n",
    "\n",
    "On doit donc créer d'abord une fonction qui prend un texte et qui le découpe en phrase. On va également lui indiquer combien de mots par phrase on souhaite. Compléter la fonction suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sentences(text: str, nb_word_sentence: int) -> List[str]:\n",
    "    '''Transforms a text in sentences.\n",
    "\n",
    "    Args:\n",
    "        text (str) : The text to cut in sentences\n",
    "        nb_word_sentence (int) : The number of words in a sentence\n",
    "    Returns:\n",
    "        list : A list of sentences\n",
    "    \n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois la fonction ci-dessus complétée, vous pouvez ouvrir le fichier `utils_tutorial_fr.py` situé au même endroit que ce notebook. A l'intérieur, vous trouverez une solution possible pour la fonction ci-dessus (vous pouvez la remplacer par votre version si vous le souhaitez). \n",
    "\n",
    "Le reste du fichier contient notre modèle de prédiction d'auteurs: \n",
    "\n",
    "La fonction `df_texts_to_df_sentences` permet de formater une liste de textes et d'auteur en un dataframe de 'phrases' (en utilisant la fonction `text_to_sentences`). Il s'agit uniquement d'un fonction de formattage et n'apporte aucune réelle plus-value.\n",
    "\n",
    "La classe `ModelAuthor` est la classe implémentant notre modèle. Notez qu'elle descend de la classe `ModelTfidfSvm`, ce qui va nous permettre très simplement d'avoir accès à toutes les méthodes voulues. \n",
    "\n",
    "Dans l'`__init__`, on rajoute simplement `nb_word_sentence`, l'argument de la fonction ci-dessus.\n",
    "\n",
    "Dans le `fit`, on ajoute simplement le découpage en 'phrases' avant d'appeler le même modèle TF-IDF+SVM que précedemment.\n",
    "\n",
    "Dans le `get_nb_sentences_author` on récupère le nombre de 'phrases' attribuées à chaque auteur\n",
    "\n",
    "Dans le `predict`, suivant que l'on veut les probabilités ou non, soit on donne le pourcentage de 'phrases' associées à chaque auteur ou le nom de l'auteur qui a le plus de 'phrases' associées.\n",
    "\n",
    "Ainsi en moins d'une centaine de ligne de code, nous avons développé un nouveau modèle qui s'incorpore totalement dans le projet et qui va pouvoir utiliser tous ses outils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons donc utiliser notre nouveau modèle en faisant les mêmes étapes que précedemment (sauf les étapes de préprocessing du texte qui ne changent pas). \n",
    "\n",
    "On va tout d'abord incorporez notre modèle au projet. Pour ce faire, copier le fichier `utils_tutorial_fr.py` à l'endroit où se trouve tous les autres modèles : `{{package_name}}/models_training/`\n",
    "\n",
    "Il suffit maintenant de modifier le script `2_training.py` pour rajouter notre modèle. En haut du script, rajouter `from {{package_name}}.models_training.utils_tutorial_fr import ModelAuthor` pour importer notre modèle. Et à la ligne 227, mettez en commentaire le modèle `ModelTfidfSvm` et à la place ajoutez le contenu de la cellule suivante:                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelAuthor(x_col=x_col, \n",
    "                    y_col=y_col, \n",
    "                    level_save=level_save,\n",
    "                    multi_label=multi_label,\n",
    "                    nb_word_sentence=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relancez le script python avec la même commande que précedemment : `python 2_training.py -f texts_train_preprocess_P1.csv --x_col preprocessed_text --y_col author`\n",
    "\n",
    "Et rechargez le nouveau modèle entrainé avec la cellule suivante (toujours en changeant le nom du modèle chargé):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, model_conf = utils_models.load_model('model_author_{YYYY_MM_DD-hh_mm_ss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On teste à nouveau notre modèle pour voir sa performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_preprocessed['prediction'] = model.predict(df_test_preprocessed['preprocessed_text'])\n",
    "print('accuracy :', len(df_test_preprocessed[df_test_preprocessed['prediction']==df_test_preprocessed['author']])/len(df_test_preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est beaucoup mieux!!\n",
    "\n",
    "Ce petit exemple illustre la manière d'utiliser les frameworks. On part d'un projet contenant tous les outils pour développer rapidement un modèle. On teste quelques modèles inclus pour voir si un correspond à notre cas d'usage. Et, si besoin, on développe son propre modèle en héritant des classes définies dans le projet pour profiter de toutes les fonctions incluses dedans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons finalement utiliser un autre outil incorporé dans les frameworks : le démonstrateur. Il est souvent nécessaire de montrer comment fonctionne un modèle à des personnes qui ne l'ont pas développé et le démonstrateur est fait pour cela. Les frameworks utilisent streamlit pour mettre en place ce démonstrateur.\n",
    "\n",
    "Il suffit de se placer dans le dossier `{{package_name}}/{{package_name}}-scripts/` et de faire `streamlit run 4_demonstrator.py` pour lancer le démonstrateur (il faut peut-être cliquer sur un lien ou le copier-coller dans un navigateur suivant votre configuration).\n",
    "\n",
    "Dans le menu de gauche, vous pouvez sélectionner le modèle que vous voulez considérer, soit le premier modèle TF-IDF-SVM que nous avons entrainé soit celui utilisant le découpage en phrase. Il suffit ensuite d'entrer le texte que vous voulez tester et appuyer sur predict pour accéder à la prédiction du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
