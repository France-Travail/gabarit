{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisites:**\n",
    "\n",
    "- This notebook must have been generated using the Gabarit's NLP template.   \n",
    "\n",
    "\n",
    "- Download the fasttext embedding matrix: https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.vec.gz and extract it in `{{package_name}}-data`\n",
    "\n",
    "\n",
    "-  Download the file `dataset_jvc.csv` here (https://github.com/OSS-Pole-Emploi/AI_frameworks/tree/main/gabarit/template_nlp/nlp_data) and place it inside `{{package_name}}-data`\n",
    "\n",
    "\n",
    "- **Launch this notebook with a kernel using your project virtual environment**. In order to create a kernel linked to your virtual environment : `python -m ipykernel install --user --name=your_venv_name` (once your virtual environment is activated). Obviously, the project must be installed on this virtual environment.\n",
    "\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understand how the NLP template works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why use the NLP template?**\n",
    "\n",
    "The NLP (Natural Language Processing) template automatically generates a NLP project including the more mainstream models and facilitating their industrialization.\n",
    "\n",
    "The generated project can be used for **classification** tasks on text data. Of course, you have to adapt it to your particular use case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structure of the generated project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".   \n",
    "├── <span style=\"color:darkred\">{{package_name}}</span><span style=\"float: right; width:70%\"><b># The package<b/></span> <br>\n",
    "│   ├── <span style=\"color:darkred\">models_training</span><span style=\"float: right; width:70%\"><b># Folder containing all the modules related to the models<b/></span> <br>\n",
    "│   ├── <span style=\"color:darkred\">monitoring</span><span style=\"float: right; width:70%\"><b># Folder containing all the modules related to the explainers and MLflow<b/></span> <br>\n",
    "│   ├── <span style=\"color:darkred\">preprocessing</span><span style=\"float: right; width:70%\"><b># Folder containing all the modules related to the preprocessing<b/></span> <br>\n",
    "├── <span style=\"color:darkred\">{{package_name}}-data</span><span style=\"float: right; width:70%\"><b># Folder containing all the data (datasets, embeddings, etc.)<b/></span> <br>\n",
    "├── <span style=\"color:darkred\">{{package_name}}-exploration</span><span style=\"float: right; width:70%\"><b># Folder where all your experiments and explorations must go<b/></span> <br>\n",
    "├── <span style=\"color:darkred\">{{package_name}}-models</span><span style=\"float: right; width:70%\"><b># Folder containing all the generated models<b/></span> <br>\n",
    "├── <span style=\"color:darkred\">{{package_name}}-ressources</span><span style=\"float: right; width:70%\"><b># Folder containing some ressources such as the instructions to upload a model<b/></span> <br>\n",
    "├── <span style=\"color:darkred\">{{package_name}}-tutorial</span><span style=\"float: right; width:70%\"><b># Folder containing notebook tutorials, including this one<b/></span> <br>\n",
    "├── <span style=\"color:darkred\">{{package_name}}-scripts</span><span style=\"float: right; width:70%\"><b># Folder containing examples script to preprocess data, train models, predict and use a demonstrator<b/></span> <br>\n",
    "│   ├── <span style=\"color:darkred\">utils</span><span style=\"float: right; width:70%\"><b># Folder containing utils scripts (such as split train/test, sampling, etc...)<b/></span> <br>\n",
    "│   ├── <span style=\"color:darkred\">utils_torch</span><span style=\"float: right; width:70%\"><b># Folder containing utils scripts related to pytorch<b/></span> <br>\n",
    "├── <span style=\"color:darkred\">{{package_name}}-transformers</span><span style=\"float: right; width:70%\"><b># Folder containing pytorch transformers<b/></span> <br>\n",
    "├── <span style=\"color:darkred\">tests</span><span style=\"float: right; width:70%\"><b># Folder containing all the unit tests<b/></span> <br>\n",
    "├── .gitignore <br>\n",
    "├── .coveragerc <br>\n",
    "├── Makefile <br>\n",
    "├── nose_setup_coverage.cfg <br>\n",
    "├── README.md    <br>\n",
    "├── requirements.txt    <br>\n",
    "├── setup.py    <br>\n",
    "└── version.txt   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General principles on the generated packages**\n",
    "\n",
    "- Data must be saved in the `{{package_name}}-data` folder<br>\n",
    "<br>\n",
    "- Trained models will automatically be saved in the `{{package_name}}-models` folder<br>\n",
    "<br>\n",
    "- Be aware that all the functions/methods for writing/reading files uses these two folders as base. Thus when a script has an argument for the path of a file/model, the given path should be **relative** to the `{{package_name}}-data`/`{{package_name}}-models` folders.<br>\n",
    "<br>\n",
    "- The provided scripts in `{{package_name}}-scripts` are given as example. You can use them to help you develop but their use is not required.<br>\n",
    "<br>\n",
    "- The file `preprocess.py` contains the various preprocessing pipelines used in this package. This file contains a dictionary of pipelines. It will be used to create datasets. Be very careful when you modify a pipeline because the already trained model won't be retrocompatible with it. It is generally advised to create a new pipeline.<br>\n",
    "<br>\n",
    "- You can use this package for mono-label and multi-labels tasks (`multi_label` argument in models' classes)<br>\n",
    "<br>\n",
    "- The modelling part is structured as follows :\n",
    "    - `ModelClass`: main class taking care of saving data and metrics (among other)\n",
    "    - `ModelPipeline`: child class of ModelClass managing all models related to a sklearn pipeline\n",
    "    - `ModelKeras`: child class of ModelClass managing all models using Keras\n",
    "    - `ModelPyTorch`: child class of ModelClass managing all models using Torch (experimental)\n",
    "    \n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use the template to train your first model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that purpose, we will use a dataset containing popular video games from the French website jeuxvideo.com.\n",
    "\n",
    "This dataset contains a description of many games and their type (Action, RPG, etc.). The goal will be to predict the type of the game from its description.\n",
    "\n",
    "Note that the dataset is in French but it is not necessary to understand French to follow this tutorial.\n",
    "\n",
    "Note: the main dataset is a CSV file with `;` as separator and `utf-8` as encoding. These are the default values for generated project. If you have generated a project with different options, you must first edit the CSV file accordingly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Exercice 1**</span>\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "- Split the main dataset in train / valid / test sets\n",
    "\n",
    "**TODO:**\n",
    "- Use the script `utils/0_split_train_valid_test.py` on the dataset `{{package_name}}-data/dataset_jvc.csv`\n",
    "- We want a 'random' split but **with a random seed set to 42** (in order to always reproduce the same results)\n",
    "- We use the default splitting ratios (0.6 / 0.2 / 0.2)\n",
    "\n",
    "**Help:**\n",
    "- The file `utils/0_split_train_valid_test.py` splits a dataset in 3 .csv files:\n",
    "    - {filename}_train.csv: the training dataset\n",
    "    - {filename}_valid.csv: the validation dataset\n",
    "    - {filename}_test.csv: the test dataset\n",
    "- You can specify the type of split : random, stratified or hierarchical (here, use random)\n",
    "- Reminder: the path to the file to process is relative to `{{package_name}}-data`\n",
    "- To get the possible arguments of the script: `python 0_split_train_valid_test.py --help`\n",
    "- Don't forget to activate your virtual environment ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 1** :  Validation\n",
    "\n",
    "~ Run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_main_tutorial\n",
    "utils_main_tutorial.test_exercice_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 1** :  Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_main_tutorial\n",
    "utils_main_tutorial.get_exercice_1_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<span style=\"color:red\">**Exercice 2**</span>\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "- Get a random sample of the file `dataset_jvc_train.csv` (n=10) (we won't use it, this exercise is just here to show what can be done)\n",
    "\n",
    "**TODO:**\n",
    "- Use the script `utils/0_create_samples.py` on the dataset `{{package_name}}-data/dataset_jvc.csv`\n",
    "- We want a sample of 10 lines\n",
    "\n",
    "**Help:**\n",
    "- The file `utils/0_create_samples.py` samples a dataset\n",
    "- To get the possible arguments of the script: `python 0_create_samples.py --help`\n",
    "- Don't forget to activate your virtual environment ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 2** :  Validation\n",
    "\n",
    "~ Run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_main_tutorial\n",
    "utils_main_tutorial.test_exercice_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 2** :  Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_main_tutorial\n",
    "utils_main_tutorial.get_exercice_2_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<span style=\"color:red\">**Exercice 3**</span>\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "- Apply the default preprocessing to `dataset_jvc_train.csv`\n",
    "\n",
    "**TODO:**\n",
    "- Use the script `1_preprocess_data.py` on the dataset `{{package_name}}-data/dataset_jvc_train.csv` to apply the default pipeline (`preprocess_P1`)\n",
    "- The preprocessing must be done on the `description` column\n",
    "\n",
    "**Help:**\n",
    "- The file `1_preprocess_data.py` applies a preprocessing pipeline **to one column of one or several .csv files**\n",
    "- Without the argument `preprocessing`, this python script creates as many files as there are **pipelines registered in `preprocessing/preprocess.py`**\n",
    "- It works as follows:<br>\n",
    "    - In `preprocessing/preprocess.py`: <br>\n",
    "        - There is a dictionary of functions (`get_preprocessors_dict`): key: str -> function <br>\n",
    "            - /!\\ Don't remove the default element 'no_preprocess': lambda x: x /!\\ <br>\n",
    "        - There are preprocessing functions (usually from words_n_fun pipelines) <br>\n",
    "    - In `1_preprocess_data.py` :<br>\n",
    "        - We retrieve the dictionary of functions from `preprocessing/preprocess.py` <br>\n",
    "        - If a `preprocessing` argument is specified, we keep only the corresponding key from the dictionnary <br>\n",
    "        - Otherwise, we keep all keys (except `no_preprocess`) <br>\n",
    "        - For each entry of the dictionary, we:<br>\n",
    "            - Get the associated preprocessing function\n",
    "            - Load data\n",
    "            - Create a column `preprocessed_text` -> apply the preprocessing function\n",
    "            - Save the result -> {file_name}_{key}.csv <br>\n",
    "- To get the possible arguments of the script: `python 1_preprocess_data.py --help`\n",
    "- Don't forget to activate your virtual environment ...\n",
    "\n",
    "**Important:**\n",
    "- Each preprocessed file is saved in the `{{package_name}}-data` folder.\n",
    "- To track which preprocessed has been done, we add a first line to these files as a metadata line (e.g. `#preprocess_P1`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 3** :  Validation\n",
    "\n",
    "~ Run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_main_tutorial\n",
    "utils_main_tutorial.test_exercice_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 3** :  Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_main_tutorial\n",
    "utils_main_tutorial.get_exercice_3_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<span style=\"color:red\">**Exercice 4**</span>\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "- Apply a \"custom\" preprocess to `dataset_jvc_train.csv` and `dataset_jvc_valid.csv`\n",
    "\n",
    "**TODO:**\n",
    "- Add a new preprocessing pipeline `preprocess_P2` in `preprocessing/preprocess.py`\n",
    "\n",
    "- '''# pipeline to use <br>\n",
    "pipeline = ['remove_non_string', 'get_true_spaces', 'remove_punct', 'to_lower','remove_stopwords', 'trim_string', 'remove_leading_and_ending_spaces']\n",
    "'''\n",
    "- Use the script `1_preprocess_data.py` to apply the new pipeline `preprocess_P2`\n",
    "- The preprocessing must be done on the `description` column\n",
    "\n",
    "**Help:**\n",
    "- You have to create a new preprocessing in `preprocessing/preprocess.py` and add it to the dictionary of `get_preprocessors_dict()`\n",
    "- Don't forget to activate your virtual environment ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 4** :  Validation\n",
    "\n",
    "~ Run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_main_tutorial\n",
    "utils_main_tutorial.test_exercice_4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 4** :  Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_main_tutorial\n",
    "utils_main_tutorial.get_exercice_4_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "<span style=\"color:red\">**Exercice 5**</span>\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "- Use the script `2_training.py` to train a mono-label TD-IDF + SVM model to predict the 'RPG' category\n",
    "\n",
    "- Training dataset : `dataset_jvc_train_preprocess_P2.csv`\n",
    "\n",
    "- Validation dataset : `dataset_jvc_valid_preprocess_P2.csv`\n",
    "\n",
    "**TODO:**\n",
    "- Use the script `2_training.py` with the proper arguments\n",
    "\n",
    "- We want to train on the column `preprocessed_text`, result of the preprocessing on the `description` column\n",
    "\n",
    "- We want to predict the `RPG` column\n",
    "\n",
    "**Help:**\n",
    "- The script `2_training.py` trains a model on a dataset\n",
    "- It works as follows:<br>\n",
    "    - Read a train .csv file as input <br>\n",
    "        - If a validation file is given, it will use it as validation data <br>\n",
    "    - Manage `y_col` argument: <br>\n",
    "        - If there is only one value, training in mono-label mode <br>\n",
    "        - If several values, training in multi-labels mode <br>\n",
    "    - **Manual modifications of the script**: <br>\n",
    "        - **To change the model used** -> you have to comment/uncomment/modify the code in the \"training\" part (not necessary for this exercise) <br>\n",
    "        - **To load datasets** -> if a dataset is not in the right format, you have to adapt the loading part (not necessary for this exercise) <br>\n",
    "- To get the possible arguments of the script: `python 2_training.py --help`\n",
    "- Don't forget to activate your virtual environment ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 5** :  Validation\n",
    "\n",
    "**~ Manual validation**\n",
    "\n",
    "After having executed the script `2_training.py`, you should see logs similar to these ones:\n",
    "\n",
    "<img src=\"images/model1.png\">\n",
    "\n",
    "With the default TF-IDF, you can see that the model overfits on the train dataset <br>\n",
    "\n",
    "A new model was created in the folder `{{package_name}}-models`. It contains the save of the model, results, statistics and plots.\n",
    "\n",
    "<img src=\"images/model1_path.png\">\n",
    "\n",
    "Details:\n",
    "- `plots/` : Folder containing the plots (here confusion matrices) <br>\n",
    "- `acc_train@​0.xx` : Empty file. The value after @ indicates the accuracy of the model on the train dataset <br>\n",
    "- `acc_valid@​0.xx` : Empty file. The value after @ indicates the accuracy of the model on the validation dataset <br>\n",
    "- `configurations.json` : **Configurations used by the model**. Mandatory to re-use a model <br>\n",
    "- `f1_train@​0.xx.csv` : Statistics per class on the train dataset. The value after @ indicates the weighted f1-score <br>\n",
    "- `f1_valid@​0.xx.csv` : Statistics per class on the validation dataset. The value after @ indicates the weighted f1-score <br>\n",
    "- `model_{name_of_the_model}.pkl` : Saved model in Pickle format. Full model object.\n",
    "- `{name_of_the_type_of_model}_standalone.pkl` : Saved model in Pickle format. Standalone version (e.g. the sklearn model only). <br>\n",
    "- `model_upload_instructions.md` : Instructions to upload a model to use it (needs to be customized).\n",
    "- `predictions_train.csv` : Predictions on the train dataset. Wrong predictions first. <br>\n",
    "- `predictions_valid.csv` : Predictions on the validation dataset. Wrong predictions first. <br>\n",
    "- `proprietes.json` : Property file to be uploaded alongside the model. Not useful for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 5** :  Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_main_tutorial\n",
    "utils_main_tutorial.get_exercice_5_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<span style=\"color:red\">**Exercice 6**</span>\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "- Use the script `2_training.py` to train a mono-label LSTM model to predict all the categories of the dataset\n",
    "\n",
    "- Training dataset : `dataset_jvc_train.csv`\n",
    "\n",
    "- Validation dataset : `dataset_jvc_valid.csv`\n",
    "\n",
    "**TODO:**\n",
    "- Generate the fasttext embedding matrix as a .pkl file -> cf. `utils/0_get_embedding_dict.py`\n",
    "\n",
    "- Modify the script `2_training.py` to select the model `ModelEmbeddingLstm`\n",
    "\n",
    "<img src=\"images/lstm_choice.jpg\">\n",
    "\n",
    "- *optionnal* : You can see the model structure directly in the script `{{package_name}}/models_training/model_embedding_lstm.py` (`_get_model()` function)\n",
    "\n",
    "- Use the script `2_training.py` with the proper arguments\n",
    "\n",
    "- We want to train on the `description` column\n",
    "\n",
    "- We want to predict the columns : \"Action\", \"Aventure\", \"RPG\", \"Plate-Forme\", \"FPS\", \"Course\", \"Strategie\", \"Sport\", \"Reflexion\", \"Combat\"\n",
    "\n",
    "**Help:**\n",
    "- Check that the file `cc.fr.300.vec` is in the folder `{{package_name}}-data`\n",
    "- We advise you to lower the number of epochs as the training can be too long\n",
    "- The strucutre of Deep Learning models are to be modified directly in the code of the suitable class (`_get_model()` function)\n",
    "- To get the possible arguments of the script: `python 2_training.py --help`\n",
    "- Don't forget to activate your virtual environment ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 6** :  Validation\n",
    "\n",
    "**~ Manual validation**\n",
    "\n",
    "After having executed the script `2_training.py`, you should see logs similar to this :\n",
    "\n",
    "<img src=\"images/model2.png\">\n",
    "\n",
    "Here we can see that our results are pretty good on some categories (Course, FPS) and quite poort on others (Strategies, Reflexion).\n",
    "\n",
    "With enough epochs, you should obtain a f1-score higher than 0.70 on the validation set. To compare, a TF-IDF/SVM gives a f1-score of roughly 0.63\n",
    "\n",
    "A new folder has been created in your folder `{{package_name}}-models`. It contains the save of the model, results, statistics and plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 6** :  Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_main_tutorial\n",
    "utils_main_tutorial.get_exercice_6_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "<span style=\"color:red\">**Exercice 7**</span>\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "- Use your model to predict on the test dataset `dataset_jvc_test.csv`\n",
    "\n",
    "**TODO:**\n",
    "- Get the name of your model which is the name of the created folder `model_embedding_lstm_{YYYY_MM_DD-hh_mm_ss}`\n",
    "\n",
    "- Use the script `3_predict.py` to predict on the test dataset `dataset_jvc_test.csv`\n",
    "\n",
    "- We want to predict using the `description` column\n",
    "\n",
    "- *optional* The argument `y_col` is optional but you can use it to evaluate the model's performances on the test dataset  : \"Action\", \"Aventure\", \"RPG\", \"Plate-Forme\", \"FPS\", \"Course\", \"Strategie\", \"Sport\", \"Reflexion\", \"Combat\"\n",
    "\n",
    "**Help:**\n",
    "- To get the possible arguments of the script: `python 3_predict.py --help`\n",
    "- Don't forget to activate your virtual environment ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 7** :  Validation\n",
    "\n",
    "**~ Manual validation**\n",
    "\n",
    "After having executed the script `3_predict.py`, you should see logs similar to this :\n",
    "\n",
    "<img src=\"images/predictions.png\">\n",
    "\n",
    "A new folder `predictions/dataset_jvc_test/` has been created in your folder `{{package_name}}-data`. It contains predictions on the test dataset and statistics and plots (if y_col has been given)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 7** :  Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_main_tutorial\n",
    "utils_main_tutorial.get_exercice_7_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "\n",
    "## 3. Use a model to predict on new data\n",
    "\n",
    "<br>\n",
    "\n",
    "In this section, we will see how to reload \"manually\" a model and how to use it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load a model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from {{package_name}}.models_training import utils_models\n",
    "\n",
    "#\n",
    "# TODO: replace the name of the model by the one you trained\n",
    "#\n",
    "model, model_conf = utils_models.load_model('model_embedding_lstm_{YYYY_MM_DD-hh_mm_ss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, feel free to imagine the description of a video game (in french) or take one from a french website (such as www.jeuxvideo.com) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = '''The Legend of Pôle emploi 2 : le retour des Data Scientists, est un jeu de tir à la première personne, \n",
    " dans lequel vous vivrez une aventure digne du Seigneur des anneaux !\n",
    " Encore meilleur que The Legend of Pôle emploi, une multitude de nouvelles armes à feu viendront enrichir votre arsenal de guerre.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. and predict its class !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "predictions = model.predict([description])\n",
    "clear_output()\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🤬 What is that ?!!! A vector ???!!! 🤬\n",
    "\n",
    "😎 Stay calm ! Do not panic ! 😎 \n",
    "\n",
    "The `inverse_transform` method is here to save the day !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.inverse_transform(predictions)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now play around with your model 😄\n",
    "\n",
    "<br>\n",
    "\n",
    "**Disclaimer : To be perfectly honest, the training dataset is really small -> the performance of your model will probably be poor  😕**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "\n",
    "## 4. BONUS : You can now showcase your best models to the world !\n",
    "\n",
    "#### Well ... Maybe you should stick to your localhost for the moment ...\n",
    "<br>\n",
    "\n",
    "You are now ready to demonstrate how good your models work. We implemented a default ***Streamlit*** app., let's try it !\n",
    "\n",
    "You juste have to open a command shell in your {{package_name}}-scripts folder and run `streamlit run 4_demonstrator.py`.\n",
    "\n",
    "It will start a Streamlit app on the default port (8501) : http://localhost:8501/\n",
    "\n",
    "<img src=\"images/demonstrator.png\">\n",
    "\n",
    "Now just have fun showing your best models 😀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
