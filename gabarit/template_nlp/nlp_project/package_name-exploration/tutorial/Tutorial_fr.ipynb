{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from {{package_name}} import utils\n",
    "from {{package_name}}.models_training import utils_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "blabla général sur les frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainement d'un modèle de détection d'auteur 'out of the box'\n",
    "\n",
    "On va partir de textes de différents auteurs du XIXème siècle et fabriquer un modèle permettant d'identifier automatiquement l'auteur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On charge les textes\n",
    "data_path = utils.get_data_path()\n",
    "df_texts = pd.read_csv(os.path.join(data_path, 'texts.csv'), sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons tout d'abord couper notre dataset en deux parties, un ensemble d'entrainement et un ensemble de test afin de pouvoir, entraîner et (vous l'avez deviné) tester notre modèle. Notez que nous ne fabriquons pas d'ensemble de validation car nous n'allons pas tuner notre modèle. \n",
    "\n",
    "Pour cela, nous allons utiliser les scripts contenus dans le package. Pour couper en deux notre dataset, procédez de la manière suivante :\n",
    "\n",
    "Lancez un terminal\n",
    "Activez votre environnement virtuel\n",
    "Placer vous dans votre package dans le répertoire suivant:\n",
    "\n",
    "`cd {{package_name}}/{{package_name}}-scripts/utils`\n",
    "\n",
    "Appelez le script de découpe du dataset pour réaliser un 'stratified split': \n",
    "\n",
    "`python 0_split_train_valid_test.py -f texts.csv --perc_train 0.6 --perc_valid 0.0 --perc_test 0.4`\n",
    "\n",
    "Si par hasard, vous avez fait une fausse manip et qu'il faut regénérez des fichiers de train/test, rajoutez l'argument `--overwrite` pour écrasez les fichiers erronés\n",
    "\n",
    "Vous pouvez alors voir que deux fichiers, `texts_train.csv` et `texts_test.csv` ont été créés dans `{{package_name}}/{{package_name}}-data`\n",
    "\n",
    "Vous pouvez regarder à quoi ils ressemblent en utilisant la cellule ci-dessous pour les charger dans le notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, metadata_train = utils.read_csv(os.path.join(data_path, 'texts_train.csv'))\n",
    "df_test, metadata_test = utils.read_csv(os.path.join(data_path, 'texts_test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant faire une étape de préprocessing sur les textes afin de les normaliser et que le modèle performe mieux. Pour cela, nous allons utiliser le script `1_preprocess_data.py` situé ici : `{{package_name}}/{{package_name}}-scripts`\n",
    "\n",
    "Pour cela :\n",
    "\n",
    "Lancez un terminal\n",
    "Activez votre environnement virtuel\n",
    "Placer vous dans votre package dans le répertoire suivant:\n",
    "\n",
    "`cd {{package_name}}/{{package_name}}-scripts`\n",
    "\n",
    "Appelez le script de préprocessing: \n",
    "\n",
    "`python 1_preprocess_data.py -f texts_train.csv texts_test.csv --input_col text`\n",
    "\n",
    "Vous pouvez remarquer que nous devons préciser la colonne sur laquelle doit agir le préprocessing. Nous pouvons voir le résultat du preprocessing sur l'ensemble de test. Notez qu'une nouvelle colonne a été ajouté, la colonne 'preprocessed_text' qui contient le texte modifié."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_preprocessed, metadata_test_preprocessed = utils.read_csv(os.path.join(data_path, 'texts_test_preprocess_P1.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le préprocessing appliqué est celui fourni de base par le package mais il est très facile d'ajouter ses propres pipelined de preprocessing . Pour cela, ouvrez le fichier : `template_nlp/template_nlp/preprocessing/preprocess.py`. Dans ce module, vous voyez que l'ensemble des pipelines de preprocessing sont renseignés dans la fonction `get_preprocessors_dict`. C'est ici que vous pouvez rajouter vos pipelines personnalisées. \n",
    "\n",
    "On peut également jeter un coup d'oeil à la fonction `preprocess_sentence_P1` qui indique les différentes étapes de préprocessing (pour plus d'informations, il suffit de lire la documentation du package words_n_fun, une librairie facilitant tout le travail de manipulation de texte).\n",
    "\n",
    "Si vous avez l'oeil, vous pouvez remarquer que les accents n'ont pas été traités correctement pas la pipeline `preprocess_sentence_P1`. On va donc corrigez cela. Pour ce faire, il suffit de rajouter `'remove_accents'` en première étape dans la pipeline de la fonction `preprocess_sentence_P1`. Puis relancez : \n",
    "\n",
    "`python 1_preprocess_data.py -f texts_train.csv texts_test.csv --input_col text`\n",
    "\n",
    "Les accents sont maintenant correctement traités:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_preprocessed, metadata_test_preprocessed = utils.read_csv(os.path.join(data_path, 'texts_test_preprocess_P1.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant entraîner notre modèle en utilisant le script `2_training.py`\n",
    "\n",
    "Pour cela :\n",
    "\n",
    "Lancez un terminal\n",
    "Activez votre environnement virtuel\n",
    "Placer vous dans votre package dans le répertoire suivant:\n",
    "\n",
    "`cd {{package_name}}/{{package_name}}-scripts`\n",
    "\n",
    "Appelez le script d'entrainement: \n",
    "\n",
    "`python 2_training.py -f texts_train_preprocess_P1.csv --x_col preprocessed_text --y_col author`\n",
    "\n",
    "Cela va appliquer un simple TF-IDF + SVM sur les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pourrions utiliser ensuite le script `3_predict.py` pour utiliser le modèle mais nous allons procéder autrement pour avoir les résultats directement dans ce notebook. Si vous allez dans `{{package_name}}/{{package_name}}-models/model_tfidf_svm` vous allez voir un dossier du type `model_tfidf_svm_{YYYY_MM_DD-hh_mm_ss}` qui contient la sauvegarde du modèle que nous venons d'entraîner. \n",
    "\n",
    "Vous pouvez par exemple voir le f1_score sur l'ensemble d'entrainement, la matrice de confusion sur l'ensemble d'entraînement etc. Notez que nous aurions pu spécifier un dataset de validation au script `2_training.py` et nous aurions alors aussi accès à des métriques sur ce dataset. \n",
    "\n",
    "Ce dossier contient également les fichiers de sauvegarde du modèle que nous allons charger dans la cellule suivante (en remplacant bien sûr `{YYYY_MM_DD-hh_mm_ss}` par les nombres correspondant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, model_conf = utils_models.load_model('model_tfidf_svm_{YYYY_MM_DD-hh_mm_ss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voilà, le modèle est chargé, prêt à être utilisé. Ce que nous allons directement faire sur l'ensemble de test chargé plus haut :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_preprocessed['prediction'] = model.predict(df_test_preprocessed['preprocessed_text'])\n",
    "print('accuracy :', len(df_test_preprocessed[df_test_preprocessed['prediction']==df_test_preprocessed['author']])/len(df_test_preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais, il est tout nul ce modèle !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_word_sentence = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On constitue les phrases\n",
    "list_phrases = []\n",
    "for index, row in df_texts.iterrows():\n",
    "    text = row['text']\n",
    "    author = row['author']\n",
    "    book = row['book']\n",
    "    # Cette fonction transforme un texte en phrases\n",
    "    sentences = text_to_sentence(text, nb_word_sentence)\n",
    "    list_phrases = list_phrases+[(sentence, author, book) for sentence in sentences]\n",
    "df_phrases = pd.DataFrame(list_phrases, columns=['sentence', 'author', 'book'])\n",
    "set_author = set(df_texts['author'])\n",
    "\n",
    "# On regarde la répartition des phrases par auteur\n",
    "df_phrases.value_counts('author')/len(df_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On mélange le dataset\n",
    "df_to_split = df_phrases.sample(frac=1)\n",
    "\n",
    "# On sélectionne un livre par auteur qui sera dans l'ensemble de validation\n",
    "dict_books_to_valid = {}\n",
    "for author in set_author:\n",
    "    set_books = set(df_texts[df_texts['author']==author]['book'])\n",
    "    dict_books_to_valid[author] = random.sample(set_books, k=1)[0]\n",
    "df_valid = df_to_split[df_to_split['book'].isin(dict_books_to_valid.values())].copy()\n",
    "\n",
    "# On sélectionne un autre livre par auteur qui sera dans l'ensemble de test\n",
    "dict_books_to_test = {}\n",
    "for author in set_author:\n",
    "    set_books = set(df_texts[df_texts['author']==author]['book'])\n",
    "    set_books.remove(dict_books_to_valid[author])\n",
    "    dict_books_to_test[author] = random.sample(set_books, k=1)[0]\n",
    "df_test = df_to_split[df_to_split['book'].isin(dict_books_to_test.values())].copy()\n",
    "\n",
    "# Tout le reste est dans l'ensemble d'entrainement\n",
    "df_train = df_to_split.copy()\n",
    "df_train = df_train[~df_train['book'].isin(dict_books_to_valid.values())]\n",
    "df_train = df_train[~df_train['book'].isin(dict_books_to_test.values())]\n",
    "\n",
    "# On sauvegarde les datasets\n",
    "utils.to_csv(df_train, os.path.join(data_path, 'dataset_texts_train.csv'))\n",
    "utils.to_csv(df_valid, os.path.join(data_path, 'dataset_texts_valid.csv'))\n",
    "utils.to_csv(df_test, os.path.join(data_path, 'dataset_texts_test.csv'))\n",
    "\n",
    "# On regarde la répartition des phrases par auteur dans l'ensemble d'entrainement\n",
    "df_train.value_counts('author')/len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On fait un léger tuning\n",
    "dict_result = {}\n",
    "count =0\n",
    "for ngram_range in [(1, 1), (1, 2)]:\n",
    "    for C in [0.1, 0.5, 1, 2]:\n",
    "        model = ModelTfidfSvm(tfidf_params = {'ngram_range': ngram_range}, svc_params={'C':C})\n",
    "        model.fit(df_train['sentence'], df_train['author'], x_valid=df_valid['sentence'], y_valid=df_valid['author'])\n",
    "        df_train['pred'] = model.predict(df_train['sentence'])\n",
    "        df_valid['pred'] = model.predict(df_valid['sentence'])\n",
    "        score_train = f1_score(df_train['author'], df_train['pred'], average='macro')\n",
    "        score_val = f1_score(df_valid['author'], df_valid['pred'], average='macro')\n",
    "        dict_tmp = {'score_train':round(score_train, 5), 'score_val':round(score_val, 5), 'ngram_range':ngram_range, 'C':C}\n",
    "        dict_result[count] = dict_tmp.copy()\n",
    "        count += 1\n",
    "        print(dict_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On sélectionne le meilleur modèle\n",
    "model = ModelTfidfSvm(tfidf_params = {'ngram_range': (1, 2)}, svc_params={'C':1})\n",
    "model.fit(df_train['sentence'], df_train['author'], x_valid=df_valid['sentence'], y_valid=df_valid['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On prédit sur le test\n",
    "df_test['pred'] = model.predict(df_test['sentence'])\n",
    "score = f1_score(df_test['author'], df_test['pred'], average='macro')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(predict_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_result = {}\n",
    "# Pour chaque auteur\n",
    "for author, book in dict_books_to_test.items():\n",
    "    # Récupère le texte\n",
    "    row = df_texts[df_texts['book']==book]\n",
    "    text = row.iloc[0]['text']\n",
    "    length = len(text)\n",
    "    # Prédit l'auteur\n",
    "    prediction, counter, nb_sentences = predict_author(text, model, nb_word_sentence, perc_sample=1.0)\n",
    "    # Enregistre le résultat\n",
    "    dict_result[author] = {'prediction': prediction, 'counter': counter, 'length': len(text), 'nb_sentences':nb_sentences}\n",
    "# Vérifie si le résultat est bon pour tous les auteurs\n",
    "print({key==value['prediction'] for key, value in dict_result.items()})\n",
    "# Montre les résultats\n",
    "dict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Venv_template_nlp",
   "language": "python",
   "name": "venv_template_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
