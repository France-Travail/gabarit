{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template NUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisites:**\n",
    "\n",
    "- This notebook must have been generated using Gabarit's numerical template.\n",
    "\n",
    "\n",
    "- **Launch this notebook with a kernel using your project virtual environment**. In order to create a kernel linked to your virtual environment : `pip install ipykernel` and then `python -m ipykernel install --user --name=your_venv_name` (once your virtual environment is activated). Obviously, the project must be installed on this virtual environment\n",
    "\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. How this template works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why use gabarit's numerical template ?**\n",
    "\n",
    "The numerical template automatically generates a project folder and python code containing mainstream models and facilitating their industrialization.\n",
    "\n",
    "The generated project can be used for **classification** and **regression** tasks on numerical data. Of course, you have to adapt it to your particular use case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structure of the generated project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family: monospace; display: grid; grid-template-columns: 1fr 2fr;\">\n",
    "  <div>.                                </div>  <div style=\"color: green;\"></div>\n",
    "  <div>.                                </div>  <div style=\"color: green;\"></div>\n",
    "  <div>├── {{package_name}}             </div>  <div style=\"color: green;\"># The package</div>\n",
    "  <div>│ ├── models_training            </div>  <div style=\"color: green;\"># Folder containing all the modules related to the models</div>\n",
    "  <div>│ ├── monitoring                 </div>  <div style=\"color: green;\"># Folder containing all the modules related to the explainers and MLflow</div>\n",
    "  <div>│ └── preprocessing              </div>  <div style=\"color: green;\"># Folder containing all the modules related to the preprocessing</div>\n",
    "  <div>├── {{package_name}}-data        </div>  <div style=\"color: green;\"># Folder containing all the data (datasets, embeddings, etc.)</div>\n",
    "  <div>├── {{package_name}}-exploration </div>  <div style=\"color: green;\"># Folder where all your experiments and explorations must go</div>\n",
    "  <div>├── {{package_name}}-models      </div>  <div style=\"color: green;\"># Folder containing all the generated models</div>\n",
    "  <div>├── {{package_name}}-pipelines   </div>  <div style=\"color: green;\"># Folder containing fitted pipelines</div>\n",
    "  <div>├── {{package_name}}-ressources  </div>  <div style=\"color: green;\"># Folder containing some ressources such as the instructions to upload a model</div>\n",
    "  <div>├── {{package_name}}-scripts     </div>  <div style=\"color: green;\"># Folder containing examples script to preprocess data, train models, predict and use a demonstrator</div>\n",
    "  <div>│ └── utils                      </div>  <div style=\"color: green;\"># Folder containing utils scripts (such as split train/test, sampling, etc...)</div>\n",
    "  <div>├── {{package_name}}-tutorials    </div>  <div style=\"color: green;\"># Folder containing notebook tutorials, including this one</div>\n",
    "  <div>├── tests                        </div>  <div style=\"color: green;\"># Folder containing all the unit tests</div>\n",
    "  <div>├── .gitignore                   </div>  <div style=\"color: green;\"></div>\n",
    "  <div>├── .coveragerc                  </div>  <div style=\"color: green;\"></div>\n",
    "  <div>├── Makefile                     </div>  <div style=\"color: green;\"></div>\n",
    "  <div>├── nose_setup_coverage.cfg      </div>  <div style=\"color: green;\"></div>\n",
    "  <div>├── README.md                    </div>  <div style=\"color: green;\"></div>\n",
    "  <div>├── requirements.txt             </div>  <div style=\"color: green;\"></div>\n",
    "  <div>├── setup.py                     </div>  <div style=\"color: green;\"></div>\n",
    "  <div>└── version.txt                  </div>  <div style=\"color: green;\"></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General principles on the generated packages**\n",
    "\n",
    "- Data must be saved in the `{{package_name}}-data` folder<br>\n",
    "<br>\n",
    "- Trained models will automatically be saved in the `{{package_name}}-models` folder<br>\n",
    "<br>\n",
    "- Fitted pipelines will automatically be saved in the `{{package_name}}-pipelines` folder<br>\n",
    "<br>\n",
    "- Be aware that all the functions/methods for writing/reading files uses these two folders as base. Thus when a script has an argument for the path of a file/model, the given path should be **relative** to the `{{package_name}}-data`/`{{package_name}}-models` folders.<br>\n",
    "<br>\n",
    "- The provided scripts in `{{package_name}}-scripts` are given as example. You can use them as accelerators, but their use is not required.<br>\n",
    "<br>\n",
    "- You can use this package for mono-label and multi-labels tasks (`multi_label` argument in models' classes)<br>\n",
    "<br>\n",
    "- The modelling part is structured as follows :\n",
    "    - `ModelClass`: main class taking care of saving data and metrics (among other)\n",
    "    - `ModelPipeline`: child class of ModelClass managing all models related to a sklearn pipeline\n",
    "    - `ModelKeras`: child class of ModelClass managing all models using Keras<br>\n",
    "<br>\n",
    "- Each task (regression and classification) has a mixin class (`ModelRegressorMixin` and `ModelClassifierMixin`) and specific models located in corresponding subfolders.\n",
    "\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load utility functions\n",
    "\n",
    "Please run the following cell to load needed utility functions. These functions are only needed in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import utility functions\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(''))\n",
    "from tutorial_exercices import answers, verify, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use the template to train your first model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wine dataset\n",
    "\n",
    "[![glass of wine](https://archive.ics.uci.edu/ml/assets/MLimages/Large109.jpg)](https://archive.ics.uci.edu/ml/datasets/wine)\n",
    "\n",
    "We are going to use the generated python package in a classification problem. \n",
    "\n",
    "We are going to work with the [Wine recognition dataset](https://archive.ics.uci.edu/ml/datasets/wine) from [sklearn](https://scikit-learn.org/stable/datasets/toy_dataset.html#wine-recognition-dataset).\n",
    "This dataset results from a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars.\n",
    "\n",
    "First we load the dataset thanks to `sklearn.datasets.load_wine` and save the dataset as a csv file in `{{package_name}}-data` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from {{package_name}}.utils import get_data_path\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Load dataset\n",
    "wine_dataset = load_wine(as_frame=True)\n",
    "df_wine = wine_dataset[\"data\"]\n",
    "df_wine[\"target\"] = wine_dataset[\"target\"]\n",
    "\n",
    "# Save dataset to {{package_name}}-data\n",
    "DATA_PATH = get_data_path()\n",
    "DATASET_WINE_FILENAME = \"wine.csv\"\n",
    "DATASET_WINE_PATH = os.path.join(DATA_PATH, DATASET_WINE_FILENAME)\n",
    "df_wine.to_csv(DATASET_WINE_PATH, sep=\";\", index=None)\n",
    "print(f\"Wine dataset saved to {DATASET_WINE_PATH}\")\n",
    "\n",
    "# Display first rows\n",
    "df_wine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify that a new file called `wine.csv` is present in your `{{package_name}}-data` directory. Notice the use of `get_data_path` function from `{{package_name}}.utils`. It returns the data folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Exercice 1**</span> : **train / valid / test split**\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "- Split the main dataset in train / valid / test sets\n",
    "\n",
    "**TODO:**\n",
    "- Use the script `utils/0_split_train_valid_test.py` on the dataset `{{package_name}}-data/wine.csv`\n",
    "- We want a 'random' split but **with a random seed set to 42** (in order to always reproduce the same results)\n",
    "- We use the default splitting ratios (0.6 / 0.2 / 0.2)\n",
    "\n",
    "**Help:**\n",
    "- The file `utils/0_split_train_valid_test.py` splits a dataset in 3 .csv files:\n",
    "    - `{filename}_train.csv`: the training dataset\n",
    "    - `{filename}_valid.csv`: the validation dataset\n",
    "    - `{filename}_test.csv`: the test dataset\n",
    "- You can specify the type of split : random, stratified or hierarchical (here, use random)\n",
    "- Reminder: the path to the file to process is relative to `{{package_name}}-data`\n",
    "- To get the possible arguments of the script: `python 0_split_train_valid_test.py --help`\n",
    "- Don't forget to activate your virtual environment ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 1** : Verify your answer ✔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify.verify_exercice_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 1** : Solution 💡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers.answer_exercice_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Exercice 2**</span> : **random sample**\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "- Get a random sample of the file `wine.csv` (n=10) (we won't use it, this exercise is just here to show what can be done)\n",
    "\n",
    "**TODO:**\n",
    "- Use the script `utils/0_create_samples.py` on the dataset `{{package_name}}-data/wine.csv`\n",
    "- We want a sample of 10 lines\n",
    "\n",
    "**Help:**\n",
    "- The file `utils/0_create_samples.py` samples a dataset\n",
    "- To get the possible arguments of the script: `python 0_create_samples.py --help`\n",
    "- Don't forget to activate your virtual environment ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 2** : Verify your answer ✔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify.verify_exercice_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 2** : Solution 💡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers.answer_exercice_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Exercice 3**</span> : **EDA**\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "- Visualize train and test dataset statistics thanks to [Sweetviz](https://github.com/fbdesignpro/sweetviz)\n",
    "\n",
    "**TODO:**\n",
    "- Use the script `utils/0_sweetviz_report.py` to generate a Sweetviz report that compare train and test datasets\n",
    "\n",
    "**Help:**\n",
    "- To get the possible arguments of the script: `python 0_sweetviz_report.py --help`\n",
    "- Use `wine_train.csv` as source and `wine_test.csv` as comparison.\n",
    "- Don't forget to activate your virtual environment ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 3** : Verify your answer ✔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify.verify_exercice_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 3** : Solution 💡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers.answer_exercice_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Exercice 4**</span> : **pre-processing**\n",
    "\n",
    "- The script `1_preprocess_data.py` applies a preprocessing pipeline **to all columns of specified files except target columns**\n",
    "- The argument `--target_cols` is used to specify which columns are targets in order to preserve these columns. Do not forget\n",
    "to specify that `target` is the target column.\n",
    "- It works as follows:\n",
    "    - In `preprocessing/preprocess.py`: \n",
    "        - There is a dictionary of functions (`pipelines_dict`): key: str -> function \n",
    "            - /!\\ Don't remove the default element 'no_preprocess': lambda x: x /!\\ \n",
    "        - There are preprocessing functions\n",
    "    - In `1_preprocess_data.py` :\n",
    "        - We retrieve the dictionary of functions from `preprocessing/preprocess.py` \n",
    "        - If a `preprocessing` argument is specified, we keep only the corresponding key from the dictionnary \n",
    "        - Otherwise, we keep all keys (except `no_preprocess`) \n",
    "        - For each entry of the dictionary, we:\n",
    "            - Get the associated preprocessing function\n",
    "            - Load data\n",
    "            - apply the preprocessing function\n",
    "            - Save the result -> {file_name}_{key}.csv \n",
    "- To get the possible arguments of the script: `python 1_preprocess_data.py --help`\n",
    "- Usually, pipelines are sklearn's ColumnTransformer objects :\n",
    "    - These transformers must specify input columns.\n",
    "    - Hence, even if all columns are loaded, some might just get removed if no transformer use it.\n",
    "    - One can specify the option remainder='passthrough' in the ColumnTransformer to keep all input columns (even the preprocessed one).\n",
    "    - Otherwise, it's easy to add a passtrough transformer to keep specific columns with no preprocess.\n",
    "- We provide an example with the `preprocess_P1`. It uses different pipelines for numerical, categorical (commented), and textual data (commented).\n",
    "- The `preprocess_P1` pipeline is displayed below (just run the next cell).\n",
    "- Don't forget to activate your virtual environment ...\n",
    "\n",
    "**Important:**\n",
    "- Each preprocessed file is saved in the `{{package_name}}-data` folder.\n",
    "- To track which pipeline has been used, we add a first line to these files as a metadata line (e.g. `#preprocess_P1_2022_09_08-17_35_06`).\n",
    "- Each fitted pipeline is saved in the `{{package_name}}-pipelines` folder.\n",
    "- If you need to use a custom preprocessing function `funcA` using `FunctionTransformer`, be aware that the pickled pipeline may not return wanted results if you later modify `funcA` definition. Please check https://github.com/OSS-Pole-Emploi/gabarit/issues/63."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from {{package_name}}.preprocessing.preprocess import preprocess_P1\n",
    "\n",
    "# Source code of preprocess_P1 function\n",
    "utils.display_source(preprocess_P1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the default pipeline `preprocess_P1` uses one transformer (called `num`, that's just a name), that applies the pipeline `numeric_pipeline` on all column with dtype `number`.  \n",
    "\n",
    "This pipeline uses a `SimpleImputer` to fill NA values with the median and applies a `StandardScaler`.\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "- Apply the default preprocessing to `wine.csv`\n",
    "\n",
    "**TODO:**\n",
    "- Use the script `1_preprocess_data.py` on the dataset `{{package_name}}-data/wine.csv` to apply the default pipeline (`preprocess_P1`)\n",
    "- The target column `target` should be preserved by the preprocessing script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 4** : Verify your answer ✔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify.verify_exercice_4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 4** : Solution 💡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers.answer_exercice_4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Exercice 5**</span> : **custom pre-processing**\n",
    "\n",
    "\n",
    "We are going to use a [`sklearn.preprocessing.MinMaxScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler) instead of the [`sklearn.preprocessing.StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) used in `preprocess_P1` to pre-process the train dataset.\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "- Apply a new preprocess with `MinMaxScaler` to the train dataset\n",
    "\n",
    "**TODO:**\n",
    "\n",
    "- Create a `preprocess_P2` function in `{{package_name}}/preprocessing/preprocess.py` that use `MinMaxScaler` instead of `StandardScaler`\n",
    "- Add `preprocess_P2` in `get_pipelines_dict` from `{{package_name}}/preprocessing/preprocess.py`\n",
    "- Use the script `1_preprocess_data.py` on the dataset `{{package_name}}-data/wine.csv` to apply `preprocess_P2` pipeline\n",
    "\n",
    "**Help:**\n",
    "\n",
    "- You can use the source code of `preprocess_P1` seen in the previous exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 5** : Verify your answer ✔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify.verify_exercice_5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 5** : Solution 💡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers.answer_exercice_5_preprocess_P2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers.answer_exercice_5_preprocess_script()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Exercice 6**</span> : **Pre-processing on train and validation data**\n",
    "\n",
    "In the previous exercice we used `1_preprocess_data.py` to apply a preprocessing pipeline to `wine.csv`. To prevent [data leakage](https://machinelearningmastery.com/data-leakage-machine-learning/) we should preprocess training data separately from validation and test data. \n",
    "\n",
    "Each time `1_preprocess_data.py` is called, a new folder is created in `{{package_name}}-pipelines`. It contains a pickled pipeline that can be used in `2_apply_existing_pipeling.py` to transform the validation dataset.\n",
    "\n",
    "We are going to use `1_preprocess_data.py` to apply the default `preprocess_P1` to `wine_train.csv` and then apply the saved pipeline to `wine_valid.csv`. \n",
    "\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "\n",
    "- Fit a `preprocess_P1` pipeline to the train dataset.\n",
    "- Apply the fitted pipeline to the validation dataset.\n",
    "\n",
    "\n",
    "**TODO:**\n",
    "\n",
    "- Apply the default `preprocess_P1` to `wine_train.csv` (previously we did it on the whole dataset, which is wrong).\n",
    "- Find the name of the fitted pipeline inside {{package_name}}-pipelines.\n",
    "- Use the script `2_apply_existing_piepline.py` to transform the validation set `wine_valid.csv` using the fitted `preprocess_P1` pipeline.\n",
    "\n",
    "\n",
    "**Important:**\n",
    "\n",
    "- Do not worry about applying the fitted pipeline to `wine_test.csv`. Our models will store the preprocessing pipelines and :\n",
    "    - The prediction script `4_predict.py` will preprocess the test dataset with the model's preprocessing pipeline before sending the data to the model's predict function. This is the **batch mode**.\n",
    "    - We also expose an agnostic `predict` function (in `utils_models`) to handle new data on the fly. It will preprocess it with the model's preprocessing pipeline before sending the data to the model's predict function. This is the **API mode**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 6** : Verify your answer ✔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify.verify_exercice_6()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 6** : Solution 💡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers.answer_exercice_6()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Exercice 7**</span> : **Train a model**\n",
    "\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "- Train a classification model on the preprocessed data. \n",
    "\n",
    "- Use the default model in `3_training_classification.py` : [`ModelRidgeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html).\n",
    "\n",
    "**TODO:**\n",
    "\n",
    "- Use the script `3_training_classification.py` to train a `ModelRidgeClassifier` on `wine_train_preprocess_P1.csv`\n",
    "- Use `wine_valid_preprocess_P1.csv` as validation dataset\n",
    "\n",
    "**Help:**\n",
    "\n",
    "- Use `3_training_classification.py -h` to see CLI helper.\n",
    "- The script `3_training_classification.py` trains a model on a dataset\n",
    "- It works as follows:\n",
    "    - Reads a train .csv file as input \n",
    "        - If a validation file is given, it will use it as validation data \n",
    "    - Manages `y_col` argument: \n",
    "        - If there is only one value, training in mono-label mode \n",
    "        - If several values, training in multi-labels mode \n",
    "    - Instantiates a model class\n",
    "    - Fits the model\n",
    "    - Saves the model & some metrics\n",
    "- **Manual modifications of the script**: \n",
    "    - **To change the model used** -> you have to comment / uncomment / modify the code in the \"training\" part (not compulsory for this exercise) \n",
    "    - **To load datasets** -> if a dataset is not in the right format, you have to adapt the loading part (not compulsory for this exercise) \n",
    "- Don't forget to activate your virtual environment ...\n",
    "- If you get an `pkg_resources.DistributionNotFound`, you should try to install {{package_name}} : `pip install -e .` when in the folder containing the setup.py\n",
    "\n",
    "\n",
    "**Important:**\n",
    "\n",
    "- If you try to use this script with not preprocessed data, it will consider the preprocessing pipeline to be a 'passtrough', i.e. no preprocessing & all columns kept.\n",
    "- The training script will raise a warning if train & validation dataset do not have the same preprocessing pipeline. We still continue even if it will probably lead to poor results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 7** : Verify your answer ✔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify.verify_exercice_7()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 7** : Solution 💡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers.answer_exercice_7()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Exercice 8**</span> : **Try another classification model**\n",
    "\n",
    "The previous model already achieves a perfect score on validation dataset but the goal here is to see how to use a different kind of model for training.\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "Train a `ModelLGBMClassifier` classification model on preprocessed data. \n",
    "\n",
    "**TODO:**\n",
    "\n",
    "- Change the script `3_training_classification.py` so it uses a `ModelLGBMClassifier`\n",
    "- Use the script `3_training_classification.py` to train `ModelLGBMClassifier` a on `wine_train_preprocess_P1.csv` with `wine_valid_preprocess_P1.csv` as validation dataset\n",
    "\n",
    "**Help:**\n",
    "\n",
    "If you look at `3_training_classification.py` you will see that a lot of models are commented :\n",
    "\n",
    "```python\n",
    "model = model_ridge_classifier.ModelRidgeClassifier(\n",
    "    x_col=x_col,\n",
    "    y_col=y_col,\n",
    "    level_save=level_save,\n",
    "    preprocess_pipeline=preprocess_pipeline,\n",
    "    ridge_params={\"alpha\": 1.0},\n",
    "    multi_label=multi_label,\n",
    ")\n",
    "# model = model_logistic_regression_classifier.ModelLogisticRegressionClassifier(\n",
    "#     x_col=x_col,\n",
    "#     y_col=y_col,\n",
    "#     level_save=level_save,\n",
    "#     preprocess_pipeline=preprocess_pipeline,\n",
    "#     lr_params={\"penalty\": \"l2\", \"C\": 1.0, \"max_iter\": 100},\n",
    "#     multi_label=multi_label,\n",
    "# )\n",
    "#\n",
    "# [...]\n",
    "```\n",
    "Comment and uncomment proper lines to use `ModelLGBMClassifier` instead of `ModelRidgeClassifier`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 8** : Verify your answer ✔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify.verify_exercice_8()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 8** : Solution 💡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers.answer_exercice_8()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see this model do not perform perfectly on validation dataset, probably due to overfitting. We are going to stick with our `ModelRidgeClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Exercice 9**</span> : **Test your model on the test dataset**\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "- Use your `ModelRidgeClassifier` model to predict on the test dataset\n",
    "\n",
    "**TODO:**\n",
    "\n",
    "- Use the script `4_predict.py` to make prediction about cultivars in test data.\n",
    "- Use argument `[-y Y_COL [Y_COL ...]]` to obtain performance on test data.\n",
    "\n",
    "**Help:**\n",
    "\n",
    "- Use `4_predict.py -h` to see CLI helper.\n",
    "- You **DO NOT** need to preprocess the test data ! As we said above, the preprocessing pipeline is saved alongside the model, and the script will preprocess the test data before sending it to the model's predict function.\n",
    "- If you get an `ValueError: There are some missing mandatory columns`, you probably made predictions on a preprocessed version of `wine_test.csv` when you should have made predictions on `wine_test.csv`. This error is caused by the first row of the preprocessed file which contains metadata infos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 9** : Verify your answer ✔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify.verify_exercice_9()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 9** : Solution 💡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers.answer_exercice_9()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "\n",
    "## 3. Use a saved model in python\n",
    "\n",
    "In this section, we will see how to load a saved model in python and use it with new data\n",
    "\n",
    "### Load a saved model\n",
    "\n",
    "First choose one of your saved models :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from {{package_name}}.utils import get_models_path\n",
    "from {{package_name}}.models_training import utils_models\n",
    "\n",
    "MODELS_PATH = Path(get_models_path())\n",
    "\n",
    "# This line list saved model in {{package_name}}-models\n",
    "saved_model_names = sorted([model.name for model in MODELS_PATH.glob(\"*/*\")])\n",
    "print(\"\\n\".join(saved_model_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then load it with `utils_models.load_model` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = saved_model_names[-1]\n",
    "print(model_name)\n",
    "\n",
    "model, model_conf = utils_models.load_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "wine_dataset = load_wine(as_frame=True)\n",
    "\n",
    "wine_dataset[\"data\"].sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model` object as a `preprocess_pipeline` argument that can be used in combination to `utils_models.apply_pipeline` to apply the same preprocessing on the new data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_dataset_prep = utils_models.apply_pipeline(wine_dataset[\"data\"], model.preprocess_pipeline)\n",
    "wine_dataset_prep.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then simply use `model.predict` to make predictions : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1 = model.predict(wine_dataset_prep)\n",
    "\n",
    "# Verifying accuracy :\n",
    "accuracy1 = sum(predictions1 == wine_dataset[\"target\"].astype(str)) / predictions1.shape[0]\n",
    "print(f\"Accuracy v1 : {accuracy1:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on new data - using the `utils_models.predict` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative is to use the provided (model agnostic) `utils_models.predict` function.  \n",
    "\n",
    "This function **does not need the data to be preprocessed**. Everthing is managed inside the function, you just have to provide the dataset and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = utils_models.predict(wine_dataset[\"data\"], model)\n",
    "\n",
    "# Verifying accuracy :\n",
    "accuracy2 = sum(predictions2 == wine_dataset[\"target\"].astype(str)) / len(predictions2)\n",
    "print(f\"Accuracy v2 : {accuracy2:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "## 4. Use the template for regression\n",
    "\n",
    "In previous sections we saw how to train a model to solve classification problem thanks to the script `3_training_classification.py`. Here we are going to see how to use `3_training_regression.py` script for regression.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reuse of wine dataset\n",
    "\n",
    "[![glass of wine](https://archive.ics.uci.edu/ml/assets/MLimages/Large109.jpg)](https://archive.ics.uci.edu/ml/datasets/wine)\n",
    "\n",
    "We are going to reuse the wine dataset but instead of predicting cultivars, we are going to predict alcohol content of samples based on the other constituents and the cultivar.\n",
    "\n",
    "We first reload the dataset and save it as `wine_reg.csv` in order to avoid conflicts with previous datasets : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload wine dataset\n",
    "wine_dataset = load_wine(as_frame=True)\n",
    "df_wine_reg = wine_dataset[\"data\"]\n",
    "df_wine_reg[\"cultivar\"] = wine_dataset[\"target\"]\n",
    "del wine_dataset[\"target\"]\n",
    "\n",
    "# Save it as wine_reg.csv\n",
    "WINE_REG_PATH = os.path.join(DATA_PATH, \"wine_reg.csv\")\n",
    "df_wine_reg.to_csv(WINE_REG_PATH, sep=\";\", index=None)\n",
    "print(f\"Wine regression dataset saved to {WINE_REG_PATH}\")\n",
    "\n",
    "# See first rows\n",
    "df_wine_reg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Exercice 10**</span> : **Preprocess, train and predict**\n",
    "\n",
    "**Goal:**\n",
    "- Use everything we learned from previous exercices to create a regressor model that is capable of predicting alcohol based on cultivar and other constituents.\n",
    "- Split `wine_reg.csv`, adapt the preprocess step and train a regressor to make predictions on test data\n",
    "\n",
    "**TODO:**\n",
    "\n",
    "- Split `wine_reg.csv` into train / valid / test datasets thanks to `utils/0_split_train_valid_test.py`\n",
    "- Create a `preprocess_P3` pipeline to handle `cultivar` feature wich is a categorical column.\n",
    "    - We want this variable to be one hot encoded.\n",
    "    - Help yourself with comments from `preprocess_P1` source code and [scikit-learn pipeline documentation](https://scikit-learn.org/stable/modules/compose.html).\n",
    "- Preprocess `wine_reg_train.csv` with your `preprocess_P3` pipeline thanks to `1_preprocess_data.py`\n",
    "- Apply preprocess pipeline on `wine_reg_valid.csv` thanks to `2_apply_existing_pipeline.py`\n",
    "- Comment / uncomment `3_training_regression.py` to use a [`ModelKNNRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)\n",
    "- Train a regressor model on `wine_reg_train_preprocess_P3.csv` with `wine_reg_valid_preprocess_P3.csv` as validation dataset thanks to `3_training_regression.py`\n",
    "- Make prediction on `wine_reg_test.csv` thanks to `4_predict.py`\n",
    "\n",
    "**Help:**\n",
    "\n",
    "- Each script has a CLI helper.\n",
    "- `cultivar` column is also of dtype `number`. We can ignore it in the numeric pipeline by adding a negative pattern to the `make_column_selector` function. (e.g. `make_column_selector(pattern=\"^(?!cultivar).*$\", dtype_include=\"number\")`)\n",
    "- To apply an OHE to `cultivar`, you will need to create a new pipeline and a new transformer. You can use the lines below :\n",
    "    ```python\n",
    "    ...\n",
    "    cat_pipeline = make_pipeline(SimpleImputer(strategy='most_frequent'), OneHotEncoder(handle_unknown='ignore'))\n",
    "    ...\n",
    "    transformers = [\n",
    "        # ...\n",
    "        (\"cat\", cat_pipeline, make_column_selector(pattern=\"cultivar\"), ),\n",
    "    ]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 10** : Verify your answer ✔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify.verify_exercice_10()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 10** : Solution 💡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers.answer_exercice_10_preprocess_P3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers.answer_exercice_10_scripts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "\n",
    "## 5. BONUS : Start up a small web app to introduce your models 🚀 \n",
    "\n",
    "You are now ready to demonstrate how good your models work. We implemented a default ***Streamlit*** app., let's try it !\n",
    "\n",
    "```bash\n",
    "# do not forget to activate your virtual environment\n",
    "# source venv_num_template/bin/activate \n",
    "\n",
    "streamlit run {{package_name}}-scripts/5_demonstrator.py\n",
    "```\n",
    "\n",
    "It will start a Streamlit app on the default port (8501).\n",
    "\n",
    "Visit [http://localhost:8501](http://localhost:8501) to see you demonstrator.\n",
    "\n",
    "*Note: the default demonstrator consider all input columns as numbers. Hence, you should adapt it to your dataset if you have other types of data.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('venv_{{package_name}}': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c4ce3d9346c91fa52fdd6bd8efd9ec857e7dd4f4537302caacbefa9813d6c40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
