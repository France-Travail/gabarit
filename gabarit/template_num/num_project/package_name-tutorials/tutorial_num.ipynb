{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical template\n",
    "\n",
    "**Prerequisites:**\n",
    "\n",
    "- This notebook must have been generated using the Gabarit's numerical template. \n",
    "- **Launch this notebook with a kernel using your project virtual environment**. In order to create a kernel linked to your virtual environment : `pip install ipykernel --user --name=your_venv_name` (once your virtual environment is activated). Obviously, the project must be installed on this virtual environment\n",
    "\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. How this template works\n",
    "\n",
    "### Why use gabarit's numerical template ?\n",
    "\n",
    "The numerical template automatically generates a project folder and python code containing mainstream models and facilitating their industrialization.\n",
    "\n",
    "The generated project can be used for **classification** and **regression** tasks on numerical data. Of course, you have to adapt it to your particular use case. \n",
    "\n",
    "### Structure of the generated project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"font-family: monospace;\">\n",
    "    <tr>\n",
    "        <td style=\"border: none; padding: 0px;\">.</td>\n",
    "        <td style=\"border: none; padding: 0px;\"></td>\n",
    "        <td style=\"border: none; padding: 0px; color: green;\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: none; padding: 0px;\">├──</td>\n",
    "        <td style=\"border: none; padding: 0px;\">{{package_name}}</td>\n",
    "        <td style=\"border: none; padding: 0px; color: green;\"># The package</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: none; padding: 0px;\">│├──</td>\n",
    "        <td style=\"border: none; padding: 0px;\">models_training</td>\n",
    "        <td style=\"border: none; padding: 0px; color: green;\"># Folder containing all the modules related to the models</td>\n",
    "    </tr>\n",
    "        <td style=\"border: none; padding: 0px;\">│├──</td>\n",
    "        <td style=\"border: none; padding: 0px;\">monitoring</td>\n",
    "        <td style=\"border: none; padding: 0px; color: green;\"># Folder containing all the modules related to the explainers and MLflow</td>\n",
    "    </tr>\n",
    "    </tr>\n",
    "        <td style=\"border: none; padding: 0px;\">│└──</td>\n",
    "        <td style=\"border: none; padding: 0px;\">preprocessing</td>\n",
    "        <td style=\"border: none; padding: 0px; color: green;\"># Folder containing all the modules related to the preprocessing</td>\n",
    "    </tr>\n",
    "    </tr>\n",
    "        <td style=\"border: none; padding: 0px;\">├──</td>\n",
    "        <td style=\"border: none; padding: 0px;\">{{package_name}}-data</td>\n",
    "        <td style=\"border: none; padding: 0px; color: green;\"># Folder containing all the data (datasets, embeddings, etc.)</td>\n",
    "    </tr>\n",
    "    </tr>\n",
    "        <td style=\"border: none; padding: 0px;\">├──</td>\n",
    "        <td style=\"border: none; padding: 0px;\">{{package_name}}-exploration</td>\n",
    "        <td style=\"border: none; padding: 0px; color: green;\"># Folder where all your experiments and explorations must go</td>\n",
    "    </tr>\n",
    "    </tr>\n",
    "        <td style=\"border: none; padding: 0px;\">├──</td>\n",
    "        <td style=\"border: none; padding: 0px;\">{{package_name}}-models</td>\n",
    "        <td style=\"border: none; padding: 0px; color: green;\"># Folder containing all the generated models</td>\n",
    "    </tr>\n",
    "    </tr>\n",
    "        <td style=\"border: none; padding: 0px;\">├──</td>\n",
    "        <td style=\"border: none; padding: 0px;\">{{package_name}}-pipelines</td>\n",
    "        <td style=\"border: none; padding: 0px; color: green;\"># Folder containing pipelines</td>\n",
    "    </tr>\n",
    "    </tr>\n",
    "        <td style=\"border: none; padding: 0px;\">├──</td>\n",
    "        <td style=\"border: none; padding: 0px;\">{{package_name}}-ressources</td>\n",
    "        <td style=\"border: none; padding: 0px; color: green;\"># Folder containing some ressources such as the instructions to upload a model</td>\n",
    "    </tr>\n",
    "    </tr>\n",
    "        <td style=\"border: none; padding: 0px;\">├──</td>\n",
    "        <td style=\"border: none; padding: 0px;\">{{package_name}}-scripts</td>\n",
    "        <td style=\"border: none; padding: 0px; color: green;\"># Folder containing examples script to preprocess data, train models, predict and use a demonstrator</td>\n",
    "    </tr>\n",
    "    </tr>\n",
    "        <td style=\"border: none; padding: 0px;\">│└──</td>\n",
    "        <td style=\"border: none; padding: 0px;\">utils</td>\n",
    "        <td style=\"border: none; padding: 0px; color: green;\"># Folder containing utils scripts (such as split train/test, sampling, etc...)</td>\n",
    "    </tr>\n",
    "    </tr>\n",
    "        <td style=\"border: none; padding: 0px;\">├──</td>\n",
    "        <td style=\"border: none; padding: 0px;\">{{package_name}}-tutorial</td>\n",
    "        <td style=\"border: none; padding: 0px; color: green;\"># Folder containing notebook tutorials, including this one</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: none; padding: 0px;\">├──</td>\n",
    "        <td style=\"border: none; padding: 0px;\">tests</td>\n",
    "        <td style=\"border: none; padding: 0px; color: green;\"># Folder containing all the unit tests</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: none; padding: 0px;\">├──</td>\n",
    "        <td style=\"border: none; padding: 0px;\">.gitignore</td>\n",
    "        <td style=\"border: none; padding: 0px; color: green;\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: none; padding: 0px;\">├──</td>\n",
    "        <td style=\"border: none; padding: 0px;\">.coveragerc</td>\n",
    "        <td style=\"border: none; padding: 0px; color: green;\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: none; padding: 0px;\">├──</td>\n",
    "        <td style=\"border: none; padding: 0px;\">Makefile</td>\n",
    "        <td style=\"border: none; padding: 0px; color: green;\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: none; padding: 0px;\">├──</td>\n",
    "        <td style=\"border: none; padding: 0px;\">nose_setup_coverage.cfg</td>\n",
    "        <td style=\"border: none; padding: 0px; color: green;\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: none; padding: 0px;\">├──</td>\n",
    "        <td style=\"border: none; padding: 0px;\">README.md</td>\n",
    "        <td style=\"border: none; padding: 0px; color: green;\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: none; padding: 0px;\">├──</td>\n",
    "        <td style=\"border: none; padding: 0px;\">requirements.txt</td>\n",
    "        <td style=\"border: none; padding: 0px; color: green;\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: none; padding: 0px;\">├──</td>\n",
    "        <td style=\"border: none; padding: 0px;\">setup.py</td>\n",
    "        <td style=\"border: none; padding: 0px; color: green;\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: none; padding: 0px;\">└──</td>\n",
    "        <td style=\"border: none; padding: 0px;\">version.txt</td>\n",
    "        <td style=\"border: none; padding: 0px; color: green;\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General principles on the generated packages\n",
    "\n",
    "- Data must be saved in the `{{package_name}}-data` folder\n",
    "\n",
    "- Trained models will automatically be saved in the `{{package_name}}-models` folder\n",
    "\n",
    "- Be aware that all the functions/methods for writing/reading files uses these two folders as base. Thus when a script has an argument for the path of a file/model, the given path should be **relative** to the `{{package_name}}-data`/`{{package_name}}-models` folders.\n",
    "\n",
    "- The provided scripts in `{{package_name}}-scripts` are given as example. You can use them to help you develop but their use is not required.\n",
    "\n",
    "- You can use this package for mono-label and multi-labels tasks (`multi_label` argument in models' classes)\n",
    "\n",
    "- The modelling part is structured as follows :\n",
    "    - `ModelClass`: main class taking care of saving data and metrics (among other)\n",
    "    - `ModelPipeline`: child class of ModelClass managing all models related to a sklearn pipeline\n",
    "    - `ModelKeras`: child class of ModelClass managing all models using Keras\n",
    "    - `ModelPyTorch`: child class of ModelClass managing all models using Torch (experimental)\n",
    "    \n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(''))\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from {{package_name}}.utils import get_data_path\n",
    "from tutorial_exercices import answers, verify, utils\n",
    "\n",
    "# La fonction get_data_path rebviue \n",
    "DATA_PATH = get_data_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use the template to train your first model\n",
    "\n",
    "### Wine dataset\n",
    "We are going to use generated python package in a classification problem. \n",
    "\n",
    "We are going to work with [Wine recognition dataset](https://archive.ics.uci.edu/ml/datasets/wine) from [sklearn](https://scikit-learn.org/stable/datasets/toy_dataset.html#wine-recognition-dataset).\n",
    "This dataset result from a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars.\n",
    "\n",
    "First we load the dataset thanks to `sklearn.datasets.load_wine` and save the dataset as a csv file in `{{package_name}}-data` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_dataset = load_wine(as_frame=True)\n",
    "\n",
    "df_wine = wine_dataset[\"data\"]\n",
    "df_wine[\"target\"] = wine_dataset[\"target\"]\n",
    "\n",
    "# Save dataset to {{package_name}}-data\n",
    "DATA_PATH = get_data_path()\n",
    "DATASET_WINE_FILENAME = \"wine.csv\"\n",
    "DATASET_WINE_PATH = os.path.join(DATA_PATH, DATASET_WINE_FILENAME)\n",
    "\n",
    "df_wine.to_csv(DATASET_WINE_PATH, sep=\";\", index=None)\n",
    "print(f\"Wine dataset saved to {DATASET_WINE_PATH}\")\n",
    "\n",
    "df_wine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify that a new file called `wine.csv` is present in your `{{package_name}}-data` directory. Notice the use of `get_data_path` function from `{{package_name}}.utils`. It returns the data folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1 : train / valid / test split\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "- Split the main dataset in train / valid / test sets\n",
    "\n",
    "**TODO:**\n",
    "- Use the script `utils/0_split_train_valid_test.py` on the dataset `{{package_name}}-data/wine.csv`\n",
    "- We want a 'random' split but **with a random seed set to 42** (in order to always reproduce the same results)\n",
    "- We use the default splitting ratios (0.6 / 0.2 / 0.2)\n",
    "\n",
    "**Help:**\n",
    "- The file `utils/0_split_train_valid_test.py` splits a dataset in 3 .csv files:\n",
    "    - `{filename}_train.csv`: the training dataset\n",
    "    - `{filename}_valid.csv`: the validation dataset\n",
    "    - `{filename}_test.csv`: the test dataset\n",
    "- You can specify the type of split : random, stratified or hierarchical (here, use random)\n",
    "- Reminder: the path to the file to process is relative to `{{package_name}}-data`\n",
    "- To get the possible arguments of the script: `python 0_split_train_valid_test.py --help`\n",
    "- Don't forget to activate your virtual environment ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 1 : Verify your answer ✔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify.verify_exercice_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 1 : Solution 💡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers.answer_exercice_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2 : random sample\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "- Get a random sample of the file `wine.csv` (n=10) (we won't use it, this exercise is just here to show what can be done)\n",
    "\n",
    "**TODO:**\n",
    "- Use the script `utils/0_create_samples.py` on the dataset `{{package_name}}-data/wine.csv`\n",
    "- We want a sample of 10 lines\n",
    "\n",
    "**Help:**\n",
    "- The file `utils/0_create_samples.py` samples a dataset\n",
    "- To get the possible arguments of the script: `python 0_create_samples.py --help`\n",
    "- Don't forget to activate your virtual environment ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 2 : Verify your answer ✔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify.verify_exercice_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 2 : Solution 💡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers.answer_exercice_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3 : pre-processing\n",
    "\n",
    "- The script `1_preprocess_data.py` applies a preprocessing pipeline **to all columns of specified files except target columns**\n",
    "- The argument `--target_cols` is used to specify which columns are targets in order to preserve these columns. Do not forget\n",
    "to scpeify that `target` is the target column.\n",
    "- It works as follows:\n",
    "    - In `preprocessing/preprocess.py`: \n",
    "        - There is a dictionary of functions (`pipelines_dict`): key: str -> function \n",
    "            - /!\\ Don't remove the default element 'no_preprocess': lambda x: x /!\\ \n",
    "        - There are preprocessing functions\n",
    "    - In `1_preprocess_data.py` :\n",
    "        - We retrieve the dictionary of functions from `preprocessing/preprocess.py` \n",
    "        - If a `preprocessing` argument is specified, we keep only the corresponding key from the dictionnary \n",
    "        - Otherwise, we keep all keys (except `no_preprocess`) \n",
    "        - For each entry of the dictionary, we:\n",
    "            - Get the associated preprocessing function\n",
    "            - Load data\n",
    "            - apply the preprocessing function\n",
    "            - Save the result -> {file_name}_{key}.csv \n",
    "- To get the possible arguments of the script: `python 1_preprocess_data.py --help`\n",
    "- By default the preprocessing pipeline used is `preprocess_P1`, you can see its content below\n",
    "- Don't forget to activate your virtual environment ...\n",
    "\n",
    "**Important:**\n",
    "- Each preprocessed file is saved in the `{{package_name}}-data` folder.\n",
    "- To track which preprocessed has been done, we add a first line to these files as a metadata line (e.g. `#preprocess_P1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from {{package_name}}.preprocessing.preprocess import preprocess_P1\n",
    "\n",
    "# Source code of preprocess_P1 function\n",
    "utils.display_source(preprocess_P1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the default pipeline `preprocess_P1` works on numerical columns (keyword `num`), and apply `numeric_pipeline` to them which fill NA values with the median and apply a `StandardScaler`.\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "- Apply the default preprocessing to `wine.csv`\n",
    "\n",
    "**TODO:**\n",
    "- Use the script `1_preprocess_data.py` on the dataset `{{package_name}}-data/wine.csv` to apply the default pipeline (`preprocess_P1`)\n",
    "- The target column `target` should be preserved by the preprocessing script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 3 : Verify your answer ✔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify.verify_exercice_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 3 : Solution 💡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers.answer_exercice_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4 : custom pre-processing\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "We are going to use a [`sklearn.preprocessing.MinMaxScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler) instead of the [`sklearn.preprocessing.StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) used in `preprocess_P1` to pre-process wine data\n",
    "\n",
    "**TODO:**\n",
    "\n",
    "- Create a `preprocess_P2` function in `{{package_name}}/preprocessing/preprocess.py` that use `MinMaxScaler` instead of `StandardScaler`\n",
    "- Add `preprocess_P2` in `get_pipelines_dict` from `{{package_name}}/preprocessing/preprocess.py`\n",
    "- Use the script `1_preprocess_data.py` on the dataset `{{package_name}}-data/wine.csv` to apply `preprocess_P2` pipeline\n",
    "\n",
    "**Help:**\n",
    "\n",
    "- You can use the source code of `preprocess_P1` seen in the previous exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 4 : Verify your answer ✔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify.verify_exercice_4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 4 : Solution 💡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers.answer_exercice_4_preprocess_P2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers.answer_exercice_4_preprocess_script()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5 : Pre-processing on train and validation data\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "In the previous exercice we used `1_preprocess_data.py` to apply a preprocessing pipeline to `wine.csv`. To prevent [data leakage](https://machinelearningmastery.com/data-leakage-machine-learning/) we should preprocess training data separately from validation and test data. \n",
    "\n",
    "Each time `1_preprocess_data.py` is called, a new folder is created in `{{package_name}}-pipelines`. It contains a pipeline pickled object that can be used by `2_apply_existing_pipeling.py` script to apply a pipeline to a file.\n",
    "\n",
    "We are going to use `1_preprocess_data.py` to apply default `preprocess_P1` to `wine_train.csv` and then apply the saved pipeline to `wine_valid.csv`. \n",
    "\n",
    "Note : Do not worry about applying `preprocess_P1` to `wine_test.csv`, it will be automatically when we will make predictions\n",
    "\n",
    "**TODO:**\n",
    "\n",
    "- Use the script `1_preprocess_data.py` to apply `preprocess_P1` pipeline on the dataset `wine_train.csv`\n",
    "- Use the script `2_apply_existing_piepline.py` to apply saved `preprocess_P1` pipeline to `wine_valid.csv`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 5 : Verify your answer ✔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify.verify_exercice_5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 5 : Solution 💡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers.answer_exercice_5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 6 : Train a model\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "Train a classification model on preprocessed data. \n",
    "\n",
    "Default model in `3_training_classification.py` is [`ModelRidgeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html).\n",
    "\n",
    "**TODO:**\n",
    "\n",
    "- Use the script `3_training_classification.py` to train a `ModelRidgeClassifier` on `wine_train_preprocess_P1.csv`\n",
    "- Use `wine_valid_preprocess_P1.csv` as validation dataset\n",
    "\n",
    "**Help:**\n",
    "\n",
    "- Use `3_training_classification.py -h` to see CLI helper.\n",
    "- The script `3_training_classification.py` trains a model on a dataset\n",
    "- It works as follows:\n",
    "    - Read a train .csv file as input \n",
    "        - If a validation file is given, it will use it as validation data \n",
    "    - Manage `y_col` argument: \n",
    "        - If there is only one value, training in mono-label mode \n",
    "        - If several values, training in multi-labels mode \n",
    "    - **Manual modifications of the script**: \n",
    "        - **To change the model used** -> you have to comment/uncomment/modify the code in the \"training\" part (not necessary for this exercise) \n",
    "        - **To load datasets** -> if a dataset is not in the right format, you have to adapt the loading part (not necessary for this exercise) \n",
    "- Don't forget to activate your virtual environment ...\n",
    "- If you get an `pkg_resources.DistributionNotFound`, you should try to install {{package_name}} : `python setup.py develop`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 6 : Verify your answer ✔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify.verify_exercice_6()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 6 : Solution 💡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers.answer_exercice_6()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 7 : Try another classification model\n",
    "\n",
    "The previous model already achieves a perfect score on validation dataset but the goal here is to see how to use a different kind of model for training.\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "Train a `ModelGBTClassifier` classification model on preprocessed data. \n",
    "\n",
    "**TODO:**\n",
    "\n",
    "- Change the script `3_training_classification.py` so it uses a `ModelGBTClassifier`\n",
    "- Use the script `3_training_classification.py` to train `ModelGBTClassifier` a on `wine_train_preprocess_P1.csv` with `wine_valid_preprocess_P1.csv` as validation dataset\n",
    "\n",
    "**Help:**\n",
    "\n",
    "If you look at `3_training_classification.py` you will see that a lot of models are commented :\n",
    "\n",
    "```python\n",
    "model = model_ridge_classifier.ModelRidgeClassifier(\n",
    "    x_col=x_col,\n",
    "    y_col=y_col,\n",
    "    level_save=level_save,\n",
    "    preprocess_pipeline=preprocess_pipeline,\n",
    "    ridge_params={\"alpha\": 1.0},\n",
    "    multi_label=multi_label,\n",
    ")\n",
    "# model = model_logistic_regression_classifier.ModelLogisticRegressionClassifier(\n",
    "#     x_col=x_col,\n",
    "#     y_col=y_col,\n",
    "#     level_save=level_save,\n",
    "#     preprocess_pipeline=preprocess_pipeline,\n",
    "#     lr_params={\"penalty\": \"l2\", \"C\": 1.0, \"max_iter\": 100},\n",
    "#     multi_label=multi_label,\n",
    "# )\n",
    "#\n",
    "# [...]\n",
    "```\n",
    "Comment and uncomment proper lines to use `ModelGBTClassifier` instead of `ModelRidgeClassifier`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 7 : Verify your answer ✔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify.verify_exercice_7()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 7 : Solution 💡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers.answer_exercice_7()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see this model do not perform perfectly on validation dataset, probably due to overfitting. We are going to stick with our `ModelRidgeClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 8 : Test ModelRidgeClassifier on test data\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "Test your `ModelRidgeClassifier` model on test data. \n",
    "\n",
    "**TODO:**\n",
    "\n",
    "- Use the script `4_predict.py` to make prediction about cultivars in test data.\n",
    "- Use argument `[-y Y_COL [Y_COL ...]]` to obtain performance on test data.\n",
    "\n",
    "**Help:**\n",
    "\n",
    "- Use `4_predict.py -h` to see CLI helper.\n",
    "- If you get an `ValueError: There are some missing mandatory columns`, you probably made predictions on a preprocessed version of `wine_test.csv` when you should have made predictions on `wine_test.csv`. This error is caused by the first row of the preprocessed file which contains metadata infos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 8 : Verify your answer ✔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify.verify_exercice_8()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 8 : Solution 💡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers.answer_exercice_8()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "\n",
    "## 3. Use a saved model in python\n",
    "\n",
    "In this section, we will see how to load a saved model in python for use with new data\n",
    "\n",
    "### Load a saved model\n",
    "\n",
    "First choose one of your saved models :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from {{package_name}}.utils import get_models_path\n",
    "from {{package_name}}.models_training import utils_models\n",
    "\n",
    "MODELS_PATH = Path(get_models_path())\n",
    "\n",
    "# This line list saved model in {{package_name}}-models\n",
    "saved_model_names = sorted([model.name for model in MODELS_PATH.glob(\"*/*\")])\n",
    "print(\"\\n\".join(saved_model_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then load it with `utils_models.load_model` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = saved_model_names[-1]\n",
    "print(model_name)\n",
    "\n",
    "model, model_conf = utils_models.load_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "wine_dataset = load_wine(as_frame=True)\n",
    "\n",
    "wine_dataset[\"data\"].sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model` object as a `preprocess_pipeline` argument that can be used in combination to `utils_models.apply_pipeline` to apply the same preprocessing on the new data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_dataset_prep = utils_models.apply_pipeline(wine_dataset[\"data\"], model.preprocess_pipeline)\n",
    "wine_dataset_prep.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then simply use `mdeol.predict` to make predictions : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(wine_dataset_prep).astype(int)\n",
    "\n",
    "# Verifying accuracy :\n",
    "accuracy = sum(predictions == wine_dataset[\"target\"]) / predictions.shape[0]\n",
    "print(f\"Accuracy : {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "\n",
    "## 4. BONUS : Start up a small web app to introduce your models 🚀 \n",
    "\n",
    "You are now ready to demonstrate how good your models work. We implemented a default ***Streamlit*** app., let's try it !\n",
    "\n",
    "```bash\n",
    "# do not forget to activate your virtual environment\n",
    "# source venv_num_template/bin/activate \n",
    "\n",
    "streamlit run {{package_name}}-scripts/5_demonstrator.py\n",
    "```\n",
    "\n",
    "It will start a Streamlit app on the default port (8501)\n",
    "\n",
    "Visit [http://localhost:8501](http://localhost:8501) to see you demonstrator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('venv_{{package_name}}': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c4ce3d9346c91fa52fdd6bd8efd9ec857e7dd4f4537302caacbefa9813d6c40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
